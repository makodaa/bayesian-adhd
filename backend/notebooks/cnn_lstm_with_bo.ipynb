{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c9fc3d7-ba3e-48bb-88a9-8ee38f5f1ab9",
   "metadata": {},
   "source": [
    "#### First Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "181da2fc-dae6-4638-8d23-8214b7560c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam, RMSprop, SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2168c3-c347-46fa-b7d7-9f352e31fdd3",
   "metadata": {},
   "source": [
    "#### Read the processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af714077-585f-4de6-b357-1e50dc4993ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Class</th>\n",
       "      <th>Window</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Fp1</th>\n",
       "      <th>Fp2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>...</th>\n",
       "      <th>Fz</th>\n",
       "      <th>Cz</th>\n",
       "      <th>Pz</th>\n",
       "      <th>Theta</th>\n",
       "      <th>Beta</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Fast_Alpha</th>\n",
       "      <th>High_Beta</th>\n",
       "      <th>TBR</th>\n",
       "      <th>TAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>v10p</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>25379.222437</td>\n",
       "      <td>13434.394810</td>\n",
       "      <td>1862.080149</td>\n",
       "      <td>3179.723538</td>\n",
       "      <td>1418.885236</td>\n",
       "      <td>5116.522701</td>\n",
       "      <td>...</td>\n",
       "      <td>2349.194501</td>\n",
       "      <td>5278.156628</td>\n",
       "      <td>935.215095</td>\n",
       "      <td>2903.203042</td>\n",
       "      <td>1487.330986</td>\n",
       "      <td>2252.515739</td>\n",
       "      <td>1170.997315</td>\n",
       "      <td>1192.159852</td>\n",
       "      <td>1.951955</td>\n",
       "      <td>1.288871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>v10p</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>35351.096151</td>\n",
       "      <td>38485.847853</td>\n",
       "      <td>12025.897606</td>\n",
       "      <td>10262.779964</td>\n",
       "      <td>3309.949812</td>\n",
       "      <td>8056.793743</td>\n",
       "      <td>...</td>\n",
       "      <td>7086.393467</td>\n",
       "      <td>9269.600745</td>\n",
       "      <td>1175.976823</td>\n",
       "      <td>2903.203042</td>\n",
       "      <td>1487.330986</td>\n",
       "      <td>2252.515739</td>\n",
       "      <td>1170.997315</td>\n",
       "      <td>1192.159852</td>\n",
       "      <td>1.951955</td>\n",
       "      <td>1.288871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v10p</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14038.273974</td>\n",
       "      <td>20836.960614</td>\n",
       "      <td>757.368253</td>\n",
       "      <td>376.379474</td>\n",
       "      <td>921.133295</td>\n",
       "      <td>1506.793907</td>\n",
       "      <td>...</td>\n",
       "      <td>682.922518</td>\n",
       "      <td>1298.915366</td>\n",
       "      <td>2283.822560</td>\n",
       "      <td>2903.203042</td>\n",
       "      <td>1487.330986</td>\n",
       "      <td>2252.515739</td>\n",
       "      <td>1170.997315</td>\n",
       "      <td>1192.159852</td>\n",
       "      <td>1.951955</td>\n",
       "      <td>1.288871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>v10p</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3299.722183</td>\n",
       "      <td>4801.627106</td>\n",
       "      <td>100.784086</td>\n",
       "      <td>589.982148</td>\n",
       "      <td>449.081281</td>\n",
       "      <td>2051.093618</td>\n",
       "      <td>...</td>\n",
       "      <td>166.249339</td>\n",
       "      <td>1869.041039</td>\n",
       "      <td>1080.292386</td>\n",
       "      <td>2903.203042</td>\n",
       "      <td>1487.330986</td>\n",
       "      <td>2252.515739</td>\n",
       "      <td>1170.997315</td>\n",
       "      <td>1192.159852</td>\n",
       "      <td>1.951955</td>\n",
       "      <td>1.288871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>v10p</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9601.231799</td>\n",
       "      <td>3313.402232</td>\n",
       "      <td>1438.078829</td>\n",
       "      <td>1473.695476</td>\n",
       "      <td>1717.483438</td>\n",
       "      <td>1573.801211</td>\n",
       "      <td>...</td>\n",
       "      <td>447.893661</td>\n",
       "      <td>890.015936</td>\n",
       "      <td>923.261576</td>\n",
       "      <td>2903.203042</td>\n",
       "      <td>1487.330986</td>\n",
       "      <td>2252.515739</td>\n",
       "      <td>1170.997315</td>\n",
       "      <td>1192.159852</td>\n",
       "      <td>1.951955</td>\n",
       "      <td>1.288871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID  Class  Window  Frequency           Fp1           Fp2            F3  \\\n",
       "0  v10p    2.0       0        2.0  25379.222437  13434.394810   1862.080149   \n",
       "1  v10p    2.0       0        2.5  35351.096151  38485.847853  12025.897606   \n",
       "2  v10p    2.0       0        3.0  14038.273974  20836.960614    757.368253   \n",
       "3  v10p    2.0       0        3.5   3299.722183   4801.627106    100.784086   \n",
       "4  v10p    2.0       0        4.0   9601.231799   3313.402232   1438.078829   \n",
       "\n",
       "             F4           C3           C4  ...           Fz           Cz  \\\n",
       "0   3179.723538  1418.885236  5116.522701  ...  2349.194501  5278.156628   \n",
       "1  10262.779964  3309.949812  8056.793743  ...  7086.393467  9269.600745   \n",
       "2    376.379474   921.133295  1506.793907  ...   682.922518  1298.915366   \n",
       "3    589.982148   449.081281  2051.093618  ...   166.249339  1869.041039   \n",
       "4   1473.695476  1717.483438  1573.801211  ...   447.893661   890.015936   \n",
       "\n",
       "            Pz        Theta         Beta        Alpha   Fast_Alpha  \\\n",
       "0   935.215095  2903.203042  1487.330986  2252.515739  1170.997315   \n",
       "1  1175.976823  2903.203042  1487.330986  2252.515739  1170.997315   \n",
       "2  2283.822560  2903.203042  1487.330986  2252.515739  1170.997315   \n",
       "3  1080.292386  2903.203042  1487.330986  2252.515739  1170.997315   \n",
       "4   923.261576  2903.203042  1487.330986  2252.515739  1170.997315   \n",
       "\n",
       "     High_Beta       TBR       TAR  \n",
       "0  1192.159852  1.951955  1.288871  \n",
       "1  1192.159852  1.951955  1.288871  \n",
       "2  1192.159852  1.951955  1.288871  \n",
       "3  1192.159852  1.951955  1.288871  \n",
       "4  1192.159852  1.951955  1.288871  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./processed_clustered_adhdata.csv\")\n",
    "\n",
    "frequency_count = len(df['Frequency'].unique())\n",
    "window_count = len(df['Window'].unique())\n",
    "numeric_df = df.drop(['ID', 'Window'], axis=1)\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39df14e-ecc4-4a5d-80c9-66305657bc36",
   "metadata": {},
   "source": [
    "#### Group the data\n",
    "\n",
    "From a 2D matrix, we need to group each row according to their window number.\n",
    "Furthermore, we add a dummy dimension for \"channel\", which is necessary for CNN, resulting in a shape of `(WINDOW_COUNT, FREQUENCY_PER_WINDOW, ELECTRODES)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fe8775d-db75-48e4-a074-6190d80c2ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13399, 77, 26, 1)\n",
      "Train shape: (13399, 77, 26, 1)\n"
     ]
    }
   ],
   "source": [
    "# shape: (windows, frequencies, electrodes)\n",
    "full_ndarray = numeric_df.values.reshape((window_count, frequency_count, numeric_df.shape[1]))\n",
    "\n",
    "X = full_ndarray[:, :, 2:]     # drop ID/Class columns\n",
    "y = full_ndarray[:, 0, 0]      # class label is repeated across freq rows\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "# Add channel dimension (N, 1, freq, electrodes)\n",
    "X_train = X_train[..., np.newaxis]   # (N, freq, electrodes, 1)\n",
    "X_test  = X_test[...,  np.newaxis]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(\"Train shape:\", X_train.shape)  # (N, freq, electrodes, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed81cb7-3951-4889-a095-e59b7e0ea1a4",
   "metadata": {},
   "source": [
    "#### Dataset Loading\n",
    "\n",
    "The EEG Dataset allows the automated extraction of the raw EEGs and the band powers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2595397-63c0-4fd7-9f9a-4242370f5bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "        # -> (N, 1, freq, electrodes)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]          # (1, 77, 26)\n",
    "\n",
    "        x_eeg  = x[:, :, :19]    # (1, 77, 19)\n",
    "        x_band = x[0, 0, 19:]    # (7,)\n",
    "\n",
    "        return x_eeg, x_band, self.y[idx]\n",
    "\n",
    "train_ds = EEGDataset(X_train, y_train)\n",
    "test_ds  = EEGDataset(X_test,  y_test)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1002d8ce-9fbc-49a2-bc2b-cd3f38ed0d1b",
   "metadata": {},
   "source": [
    "#### Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e658ff5-0dd2-4898-9f4a-d71d0cc79bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class EEGCNNLSTM(nn.Module):\n",
    "    def __init__(self, num_band_features=7, num_classes=4,\n",
    "                 cnn_kernels_1=32,\n",
    "                 cnn_kernel_size_1=3,\n",
    "                 cnn_kernels_2=32,\n",
    "                 cnn_kernel_size_2=3,\n",
    "                 cnn_dropout=0.3,\n",
    "                 cnn_dense=16,\n",
    "                 lstm_hidden_size=32,\n",
    "                 lstm_layers=4,\n",
    "                 lstm_dense=64,\n",
    "                 dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        pad1 = cnn_kernel_size_1 // 2\n",
    "        self.conv1   = nn.Conv2d(1, int(cnn_kernels_1), kernel_size=cnn_kernel_size_1, padding=pad1)\n",
    "        self.pool1 = nn.AvgPool2d(2)\n",
    "        \n",
    "        pad2 = cnn_kernel_size_2 // 2\n",
    "        self.conv2 = nn.Conv2d(int(cnn_kernels_1), int(cnn_kernels_2), kernel_size=cnn_kernel_size_2, padding=pad2)\n",
    "        self.cnn_dropout = nn.Dropout(cnn_dropout)\n",
    "\n",
    "        # Compute flatten size dynamically\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 1, X_train.shape[1], 19)\n",
    "            out = self._forward_cnn(dummy)   # [B, C, H, W]\n",
    "            b, c, h, w = out.shape\n",
    "            self.seq_len = h                      # sequence length (rows)\n",
    "            self.cnn_feat_dim = c * w             # CNN features per timestep\n",
    "\n",
    "        # Dense layer BEFORE LSTM\n",
    "        self.cnn_dense = nn.Linear(self.cnn_feat_dim, int(cnn_dense))\n",
    "\n",
    "        # Two stacked LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=int(cnn_dense),\n",
    "            hidden_size=int(lstm_hidden_size),\n",
    "            num_layers=int(lstm_layers),\n",
    "            batch_first=True,\n",
    "            dropout=dropout if lstm_layers > 1 else 0.0\n",
    "        )\n",
    "        # self.lstm_dense = nn.Linear(int(lstm_hidden_size), int(lstm_dense))\n",
    "        self.band_fc = nn.Sequential(\n",
    "            nn.Linear(num_band_features, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        \n",
    "        # final classifier (match your original final style: dropout + linear)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(int(lstm_hidden_size) + 32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes),\n",
    "        )\n",
    "\n",
    "    def _forward_cnn(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.cnn_dropout(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x_eeg, x_band):\n",
    "        # 1️⃣ CNN feature extraction\n",
    "        x = self._forward_cnn(x_eeg)             # [B, C, H, W]\n",
    "\n",
    "        # 2️⃣ Prepare sequence for LSTM\n",
    "        x = x.permute(0, 2, 1, 3)                 # [B, H, C, W]\n",
    "        x = x.contiguous().view(x.size(0), x.size(1), -1)  # [B, H, C*W]\n",
    "\n",
    "        # 3️⃣ Dense layer for each timestep\n",
    "        x = F.relu(self.cnn_dense(x))     # [B, H, dense_size]\n",
    "\n",
    "        # 4️⃣ Two-layer LSTM\n",
    "        lstm_out, _ = self.lstm(x)                # [B, H, hidden_size]\n",
    "\n",
    "        # 5️⃣ Use last time step (or mean/attention if preferred)\n",
    "        eeg_feat = lstm_out[:, -1, :]\n",
    "        # eeg_feat = lstm_out.mean(dim=1)                    # [B, hidden_size]\n",
    "        # eeg_feat = self.lstm_dense(eeg_feat)\n",
    "\n",
    "        # --- Band features ---\n",
    "        band_feat = self.band_fc(x_band)        # [B, 32]\n",
    "\n",
    "        # --- Fusion ---\n",
    "        fused = torch.cat([eeg_feat, band_feat], dim=1)\n",
    "\n",
    "        # 6️⃣ Fully connected head\n",
    "        x = self.classifier(fused)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def fit(self, train_loader, test_loader, epochs, criterion, optimizer, device, patience=100):\n",
    "        best_val_loss = float('inf')\n",
    "        no_improve = 0\n",
    "\n",
    "        train_losses, train_accs = [], []\n",
    "        val_losses, val_accs     = [], []\n",
    "\n",
    "        best_state = None\n",
    "        for epoch in range(epochs):\n",
    "            # --- Train ---\n",
    "            self.train()\n",
    "            train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "            for xb_eeg, xb_band, yb in train_loader:\n",
    "                xb_eeg = xb_eeg.to(device)\n",
    "                xb_band = xb_band.to(device)\n",
    "                yb = yb.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                out = self(xb_eeg, xb_band)\n",
    "                loss = criterion(out, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item() * xb_eeg.size(0)\n",
    "                train_correct += (out.argmax(1) == yb).sum().item()\n",
    "                train_total += yb.size(0)\n",
    "\n",
    "            train_loss /= train_total\n",
    "            train_acc  = train_correct / train_total\n",
    "            train_losses.append(train_loss)\n",
    "            train_accs.append(train_acc)\n",
    "\n",
    "            # --- Validate ---\n",
    "            self.eval()\n",
    "            val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "            with torch.no_grad():\n",
    "                for xb_eeg, xb_band, yb in test_loader:\n",
    "                    xb_eeg = xb_eeg.to(device)\n",
    "                    xb_band = xb_band.to(device)\n",
    "                    yb = yb.to(device)\n",
    "                    out = self(xb_eeg, xb_band)\n",
    "                    loss = criterion(out, yb)\n",
    "                    val_loss += loss.item() * xb_eeg.size(0)\n",
    "                    val_correct += (out.argmax(1) == yb).sum().item()\n",
    "                    val_total += yb.size(0)\n",
    "\n",
    "            val_loss /= val_total\n",
    "            val_acc  = val_correct / val_total\n",
    "            val_losses.append(val_loss)\n",
    "            val_accs.append(val_acc)\n",
    "\n",
    "            print(f\"Epoch {epoch+1:03d} | \"\n",
    "                  f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n",
    "\n",
    "            # if val_loss - train_loss > 0.2:\n",
    "            #     print(\"Overfitting detected.\")\n",
    "            #     break\n",
    "\n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_state = self.state_dict()\n",
    "                no_improve = 0\n",
    "            else:\n",
    "                no_improve += 1\n",
    "                if no_improve >= patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break\n",
    "\n",
    "        if best_state is not None:\n",
    "            self.load_state_dict(best_state)\n",
    "        return {\n",
    "            \"train_accs\": np.array(train_accs),\n",
    "            \"train_losses\": np.array(train_losses),\n",
    "            \"val_accs\":   np.array(val_accs),\n",
    "            \"val_losses\": np.array(val_losses)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ec1eaa-dcab-40b1-a171-1cf366326818",
   "metadata": {},
   "source": [
    "#### First Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f87827b-31e7-417d-b64f-8b93427bc531",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "01683741-6af7-471c-8440-c72a1b8fd652",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Train Loss: 197.6154 Acc: 0.3104 | Val Loss: 36.3389 Acc: 0.3916\n",
      "Epoch 002 | Train Loss: 56.6425 Acc: 0.3503 | Val Loss: 18.3807 Acc: 0.3916\n",
      "Epoch 003 | Train Loss: 33.3668 Acc: 0.3587 | Val Loss: 9.4501 Acc: 0.3937\n",
      "Epoch 004 | Train Loss: 23.6421 Acc: 0.3740 | Val Loss: 6.8079 Acc: 0.4185\n",
      "Epoch 005 | Train Loss: 18.0504 Acc: 0.3909 | Val Loss: 3.9681 Acc: 0.4928\n",
      "Epoch 006 | Train Loss: 14.7918 Acc: 0.4030 | Val Loss: 5.5033 Acc: 0.4991\n",
      "Epoch 007 | Train Loss: 12.3006 Acc: 0.4217 | Val Loss: 3.2440 Acc: 0.5242\n",
      "Epoch 008 | Train Loss: 10.4925 Acc: 0.4252 | Val Loss: 3.4945 Acc: 0.5269\n",
      "Epoch 009 | Train Loss: 9.0134 Acc: 0.4332 | Val Loss: 2.3641 Acc: 0.5248\n",
      "Epoch 010 | Train Loss: 8.0745 Acc: 0.4312 | Val Loss: 2.7282 Acc: 0.5490\n",
      "Epoch 011 | Train Loss: 7.0234 Acc: 0.4470 | Val Loss: 2.0112 Acc: 0.5463\n",
      "Epoch 012 | Train Loss: 6.2991 Acc: 0.4535 | Val Loss: 2.0192 Acc: 0.5725\n",
      "Epoch 013 | Train Loss: 5.8080 Acc: 0.4664 | Val Loss: 2.4242 Acc: 0.5549\n",
      "Epoch 014 | Train Loss: 5.1102 Acc: 0.4806 | Val Loss: 2.0331 Acc: 0.5985\n",
      "Epoch 015 | Train Loss: 4.6775 Acc: 0.4797 | Val Loss: 1.8884 Acc: 0.5707\n",
      "Epoch 016 | Train Loss: 4.2908 Acc: 0.4910 | Val Loss: 1.6514 Acc: 0.6296\n",
      "Epoch 017 | Train Loss: 3.9684 Acc: 0.4988 | Val Loss: 1.4484 Acc: 0.6669\n",
      "Epoch 018 | Train Loss: 3.7547 Acc: 0.5117 | Val Loss: 1.6189 Acc: 0.6218\n",
      "Epoch 019 | Train Loss: 3.4724 Acc: 0.5184 | Val Loss: 1.1847 Acc: 0.6779\n",
      "Epoch 020 | Train Loss: 3.2329 Acc: 0.5254 | Val Loss: 1.1456 Acc: 0.6585\n",
      "Epoch 021 | Train Loss: 2.9939 Acc: 0.5366 | Val Loss: 1.4078 Acc: 0.5928\n",
      "Epoch 022 | Train Loss: 2.7916 Acc: 0.5364 | Val Loss: 1.1380 Acc: 0.6122\n",
      "Epoch 023 | Train Loss: 2.6163 Acc: 0.5419 | Val Loss: 1.2277 Acc: 0.6815\n",
      "Epoch 024 | Train Loss: 2.3753 Acc: 0.5603 | Val Loss: 1.2468 Acc: 0.6200\n",
      "Epoch 025 | Train Loss: 2.3545 Acc: 0.5612 | Val Loss: 0.9633 Acc: 0.7018\n",
      "Epoch 026 | Train Loss: 2.2539 Acc: 0.5673 | Val Loss: 0.9651 Acc: 0.7018\n",
      "Epoch 027 | Train Loss: 2.1465 Acc: 0.5715 | Val Loss: 1.1761 Acc: 0.6048\n",
      "Epoch 028 | Train Loss: 2.0205 Acc: 0.5840 | Val Loss: 1.1397 Acc: 0.5985\n",
      "Epoch 029 | Train Loss: 1.8804 Acc: 0.5953 | Val Loss: 1.1768 Acc: 0.6191\n",
      "Epoch 030 | Train Loss: 1.8250 Acc: 0.5962 | Val Loss: 0.9803 Acc: 0.6275\n",
      "Epoch 031 | Train Loss: 1.6740 Acc: 0.6100 | Val Loss: 0.9792 Acc: 0.6642\n",
      "Epoch 032 | Train Loss: 1.5884 Acc: 0.6222 | Val Loss: 0.7698 Acc: 0.7221\n",
      "Epoch 033 | Train Loss: 1.5441 Acc: 0.6312 | Val Loss: 0.9332 Acc: 0.7346\n",
      "Epoch 034 | Train Loss: 1.4899 Acc: 0.6383 | Val Loss: 0.8859 Acc: 0.7230\n",
      "Epoch 035 | Train Loss: 1.3561 Acc: 0.6555 | Val Loss: 0.8591 Acc: 0.7463\n",
      "Epoch 036 | Train Loss: 1.3085 Acc: 0.6577 | Val Loss: 0.7939 Acc: 0.7221\n",
      "Epoch 037 | Train Loss: 1.2192 Acc: 0.6705 | Val Loss: 0.7795 Acc: 0.7675\n",
      "Epoch 038 | Train Loss: 1.2507 Acc: 0.6698 | Val Loss: 0.7637 Acc: 0.7642\n",
      "Epoch 039 | Train Loss: 1.2088 Acc: 0.6792 | Val Loss: 0.7553 Acc: 0.7728\n",
      "Epoch 040 | Train Loss: 1.1350 Acc: 0.6883 | Val Loss: 0.6242 Acc: 0.7949\n",
      "Epoch 041 | Train Loss: 1.0988 Acc: 0.6894 | Val Loss: 0.8711 Acc: 0.7376\n",
      "Epoch 042 | Train Loss: 1.0778 Acc: 0.7010 | Val Loss: 0.7543 Acc: 0.7678\n",
      "Epoch 043 | Train Loss: 1.0313 Acc: 0.7126 | Val Loss: 0.6664 Acc: 0.7794\n",
      "Epoch 044 | Train Loss: 0.9367 Acc: 0.7245 | Val Loss: 0.6874 Acc: 0.7845\n",
      "Epoch 045 | Train Loss: 0.9757 Acc: 0.7207 | Val Loss: 0.7362 Acc: 0.7973\n",
      "Epoch 046 | Train Loss: 0.9319 Acc: 0.7310 | Val Loss: 0.5910 Acc: 0.8116\n",
      "Epoch 047 | Train Loss: 0.9036 Acc: 0.7320 | Val Loss: 0.7027 Acc: 0.7884\n",
      "Epoch 048 | Train Loss: 0.8640 Acc: 0.7427 | Val Loss: 0.5853 Acc: 0.8251\n",
      "Epoch 049 | Train Loss: 0.8076 Acc: 0.7521 | Val Loss: 0.6200 Acc: 0.8048\n",
      "Epoch 050 | Train Loss: 0.7891 Acc: 0.7577 | Val Loss: 0.7937 Acc: 0.7881\n",
      "Epoch 051 | Train Loss: 0.7460 Acc: 0.7688 | Val Loss: 0.5682 Acc: 0.8352\n",
      "Epoch 052 | Train Loss: 0.6967 Acc: 0.7792 | Val Loss: 0.4938 Acc: 0.8200\n",
      "Epoch 053 | Train Loss: 0.6785 Acc: 0.7819 | Val Loss: 0.5767 Acc: 0.8307\n",
      "Epoch 054 | Train Loss: 0.6645 Acc: 0.7854 | Val Loss: 0.6311 Acc: 0.8119\n",
      "Epoch 055 | Train Loss: 0.6794 Acc: 0.7830 | Val Loss: 0.4806 Acc: 0.8340\n",
      "Epoch 056 | Train Loss: 0.6128 Acc: 0.8034 | Val Loss: 0.4144 Acc: 0.8669\n",
      "Epoch 057 | Train Loss: 0.5881 Acc: 0.8084 | Val Loss: 0.4658 Acc: 0.8424\n",
      "Epoch 058 | Train Loss: 0.5772 Acc: 0.8119 | Val Loss: 0.3979 Acc: 0.8543\n",
      "Epoch 059 | Train Loss: 0.5630 Acc: 0.8186 | Val Loss: 0.4173 Acc: 0.8567\n",
      "Epoch 060 | Train Loss: 0.5205 Acc: 0.8304 | Val Loss: 0.4284 Acc: 0.8582\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = EEGCNNLSTM().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "history = model.fit(train_loader, test_loader, epochs=60, criterion=criterion,\n",
    "                    optimizer=optimizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e20ab9a3-cf9a-4fbf-b321-8d29d0797a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.8582089552238806\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.63      0.73       268\n",
      "           1       0.94      0.87      0.90       682\n",
      "           2       0.75      0.99      0.85       919\n",
      "           3       0.92      0.81      0.86      1481\n",
      "\n",
      "    accuracy                           0.86      3350\n",
      "   macro avg       0.87      0.83      0.84      3350\n",
      "weighted avg       0.87      0.86      0.86      3350\n",
      "\n",
      "[[ 168    0   53   47]\n",
      " [   0  593   36   53]\n",
      " [   0    0  914    5]\n",
      " [  24   40  217 1200]]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for xb_eeg, xb_band, yb in test_loader:\n",
    "        xb_eeg = xb_eeg.to(device)\n",
    "        xb_band = xb_band.to(device)\n",
    "        preds = model(xb_eeg, xb_band).argmax(1).cpu().numpy()\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(yb.numpy())\n",
    "\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(all_labels, all_preds))\n",
    "print(classification_report(all_labels, all_preds))\n",
    "print(confusion_matrix(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5ba6aae-6a97-4f20-9edd-a31e8d0a355d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_accs'], label='Training Accuracy')\n",
    "plt.plot(history['val_accs'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['train_losses'], label='Training Loss')\n",
    "plt.plot(history['val_losses'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159c22db-c687-481f-8529-b23ffae9a542",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0e739b5-97ce-4907-8c34-e24550813939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def batched(iterable, n, *, strict=False):\n",
    "    # batched('ABCDEFG', 2) → AB CD EF G\n",
    "    if n < 1:\n",
    "        raise ValueError('n must be at least one')\n",
    "    iterator = iter(iterable)\n",
    "    while batch := tuple(itertools.islice(iterator, n)):\n",
    "        if strict and len(batch) != n:\n",
    "            raise ValueError('batched(): incomplete batch')\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49309de8-22a6-422f-a577-69ee748d5178",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def get_timestamp():\n",
    "    ts = time.time()\n",
    "    return datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "def get_model(params):\n",
    "    model = EEGCNNLSTM(\n",
    "        cnn_kernels_1=params['cnn_kernels_1'],\n",
    "        cnn_kernel_size_1=params['cnn_kernel_size_1'],\n",
    "        cnn_kernels_2=params['cnn_kernels_2'],\n",
    "        cnn_dropout=float(params['cnn_dropout']),\n",
    "        cnn_dense=params['cnn_dense'],\n",
    "        lstm_hidden_size=params['lstm_hidden_size'],\n",
    "        lstm_layers=params['lstm_layers'],\n",
    "        lstm_dense=params['lstm_dense'],\n",
    "        dropout=float(params['cnn_dropout']),  # use cnn_dropout as a simple shared dropout param\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if params['optimizer'] == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'], weight_decay=1e-4)\n",
    "    elif params['optimizer'] == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=params['learning_rate'], weight_decay=1e-4)\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=params['learning_rate'], momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "    return model, criterion, optimizer\n",
    "\n",
    "def get_validation(model, data_loader, device, matrix=True):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb_eeg, xb_band, yb in data_loader:\n",
    "            xb_eeg = xb_eeg.to(device)\n",
    "            xb_band = xb_band.to(device)\n",
    "            preds = model(xb_eeg, xb_band).argmax(1).cpu().numpy()\n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(yb.numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    report = classification_report(all_labels, all_preds)\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds) if matrix else None\n",
    "\n",
    "    return acc, report, conf_matrix\n",
    "\n",
    "def get_dataset(df, is_train=False, batch_size=36):\n",
    "    frequency_count = len(df['Frequency'].unique())\n",
    "    window_count = len(df['Window'].unique())\n",
    "    numeric_df = df.drop(['ID', 'Window'], axis=1)\n",
    "\n",
    "    # shape: (windows, freqs, features)\n",
    "    full_ndarray = numeric_df.values.reshape((window_count, frequency_count, numeric_df.shape[1]))\n",
    "\n",
    "    X = full_ndarray[:, :, 2:]     # drop ID/Class columns\n",
    "    y = full_ndarray[:, 0, 0]      # class label is repeated across freq rows\n",
    "\n",
    "    # Add channel dimension (N, 1, freq, electrodes)\n",
    "    X = X[..., np.newaxis]          # (N, freq, electrodes, 1)\n",
    "\n",
    "    print(X.shape)\n",
    "\n",
    "    return DataLoader(EEGDataset(X, y), batch_size=batch_size, shuffle=is_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfcbdbb-a909-4bd8-b736-258d0265d095",
   "metadata": {},
   "source": [
    "#### Search Space Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32e30627-801f-426c-985f-887d0b0b04c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK\n",
    "\n",
    "# -------------------------\n",
    "# Hyperopt search space\n",
    "# -------------------------\n",
    "space = {\n",
    "    'cnn_kernels_1'    : hp.choice('cnn_kernels_1', [16, 32, 48, 64]),\n",
    "    'cnn_kernel_size_1': hp.choice('cnn_kernel_size_1', [3, 5]),\n",
    "    'cnn_kernels_2'    : hp.choice('cnn_kernels_2', [16, 32, 64, 96]),\n",
    "    'cnn_kernel_size_2': hp.choice('cnn_kernel_size_2', [3, 5]),\n",
    "    'cnn_dropout'      : hp.uniform('cnn_dropout', 0.0, 0.7),\n",
    "    'cnn_dense'        : hp.choice('cnn_dense', [32, 64, 128, 256]),\n",
    "    'lstm_hidden_size' : hp.choice('lstm_hidden_size', [32, 64, 96, 128]),\n",
    "    'lstm_layers'      : hp.choice('lstm_layers', [1, 2, 3, 4, 5, 6]),\n",
    "    'lstm_dense'       : hp.choice('lstm_dense', [32, 64, 128, 256]),\n",
    "    'learning_rate'    : hp.loguniform('learning_rate', np.log(1e-5), np.log(1e-2)),\n",
    "    'optimizer'        : hp.choice('optimizer', ['adam', 'rmsprop', 'sgd']),\n",
    "    'batch_size'       : hp.choice('batch_size', [32, 36, 48, 64, 80, 96])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3db2be-a073-42a6-9466-4468ebd16381",
   "metadata": {},
   "source": [
    "#### Search Objective Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69653680-cc67-4a43-9456-5551beeed641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# -------------------------\n",
    "# k-Fold CV for Hyperopt\n",
    "# -------------------------\n",
    "def objective(params):\n",
    "    print(\"Trial params:\", params)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    unique_subjects = df['ID'].unique()\n",
    "    losses = []\n",
    "    variances = []\n",
    "    batch_size = params['batch_size']\n",
    "\n",
    "    K_FOLDS = 5\n",
    "    fold_size = len(unique_subjects) // K_FOLDS\n",
    "\n",
    "    cyclic = itertools.cycle(unique_subjects)\n",
    "    batched_cyclic = batched(cyclic, n=fold_size)\n",
    "    folds = itertools.islice(batched_cyclic, K_FOLDS)\n",
    "\n",
    "    for i, fold in enumerate(folds):\n",
    "        print(f\"Starting fold {i + 1}/{K_FOLDS}\")\n",
    "\n",
    "        train_df = df[~df['ID'].isin(fold)]\n",
    "        test_df  = df[df['ID'].isin(fold)]\n",
    "\n",
    "        print(train_df.shape, test_df.shape)\n",
    "\n",
    "        train_loader = get_dataset(train_df, batch_size=batch_size)\n",
    "        test_loader  = get_dataset(test_df, batch_size=batch_size)\n",
    "        model, criterion, optimizer = get_model(params)\n",
    "\n",
    "        # Train with modest epochs; early stopping inside fit handles rest\n",
    "        history = model.fit(\n",
    "            train_loader=train_loader,\n",
    "            test_loader=test_loader,\n",
    "            epochs=60,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "            patience=15\n",
    "        )\n",
    "\n",
    "        acc, *_ = get_validation(model, test_loader, device)\n",
    "        loss = history['val_losses']\n",
    "        mean_loss = np.min(loss)\n",
    "        losses.append(mean_loss)\n",
    "\n",
    "        last_5_or_less = history[\"val_losses\"]\n",
    "        last_5_or_less = last_5_or_less[-min(len(last_5_or_less), 5):]\n",
    "        variance = np.var(last_5_or_less) if len(last_5_or_less) > 1 else 1\n",
    "        variances.append(variance)\n",
    "\n",
    "        print(f\"Fold {i + 1} Accuracy:\", acc)\n",
    "\n",
    "    loss = np.mean(losses)\n",
    "    tail_variance = np.var(variances)\n",
    "    print(variances)\n",
    "    score = loss + tail_variance\n",
    "\n",
    "    print(f\"k-Fold CV Mean Loss: {loss:.4f} ± {np.std(losses):.4f}\")\n",
    "    print(f\"k-Fold CV Tail Variance: {tail_variance:.4f}\")\n",
    "\n",
    "    # Hyperopt minimizes -> return negative accuracy\n",
    "    return {'loss': score, 'status': STATUS_OK, 'attachments': {'history': history}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fdbeed8-2f69-45bb-b4ee-d5e73b4c2f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Single Objective for Hyperopt\n",
    "# -------------------------\n",
    "def objective(params):\n",
    "    print(\"Trial params:\", params)\n",
    "\n",
    "    # build dataloaders from the existing train_ds/test_ds in this session\n",
    "    train_loader = DataLoader(train_ds, batch_size=params['batch_size'], shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=params['batch_size'], shuffle=False)\n",
    "\n",
    "    # create model (note we pass dropout into lstm dropout and cnn dropout)\n",
    "    model, criterion, optimizer = get_model(params)\n",
    "\n",
    "    # Train with modest epochs; early stopping inside fit handles rest\n",
    "    history = model.fit(\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        epochs=60,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        patience=10\n",
    "    )\n",
    "\n",
    "    best_val_loss = float(np.min(history['val_losses'])) if len(history['val_losses']) > 0 else 0.0\n",
    "\n",
    "    # Hyperopt minimizes -> return negative accuracy\n",
    "    return {'loss': best_val_loss, 'status': STATUS_OK, 'attachments': {'history': history, 'best_val_loss': best_val_loss}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bbef8d-8af5-4fbc-bf88-ad2a8ae921af",
   "metadata": {},
   "source": [
    "#### Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "086894b3-97e9-4755-9b94-886320886f39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from hyperopt import Trials, fmin\n",
    "\n",
    "def hyperparameter_search(max_evals=30):\n",
    "    trials = Trials()\n",
    "    \n",
    "    print(\"Starting TPE search...\")\n",
    "    t0 = time.time()\n",
    "    best = fmin(\n",
    "        fn=objective,\n",
    "        space=space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=max_evals,   # increase for more thorough search\n",
    "        trials=trials\n",
    "    )\n",
    "    \n",
    "    print(\"Best hyperparameters:\", best)\n",
    "    t1 = time.time()\n",
    "    duration = t1 - t0\n",
    "    print(f\"TPE search finished in {duration:.2f} seconds\")\n",
    "    print(\"Best (raw indices):\", best)\n",
    "    \n",
    "    # Convert choice indices back to values for readability:\n",
    "    def choice_value(key, val):\n",
    "        mapping = {\n",
    "            'cnn_kernels_1': [16, 32, 48, 64],\n",
    "            'cnn_kernel_size_1': [3, 5],\n",
    "            'cnn_kernels_2': [16, 32, 64, 96],\n",
    "            'cnn_kernel_size_2': [3, 5],\n",
    "            'cnn_dense': [32, 64, 128, 256],\n",
    "            'lstm_hidden_size': [32, 64, 96, 128],\n",
    "            'lstm_layers': [1, 2, 3, 4, 5, 6],\n",
    "            'lstm_dense': [32, 64, 128, 256],\n",
    "            'optimizer': ['adam', 'rmsprop', 'sgd'],\n",
    "            'batch_size': [32, 36, 48, 64, 80, 96]\n",
    "        }\n",
    "        return mapping[key][int(val)] if key in mapping else val\n",
    "    \n",
    "    readable = {k: choice_value(k, v) if k in ['cnn_kernels_1','cnn_kernel_size_1','cnn_kernels_2',\n",
    "                                               'cnn_kernel_size_2', 'cnn_dense','lstm_hidden_size',\n",
    "                                               'lstm_layers','lstm_dense','optimizer','batch_size'] else v\n",
    "                for k,v in best.items()}\n",
    "    print(\"Best (interpreted):\", readable)\n",
    "\n",
    "    params = dict(readable)\n",
    "    params['cnn_kernels_1'] = int(params['cnn_kernels_1'])\n",
    "    params['cnn_kernel_size_1'] = int(params['cnn_kernel_size_1'])\n",
    "    params['cnn_kernels_2'] = int(params['cnn_kernels_2'])\n",
    "    params['cnn_kernel_size_2'] = int(params['cnn_kernel_size_2'])\n",
    "    params['cnn_dense'] = int(params['cnn_dense'])\n",
    "    params['lstm_hidden_size'] = int(params['lstm_hidden_size'])\n",
    "    params['lstm_layers'] = int(params['lstm_layers'])\n",
    "    params['lstm_dense'] = int(params['lstm_dense'])\n",
    "    params['batch_size'] = int(params['batch_size'])\n",
    "    params['cnn_dropout'] = float(params['cnn_dropout'])\n",
    "    params['dropout'] = float(params['cnn_dropout'])\n",
    "\n",
    "    return trials, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6f549aa-3b7b-4bce-bd19-460597af1f5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting TPE search...\n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 128, 'cnn_dropout': 0.6428092194630661, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 32, 'cnn_kernels_2': 64, 'learning_rate': 0.00011678133243159471, 'lstm_dense': 32, 'lstm_hidden_size': 32, 'lstm_layers': 6, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 9.8554 Acc: 0.3809 | Val Loss: 0.9777 Acc: 0.5370                                               \n",
      "Epoch 002 | Train Loss: 1.0547 Acc: 0.4644 | Val Loss: 0.9393 Acc: 0.5522                                               \n",
      "Epoch 003 | Train Loss: 1.0193 Acc: 0.4723 | Val Loss: 0.9112 Acc: 0.5433                                               \n",
      "Epoch 004 | Train Loss: 1.0000 Acc: 0.4708 | Val Loss: 0.8897 Acc: 0.5716                                               \n",
      "Epoch 005 | Train Loss: 0.9991 Acc: 0.4773 | Val Loss: 0.8779 Acc: 0.6137                                               \n",
      "Epoch 006 | Train Loss: 0.9809 Acc: 0.4651 | Val Loss: 0.8677 Acc: 0.5561                                               \n",
      "Epoch 007 | Train Loss: 0.9654 Acc: 0.4739 | Val Loss: 0.8614 Acc: 0.5669                                               \n",
      "Epoch 008 | Train Loss: 0.9542 Acc: 0.4739 | Val Loss: 0.8354 Acc: 0.6024                                               \n",
      "Epoch 009 | Train Loss: 0.9488 Acc: 0.4748 | Val Loss: 0.8249 Acc: 0.5830                                               \n",
      "Epoch 010 | Train Loss: 0.9368 Acc: 0.4956 | Val Loss: 0.8112 Acc: 0.5749                                               \n",
      "Epoch 011 | Train Loss: 0.9309 Acc: 0.4991 | Val Loss: 0.8175 Acc: 0.5552                                               \n",
      "Epoch 012 | Train Loss: 0.9245 Acc: 0.5003 | Val Loss: 0.8036 Acc: 0.5824                                               \n",
      "Epoch 013 | Train Loss: 0.9168 Acc: 0.5074 | Val Loss: 0.8058 Acc: 0.6072                                               \n",
      "Epoch 014 | Train Loss: 0.9176 Acc: 0.5124 | Val Loss: 0.7968 Acc: 0.5964                                               \n",
      "Epoch 015 | Train Loss: 0.9206 Acc: 0.5082 | Val Loss: 0.8131 Acc: 0.6057                                               \n",
      "Epoch 016 | Train Loss: 0.9148 Acc: 0.5117 | Val Loss: 0.8030 Acc: 0.6051                                               \n",
      "Epoch 017 | Train Loss: 0.9082 Acc: 0.5182 | Val Loss: 0.7898 Acc: 0.6636                                               \n",
      "Epoch 018 | Train Loss: 0.9031 Acc: 0.5235 | Val Loss: 0.7751 Acc: 0.6367                                               \n",
      "Epoch 019 | Train Loss: 0.8939 Acc: 0.5266 | Val Loss: 0.7735 Acc: 0.6684                                               \n",
      "Epoch 020 | Train Loss: 0.9004 Acc: 0.5279 | Val Loss: 0.7859 Acc: 0.5770                                               \n",
      "Epoch 021 | Train Loss: 0.8958 Acc: 0.5330 | Val Loss: 0.7839 Acc: 0.6397                                               \n",
      "Epoch 022 | Train Loss: 0.8942 Acc: 0.5281 | Val Loss: 0.7824 Acc: 0.6248                                               \n",
      "Epoch 023 | Train Loss: 0.8921 Acc: 0.5276 | Val Loss: 0.7649 Acc: 0.6457                                               \n",
      "Epoch 024 | Train Loss: 0.8962 Acc: 0.5340 | Val Loss: 0.7788 Acc: 0.6704                                               \n",
      "Epoch 025 | Train Loss: 0.8913 Acc: 0.5291 | Val Loss: 0.7564 Acc: 0.6415                                               \n",
      "Epoch 026 | Train Loss: 0.8869 Acc: 0.5332 | Val Loss: 0.7557 Acc: 0.6719                                               \n",
      "Epoch 027 | Train Loss: 0.8867 Acc: 0.5370 | Val Loss: 0.7723 Acc: 0.5964                                               \n",
      "Epoch 028 | Train Loss: 0.8827 Acc: 0.5406 | Val Loss: 0.7839 Acc: 0.6507                                               \n",
      "Epoch 029 | Train Loss: 0.8881 Acc: 0.5353 | Val Loss: 0.7753 Acc: 0.6281                                               \n",
      "Epoch 030 | Train Loss: 0.8850 Acc: 0.5451 | Val Loss: 0.7489 Acc: 0.6469                                               \n",
      "Epoch 031 | Train Loss: 0.8861 Acc: 0.5401 | Val Loss: 0.7555 Acc: 0.6582                                               \n",
      "Epoch 032 | Train Loss: 0.8786 Acc: 0.5531 | Val Loss: 0.7883 Acc: 0.6090                                               \n",
      "Epoch 033 | Train Loss: 0.8848 Acc: 0.5409 | Val Loss: 0.7393 Acc: 0.6797                                               \n",
      "Epoch 034 | Train Loss: 0.8710 Acc: 0.5539 | Val Loss: 0.7393 Acc: 0.6722                                               \n",
      "Epoch 035 | Train Loss: 0.8668 Acc: 0.5565 | Val Loss: 0.7442 Acc: 0.6567                                               \n",
      "Epoch 036 | Train Loss: 0.8642 Acc: 0.5606 | Val Loss: 0.7282 Acc: 0.6612                                               \n",
      "Epoch 037 | Train Loss: 0.8673 Acc: 0.5590 | Val Loss: 0.7799 Acc: 0.5961                                               \n",
      "Epoch 038 | Train Loss: 0.8690 Acc: 0.5565 | Val Loss: 0.7376 Acc: 0.6576                                               \n",
      "Epoch 039 | Train Loss: 0.8592 Acc: 0.5665 | Val Loss: 0.7320 Acc: 0.6985                                               \n",
      "Epoch 040 | Train Loss: 0.8582 Acc: 0.5592 | Val Loss: 0.7275 Acc: 0.6666                                               \n",
      "Epoch 041 | Train Loss: 0.8667 Acc: 0.5586 | Val Loss: 0.7341 Acc: 0.6490                                               \n",
      "Epoch 042 | Train Loss: 0.8646 Acc: 0.5678 | Val Loss: 0.8242 Acc: 0.5684                                               \n",
      "Epoch 043 | Train Loss: 0.8664 Acc: 0.5640 | Val Loss: 0.7327 Acc: 0.6639                                               \n",
      "Epoch 044 | Train Loss: 0.8521 Acc: 0.5779 | Val Loss: 0.7302 Acc: 0.6600                                               \n",
      "Epoch 045 | Train Loss: 0.8572 Acc: 0.5762 | Val Loss: 0.7548 Acc: 0.6266                                               \n",
      "Epoch 046 | Train Loss: 0.8443 Acc: 0.5845 | Val Loss: 0.7297 Acc: 0.6149                                               \n",
      "Epoch 047 | Train Loss: 0.8525 Acc: 0.5750 | Val Loss: 0.7284 Acc: 0.6552                                               \n",
      "Epoch 048 | Train Loss: 0.8538 Acc: 0.5780 | Val Loss: 0.7391 Acc: 0.6752                                               \n",
      "Epoch 049 | Train Loss: 0.8528 Acc: 0.5787 | Val Loss: 0.7145 Acc: 0.6851                                               \n",
      "Epoch 050 | Train Loss: 0.8520 Acc: 0.5751 | Val Loss: 0.8053 Acc: 0.5418                                               \n",
      "Epoch 051 | Train Loss: 0.8551 Acc: 0.5785 | Val Loss: 0.7633 Acc: 0.6394                                               \n",
      "Epoch 052 | Train Loss: 0.8459 Acc: 0.5792 | Val Loss: 0.7160 Acc: 0.6728                                               \n",
      "Epoch 053 | Train Loss: 0.8483 Acc: 0.5750 | Val Loss: 0.7447 Acc: 0.6397                                               \n",
      "Epoch 054 | Train Loss: 0.8430 Acc: 0.5780 | Val Loss: 0.7239 Acc: 0.6690                                               \n",
      "Epoch 055 | Train Loss: 0.8441 Acc: 0.5797 | Val Loss: 0.7622 Acc: 0.6269                                               \n",
      "Epoch 056 | Train Loss: 0.8427 Acc: 0.5759 | Val Loss: 0.7378 Acc: 0.7075                                               \n",
      "Epoch 057 | Train Loss: 0.8410 Acc: 0.5879 | Val Loss: 0.7429 Acc: 0.6651                                               \n",
      "Epoch 058 | Train Loss: 0.8555 Acc: 0.5759 | Val Loss: 0.7295 Acc: 0.6481                                               \n",
      "Epoch 059 | Train Loss: 0.8408 Acc: 0.5845 | Val Loss: 0.7450 Acc: 0.6263                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 128, 'cnn_dropout': 0.6510683796983017, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 96, 'learning_rate': 6.879245085523463e-05, 'lstm_dense': 32, 'lstm_hidden_size': 64, 'lstm_layers': 3, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 15.8975 Acc: 0.3807 | Val Loss: 2.6235 Acc: 0.5239                                              \n",
      "Epoch 002 | Train Loss: 6.0608 Acc: 0.4186 | Val Loss: 1.8168 Acc: 0.4982                                               \n",
      "Epoch 003 | Train Loss: 3.6590 Acc: 0.4527 | Val Loss: 1.5056 Acc: 0.4767                                               \n",
      "Epoch 004 | Train Loss: 2.6449 Acc: 0.4679 | Val Loss: 1.5366 Acc: 0.5137                                               \n",
      "Epoch 005 | Train Loss: 2.0045 Acc: 0.5138 | Val Loss: 1.1197 Acc: 0.5872                                               \n",
      "Epoch 006 | Train Loss: 1.6630 Acc: 0.5412 | Val Loss: 0.9423 Acc: 0.5937                                               \n",
      "Epoch 007 | Train Loss: 1.4094 Acc: 0.5696 | Val Loss: 0.9152 Acc: 0.5755                                               \n",
      "Epoch 008 | Train Loss: 1.2678 Acc: 0.5941 | Val Loss: 0.8507 Acc: 0.6663                                               \n",
      "Epoch 009 | Train Loss: 1.1305 Acc: 0.6113 | Val Loss: 1.0620 Acc: 0.6284                                               \n",
      "Epoch 010 | Train Loss: 1.0209 Acc: 0.6487 | Val Loss: 0.7165 Acc: 0.7206                                               \n",
      "Epoch 011 | Train Loss: 0.9184 Acc: 0.6721 | Val Loss: 0.7271 Acc: 0.7301                                               \n",
      "Epoch 012 | Train Loss: 0.8562 Acc: 0.7018 | Val Loss: 0.6211 Acc: 0.7507                                               \n",
      "Epoch 013 | Train Loss: 0.8029 Acc: 0.7171 | Val Loss: 0.7630 Acc: 0.7287                                               \n",
      "Epoch 014 | Train Loss: 0.7265 Acc: 0.7445 | Val Loss: 0.6265 Acc: 0.7537                                               \n",
      "Epoch 015 | Train Loss: 0.6628 Acc: 0.7618 | Val Loss: 0.6602 Acc: 0.7567                                               \n",
      "Epoch 016 | Train Loss: 0.6104 Acc: 0.7824 | Val Loss: 0.5111 Acc: 0.8194                                               \n",
      "Epoch 017 | Train Loss: 0.5674 Acc: 0.7980 | Val Loss: 0.4649 Acc: 0.8275                                               \n",
      "Epoch 018 | Train Loss: 0.5328 Acc: 0.8130 | Val Loss: 0.4441 Acc: 0.8248                                               \n",
      "Epoch 019 | Train Loss: 0.4915 Acc: 0.8260 | Val Loss: 0.4141 Acc: 0.8433                                               \n",
      "Epoch 020 | Train Loss: 0.4463 Acc: 0.8448 | Val Loss: 0.3993 Acc: 0.8630                                               \n",
      "Epoch 021 | Train Loss: 0.4210 Acc: 0.8545 | Val Loss: 0.3569 Acc: 0.8642                                               \n",
      "Epoch 022 | Train Loss: 0.3811 Acc: 0.8675 | Val Loss: 0.2827 Acc: 0.8991                                               \n",
      "Epoch 023 | Train Loss: 0.3627 Acc: 0.8748 | Val Loss: 0.3352 Acc: 0.8809                                               \n",
      "Epoch 024 | Train Loss: 0.3224 Acc: 0.8882 | Val Loss: 0.2843 Acc: 0.8988                                               \n",
      "Epoch 025 | Train Loss: 0.3174 Acc: 0.8935 | Val Loss: 0.2938 Acc: 0.8934                                               \n",
      "Epoch 026 | Train Loss: 0.2746 Acc: 0.9071 | Val Loss: 0.2658 Acc: 0.9107                                               \n",
      "Epoch 027 | Train Loss: 0.2711 Acc: 0.9100 | Val Loss: 0.2027 Acc: 0.9382                                               \n",
      "Epoch 028 | Train Loss: 0.2489 Acc: 0.9182 | Val Loss: 0.1807 Acc: 0.9445                                               \n",
      "Epoch 029 | Train Loss: 0.2423 Acc: 0.9202 | Val Loss: 0.2013 Acc: 0.9400                                               \n",
      "Epoch 030 | Train Loss: 0.2122 Acc: 0.9307 | Val Loss: 0.2393 Acc: 0.9206                                               \n",
      "Epoch 031 | Train Loss: 0.2111 Acc: 0.9296 | Val Loss: 0.1831 Acc: 0.9430                                               \n",
      "Epoch 032 | Train Loss: 0.1956 Acc: 0.9351 | Val Loss: 0.2379 Acc: 0.9197                                               \n",
      "Epoch 033 | Train Loss: 0.1836 Acc: 0.9397 | Val Loss: 0.1571 Acc: 0.9478                                               \n",
      "Epoch 034 | Train Loss: 0.1710 Acc: 0.9434 | Val Loss: 0.1356 Acc: 0.9549                                               \n",
      "Epoch 035 | Train Loss: 0.1643 Acc: 0.9445 | Val Loss: 0.1510 Acc: 0.9519                                               \n",
      "Epoch 036 | Train Loss: 0.1557 Acc: 0.9486 | Val Loss: 0.1375 Acc: 0.9570                                               \n",
      "Epoch 037 | Train Loss: 0.1441 Acc: 0.9528 | Val Loss: 0.1391 Acc: 0.9549                                               \n",
      "Epoch 038 | Train Loss: 0.1428 Acc: 0.9534 | Val Loss: 0.1214 Acc: 0.9615                                               \n",
      "Epoch 039 | Train Loss: 0.1334 Acc: 0.9557 | Val Loss: 0.1214 Acc: 0.9618                                               \n",
      "Epoch 040 | Train Loss: 0.1296 Acc: 0.9615 | Val Loss: 0.1426 Acc: 0.9609                                               \n",
      "Epoch 041 | Train Loss: 0.1232 Acc: 0.9616 | Val Loss: 0.1279 Acc: 0.9627                                               \n",
      "Epoch 042 | Train Loss: 0.1173 Acc: 0.9621 | Val Loss: 0.1062 Acc: 0.9648                                               \n",
      "Epoch 043 | Train Loss: 0.1126 Acc: 0.9635 | Val Loss: 0.1536 Acc: 0.9499                                               \n",
      "Epoch 044 | Train Loss: 0.1121 Acc: 0.9645 | Val Loss: 0.1653 Acc: 0.9570                                               \n",
      "Epoch 045 | Train Loss: 0.1077 Acc: 0.9667 | Val Loss: 0.1183 Acc: 0.9618                                               \n",
      "Epoch 046 | Train Loss: 0.1035 Acc: 0.9666 | Val Loss: 0.1194 Acc: 0.9627                                               \n",
      "Epoch 047 | Train Loss: 0.0924 Acc: 0.9713 | Val Loss: 0.0883 Acc: 0.9707                                               \n",
      "Epoch 048 | Train Loss: 0.0926 Acc: 0.9706 | Val Loss: 0.1410 Acc: 0.9552                                               \n",
      "Epoch 049 | Train Loss: 0.0825 Acc: 0.9743 | Val Loss: 0.1000 Acc: 0.9713                                               \n",
      "Epoch 050 | Train Loss: 0.0840 Acc: 0.9723 | Val Loss: 0.0793 Acc: 0.9770                                               \n",
      "Epoch 051 | Train Loss: 0.0773 Acc: 0.9749 | Val Loss: 0.1057 Acc: 0.9719                                               \n",
      "Epoch 052 | Train Loss: 0.0772 Acc: 0.9778 | Val Loss: 0.1139 Acc: 0.9648                                               \n",
      "Epoch 053 | Train Loss: 0.0793 Acc: 0.9755 | Val Loss: 0.1275 Acc: 0.9651                                               \n",
      "Epoch 054 | Train Loss: 0.0724 Acc: 0.9778 | Val Loss: 0.0892 Acc: 0.9716                                               \n",
      "Epoch 055 | Train Loss: 0.0712 Acc: 0.9774 | Val Loss: 0.0961 Acc: 0.9716                                               \n",
      "Epoch 056 | Train Loss: 0.0689 Acc: 0.9787 | Val Loss: 0.1100 Acc: 0.9672                                               \n",
      "Epoch 057 | Train Loss: 0.0724 Acc: 0.9774 | Val Loss: 0.1144 Acc: 0.9669                                               \n",
      "Epoch 058 | Train Loss: 0.0646 Acc: 0.9812 | Val Loss: 0.0853 Acc: 0.9743                                               \n",
      "Epoch 059 | Train Loss: 0.0617 Acc: 0.9793 | Val Loss: 0.1038 Acc: 0.9701                                               \n",
      "Epoch 060 | Train Loss: 0.0609 Acc: 0.9806 | Val Loss: 0.0674 Acc: 0.9794                                               \n",
      "Epoch 061 | Train Loss: 0.0575 Acc: 0.9830 | Val Loss: 0.0961 Acc: 0.9701                                               \n",
      "Epoch 062 | Train Loss: 0.0616 Acc: 0.9811 | Val Loss: 0.0687 Acc: 0.9782                                               \n",
      "Epoch 063 | Train Loss: 0.0560 Acc: 0.9829 | Val Loss: 0.0939 Acc: 0.9734                                               \n",
      "Epoch 064 | Train Loss: 0.0535 Acc: 0.9817 | Val Loss: 0.0892 Acc: 0.9758                                               \n",
      "Epoch 065 | Train Loss: 0.0521 Acc: 0.9833 | Val Loss: 0.0977 Acc: 0.9731                                               \n",
      "Epoch 066 | Train Loss: 0.0513 Acc: 0.9831 | Val Loss: 0.0721 Acc: 0.9779                                               \n",
      "Epoch 067 | Train Loss: 0.0517 Acc: 0.9833 | Val Loss: 0.0709 Acc: 0.9797                                               \n",
      "Epoch 068 | Train Loss: 0.0447 Acc: 0.9854 | Val Loss: 0.0769 Acc: 0.9767                                               \n",
      "Epoch 069 | Train Loss: 0.0503 Acc: 0.9846 | Val Loss: 0.0708 Acc: 0.9809                                               \n",
      "Epoch 070 | Train Loss: 0.0459 Acc: 0.9867 | Val Loss: 0.0764 Acc: 0.9770                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 128, 'cnn_dropout': 0.47554449835530127, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 32, 'cnn_kernels_2': 96, 'learning_rate': 0.009201829548805746, 'lstm_dense': 64, 'lstm_hidden_size': 96, 'lstm_layers': 4, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 113.9188 Acc: 0.4227 | Val Loss: 1.2425 Acc: 0.4421                                             \n",
      "Epoch 002 | Train Loss: 1.2438 Acc: 0.4422 | Val Loss: 1.2421 Acc: 0.4421                                               \n",
      "Epoch 003 | Train Loss: 1.2432 Acc: 0.4422 | Val Loss: 1.2424 Acc: 0.4421                                               \n",
      "Epoch 004 | Train Loss: 1.2442 Acc: 0.4422 | Val Loss: 1.2452 Acc: 0.4421                                               \n",
      "Epoch 005 | Train Loss: 1.2432 Acc: 0.4422 | Val Loss: 1.2420 Acc: 0.4421                                               \n",
      "Epoch 006 | Train Loss: 1.2435 Acc: 0.4422 | Val Loss: 1.2425 Acc: 0.4421                                               \n",
      "Epoch 007 | Train Loss: 1.2428 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 008 | Train Loss: 1.2428 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 009 | Train Loss: 1.2430 Acc: 0.4422 | Val Loss: 1.2415 Acc: 0.4421                                               \n",
      "Epoch 010 | Train Loss: 1.2425 Acc: 0.4422 | Val Loss: 1.2422 Acc: 0.4421                                               \n",
      "Epoch 011 | Train Loss: 1.2429 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 012 | Train Loss: 1.2424 Acc: 0.4422 | Val Loss: 1.2413 Acc: 0.4421                                               \n",
      "Epoch 013 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 014 | Train Loss: 1.2409 Acc: 0.4422 | Val Loss: 1.2385 Acc: 0.4421                                               \n",
      "Epoch 015 | Train Loss: 1.2346 Acc: 0.4422 | Val Loss: 1.2272 Acc: 0.4421                                               \n",
      "Epoch 016 | Train Loss: 1.2151 Acc: 0.4422 | Val Loss: 1.2032 Acc: 0.4421                                               \n",
      "Epoch 017 | Train Loss: 1.1691 Acc: 0.4441 | Val Loss: 1.2409 Acc: 0.4125                                               \n",
      "Epoch 018 | Train Loss: 1.0802 Acc: 0.4985 | Val Loss: 1.0040 Acc: 0.5552                                               \n",
      "Epoch 019 | Train Loss: 0.9822 Acc: 0.5728 | Val Loss: 0.9225 Acc: 0.6069                                               \n",
      "Epoch 020 | Train Loss: 0.9735 Acc: 0.5920 | Val Loss: 0.9556 Acc: 0.6382                                               \n",
      "Epoch 021 | Train Loss: 0.9214 Acc: 0.6333 | Val Loss: 0.8398 Acc: 0.6752                                               \n",
      "Epoch 022 | Train Loss: 0.8435 Acc: 0.6773 | Val Loss: 0.8526 Acc: 0.6973                                               \n",
      "Epoch 023 | Train Loss: 0.8403 Acc: 0.6817 | Val Loss: 0.9188 Acc: 0.6310                                               \n",
      "Epoch 024 | Train Loss: 0.8438 Acc: 0.6807 | Val Loss: 0.8820 Acc: 0.6716                                               \n",
      "Epoch 025 | Train Loss: 0.8032 Acc: 0.6990 | Val Loss: 1.1129 Acc: 0.5591                                               \n",
      "Epoch 026 | Train Loss: 1.2075 Acc: 0.4578 | Val Loss: 1.2056 Acc: 0.4478                                               \n",
      "Epoch 027 | Train Loss: 1.2262 Acc: 0.4419 | Val Loss: 1.2575 Acc: 0.4421                                               \n",
      "Epoch 028 | Train Loss: 1.2419 Acc: 0.4415 | Val Loss: 1.2256 Acc: 0.4421                                               \n",
      "Epoch 029 | Train Loss: 1.2430 Acc: 0.4385 | Val Loss: 1.2311 Acc: 0.4421                                               \n",
      "Epoch 030 | Train Loss: 1.2359 Acc: 0.4423 | Val Loss: 1.2338 Acc: 0.4463                                               \n",
      "Epoch 031 | Train Loss: 1.2322 Acc: 0.4415 | Val Loss: 1.2430 Acc: 0.4081                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 256, 'cnn_dropout': 0.6242829372555561, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 96, 'learning_rate': 0.003367620584979419, 'lstm_dense': 256, 'lstm_hidden_size': 32, 'lstm_layers': 6, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 6099.8136 Acc: 0.4346 | Val Loss: 1.2442 Acc: 0.4421                                            \n",
      "Epoch 002 | Train Loss: 1.2432 Acc: 0.4422 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 003 | Train Loss: 1.2429 Acc: 0.4422 | Val Loss: 1.2442 Acc: 0.4421                                               \n",
      "Epoch 004 | Train Loss: 1.2431 Acc: 0.4422 | Val Loss: 1.2420 Acc: 0.4421                                               \n",
      "Epoch 005 | Train Loss: 1.2425 Acc: 0.4422 | Val Loss: 1.2436 Acc: 0.4421                                               \n",
      "Epoch 006 | Train Loss: 1.2427 Acc: 0.4422 | Val Loss: 1.2422 Acc: 0.4421                                               \n",
      "Epoch 007 | Train Loss: 1.2431 Acc: 0.4422 | Val Loss: 1.2444 Acc: 0.4421                                               \n",
      "Epoch 008 | Train Loss: 1.2430 Acc: 0.4422 | Val Loss: 1.2445 Acc: 0.4421                                               \n",
      "Epoch 009 | Train Loss: 1.2431 Acc: 0.4422 | Val Loss: 1.2420 Acc: 0.4421                                               \n",
      "Epoch 010 | Train Loss: 1.2427 Acc: 0.4422 | Val Loss: 1.2422 Acc: 0.4421                                               \n",
      "Epoch 011 | Train Loss: 1.2428 Acc: 0.4422 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 012 | Train Loss: 1.2431 Acc: 0.4422 | Val Loss: 1.2430 Acc: 0.4421                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 128, 'cnn_dropout': 0.08213795560401194, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 32, 'cnn_kernels_2': 96, 'learning_rate': 3.533513290775202e-05, 'lstm_dense': 32, 'lstm_hidden_size': 96, 'lstm_layers': 5, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 47.3174 Acc: 0.3349 | Val Loss: 19.5596 Acc: 0.4355                                             \n",
      "Epoch 002 | Train Loss: 29.9338 Acc: 0.3580 | Val Loss: 13.1112 Acc: 0.4490                                             \n",
      "Epoch 003 | Train Loss: 21.6672 Acc: 0.3645 | Val Loss: 8.5098 Acc: 0.4442                                              \n",
      "Epoch 004 | Train Loss: 16.6676 Acc: 0.3832 | Val Loss: 5.1564 Acc: 0.4093                                              \n",
      "Epoch 005 | Train Loss: 13.6474 Acc: 0.3887 | Val Loss: 3.8054 Acc: 0.3982                                              \n",
      "Epoch 006 | Train Loss: 11.9941 Acc: 0.3923 | Val Loss: 3.0783 Acc: 0.4651                                              \n",
      "Epoch 007 | Train Loss: 10.4658 Acc: 0.4047 | Val Loss: 2.6043 Acc: 0.4260                                              \n",
      "Epoch 008 | Train Loss: 9.0283 Acc: 0.4132 | Val Loss: 1.9127 Acc: 0.4322                                               \n",
      "Epoch 009 | Train Loss: 8.0788 Acc: 0.4208 | Val Loss: 1.7195 Acc: 0.5093                                               \n",
      "Epoch 010 | Train Loss: 7.1335 Acc: 0.4229 | Val Loss: 1.6955 Acc: 0.4660                                               \n",
      "Epoch 011 | Train Loss: 6.4117 Acc: 0.4296 | Val Loss: 1.1690 Acc: 0.5161                                               \n",
      "Epoch 012 | Train Loss: 5.7285 Acc: 0.4433 | Val Loss: 1.3281 Acc: 0.4558                                               \n",
      "Epoch 013 | Train Loss: 5.2149 Acc: 0.4527 | Val Loss: 1.3155 Acc: 0.5299                                               \n",
      "Epoch 014 | Train Loss: 4.7950 Acc: 0.4603 | Val Loss: 1.5461 Acc: 0.5236                                               \n",
      "Epoch 015 | Train Loss: 4.3477 Acc: 0.4700 | Val Loss: 1.2047 Acc: 0.6075                                               \n",
      "Epoch 016 | Train Loss: 4.0432 Acc: 0.4825 | Val Loss: 1.1107 Acc: 0.6206                                               \n",
      "Epoch 017 | Train Loss: 3.8015 Acc: 0.4844 | Val Loss: 1.2258 Acc: 0.6409                                               \n",
      "Epoch 018 | Train Loss: 3.4324 Acc: 0.5059 | Val Loss: 1.0394 Acc: 0.5997                                               \n",
      "Epoch 019 | Train Loss: 3.1616 Acc: 0.5118 | Val Loss: 0.9346 Acc: 0.6663                                               \n",
      "Epoch 020 | Train Loss: 3.0265 Acc: 0.5248 | Val Loss: 1.2567 Acc: 0.5887                                               \n",
      "Epoch 021 | Train Loss: 2.6645 Acc: 0.5434 | Val Loss: 0.9820 Acc: 0.6636                                               \n",
      "Epoch 022 | Train Loss: 2.4980 Acc: 0.5615 | Val Loss: 1.2312 Acc: 0.5606                                               \n",
      "Epoch 023 | Train Loss: 2.3402 Acc: 0.5673 | Val Loss: 0.8936 Acc: 0.7093                                               \n",
      "Epoch 024 | Train Loss: 2.2364 Acc: 0.5768 | Val Loss: 0.8849 Acc: 0.6940                                               \n",
      "Epoch 025 | Train Loss: 2.0779 Acc: 0.5953 | Val Loss: 0.9359 Acc: 0.6197                                               \n",
      "Epoch 026 | Train Loss: 1.9437 Acc: 0.6050 | Val Loss: 0.8085 Acc: 0.6773                                               \n",
      "Epoch 027 | Train Loss: 1.7823 Acc: 0.6175 | Val Loss: 0.7925 Acc: 0.6833                                               \n",
      "Epoch 028 | Train Loss: 1.7452 Acc: 0.6278 | Val Loss: 0.8078 Acc: 0.6845                                               \n",
      "Epoch 029 | Train Loss: 1.6190 Acc: 0.6407 | Val Loss: 0.7712 Acc: 0.6746                                               \n",
      "Epoch 030 | Train Loss: 1.5408 Acc: 0.6539 | Val Loss: 0.6436 Acc: 0.7475                                               \n",
      "Epoch 031 | Train Loss: 1.4574 Acc: 0.6586 | Val Loss: 0.6377 Acc: 0.7904                                               \n",
      "Epoch 032 | Train Loss: 1.3561 Acc: 0.6757 | Val Loss: 0.5820 Acc: 0.7994                                               \n",
      "Epoch 033 | Train Loss: 1.3038 Acc: 0.6822 | Val Loss: 0.6370 Acc: 0.7794                                               \n",
      "Epoch 034 | Train Loss: 1.2476 Acc: 0.6942 | Val Loss: 0.5670 Acc: 0.7794                                               \n",
      "Epoch 035 | Train Loss: 1.2056 Acc: 0.6991 | Val Loss: 0.8117 Acc: 0.7499                                               \n",
      "Epoch 036 | Train Loss: 1.1062 Acc: 0.7125 | Val Loss: 0.5608 Acc: 0.8084                                               \n",
      "Epoch 037 | Train Loss: 1.0690 Acc: 0.7244 | Val Loss: 0.5528 Acc: 0.7767                                               \n",
      "Epoch 038 | Train Loss: 1.0142 Acc: 0.7321 | Val Loss: 0.5266 Acc: 0.8227                                               \n",
      "Epoch 039 | Train Loss: 1.0251 Acc: 0.7341 | Val Loss: 0.4948 Acc: 0.8269                                               \n",
      "Epoch 040 | Train Loss: 0.9617 Acc: 0.7426 | Val Loss: 0.5793 Acc: 0.8069                                               \n",
      "Epoch 041 | Train Loss: 0.9029 Acc: 0.7532 | Val Loss: 0.4873 Acc: 0.8364                                               \n",
      "Epoch 042 | Train Loss: 0.8770 Acc: 0.7601 | Val Loss: 0.4739 Acc: 0.8355                                               \n",
      "Epoch 043 | Train Loss: 0.8759 Acc: 0.7658 | Val Loss: 0.4842 Acc: 0.8430                                               \n",
      "Epoch 044 | Train Loss: 0.8162 Acc: 0.7743 | Val Loss: 0.5008 Acc: 0.7931                                               \n",
      "Epoch 045 | Train Loss: 0.7858 Acc: 0.7783 | Val Loss: 0.4674 Acc: 0.8158                                               \n",
      "Epoch 046 | Train Loss: 0.7512 Acc: 0.7878 | Val Loss: 0.4297 Acc: 0.8555                                               \n",
      "Epoch 047 | Train Loss: 0.7205 Acc: 0.7983 | Val Loss: 0.4746 Acc: 0.8484                                               \n",
      "Epoch 048 | Train Loss: 0.7109 Acc: 0.7969 | Val Loss: 0.8308 Acc: 0.7325                                               \n",
      "Epoch 049 | Train Loss: 0.6837 Acc: 0.8041 | Val Loss: 0.3806 Acc: 0.8693                                               \n",
      "Epoch 050 | Train Loss: 0.6238 Acc: 0.8127 | Val Loss: 0.5420 Acc: 0.8290                                               \n",
      "Epoch 051 | Train Loss: 0.6179 Acc: 0.8205 | Val Loss: 0.3774 Acc: 0.8678                                               \n",
      "Epoch 052 | Train Loss: 0.6212 Acc: 0.8263 | Val Loss: 0.4633 Acc: 0.8493                                               \n",
      "Epoch 053 | Train Loss: 0.5805 Acc: 0.8268 | Val Loss: 0.4887 Acc: 0.8110                                               \n",
      "Epoch 054 | Train Loss: 0.5829 Acc: 0.8299 | Val Loss: 0.4001 Acc: 0.8696                                               \n",
      "Epoch 055 | Train Loss: 0.5471 Acc: 0.8413 | Val Loss: 0.3520 Acc: 0.8779                                               \n",
      "Epoch 056 | Train Loss: 0.5370 Acc: 0.8416 | Val Loss: 0.3168 Acc: 0.8952                                               \n",
      "Epoch 057 | Train Loss: 0.4973 Acc: 0.8528 | Val Loss: 0.4137 Acc: 0.8567                                               \n",
      "Epoch 058 | Train Loss: 0.5149 Acc: 0.8545 | Val Loss: 0.3602 Acc: 0.8782                                               \n",
      "Epoch 059 | Train Loss: 0.4912 Acc: 0.8569 | Val Loss: 0.4520 Acc: 0.8540                                               \n",
      "Epoch 060 | Train Loss: 0.4640 Acc: 0.8625 | Val Loss: 0.4684 Acc: 0.8457                                               \n",
      "Epoch 061 | Train Loss: 0.4540 Acc: 0.8652 | Val Loss: 0.4509 Acc: 0.8376                                               \n",
      "Epoch 062 | Train Loss: 0.4437 Acc: 0.8661 | Val Loss: 0.3334 Acc: 0.8875                                               \n",
      "Epoch 063 | Train Loss: 0.4213 Acc: 0.8703 | Val Loss: 0.3774 Acc: 0.8770                                               \n",
      "Epoch 064 | Train Loss: 0.4243 Acc: 0.8781 | Val Loss: 0.3110 Acc: 0.8976                                               \n",
      "Epoch 065 | Train Loss: 0.3954 Acc: 0.8769 | Val Loss: 0.4307 Acc: 0.8624                                               \n",
      "Epoch 066 | Train Loss: 0.4018 Acc: 0.8804 | Val Loss: 0.4337 Acc: 0.8645                                               \n",
      "Epoch 067 | Train Loss: 0.3902 Acc: 0.8820 | Val Loss: 0.4728 Acc: 0.8424                                               \n",
      "Epoch 068 | Train Loss: 0.3796 Acc: 0.8856 | Val Loss: 0.3070 Acc: 0.8988                                               \n",
      "Epoch 069 | Train Loss: 0.3659 Acc: 0.8927 | Val Loss: 0.3121 Acc: 0.8937                                               \n",
      "Epoch 070 | Train Loss: 0.3627 Acc: 0.8922 | Val Loss: 0.3269 Acc: 0.8949                                               \n",
      "Epoch 071 | Train Loss: 0.3340 Acc: 0.9019 | Val Loss: 0.4177 Acc: 0.8800                                               \n",
      "Epoch 072 | Train Loss: 0.3215 Acc: 0.9061 | Val Loss: 0.2785 Acc: 0.9042                                               \n",
      "Epoch 073 | Train Loss: 0.3227 Acc: 0.9039 | Val Loss: 0.2934 Acc: 0.9045                                               \n",
      "Epoch 074 | Train Loss: 0.3069 Acc: 0.9069 | Val Loss: 0.2560 Acc: 0.9176                                               \n",
      "Epoch 075 | Train Loss: 0.3006 Acc: 0.9107 | Val Loss: 0.2701 Acc: 0.9119                                               \n",
      "Epoch 076 | Train Loss: 0.2803 Acc: 0.9160 | Val Loss: 0.2879 Acc: 0.9101                                               \n",
      "Epoch 077 | Train Loss: 0.2888 Acc: 0.9122 | Val Loss: 0.3051 Acc: 0.9051                                               \n",
      "Epoch 078 | Train Loss: 0.2861 Acc: 0.9149 | Val Loss: 0.2523 Acc: 0.9149                                               \n",
      "Epoch 079 | Train Loss: 0.2571 Acc: 0.9228 | Val Loss: 0.3966 Acc: 0.8913                                               \n",
      "Epoch 080 | Train Loss: 0.2504 Acc: 0.9273 | Val Loss: 0.3346 Acc: 0.9033                                               \n",
      "Epoch 081 | Train Loss: 0.2516 Acc: 0.9257 | Val Loss: 0.2871 Acc: 0.9134                                               \n",
      "Epoch 082 | Train Loss: 0.2515 Acc: 0.9259 | Val Loss: 0.2932 Acc: 0.9096                                               \n",
      "Epoch 083 | Train Loss: 0.2337 Acc: 0.9334 | Val Loss: 0.3275 Acc: 0.8916                                               \n",
      "Epoch 084 | Train Loss: 0.2366 Acc: 0.9272 | Val Loss: 0.3108 Acc: 0.9048                                               \n",
      "Epoch 085 | Train Loss: 0.2319 Acc: 0.9322 | Val Loss: 0.2964 Acc: 0.9104                                               \n",
      "Epoch 086 | Train Loss: 0.2214 Acc: 0.9330 | Val Loss: 0.3090 Acc: 0.9018                                               \n",
      "Epoch 087 | Train Loss: 0.2269 Acc: 0.9338 | Val Loss: 0.2963 Acc: 0.8970                                               \n",
      "Epoch 088 | Train Loss: 0.2146 Acc: 0.9378 | Val Loss: 0.2570 Acc: 0.9176                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 256, 'cnn_dropout': 0.4388812629880132, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 32, 'cnn_kernels_2': 16, 'learning_rate': 0.00026700637200875304, 'lstm_dense': 32, 'lstm_hidden_size': 96, 'lstm_layers': 5, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 5.4749 Acc: 0.2905 | Val Loss: 1.3572 Acc: 0.2743                                               \n",
      "Epoch 002 | Train Loss: 1.3291 Acc: 0.3187 | Val Loss: 1.3245 Acc: 0.4421                                               \n",
      "Epoch 003 | Train Loss: 1.3014 Acc: 0.4429 | Val Loss: 1.3018 Acc: 0.4421                                               \n",
      "Epoch 004 | Train Loss: 1.2826 Acc: 0.4425 | Val Loss: 1.2860 Acc: 0.4421                                               \n",
      "Epoch 005 | Train Loss: 1.2702 Acc: 0.4423 | Val Loss: 1.2748 Acc: 0.4421                                               \n",
      "Epoch 006 | Train Loss: 1.2607 Acc: 0.4425 | Val Loss: 1.2669 Acc: 0.4421                                               \n",
      "Epoch 007 | Train Loss: 1.2541 Acc: 0.4423 | Val Loss: 1.2612 Acc: 0.4421                                               \n",
      "Epoch 008 | Train Loss: 1.2507 Acc: 0.4415 | Val Loss: 1.2571 Acc: 0.4421                                               \n",
      "Epoch 009 | Train Loss: 1.2465 Acc: 0.4423 | Val Loss: 1.2540 Acc: 0.4421                                               \n",
      "Epoch 010 | Train Loss: 1.2443 Acc: 0.4423 | Val Loss: 1.2516 Acc: 0.4421                                               \n",
      "Epoch 011 | Train Loss: 1.2413 Acc: 0.4426 | Val Loss: 1.2499 Acc: 0.4421                                               \n",
      "Epoch 012 | Train Loss: 1.2388 Acc: 0.4436 | Val Loss: 1.2484 Acc: 0.4421                                               \n",
      "Epoch 013 | Train Loss: 1.2379 Acc: 0.4432 | Val Loss: 1.2473 Acc: 0.4421                                               \n",
      "Epoch 014 | Train Loss: 1.2373 Acc: 0.4425 | Val Loss: 1.2464 Acc: 0.4421                                               \n",
      "Epoch 015 | Train Loss: 1.2366 Acc: 0.4428 | Val Loss: 1.2456 Acc: 0.4421                                               \n",
      "Epoch 016 | Train Loss: 1.2360 Acc: 0.4429 | Val Loss: 1.2450 Acc: 0.4421                                               \n",
      "Epoch 017 | Train Loss: 1.2345 Acc: 0.4429 | Val Loss: 1.2446 Acc: 0.4421                                               \n",
      "Epoch 018 | Train Loss: 1.2350 Acc: 0.4428 | Val Loss: 1.2442 Acc: 0.4421                                               \n",
      "Epoch 019 | Train Loss: 1.2357 Acc: 0.4428 | Val Loss: 1.2438 Acc: 0.4421                                               \n",
      "Epoch 020 | Train Loss: 1.2344 Acc: 0.4425 | Val Loss: 1.2435 Acc: 0.4421                                               \n",
      "Epoch 021 | Train Loss: 1.2339 Acc: 0.4420 | Val Loss: 1.2433 Acc: 0.4421                                               \n",
      "Epoch 022 | Train Loss: 1.2337 Acc: 0.4424 | Val Loss: 1.2431 Acc: 0.4421                                               \n",
      "Epoch 023 | Train Loss: 1.2340 Acc: 0.4421 | Val Loss: 1.2429 Acc: 0.4421                                               \n",
      "Epoch 024 | Train Loss: 1.2338 Acc: 0.4417 | Val Loss: 1.2428 Acc: 0.4421                                               \n",
      "Epoch 025 | Train Loss: 1.2325 Acc: 0.4428 | Val Loss: 1.2427 Acc: 0.4421                                               \n",
      "Epoch 026 | Train Loss: 1.2322 Acc: 0.4425 | Val Loss: 1.2426 Acc: 0.4421                                               \n",
      "Epoch 027 | Train Loss: 1.2349 Acc: 0.4416 | Val Loss: 1.2425 Acc: 0.4421                                               \n",
      "Epoch 028 | Train Loss: 1.2325 Acc: 0.4422 | Val Loss: 1.2424 Acc: 0.4421                                               \n",
      "Epoch 029 | Train Loss: 1.2330 Acc: 0.4424 | Val Loss: 1.2423 Acc: 0.4421                                               \n",
      "Epoch 030 | Train Loss: 1.2332 Acc: 0.4422 | Val Loss: 1.2423 Acc: 0.4421                                               \n",
      "Epoch 031 | Train Loss: 1.2316 Acc: 0.4431 | Val Loss: 1.2422 Acc: 0.4421                                               \n",
      "Epoch 032 | Train Loss: 1.2309 Acc: 0.4426 | Val Loss: 1.2422 Acc: 0.4421                                               \n",
      "Epoch 033 | Train Loss: 1.2292 Acc: 0.4427 | Val Loss: 1.2421 Acc: 0.4421                                               \n",
      "Epoch 034 | Train Loss: 1.2307 Acc: 0.4429 | Val Loss: 1.2421 Acc: 0.4421                                               \n",
      "Epoch 035 | Train Loss: 1.2323 Acc: 0.4426 | Val Loss: 1.2421 Acc: 0.4421                                               \n",
      "Epoch 036 | Train Loss: 1.2309 Acc: 0.4425 | Val Loss: 1.2420 Acc: 0.4421                                               \n",
      "Epoch 037 | Train Loss: 1.2317 Acc: 0.4430 | Val Loss: 1.2420 Acc: 0.4421                                               \n",
      "Epoch 038 | Train Loss: 1.2313 Acc: 0.4430 | Val Loss: 1.2420 Acc: 0.4421                                               \n",
      "Epoch 039 | Train Loss: 1.2328 Acc: 0.4420 | Val Loss: 1.2420 Acc: 0.4421                                               \n",
      "Epoch 040 | Train Loss: 1.2315 Acc: 0.4429 | Val Loss: 1.2420 Acc: 0.4421                                               \n",
      "Epoch 041 | Train Loss: 1.2322 Acc: 0.4426 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 042 | Train Loss: 1.2313 Acc: 0.4428 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 043 | Train Loss: 1.2324 Acc: 0.4427 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 044 | Train Loss: 1.2302 Acc: 0.4430 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 045 | Train Loss: 1.2304 Acc: 0.4426 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 046 | Train Loss: 1.2307 Acc: 0.4430 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 047 | Train Loss: 1.2301 Acc: 0.4430 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 048 | Train Loss: 1.2293 Acc: 0.4428 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 049 | Train Loss: 1.2314 Acc: 0.4426 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 050 | Train Loss: 1.2320 Acc: 0.4428 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 051 | Train Loss: 1.2298 Acc: 0.4429 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 052 | Train Loss: 1.2299 Acc: 0.4434 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 053 | Train Loss: 1.2313 Acc: 0.4426 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 054 | Train Loss: 1.2298 Acc: 0.4426 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 055 | Train Loss: 1.2313 Acc: 0.4429 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 056 | Train Loss: 1.2296 Acc: 0.4434 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 057 | Train Loss: 1.2305 Acc: 0.4427 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 058 | Train Loss: 1.2298 Acc: 0.4427 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 059 | Train Loss: 1.2307 Acc: 0.4426 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 060 | Train Loss: 1.2315 Acc: 0.4429 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 061 | Train Loss: 1.2311 Acc: 0.4426 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 062 | Train Loss: 1.2322 Acc: 0.4423 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 063 | Train Loss: 1.2302 Acc: 0.4426 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 064 | Train Loss: 1.2313 Acc: 0.4420 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 065 | Train Loss: 1.2308 Acc: 0.4426 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 066 | Train Loss: 1.2301 Acc: 0.4429 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 067 | Train Loss: 1.2300 Acc: 0.4435 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 068 | Train Loss: 1.2301 Acc: 0.4429 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 069 | Train Loss: 1.2308 Acc: 0.4419 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 070 | Train Loss: 1.2305 Acc: 0.4418 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 071 | Train Loss: 1.2287 Acc: 0.4433 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 072 | Train Loss: 1.2308 Acc: 0.4429 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 073 | Train Loss: 1.2301 Acc: 0.4429 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 074 | Train Loss: 1.2292 Acc: 0.4429 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 075 | Train Loss: 1.2301 Acc: 0.4429 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 076 | Train Loss: 1.2314 Acc: 0.4426 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 077 | Train Loss: 1.2302 Acc: 0.4429 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 078 | Train Loss: 1.2309 Acc: 0.4420 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 079 | Train Loss: 1.2303 Acc: 0.4425 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 080 | Train Loss: 1.2300 Acc: 0.4430 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 081 | Train Loss: 1.2315 Acc: 0.4426 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 082 | Train Loss: 1.2309 Acc: 0.4427 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 083 | Train Loss: 1.2294 Acc: 0.4431 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 084 | Train Loss: 1.2301 Acc: 0.4431 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 085 | Train Loss: 1.2299 Acc: 0.4424 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 086 | Train Loss: 1.2305 Acc: 0.4426 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 087 | Train Loss: 1.2291 Acc: 0.4435 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 088 | Train Loss: 1.2360 Acc: 0.4430 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 089 | Train Loss: 1.2284 Acc: 0.4449 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 090 | Train Loss: 1.2320 Acc: 0.4426 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 091 | Train Loss: 1.2304 Acc: 0.4430 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 092 | Train Loss: 1.2304 Acc: 0.4428 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 093 | Train Loss: 1.2302 Acc: 0.4426 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 094 | Train Loss: 1.2299 Acc: 0.4426 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 095 | Train Loss: 1.2298 Acc: 0.4429 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 096 | Train Loss: 1.2292 Acc: 0.4432 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 097 | Train Loss: 1.2297 Acc: 0.4426 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 098 | Train Loss: 1.2293 Acc: 0.4432 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 099 | Train Loss: 1.2326 Acc: 0.4424 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 100 | Train Loss: 1.2311 Acc: 0.4417 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 101 | Train Loss: 1.2304 Acc: 0.4432 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 102 | Train Loss: 1.2281 Acc: 0.4426 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 256, 'cnn_dropout': 0.23974561099746244, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 96, 'learning_rate': 0.0002151014855074714, 'lstm_dense': 64, 'lstm_hidden_size': 64, 'lstm_layers': 1, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 26.8282 Acc: 0.3661 | Val Loss: 6.1087 Acc: 0.3439                                              \n",
      "Epoch 002 | Train Loss: 8.9079 Acc: 0.4035 | Val Loss: 2.8010 Acc: 0.3884                                               \n",
      "Epoch 003 | Train Loss: 5.7797 Acc: 0.4173 | Val Loss: 1.2783 Acc: 0.5454                                               \n",
      "Epoch 004 | Train Loss: 4.1901 Acc: 0.4297 | Val Loss: 2.7246 Acc: 0.4630                                               \n",
      "Epoch 005 | Train Loss: 3.3264 Acc: 0.4438 | Val Loss: 1.5907 Acc: 0.5442                                               \n",
      "Epoch 006 | Train Loss: 2.5603 Acc: 0.4547 | Val Loss: 1.3275 Acc: 0.5060                                               \n",
      "Epoch 007 | Train Loss: 2.1685 Acc: 0.4806 | Val Loss: 0.8955 Acc: 0.6221                                               \n",
      "Epoch 008 | Train Loss: 1.8452 Acc: 0.5030 | Val Loss: 1.0139 Acc: 0.5654                                               \n",
      "Epoch 009 | Train Loss: 1.5869 Acc: 0.5204 | Val Loss: 0.9749 Acc: 0.6000                                               \n",
      "Epoch 010 | Train Loss: 1.4588 Acc: 0.5231 | Val Loss: 0.8820 Acc: 0.6012                                               \n",
      "Epoch 011 | Train Loss: 1.2701 Acc: 0.5354 | Val Loss: 0.9314 Acc: 0.5690                                               \n",
      "Epoch 012 | Train Loss: 1.1993 Acc: 0.5327 | Val Loss: 0.7262 Acc: 0.6388                                               \n",
      "Epoch 013 | Train Loss: 1.1514 Acc: 0.5394 | Val Loss: 1.0342 Acc: 0.5555                                               \n",
      "Epoch 014 | Train Loss: 1.0407 Acc: 0.5577 | Val Loss: 1.1785 Acc: 0.5609                                               \n",
      "Epoch 015 | Train Loss: 1.0152 Acc: 0.5609 | Val Loss: 0.8376 Acc: 0.6660                                               \n",
      "Epoch 016 | Train Loss: 0.9720 Acc: 0.5746 | Val Loss: 0.6742 Acc: 0.7045                                               \n",
      "Epoch 017 | Train Loss: 0.9244 Acc: 0.5778 | Val Loss: 0.7428 Acc: 0.6403                                               \n",
      "Epoch 018 | Train Loss: 0.8662 Acc: 0.6062 | Val Loss: 0.7566 Acc: 0.6087                                               \n",
      "Epoch 019 | Train Loss: 0.8599 Acc: 0.6065 | Val Loss: 0.6416 Acc: 0.6973                                               \n",
      "Epoch 020 | Train Loss: 0.8256 Acc: 0.6090 | Val Loss: 0.7721 Acc: 0.5961                                               \n",
      "Epoch 021 | Train Loss: 0.7790 Acc: 0.6337 | Val Loss: 0.6468 Acc: 0.7131                                               \n",
      "Epoch 022 | Train Loss: 0.7623 Acc: 0.6488 | Val Loss: 0.5965 Acc: 0.7630                                               \n",
      "Epoch 023 | Train Loss: 0.7609 Acc: 0.6512 | Val Loss: 0.6594 Acc: 0.6803                                               \n",
      "Epoch 024 | Train Loss: 0.7400 Acc: 0.6618 | Val Loss: 0.6206 Acc: 0.7293                                               \n",
      "Epoch 025 | Train Loss: 0.7260 Acc: 0.6681 | Val Loss: 0.6093 Acc: 0.7412                                               \n",
      "Epoch 026 | Train Loss: 0.7044 Acc: 0.6811 | Val Loss: 0.6687 Acc: 0.6752                                               \n",
      "Epoch 027 | Train Loss: 0.6924 Acc: 0.6856 | Val Loss: 0.5493 Acc: 0.7696                                               \n",
      "Epoch 028 | Train Loss: 0.6689 Acc: 0.6961 | Val Loss: 0.6238 Acc: 0.7078                                               \n",
      "Epoch 029 | Train Loss: 0.6479 Acc: 0.7066 | Val Loss: 0.5915 Acc: 0.7412                                               \n",
      "Epoch 030 | Train Loss: 0.6364 Acc: 0.7181 | Val Loss: 0.7023 Acc: 0.6355                                               \n",
      "Epoch 031 | Train Loss: 0.6406 Acc: 0.7150 | Val Loss: 0.5806 Acc: 0.7182                                               \n",
      "Epoch 032 | Train Loss: 0.6082 Acc: 0.7371 | Val Loss: 0.5590 Acc: 0.7469                                               \n",
      "Epoch 033 | Train Loss: 0.5873 Acc: 0.7519 | Val Loss: 0.5720 Acc: 0.7501                                               \n",
      "Epoch 034 | Train Loss: 0.5946 Acc: 0.7474 | Val Loss: 0.5288 Acc: 0.7767                                               \n",
      "Epoch 035 | Train Loss: 0.5514 Acc: 0.7689 | Val Loss: 0.4634 Acc: 0.8075                                               \n",
      "Epoch 036 | Train Loss: 0.5317 Acc: 0.7753 | Val Loss: 0.4885 Acc: 0.7991                                               \n",
      "Epoch 037 | Train Loss: 0.5154 Acc: 0.7843 | Val Loss: 0.5368 Acc: 0.7490                                               \n",
      "Epoch 038 | Train Loss: 0.5143 Acc: 0.7874 | Val Loss: 0.4279 Acc: 0.8313                                               \n",
      "Epoch 039 | Train Loss: 0.4922 Acc: 0.7989 | Val Loss: 0.4857 Acc: 0.7910                                               \n",
      "Epoch 040 | Train Loss: 0.4833 Acc: 0.8019 | Val Loss: 0.4089 Acc: 0.8325                                               \n",
      "Epoch 041 | Train Loss: 0.4320 Acc: 0.8289 | Val Loss: 0.3642 Acc: 0.8549                                               \n",
      "Epoch 042 | Train Loss: 0.4000 Acc: 0.8416 | Val Loss: 0.3284 Acc: 0.8818                                               \n",
      "Epoch 043 | Train Loss: 0.3710 Acc: 0.8567 | Val Loss: 0.3220 Acc: 0.8785                                               \n",
      "Epoch 044 | Train Loss: 0.3330 Acc: 0.8726 | Val Loss: 0.3180 Acc: 0.8734                                               \n",
      "Epoch 045 | Train Loss: 0.3152 Acc: 0.8853 | Val Loss: 0.2417 Acc: 0.9066                                               \n",
      "Epoch 046 | Train Loss: 0.2796 Acc: 0.8944 | Val Loss: 0.2408 Acc: 0.9024                                               \n",
      "Epoch 047 | Train Loss: 0.2496 Acc: 0.9104 | Val Loss: 0.2506 Acc: 0.9006                                               \n",
      "Epoch 048 | Train Loss: 0.2266 Acc: 0.9167 | Val Loss: 0.2218 Acc: 0.9093                                               \n",
      "Epoch 049 | Train Loss: 0.2069 Acc: 0.9295 | Val Loss: 0.1799 Acc: 0.9278                                               \n",
      "Epoch 050 | Train Loss: 0.1801 Acc: 0.9381 | Val Loss: 0.1875 Acc: 0.9251                                               \n",
      "Epoch 051 | Train Loss: 0.1647 Acc: 0.9428 | Val Loss: 0.1484 Acc: 0.9436                                               \n",
      "Epoch 052 | Train Loss: 0.1445 Acc: 0.9474 | Val Loss: 0.1675 Acc: 0.9394                                               \n",
      "Epoch 053 | Train Loss: 0.1203 Acc: 0.9581 | Val Loss: 0.1905 Acc: 0.9299                                               \n",
      "Epoch 054 | Train Loss: 0.1328 Acc: 0.9534 | Val Loss: 0.1106 Acc: 0.9591                                               \n",
      "Epoch 055 | Train Loss: 0.1064 Acc: 0.9640 | Val Loss: 0.1537 Acc: 0.9394                                               \n",
      "Epoch 056 | Train Loss: 0.1081 Acc: 0.9645 | Val Loss: 0.1196 Acc: 0.9564                                               \n",
      "Epoch 057 | Train Loss: 0.0986 Acc: 0.9649 | Val Loss: 0.1177 Acc: 0.9540                                               \n",
      "Epoch 058 | Train Loss: 0.0811 Acc: 0.9726 | Val Loss: 0.1325 Acc: 0.9501                                               \n",
      "Epoch 059 | Train Loss: 0.0928 Acc: 0.9687 | Val Loss: 0.0850 Acc: 0.9687                                               \n",
      "Epoch 060 | Train Loss: 0.0799 Acc: 0.9718 | Val Loss: 0.1218 Acc: 0.9567                                               \n",
      "Epoch 061 | Train Loss: 0.0754 Acc: 0.9757 | Val Loss: 0.1019 Acc: 0.9633                                               \n",
      "Epoch 062 | Train Loss: 0.0593 Acc: 0.9799 | Val Loss: 0.1013 Acc: 0.9627                                               \n",
      "Epoch 063 | Train Loss: 0.0618 Acc: 0.9791 | Val Loss: 0.0753 Acc: 0.9761                                               \n",
      "Epoch 064 | Train Loss: 0.0578 Acc: 0.9801 | Val Loss: 0.0765 Acc: 0.9731                                               \n",
      "Epoch 065 | Train Loss: 0.0609 Acc: 0.9793 | Val Loss: 0.0931 Acc: 0.9684                                               \n",
      "Epoch 066 | Train Loss: 0.0509 Acc: 0.9826 | Val Loss: 0.0859 Acc: 0.9740                                               \n",
      "Epoch 067 | Train Loss: 0.0424 Acc: 0.9859 | Val Loss: 0.0659 Acc: 0.9797                                               \n",
      "Epoch 068 | Train Loss: 0.0393 Acc: 0.9872 | Val Loss: 0.0771 Acc: 0.9758                                               \n",
      "Epoch 069 | Train Loss: 0.0443 Acc: 0.9848 | Val Loss: 0.0845 Acc: 0.9743                                               \n",
      "Epoch 070 | Train Loss: 0.0419 Acc: 0.9862 | Val Loss: 0.0937 Acc: 0.9713                                               \n",
      "Epoch 071 | Train Loss: 0.0397 Acc: 0.9867 | Val Loss: 0.0658 Acc: 0.9770                                               \n",
      "Epoch 072 | Train Loss: 0.0375 Acc: 0.9868 | Val Loss: 0.0812 Acc: 0.9731                                               \n",
      "Epoch 073 | Train Loss: 0.0586 Acc: 0.9818 | Val Loss: 0.0761 Acc: 0.9755                                               \n",
      "Epoch 074 | Train Loss: 0.0315 Acc: 0.9896 | Val Loss: 0.0691 Acc: 0.9767                                               \n",
      "Epoch 075 | Train Loss: 0.0293 Acc: 0.9908 | Val Loss: 0.0865 Acc: 0.9725                                               \n",
      "Epoch 076 | Train Loss: 0.0192 Acc: 0.9942 | Val Loss: 0.0945 Acc: 0.9713                                               \n",
      "Epoch 077 | Train Loss: 0.0334 Acc: 0.9893 | Val Loss: 0.1156 Acc: 0.9657                                               \n",
      "Epoch 078 | Train Loss: 0.0321 Acc: 0.9887 | Val Loss: 0.0618 Acc: 0.9782                                               \n",
      "Epoch 079 | Train Loss: 0.0327 Acc: 0.9896 | Val Loss: 0.1134 Acc: 0.9651                                               \n",
      "Epoch 080 | Train Loss: 0.0313 Acc: 0.9897 | Val Loss: 0.0671 Acc: 0.9776                                               \n",
      "Epoch 081 | Train Loss: 0.0227 Acc: 0.9935 | Val Loss: 0.0843 Acc: 0.9761                                               \n",
      "Epoch 082 | Train Loss: 0.0205 Acc: 0.9934 | Val Loss: 0.1023 Acc: 0.9684                                               \n",
      "Epoch 083 | Train Loss: 0.0301 Acc: 0.9890 | Val Loss: 0.0799 Acc: 0.9755                                               \n",
      "Epoch 084 | Train Loss: 0.0282 Acc: 0.9903 | Val Loss: 0.0511 Acc: 0.9827                                               \n",
      "Epoch 085 | Train Loss: 0.0252 Acc: 0.9916 | Val Loss: 0.0541 Acc: 0.9842                                               \n",
      "Epoch 086 | Train Loss: 0.0160 Acc: 0.9950 | Val Loss: 0.0629 Acc: 0.9827                                               \n",
      "Epoch 087 | Train Loss: 0.0234 Acc: 0.9929 | Val Loss: 0.0593 Acc: 0.9821                                               \n",
      "Epoch 088 | Train Loss: 0.0286 Acc: 0.9906 | Val Loss: 0.0611 Acc: 0.9833                                               \n",
      "Epoch 089 | Train Loss: 0.0164 Acc: 0.9951 | Val Loss: 0.0531 Acc: 0.9812                                               \n",
      "Epoch 090 | Train Loss: 0.0160 Acc: 0.9953 | Val Loss: 0.0620 Acc: 0.9827                                               \n",
      "Epoch 091 | Train Loss: 0.0153 Acc: 0.9956 | Val Loss: 0.0950 Acc: 0.9761                                               \n",
      "Epoch 092 | Train Loss: 0.0139 Acc: 0.9956 | Val Loss: 0.0709 Acc: 0.9821                                               \n",
      "Epoch 093 | Train Loss: 0.0111 Acc: 0.9966 | Val Loss: 0.0877 Acc: 0.9779                                               \n",
      "Epoch 094 | Train Loss: 0.0218 Acc: 0.9929 | Val Loss: 0.0719 Acc: 0.9812                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 32, 'cnn_dropout': 0.2813378727470812, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 48, 'cnn_kernels_2': 64, 'learning_rate': 9.583066702530785e-05, 'lstm_dense': 256, 'lstm_hidden_size': 128, 'lstm_layers': 4, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 21.3152 Acc: 0.3664 | Val Loss: 2.7766 Acc: 0.5693                                              \n",
      "Epoch 002 | Train Loss: 5.9160 Acc: 0.4432 | Val Loss: 1.6231 Acc: 0.5913                                               \n",
      "Epoch 003 | Train Loss: 3.5591 Acc: 0.5087 | Val Loss: 0.9886 Acc: 0.6842                                               \n",
      "Epoch 004 | Train Loss: 2.4251 Acc: 0.5630 | Val Loss: 0.9994 Acc: 0.7101                                               \n",
      "Epoch 005 | Train Loss: 1.8443 Acc: 0.6003 | Val Loss: 0.9701 Acc: 0.7167                                               \n",
      "Epoch 006 | Train Loss: 1.3727 Acc: 0.6535 | Val Loss: 0.7806 Acc: 0.7293                                               \n",
      "Epoch 007 | Train Loss: 1.1439 Acc: 0.6959 | Val Loss: 1.0905 Acc: 0.6594                                               \n",
      "Epoch 008 | Train Loss: 0.9456 Acc: 0.7357 | Val Loss: 0.6235 Acc: 0.7931                                               \n",
      "Epoch 009 | Train Loss: 0.7948 Acc: 0.7639 | Val Loss: 0.5082 Acc: 0.8406                                               \n",
      "Epoch 010 | Train Loss: 0.7091 Acc: 0.7839 | Val Loss: 0.5419 Acc: 0.8194                                               \n",
      "Epoch 011 | Train Loss: 0.5845 Acc: 0.8152 | Val Loss: 0.4028 Acc: 0.8421                                               \n",
      "Epoch 012 | Train Loss: 0.4953 Acc: 0.8372 | Val Loss: 0.4151 Acc: 0.8600                                               \n",
      "Epoch 013 | Train Loss: 0.4273 Acc: 0.8612 | Val Loss: 0.3234 Acc: 0.8845                                               \n",
      "Epoch 014 | Train Loss: 0.3620 Acc: 0.8807 | Val Loss: 0.3492 Acc: 0.8743                                               \n",
      "Epoch 015 | Train Loss: 0.3095 Acc: 0.9002 | Val Loss: 0.2390 Acc: 0.9200                                               \n",
      "Epoch 016 | Train Loss: 0.2775 Acc: 0.9095 | Val Loss: 0.3971 Acc: 0.8693                                               \n",
      "Epoch 017 | Train Loss: 0.2474 Acc: 0.9216 | Val Loss: 0.2489 Acc: 0.9131                                               \n",
      "Epoch 018 | Train Loss: 0.2022 Acc: 0.9350 | Val Loss: 0.1666 Acc: 0.9487                                               \n",
      "Epoch 019 | Train Loss: 0.1954 Acc: 0.9382 | Val Loss: 0.2240 Acc: 0.9257                                               \n",
      "Epoch 020 | Train Loss: 0.1807 Acc: 0.9404 | Val Loss: 0.1407 Acc: 0.9519                                               \n",
      "Epoch 021 | Train Loss: 0.1581 Acc: 0.9504 | Val Loss: 0.1440 Acc: 0.9504                                               \n",
      "Epoch 022 | Train Loss: 0.1406 Acc: 0.9545 | Val Loss: 0.1450 Acc: 0.9460                                               \n",
      "Epoch 023 | Train Loss: 0.1352 Acc: 0.9566 | Val Loss: 0.2163 Acc: 0.9257                                               \n",
      "Epoch 024 | Train Loss: 0.1292 Acc: 0.9591 | Val Loss: 0.1428 Acc: 0.9555                                               \n",
      "Epoch 025 | Train Loss: 0.1206 Acc: 0.9630 | Val Loss: 0.1191 Acc: 0.9606                                               \n",
      "Epoch 026 | Train Loss: 0.1075 Acc: 0.9672 | Val Loss: 0.1706 Acc: 0.9499                                               \n",
      "Epoch 027 | Train Loss: 0.0950 Acc: 0.9695 | Val Loss: 0.1528 Acc: 0.9528                                               \n",
      "Epoch 028 | Train Loss: 0.0962 Acc: 0.9700 | Val Loss: 0.1122 Acc: 0.9633                                               \n",
      "Epoch 029 | Train Loss: 0.0922 Acc: 0.9709 | Val Loss: 0.1169 Acc: 0.9630                                               \n",
      "Epoch 030 | Train Loss: 0.0835 Acc: 0.9743 | Val Loss: 0.1623 Acc: 0.9496                                               \n",
      "Epoch 031 | Train Loss: 0.0716 Acc: 0.9780 | Val Loss: 0.1023 Acc: 0.9642                                               \n",
      "Epoch 032 | Train Loss: 0.0789 Acc: 0.9746 | Val Loss: 0.1032 Acc: 0.9681                                               \n",
      "Epoch 033 | Train Loss: 0.0717 Acc: 0.9774 | Val Loss: 0.0856 Acc: 0.9725                                               \n",
      "Epoch 034 | Train Loss: 0.0601 Acc: 0.9817 | Val Loss: 0.0907 Acc: 0.9699                                               \n",
      "Epoch 035 | Train Loss: 0.0582 Acc: 0.9808 | Val Loss: 0.0844 Acc: 0.9734                                               \n",
      "Epoch 036 | Train Loss: 0.0551 Acc: 0.9817 | Val Loss: 0.0767 Acc: 0.9758                                               \n",
      "Epoch 037 | Train Loss: 0.0591 Acc: 0.9817 | Val Loss: 0.1070 Acc: 0.9684                                               \n",
      "Epoch 038 | Train Loss: 0.0539 Acc: 0.9844 | Val Loss: 0.0925 Acc: 0.9699                                               \n",
      "Epoch 039 | Train Loss: 0.0535 Acc: 0.9842 | Val Loss: 0.1018 Acc: 0.9713                                               \n",
      "Epoch 040 | Train Loss: 0.0543 Acc: 0.9835 | Val Loss: 0.1049 Acc: 0.9687                                               \n",
      "Epoch 041 | Train Loss: 0.0475 Acc: 0.9860 | Val Loss: 0.1152 Acc: 0.9678                                               \n",
      "Epoch 042 | Train Loss: 0.0481 Acc: 0.9848 | Val Loss: 0.0846 Acc: 0.9770                                               \n",
      "Epoch 043 | Train Loss: 0.0466 Acc: 0.9855 | Val Loss: 0.1572 Acc: 0.9567                                               \n",
      "Epoch 044 | Train Loss: 0.0399 Acc: 0.9876 | Val Loss: 0.1049 Acc: 0.9693                                               \n",
      "Epoch 045 | Train Loss: 0.0349 Acc: 0.9887 | Val Loss: 0.0779 Acc: 0.9776                                               \n",
      "Epoch 046 | Train Loss: 0.0371 Acc: 0.9895 | Val Loss: 0.1376 Acc: 0.9666                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 32, 'cnn_dropout': 0.45011225265358445, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 16, 'cnn_kernels_2': 16, 'learning_rate': 3.353742402679978e-05, 'lstm_dense': 256, 'lstm_hidden_size': 128, 'lstm_layers': 4, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 21.6480 Acc: 0.3413 | Val Loss: 9.4966 Acc: 0.4379                                              \n",
      "Epoch 002 | Train Loss: 11.7694 Acc: 0.3667 | Val Loss: 5.4616 Acc: 0.4075                                              \n",
      "Epoch 003 | Train Loss: 8.2590 Acc: 0.3832 | Val Loss: 2.4452 Acc: 0.4904                                               \n",
      "Epoch 004 | Train Loss: 6.5537 Acc: 0.3966 | Val Loss: 1.7133 Acc: 0.4982                                               \n",
      "Epoch 005 | Train Loss: 5.3233 Acc: 0.4089 | Val Loss: 1.6969 Acc: 0.4406                                               \n",
      "Epoch 006 | Train Loss: 4.6242 Acc: 0.4281 | Val Loss: 1.5130 Acc: 0.5012                                               \n",
      "Epoch 007 | Train Loss: 3.9790 Acc: 0.4228 | Val Loss: 1.3586 Acc: 0.5248                                               \n",
      "Epoch 008 | Train Loss: 3.4375 Acc: 0.4420 | Val Loss: 1.2565 Acc: 0.5704                                               \n",
      "Epoch 009 | Train Loss: 3.0593 Acc: 0.4585 | Val Loss: 1.2652 Acc: 0.5284                                               \n",
      "Epoch 010 | Train Loss: 2.6365 Acc: 0.4778 | Val Loss: 1.2790 Acc: 0.5382                                               \n",
      "Epoch 011 | Train Loss: 2.3906 Acc: 0.4869 | Val Loss: 1.0200 Acc: 0.6257                                               \n",
      "Epoch 012 | Train Loss: 2.2344 Acc: 0.4916 | Val Loss: 1.1064 Acc: 0.5463                                               \n",
      "Epoch 013 | Train Loss: 1.9634 Acc: 0.5088 | Val Loss: 1.0110 Acc: 0.5549                                               \n",
      "Epoch 014 | Train Loss: 1.8525 Acc: 0.5249 | Val Loss: 0.9511 Acc: 0.6382                                               \n",
      "Epoch 015 | Train Loss: 1.7154 Acc: 0.5317 | Val Loss: 1.0916 Acc: 0.6182                                               \n",
      "Epoch 016 | Train Loss: 1.5591 Acc: 0.5456 | Val Loss: 1.0463 Acc: 0.6200                                               \n",
      "Epoch 017 | Train Loss: 1.4921 Acc: 0.5533 | Val Loss: 1.0963 Acc: 0.6176                                               \n",
      "Epoch 018 | Train Loss: 1.3575 Acc: 0.5694 | Val Loss: 1.0132 Acc: 0.6361                                               \n",
      "Epoch 019 | Train Loss: 1.3239 Acc: 0.5730 | Val Loss: 1.2335 Acc: 0.5651                                               \n",
      "Epoch 020 | Train Loss: 1.2259 Acc: 0.5915 | Val Loss: 1.1102 Acc: 0.5976                                               \n",
      "Epoch 021 | Train Loss: 1.1574 Acc: 0.6033 | Val Loss: 1.2044 Acc: 0.5866                                               \n",
      "Epoch 022 | Train Loss: 1.1129 Acc: 0.6116 | Val Loss: 1.2326 Acc: 0.5564                                               \n",
      "Epoch 023 | Train Loss: 1.0649 Acc: 0.6246 | Val Loss: 1.0212 Acc: 0.6281                                               \n",
      "Epoch 024 | Train Loss: 1.0182 Acc: 0.6290 | Val Loss: 0.8178 Acc: 0.6696                                               \n",
      "Epoch 025 | Train Loss: 0.9857 Acc: 0.6429 | Val Loss: 0.9054 Acc: 0.6663                                               \n",
      "Epoch 026 | Train Loss: 0.9621 Acc: 0.6525 | Val Loss: 0.9498 Acc: 0.6496                                               \n",
      "Epoch 027 | Train Loss: 0.9367 Acc: 0.6541 | Val Loss: 0.8947 Acc: 0.6496                                               \n",
      "Epoch 028 | Train Loss: 0.8850 Acc: 0.6629 | Val Loss: 0.8454 Acc: 0.6528                                               \n",
      "Epoch 029 | Train Loss: 0.8463 Acc: 0.6712 | Val Loss: 0.7687 Acc: 0.6845                                               \n",
      "Epoch 030 | Train Loss: 0.8355 Acc: 0.6771 | Val Loss: 1.0102 Acc: 0.6021                                               \n",
      "Epoch 031 | Train Loss: 0.8003 Acc: 0.6842 | Val Loss: 0.7277 Acc: 0.6955                                               \n",
      "Epoch 032 | Train Loss: 0.7951 Acc: 0.6849 | Val Loss: 0.7271 Acc: 0.7048                                               \n",
      "Epoch 033 | Train Loss: 0.7612 Acc: 0.7008 | Val Loss: 0.8905 Acc: 0.6439                                               \n",
      "Epoch 034 | Train Loss: 0.7469 Acc: 0.7004 | Val Loss: 0.8139 Acc: 0.6740                                               \n",
      "Epoch 035 | Train Loss: 0.7351 Acc: 0.7035 | Val Loss: 0.8314 Acc: 0.6663                                               \n",
      "Epoch 036 | Train Loss: 0.7217 Acc: 0.7129 | Val Loss: 0.7925 Acc: 0.6881                                               \n",
      "Epoch 037 | Train Loss: 0.6965 Acc: 0.7174 | Val Loss: 0.7239 Acc: 0.7075                                               \n",
      "Epoch 038 | Train Loss: 0.6727 Acc: 0.7297 | Val Loss: 0.6261 Acc: 0.7376                                               \n",
      "Epoch 039 | Train Loss: 0.6459 Acc: 0.7343 | Val Loss: 0.6274 Acc: 0.7439                                               \n",
      "Epoch 040 | Train Loss: 0.6499 Acc: 0.7392 | Val Loss: 0.7896 Acc: 0.6857                                               \n",
      "Epoch 041 | Train Loss: 0.6129 Acc: 0.7512 | Val Loss: 0.6040 Acc: 0.7552                                               \n",
      "Epoch 042 | Train Loss: 0.6151 Acc: 0.7527 | Val Loss: 0.5761 Acc: 0.7627                                               \n",
      "Epoch 043 | Train Loss: 0.5915 Acc: 0.7630 | Val Loss: 0.4826 Acc: 0.8048                                               \n",
      "Epoch 044 | Train Loss: 0.5871 Acc: 0.7615 | Val Loss: 0.7572 Acc: 0.6904                                               \n",
      "Epoch 045 | Train Loss: 0.5835 Acc: 0.7650 | Val Loss: 0.8891 Acc: 0.6481                                               \n",
      "Epoch 046 | Train Loss: 0.5707 Acc: 0.7694 | Val Loss: 0.6601 Acc: 0.7287                                               \n",
      "Epoch 047 | Train Loss: 0.5547 Acc: 0.7768 | Val Loss: 0.4575 Acc: 0.8266                                               \n",
      "Epoch 048 | Train Loss: 0.5436 Acc: 0.7808 | Val Loss: 0.6805 Acc: 0.7251                                               \n",
      "Epoch 049 | Train Loss: 0.5278 Acc: 0.7892 | Val Loss: 0.5124 Acc: 0.7946                                               \n",
      "Epoch 050 | Train Loss: 0.5269 Acc: 0.7904 | Val Loss: 0.4379 Acc: 0.8287                                               \n",
      "Epoch 051 | Train Loss: 0.5039 Acc: 0.8007 | Val Loss: 0.4696 Acc: 0.8179                                               \n",
      "Epoch 052 | Train Loss: 0.4939 Acc: 0.8010 | Val Loss: 0.4673 Acc: 0.8203                                               \n",
      "Epoch 053 | Train Loss: 0.4915 Acc: 0.8065 | Val Loss: 0.4698 Acc: 0.8167                                               \n",
      "Epoch 054 | Train Loss: 0.4726 Acc: 0.8098 | Val Loss: 0.5006 Acc: 0.8027                                               \n",
      "Epoch 055 | Train Loss: 0.4836 Acc: 0.8075 | Val Loss: 0.4260 Acc: 0.8403                                               \n",
      "Epoch 056 | Train Loss: 0.4657 Acc: 0.8177 | Val Loss: 0.4347 Acc: 0.8257                                               \n",
      "Epoch 057 | Train Loss: 0.4663 Acc: 0.8192 | Val Loss: 0.4406 Acc: 0.8391                                               \n",
      "Epoch 058 | Train Loss: 0.4500 Acc: 0.8223 | Val Loss: 0.4643 Acc: 0.8093                                               \n",
      "Epoch 059 | Train Loss: 0.4506 Acc: 0.8239 | Val Loss: 0.4217 Acc: 0.8439                                               \n",
      "Epoch 060 | Train Loss: 0.4240 Acc: 0.8390 | Val Loss: 0.4203 Acc: 0.8445                                               \n",
      "Epoch 061 | Train Loss: 0.4290 Acc: 0.8338 | Val Loss: 0.4068 Acc: 0.8448                                               \n",
      "Epoch 062 | Train Loss: 0.4173 Acc: 0.8378 | Val Loss: 0.4929 Acc: 0.8099                                               \n",
      "Epoch 063 | Train Loss: 0.4100 Acc: 0.8431 | Val Loss: 0.3692 Acc: 0.8624                                               \n",
      "Epoch 064 | Train Loss: 0.4112 Acc: 0.8395 | Val Loss: 0.4330 Acc: 0.8313                                               \n",
      "Epoch 065 | Train Loss: 0.3999 Acc: 0.8463 | Val Loss: 0.3706 Acc: 0.8630                                               \n",
      "Epoch 066 | Train Loss: 0.3981 Acc: 0.8477 | Val Loss: 0.3591 Acc: 0.8642                                               \n",
      "Epoch 067 | Train Loss: 0.3750 Acc: 0.8579 | Val Loss: 0.3962 Acc: 0.8552                                               \n",
      "Epoch 068 | Train Loss: 0.3769 Acc: 0.8609 | Val Loss: 0.4056 Acc: 0.8451                                               \n",
      "Epoch 069 | Train Loss: 0.3766 Acc: 0.8588 | Val Loss: 0.3383 Acc: 0.8740                                               \n",
      "Epoch 070 | Train Loss: 0.3666 Acc: 0.8579 | Val Loss: 0.4537 Acc: 0.8206                                               \n",
      "Epoch 071 | Train Loss: 0.3526 Acc: 0.8663 | Val Loss: 0.3293 Acc: 0.8710                                               \n",
      "Epoch 072 | Train Loss: 0.3425 Acc: 0.8680 | Val Loss: 0.3477 Acc: 0.8609                                               \n",
      "Epoch 073 | Train Loss: 0.3399 Acc: 0.8678 | Val Loss: 0.3230 Acc: 0.8719                                               \n",
      "Epoch 074 | Train Loss: 0.3370 Acc: 0.8712 | Val Loss: 0.2933 Acc: 0.8824                                               \n",
      "Epoch 075 | Train Loss: 0.3339 Acc: 0.8721 | Val Loss: 0.3777 Acc: 0.8430                                               \n",
      "Epoch 076 | Train Loss: 0.3162 Acc: 0.8760 | Val Loss: 0.2737 Acc: 0.8931                                               \n",
      "Epoch 077 | Train Loss: 0.3103 Acc: 0.8822 | Val Loss: 0.3334 Acc: 0.8624                                               \n",
      "Epoch 078 | Train Loss: 0.3076 Acc: 0.8813 | Val Loss: 0.2633 Acc: 0.8910                                               \n",
      "Epoch 079 | Train Loss: 0.3012 Acc: 0.8831 | Val Loss: 0.3443 Acc: 0.8639                                               \n",
      "Epoch 080 | Train Loss: 0.2898 Acc: 0.8908 | Val Loss: 0.2749 Acc: 0.8887                                               \n",
      "Epoch 081 | Train Loss: 0.2923 Acc: 0.8892 | Val Loss: 0.2719 Acc: 0.8925                                               \n",
      "Epoch 082 | Train Loss: 0.2841 Acc: 0.8930 | Val Loss: 0.2866 Acc: 0.8842                                               \n",
      "Epoch 083 | Train Loss: 0.2766 Acc: 0.8984 | Val Loss: 0.2678 Acc: 0.8872                                               \n",
      "Epoch 084 | Train Loss: 0.2697 Acc: 0.9004 | Val Loss: 0.2717 Acc: 0.8922                                               \n",
      "Epoch 085 | Train Loss: 0.2672 Acc: 0.8979 | Val Loss: 0.2176 Acc: 0.9096                                               \n",
      "Epoch 086 | Train Loss: 0.2636 Acc: 0.9036 | Val Loss: 0.2472 Acc: 0.8976                                               \n",
      "Epoch 087 | Train Loss: 0.2562 Acc: 0.9049 | Val Loss: 0.2156 Acc: 0.9090                                               \n",
      "Epoch 088 | Train Loss: 0.2593 Acc: 0.9065 | Val Loss: 0.2478 Acc: 0.8988                                               \n",
      "Epoch 089 | Train Loss: 0.2409 Acc: 0.9101 | Val Loss: 0.2827 Acc: 0.8893                                               \n",
      "Epoch 090 | Train Loss: 0.2434 Acc: 0.9110 | Val Loss: 0.2455 Acc: 0.8985                                               \n",
      "Epoch 091 | Train Loss: 0.2400 Acc: 0.9137 | Val Loss: 0.2358 Acc: 0.9045                                               \n",
      "Epoch 092 | Train Loss: 0.2285 Acc: 0.9138 | Val Loss: 0.2197 Acc: 0.9122                                               \n",
      "Epoch 093 | Train Loss: 0.2320 Acc: 0.9167 | Val Loss: 0.2423 Acc: 0.9045                                               \n",
      "Epoch 094 | Train Loss: 0.2302 Acc: 0.9182 | Val Loss: 0.2100 Acc: 0.9107                                               \n",
      "Epoch 095 | Train Loss: 0.2235 Acc: 0.9197 | Val Loss: 0.2036 Acc: 0.9170                                               \n",
      "Epoch 096 | Train Loss: 0.2179 Acc: 0.9219 | Val Loss: 0.2163 Acc: 0.9122                                               \n",
      "Epoch 097 | Train Loss: 0.2083 Acc: 0.9261 | Val Loss: 0.2584 Acc: 0.8979                                               \n",
      "Epoch 098 | Train Loss: 0.2014 Acc: 0.9269 | Val Loss: 0.1996 Acc: 0.9194                                               \n",
      "Epoch 099 | Train Loss: 0.2022 Acc: 0.9291 | Val Loss: 0.2776 Acc: 0.8946                                               \n",
      "Epoch 100 | Train Loss: 0.1974 Acc: 0.9292 | Val Loss: 0.1827 Acc: 0.9218                                               \n",
      "Epoch 101 | Train Loss: 0.1987 Acc: 0.9290 | Val Loss: 0.2242 Acc: 0.9113                                               \n",
      "Epoch 102 | Train Loss: 0.1904 Acc: 0.9342 | Val Loss: 0.2212 Acc: 0.9131                                               \n",
      "Epoch 103 | Train Loss: 0.1850 Acc: 0.9331 | Val Loss: 0.2533 Acc: 0.9027                                               \n",
      "Epoch 104 | Train Loss: 0.1836 Acc: 0.9350 | Val Loss: 0.1948 Acc: 0.9203                                               \n",
      "Epoch 105 | Train Loss: 0.1843 Acc: 0.9360 | Val Loss: 0.1919 Acc: 0.9215                                               \n",
      "Epoch 106 | Train Loss: 0.1749 Acc: 0.9382 | Val Loss: 0.1925 Acc: 0.9212                                               \n",
      "Epoch 107 | Train Loss: 0.1758 Acc: 0.9356 | Val Loss: 0.1910 Acc: 0.9248                                               \n",
      "Epoch 108 | Train Loss: 0.1681 Acc: 0.9427 | Val Loss: 0.1674 Acc: 0.9296                                               \n",
      "Epoch 109 | Train Loss: 0.1721 Acc: 0.9392 | Val Loss: 0.1890 Acc: 0.9242                                               \n",
      "Epoch 110 | Train Loss: 0.1717 Acc: 0.9421 | Val Loss: 0.2277 Acc: 0.9101                                               \n",
      "Epoch 111 | Train Loss: 0.1597 Acc: 0.9448 | Val Loss: 0.1896 Acc: 0.9236                                               \n",
      "Epoch 112 | Train Loss: 0.1657 Acc: 0.9425 | Val Loss: 0.1897 Acc: 0.9254                                               \n",
      "Epoch 113 | Train Loss: 0.1567 Acc: 0.9447 | Val Loss: 0.1739 Acc: 0.9293                                               \n",
      "Epoch 114 | Train Loss: 0.1560 Acc: 0.9425 | Val Loss: 0.1877 Acc: 0.9260                                               \n",
      "Epoch 115 | Train Loss: 0.1586 Acc: 0.9446 | Val Loss: 0.1553 Acc: 0.9400                                               \n",
      "Epoch 116 | Train Loss: 0.1496 Acc: 0.9481 | Val Loss: 0.1778 Acc: 0.9266                                               \n",
      "Epoch 117 | Train Loss: 0.1467 Acc: 0.9485 | Val Loss: 0.1743 Acc: 0.9322                                               \n",
      "Epoch 118 | Train Loss: 0.1397 Acc: 0.9526 | Val Loss: 0.1609 Acc: 0.9319                                               \n",
      "Epoch 119 | Train Loss: 0.1459 Acc: 0.9507 | Val Loss: 0.1625 Acc: 0.9364                                               \n",
      "Epoch 120 | Train Loss: 0.1390 Acc: 0.9526 | Val Loss: 0.1622 Acc: 0.9343                                               \n",
      "Epoch 121 | Train Loss: 0.1354 Acc: 0.9536 | Val Loss: 0.2008 Acc: 0.9224                                               \n",
      "Epoch 122 | Train Loss: 0.1386 Acc: 0.9516 | Val Loss: 0.1706 Acc: 0.9316                                               \n",
      "Epoch 123 | Train Loss: 0.1379 Acc: 0.9533 | Val Loss: 0.2215 Acc: 0.9164                                               \n",
      "Epoch 124 | Train Loss: 0.1300 Acc: 0.9571 | Val Loss: 0.1582 Acc: 0.9370                                               \n",
      "Epoch 125 | Train Loss: 0.1299 Acc: 0.9565 | Val Loss: 0.1772 Acc: 0.9301                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 36, 'cnn_dense': 64, 'cnn_dropout': 0.5205242827881843, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 64, 'learning_rate': 0.001614474546870474, 'lstm_dense': 32, 'lstm_hidden_size': 32, 'lstm_layers': 5, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 11.0254 Acc: 0.4709 | Val Loss: 0.8922 Acc: 0.6537                                              \n",
      "Epoch 002 | Train Loss: 0.9771 Acc: 0.6057 | Val Loss: 0.9200 Acc: 0.6224                                               \n",
      "Epoch 003 | Train Loss: 0.8300 Acc: 0.6230 | Val Loss: 0.7150 Acc: 0.6890                                               \n",
      "Epoch 004 | Train Loss: 0.7736 Acc: 0.6362 | Val Loss: 0.7471 Acc: 0.6272                                               \n",
      "Epoch 005 | Train Loss: 0.7479 Acc: 0.6580 | Val Loss: 0.8165 Acc: 0.6516                                               \n",
      "Epoch 006 | Train Loss: 0.6843 Acc: 0.6788 | Val Loss: 0.9553 Acc: 0.5409                                               \n",
      "Epoch 007 | Train Loss: 0.7016 Acc: 0.6814 | Val Loss: 1.1282 Acc: 0.4758                                               \n",
      "Epoch 008 | Train Loss: 0.6603 Acc: 0.6884 | Val Loss: 0.5944 Acc: 0.7412                                               \n",
      "Epoch 009 | Train Loss: 0.6338 Acc: 0.7120 | Val Loss: 0.5355 Acc: 0.7901                                               \n",
      "Epoch 010 | Train Loss: 0.6438 Acc: 0.7040 | Val Loss: 0.6025 Acc: 0.7331                                               \n",
      "Epoch 011 | Train Loss: 0.6238 Acc: 0.7118 | Val Loss: 0.8249 Acc: 0.6090                                               \n",
      "Epoch 012 | Train Loss: 0.6255 Acc: 0.7130 | Val Loss: 0.5957 Acc: 0.7325                                               \n",
      "Epoch 013 | Train Loss: 0.6218 Acc: 0.7156 | Val Loss: 0.7448 Acc: 0.6755                                               \n",
      "Epoch 014 | Train Loss: 0.6168 Acc: 0.7220 | Val Loss: 0.6599 Acc: 0.6940                                               \n",
      "Epoch 015 | Train Loss: 0.6061 Acc: 0.7207 | Val Loss: 0.5242 Acc: 0.7433                                               \n",
      "Epoch 016 | Train Loss: 0.5892 Acc: 0.7276 | Val Loss: 0.4964 Acc: 0.7973                                               \n",
      "Epoch 017 | Train Loss: 0.6002 Acc: 0.7280 | Val Loss: 0.7961 Acc: 0.6833                                               \n",
      "Epoch 018 | Train Loss: 0.5938 Acc: 0.7232 | Val Loss: 0.5995 Acc: 0.7839                                               \n",
      "Epoch 019 | Train Loss: 0.5862 Acc: 0.7314 | Val Loss: 0.8171 Acc: 0.6266                                               \n",
      "Epoch 020 | Train Loss: 0.5823 Acc: 0.7398 | Val Loss: 0.8088 Acc: 0.6391                                               \n",
      "Epoch 021 | Train Loss: 0.5760 Acc: 0.7363 | Val Loss: 0.6677 Acc: 0.7248                                               \n",
      "Epoch 022 | Train Loss: 0.5643 Acc: 0.7511 | Val Loss: 0.6829 Acc: 0.7349                                               \n",
      "Epoch 023 | Train Loss: 0.5726 Acc: 0.7529 | Val Loss: 0.6822 Acc: 0.6970                                               \n",
      "Epoch 024 | Train Loss: 0.5521 Acc: 0.7509 | Val Loss: 0.6517 Acc: 0.7430                                               \n",
      "Epoch 025 | Train Loss: 0.5677 Acc: 0.7551 | Val Loss: 0.6235 Acc: 0.7087                                               \n",
      "Epoch 026 | Train Loss: 0.5414 Acc: 0.7557 | Val Loss: 0.5741 Acc: 0.6985                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 64, 'cnn_dropout': 0.45767155349328287, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 48, 'cnn_kernels_2': 64, 'learning_rate': 4.4327034965589666e-05, 'lstm_dense': 64, 'lstm_hidden_size': 32, 'lstm_layers': 6, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 7.1839 Acc: 0.4317 | Val Loss: 0.8995 Acc: 0.5051                                               \n",
      "Epoch 002 | Train Loss: 1.0080 Acc: 0.4875 | Val Loss: 0.8295 Acc: 0.6107                                               \n",
      "Epoch 003 | Train Loss: 0.9547 Acc: 0.5108 | Val Loss: 0.8024 Acc: 0.6021                                               \n",
      "Epoch 004 | Train Loss: 0.9275 Acc: 0.5437 | Val Loss: 0.7688 Acc: 0.6328                                               \n",
      "Epoch 005 | Train Loss: 0.9098 Acc: 0.5494 | Val Loss: 0.7435 Acc: 0.6684                                               \n",
      "Epoch 006 | Train Loss: 0.8899 Acc: 0.5532 | Val Loss: 0.7333 Acc: 0.6890                                               \n",
      "Epoch 007 | Train Loss: 0.8833 Acc: 0.5730 | Val Loss: 0.7332 Acc: 0.6675                                               \n",
      "Epoch 008 | Train Loss: 0.8787 Acc: 0.5739 | Val Loss: 0.7147 Acc: 0.6734                                               \n",
      "Epoch 009 | Train Loss: 0.8620 Acc: 0.5867 | Val Loss: 0.7148 Acc: 0.6609                                               \n",
      "Epoch 010 | Train Loss: 0.8579 Acc: 0.5968 | Val Loss: 0.6967 Acc: 0.7084                                               \n",
      "Epoch 011 | Train Loss: 0.8494 Acc: 0.5936 | Val Loss: 0.7001 Acc: 0.6988                                               \n",
      "Epoch 012 | Train Loss: 0.8531 Acc: 0.5937 | Val Loss: 0.7085 Acc: 0.6875                                               \n",
      "Epoch 013 | Train Loss: 0.8441 Acc: 0.5964 | Val Loss: 0.7015 Acc: 0.6931                                               \n",
      "Epoch 014 | Train Loss: 0.8390 Acc: 0.6054 | Val Loss: 0.7047 Acc: 0.6830                                               \n",
      "Epoch 015 | Train Loss: 0.8314 Acc: 0.6151 | Val Loss: 0.6859 Acc: 0.7263                                               \n",
      "Epoch 016 | Train Loss: 0.8338 Acc: 0.6171 | Val Loss: 0.6775 Acc: 0.7170                                               \n",
      "Epoch 017 | Train Loss: 0.8176 Acc: 0.6244 | Val Loss: 0.6728 Acc: 0.7412                                               \n",
      "Epoch 018 | Train Loss: 0.8134 Acc: 0.6285 | Val Loss: 0.6885 Acc: 0.7012                                               \n",
      "Epoch 019 | Train Loss: 0.8255 Acc: 0.6207 | Val Loss: 0.6786 Acc: 0.6979                                               \n",
      "Epoch 020 | Train Loss: 0.8141 Acc: 0.6204 | Val Loss: 0.6653 Acc: 0.7469                                               \n",
      "Epoch 021 | Train Loss: 0.8014 Acc: 0.6376 | Val Loss: 0.6571 Acc: 0.7299                                               \n",
      "Epoch 022 | Train Loss: 0.8091 Acc: 0.6300 | Val Loss: 0.6578 Acc: 0.7093                                               \n",
      "Epoch 023 | Train Loss: 0.7913 Acc: 0.6371 | Val Loss: 0.6471 Acc: 0.7173                                               \n",
      "Epoch 024 | Train Loss: 0.7960 Acc: 0.6383 | Val Loss: 0.6470 Acc: 0.7534                                               \n",
      "Epoch 025 | Train Loss: 0.7981 Acc: 0.6468 | Val Loss: 0.6384 Acc: 0.7737                                               \n",
      "Epoch 026 | Train Loss: 0.7920 Acc: 0.6440 | Val Loss: 0.6401 Acc: 0.7630                                               \n",
      "Epoch 027 | Train Loss: 0.7788 Acc: 0.6527 | Val Loss: 0.6346 Acc: 0.7510                                               \n",
      "Epoch 028 | Train Loss: 0.7775 Acc: 0.6480 | Val Loss: 0.6333 Acc: 0.7681                                               \n",
      "Epoch 029 | Train Loss: 0.7829 Acc: 0.6430 | Val Loss: 0.6268 Acc: 0.7884                                               \n",
      "Epoch 030 | Train Loss: 0.7728 Acc: 0.6534 | Val Loss: 0.6194 Acc: 0.7612                                               \n",
      "Epoch 031 | Train Loss: 0.7713 Acc: 0.6505 | Val Loss: 0.6298 Acc: 0.7627                                               \n",
      "Epoch 032 | Train Loss: 0.7640 Acc: 0.6547 | Val Loss: 0.6156 Acc: 0.7319                                               \n",
      "Epoch 033 | Train Loss: 0.7694 Acc: 0.6515 | Val Loss: 0.6062 Acc: 0.7591                                               \n",
      "Epoch 034 | Train Loss: 0.7536 Acc: 0.6515 | Val Loss: 0.5933 Acc: 0.7845                                               \n",
      "Epoch 035 | Train Loss: 0.7689 Acc: 0.6477 | Val Loss: 0.6036 Acc: 0.7630                                               \n",
      "Epoch 036 | Train Loss: 0.7580 Acc: 0.6530 | Val Loss: 0.6086 Acc: 0.7322                                               \n",
      "Epoch 037 | Train Loss: 0.7638 Acc: 0.6571 | Val Loss: 0.6072 Acc: 0.7367                                               \n",
      "Epoch 038 | Train Loss: 0.7505 Acc: 0.6604 | Val Loss: 0.5980 Acc: 0.7418                                               \n",
      "Epoch 039 | Train Loss: 0.7483 Acc: 0.6630 | Val Loss: 0.5985 Acc: 0.7884                                               \n",
      "Epoch 040 | Train Loss: 0.7342 Acc: 0.6642 | Val Loss: 0.5978 Acc: 0.7313                                               \n",
      "Epoch 041 | Train Loss: 0.7526 Acc: 0.6646 | Val Loss: 0.6008 Acc: 0.8242                                               \n",
      "Epoch 042 | Train Loss: 0.7575 Acc: 0.6620 | Val Loss: 0.6081 Acc: 0.7513                                               \n",
      "Epoch 043 | Train Loss: 0.7444 Acc: 0.6668 | Val Loss: 0.5977 Acc: 0.7481                                               \n",
      "Epoch 044 | Train Loss: 0.7441 Acc: 0.6663 | Val Loss: 0.5887 Acc: 0.7863                                               \n",
      "Epoch 045 | Train Loss: 0.7472 Acc: 0.6687 | Val Loss: 0.6052 Acc: 0.7466                                               \n",
      "Epoch 046 | Train Loss: 0.7464 Acc: 0.6645 | Val Loss: 0.6045 Acc: 0.7475                                               \n",
      "Epoch 047 | Train Loss: 0.7393 Acc: 0.6652 | Val Loss: 0.5831 Acc: 0.7299                                               \n",
      "Epoch 048 | Train Loss: 0.7387 Acc: 0.6710 | Val Loss: 0.5775 Acc: 0.7737                                               \n",
      "Epoch 049 | Train Loss: 0.7449 Acc: 0.6692 | Val Loss: 0.5791 Acc: 0.7791                                               \n",
      "Epoch 050 | Train Loss: 0.7324 Acc: 0.6706 | Val Loss: 0.5724 Acc: 0.7875                                               \n",
      "Epoch 051 | Train Loss: 0.7400 Acc: 0.6726 | Val Loss: 0.5813 Acc: 0.7478                                               \n",
      "Epoch 052 | Train Loss: 0.7234 Acc: 0.6672 | Val Loss: 0.5754 Acc: 0.7600                                               \n",
      "Epoch 053 | Train Loss: 0.7323 Acc: 0.6668 | Val Loss: 0.5719 Acc: 0.7913                                               \n",
      "Epoch 054 | Train Loss: 0.7238 Acc: 0.6784 | Val Loss: 0.5585 Acc: 0.8000                                               \n",
      "Epoch 055 | Train Loss: 0.7242 Acc: 0.6766 | Val Loss: 0.5752 Acc: 0.8412                                               \n",
      "Epoch 056 | Train Loss: 0.7227 Acc: 0.6719 | Val Loss: 0.5487 Acc: 0.8191                                               \n",
      "Epoch 057 | Train Loss: 0.7197 Acc: 0.6818 | Val Loss: 0.5624 Acc: 0.7609                                               \n",
      "Epoch 058 | Train Loss: 0.7220 Acc: 0.6830 | Val Loss: 0.5445 Acc: 0.8245                                               \n",
      "Epoch 059 | Train Loss: 0.7096 Acc: 0.6862 | Val Loss: 0.5606 Acc: 0.7857                                               \n",
      "Epoch 060 | Train Loss: 0.7254 Acc: 0.6823 | Val Loss: 0.5527 Acc: 0.7475                                               \n",
      "Epoch 061 | Train Loss: 0.7243 Acc: 0.6843 | Val Loss: 0.5360 Acc: 0.8200                                               \n",
      "Epoch 062 | Train Loss: 0.7178 Acc: 0.6827 | Val Loss: 0.5728 Acc: 0.7884                                               \n",
      "Epoch 063 | Train Loss: 0.7136 Acc: 0.6814 | Val Loss: 0.5417 Acc: 0.7937                                               \n",
      "Epoch 064 | Train Loss: 0.7079 Acc: 0.6878 | Val Loss: 0.5536 Acc: 0.7424                                               \n",
      "Epoch 065 | Train Loss: 0.7091 Acc: 0.6874 | Val Loss: 0.5434 Acc: 0.8042                                               \n",
      "Epoch 066 | Train Loss: 0.7082 Acc: 0.6901 | Val Loss: 0.5602 Acc: 0.8113                                               \n",
      "Epoch 067 | Train Loss: 0.7186 Acc: 0.6894 | Val Loss: 0.5337 Acc: 0.7955                                               \n",
      "Epoch 068 | Train Loss: 0.6999 Acc: 0.7018 | Val Loss: 0.5310 Acc: 0.8391                                               \n",
      "Epoch 069 | Train Loss: 0.7105 Acc: 0.6862 | Val Loss: 0.5339 Acc: 0.7961                                               \n",
      "Epoch 070 | Train Loss: 0.7101 Acc: 0.6890 | Val Loss: 0.5585 Acc: 0.8036                                               \n",
      "Epoch 071 | Train Loss: 0.7082 Acc: 0.6877 | Val Loss: 0.5255 Acc: 0.8460                                               \n",
      "Epoch 072 | Train Loss: 0.6942 Acc: 0.6882 | Val Loss: 0.5291 Acc: 0.7815                                               \n",
      "Epoch 073 | Train Loss: 0.7025 Acc: 0.6881 | Val Loss: 0.5301 Acc: 0.8546                                               \n",
      "Epoch 074 | Train Loss: 0.7082 Acc: 0.6834 | Val Loss: 0.5309 Acc: 0.8230                                               \n",
      "Epoch 075 | Train Loss: 0.7060 Acc: 0.6856 | Val Loss: 0.5383 Acc: 0.7946                                               \n",
      "Epoch 076 | Train Loss: 0.6927 Acc: 0.6883 | Val Loss: 0.5283 Acc: 0.8158                                               \n",
      "Epoch 077 | Train Loss: 0.7043 Acc: 0.6927 | Val Loss: 0.5470 Acc: 0.8173                                               \n",
      "Epoch 078 | Train Loss: 0.6966 Acc: 0.6836 | Val Loss: 0.5405 Acc: 0.8200                                               \n",
      "Epoch 079 | Train Loss: 0.6895 Acc: 0.6933 | Val Loss: 0.5483 Acc: 0.8212                                               \n",
      "Epoch 080 | Train Loss: 0.6946 Acc: 0.6949 | Val Loss: 0.5344 Acc: 0.8206                                               \n",
      "Epoch 081 | Train Loss: 0.6862 Acc: 0.6935 | Val Loss: 0.5558 Acc: 0.7313                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 256, 'cnn_dropout': 0.26580158935880754, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 32, 'cnn_kernels_2': 16, 'learning_rate': 1.8689929990918273e-05, 'lstm_dense': 32, 'lstm_hidden_size': 128, 'lstm_layers': 2, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 35.2170 Acc: 0.2766 | Val Loss: 8.4998 Acc: 0.4678                                              \n",
      "Epoch 002 | Train Loss: 22.8483 Acc: 0.3596 | Val Loss: 5.7863 Acc: 0.5075                                              \n",
      "Epoch 003 | Train Loss: 16.5286 Acc: 0.3817 | Val Loss: 4.3900 Acc: 0.4955                                              \n",
      "Epoch 004 | Train Loss: 12.5285 Acc: 0.3976 | Val Loss: 3.1105 Acc: 0.4806                                              \n",
      "Epoch 005 | Train Loss: 10.3510 Acc: 0.4083 | Val Loss: 3.0091 Acc: 0.5221                                              \n",
      "Epoch 006 | Train Loss: 8.9338 Acc: 0.4204 | Val Loss: 3.0293 Acc: 0.5236                                               \n",
      "Epoch 007 | Train Loss: 7.8316 Acc: 0.4270 | Val Loss: 2.4409 Acc: 0.5146                                               \n",
      "Epoch 008 | Train Loss: 7.0845 Acc: 0.4315 | Val Loss: 2.3661 Acc: 0.5451                                               \n",
      "Epoch 009 | Train Loss: 6.2151 Acc: 0.4532 | Val Loss: 2.3177 Acc: 0.5848                                               \n",
      "Epoch 010 | Train Loss: 5.4806 Acc: 0.4614 | Val Loss: 1.9751 Acc: 0.5316                                               \n",
      "Epoch 011 | Train Loss: 4.9389 Acc: 0.4735 | Val Loss: 1.9200 Acc: 0.6149                                               \n",
      "Epoch 012 | Train Loss: 4.3881 Acc: 0.4854 | Val Loss: 1.7737 Acc: 0.5761                                               \n",
      "Epoch 013 | Train Loss: 4.0759 Acc: 0.4945 | Val Loss: 1.8603 Acc: 0.6131                                               \n",
      "Epoch 014 | Train Loss: 3.8428 Acc: 0.5070 | Val Loss: 1.6632 Acc: 0.6155                                               \n",
      "Epoch 015 | Train Loss: 3.4071 Acc: 0.5244 | Val Loss: 1.7933 Acc: 0.6054                                               \n",
      "Epoch 016 | Train Loss: 3.2425 Acc: 0.5283 | Val Loss: 1.5046 Acc: 0.6242                                               \n",
      "Epoch 017 | Train Loss: 3.1382 Acc: 0.5262 | Val Loss: 1.4399 Acc: 0.6167                                               \n",
      "Epoch 018 | Train Loss: 2.9534 Acc: 0.5336 | Val Loss: 1.6142 Acc: 0.6170                                               \n",
      "Epoch 019 | Train Loss: 2.8054 Acc: 0.5365 | Val Loss: 1.3150 Acc: 0.6579                                               \n",
      "Epoch 020 | Train Loss: 2.5331 Acc: 0.5564 | Val Loss: 1.4205 Acc: 0.6331                                               \n",
      "Epoch 021 | Train Loss: 2.4840 Acc: 0.5615 | Val Loss: 1.4161 Acc: 0.6400                                               \n",
      "Epoch 022 | Train Loss: 2.3838 Acc: 0.5676 | Val Loss: 1.1903 Acc: 0.6597                                               \n",
      "Epoch 023 | Train Loss: 2.2031 Acc: 0.5750 | Val Loss: 1.1211 Acc: 0.6624                                               \n",
      "Epoch 024 | Train Loss: 2.1525 Acc: 0.5753 | Val Loss: 1.3631 Acc: 0.6528                                               \n",
      "Epoch 025 | Train Loss: 2.0659 Acc: 0.5843 | Val Loss: 1.2614 Acc: 0.6421                                               \n",
      "Epoch 026 | Train Loss: 1.9353 Acc: 0.5853 | Val Loss: 1.0793 Acc: 0.6707                                               \n",
      "Epoch 027 | Train Loss: 1.8750 Acc: 0.6030 | Val Loss: 1.1512 Acc: 0.6836                                               \n",
      "Epoch 028 | Train Loss: 1.8250 Acc: 0.5991 | Val Loss: 1.2224 Acc: 0.6663                                               \n",
      "Epoch 029 | Train Loss: 1.6797 Acc: 0.6035 | Val Loss: 1.3000 Acc: 0.6478                                               \n",
      "Epoch 030 | Train Loss: 1.6953 Acc: 0.6055 | Val Loss: 0.9863 Acc: 0.6943                                               \n",
      "Epoch 031 | Train Loss: 1.6226 Acc: 0.6141 | Val Loss: 1.0823 Acc: 0.6797                                               \n",
      "Epoch 032 | Train Loss: 1.6072 Acc: 0.6119 | Val Loss: 0.9004 Acc: 0.7018                                               \n",
      "Epoch 033 | Train Loss: 1.5053 Acc: 0.6147 | Val Loss: 0.8875 Acc: 0.6982                                               \n",
      "Epoch 034 | Train Loss: 1.4805 Acc: 0.6250 | Val Loss: 0.9733 Acc: 0.6863                                               \n",
      "Epoch 035 | Train Loss: 1.4266 Acc: 0.6291 | Val Loss: 0.9301 Acc: 0.6970                                               \n",
      "Epoch 036 | Train Loss: 1.3550 Acc: 0.6347 | Val Loss: 0.9063 Acc: 0.6884                                               \n",
      "Epoch 037 | Train Loss: 1.3521 Acc: 0.6380 | Val Loss: 0.9001 Acc: 0.6982                                               \n",
      "Epoch 038 | Train Loss: 1.2975 Acc: 0.6409 | Val Loss: 0.8245 Acc: 0.7116                                               \n",
      "Epoch 039 | Train Loss: 1.2071 Acc: 0.6473 | Val Loss: 0.8698 Acc: 0.7033                                               \n",
      "Epoch 040 | Train Loss: 1.1677 Acc: 0.6596 | Val Loss: 0.8495 Acc: 0.7030                                               \n",
      "Epoch 041 | Train Loss: 1.1806 Acc: 0.6609 | Val Loss: 0.8536 Acc: 0.6845                                               \n",
      "Epoch 042 | Train Loss: 1.1289 Acc: 0.6648 | Val Loss: 1.0528 Acc: 0.6582                                               \n",
      "Epoch 043 | Train Loss: 1.1112 Acc: 0.6621 | Val Loss: 0.8356 Acc: 0.7018                                               \n",
      "Epoch 044 | Train Loss: 1.0780 Acc: 0.6699 | Val Loss: 0.8793 Acc: 0.6967                                               \n",
      "Epoch 045 | Train Loss: 1.0354 Acc: 0.6834 | Val Loss: 0.7626 Acc: 0.7134                                               \n",
      "Epoch 046 | Train Loss: 1.0240 Acc: 0.6862 | Val Loss: 0.7351 Acc: 0.7316                                               \n",
      "Epoch 047 | Train Loss: 0.9657 Acc: 0.6960 | Val Loss: 0.6842 Acc: 0.7364                                               \n",
      "Epoch 048 | Train Loss: 0.9522 Acc: 0.6953 | Val Loss: 0.6637 Acc: 0.7370                                               \n",
      "Epoch 049 | Train Loss: 0.9028 Acc: 0.7035 | Val Loss: 0.7085 Acc: 0.7170                                               \n",
      "Epoch 050 | Train Loss: 0.8895 Acc: 0.7060 | Val Loss: 0.6577 Acc: 0.7427                                               \n",
      "Epoch 051 | Train Loss: 0.8879 Acc: 0.7126 | Val Loss: 0.6529 Acc: 0.7373                                               \n",
      "Epoch 052 | Train Loss: 0.8547 Acc: 0.7201 | Val Loss: 0.5801 Acc: 0.7585                                               \n",
      "Epoch 053 | Train Loss: 0.8132 Acc: 0.7290 | Val Loss: 0.6497 Acc: 0.7299                                               \n",
      "Epoch 054 | Train Loss: 0.7958 Acc: 0.7311 | Val Loss: 0.6681 Acc: 0.7475                                               \n",
      "Epoch 055 | Train Loss: 0.7860 Acc: 0.7330 | Val Loss: 0.6816 Acc: 0.7206                                               \n",
      "Epoch 056 | Train Loss: 0.7710 Acc: 0.7340 | Val Loss: 0.6234 Acc: 0.7445                                               \n",
      "Epoch 057 | Train Loss: 0.7573 Acc: 0.7442 | Val Loss: 0.6584 Acc: 0.7463                                               \n",
      "Epoch 058 | Train Loss: 0.7349 Acc: 0.7494 | Val Loss: 0.6146 Acc: 0.7585                                               \n",
      "Epoch 059 | Train Loss: 0.7096 Acc: 0.7522 | Val Loss: 0.5624 Acc: 0.7761                                               \n",
      "Epoch 060 | Train Loss: 0.6819 Acc: 0.7633 | Val Loss: 0.5439 Acc: 0.7752                                               \n",
      "Epoch 061 | Train Loss: 0.6935 Acc: 0.7621 | Val Loss: 0.5713 Acc: 0.7627                                               \n",
      "Epoch 062 | Train Loss: 0.6530 Acc: 0.7666 | Val Loss: 0.5424 Acc: 0.7648                                               \n",
      "Epoch 063 | Train Loss: 0.6228 Acc: 0.7810 | Val Loss: 0.5414 Acc: 0.7794                                               \n",
      "Epoch 064 | Train Loss: 0.6229 Acc: 0.7817 | Val Loss: 0.4985 Acc: 0.8033                                               \n",
      "Epoch 065 | Train Loss: 0.5963 Acc: 0.7883 | Val Loss: 0.4652 Acc: 0.8066                                               \n",
      "Epoch 066 | Train Loss: 0.5844 Acc: 0.7929 | Val Loss: 0.5597 Acc: 0.7755                                               \n",
      "Epoch 067 | Train Loss: 0.5714 Acc: 0.7989 | Val Loss: 0.5043 Acc: 0.7878                                               \n",
      "Epoch 068 | Train Loss: 0.5647 Acc: 0.7992 | Val Loss: 0.4744 Acc: 0.8072                                               \n",
      "Epoch 069 | Train Loss: 0.5516 Acc: 0.8085 | Val Loss: 0.4615 Acc: 0.8170                                               \n",
      "Epoch 070 | Train Loss: 0.5212 Acc: 0.8147 | Val Loss: 0.4502 Acc: 0.8057                                               \n",
      "Epoch 071 | Train Loss: 0.5138 Acc: 0.8158 | Val Loss: 0.4522 Acc: 0.8137                                               \n",
      "Epoch 072 | Train Loss: 0.4878 Acc: 0.8242 | Val Loss: 0.4411 Acc: 0.8221                                               \n",
      "Epoch 073 | Train Loss: 0.4798 Acc: 0.8269 | Val Loss: 0.3992 Acc: 0.8403                                               \n",
      "Epoch 074 | Train Loss: 0.4749 Acc: 0.8347 | Val Loss: 0.4304 Acc: 0.8206                                               \n",
      "Epoch 075 | Train Loss: 0.4475 Acc: 0.8370 | Val Loss: 0.3704 Acc: 0.8588                                               \n",
      "Epoch 076 | Train Loss: 0.4362 Acc: 0.8408 | Val Loss: 0.3668 Acc: 0.8555                                               \n",
      "Epoch 077 | Train Loss: 0.4229 Acc: 0.8491 | Val Loss: 0.3276 Acc: 0.8755                                               \n",
      "Epoch 078 | Train Loss: 0.4169 Acc: 0.8499 | Val Loss: 0.4048 Acc: 0.8427                                               \n",
      "Epoch 079 | Train Loss: 0.4105 Acc: 0.8533 | Val Loss: 0.3629 Acc: 0.8567                                               \n",
      "Epoch 080 | Train Loss: 0.3941 Acc: 0.8551 | Val Loss: 0.3619 Acc: 0.8615                                               \n",
      "Epoch 081 | Train Loss: 0.3849 Acc: 0.8616 | Val Loss: 0.3639 Acc: 0.8561                                               \n",
      "Epoch 082 | Train Loss: 0.3699 Acc: 0.8710 | Val Loss: 0.3265 Acc: 0.8725                                               \n",
      "Epoch 083 | Train Loss: 0.3536 Acc: 0.8743 | Val Loss: 0.3204 Acc: 0.8719                                               \n",
      "Epoch 084 | Train Loss: 0.3483 Acc: 0.8791 | Val Loss: 0.3187 Acc: 0.8791                                               \n",
      "Epoch 085 | Train Loss: 0.3370 Acc: 0.8786 | Val Loss: 0.3042 Acc: 0.8812                                               \n",
      "Epoch 086 | Train Loss: 0.3211 Acc: 0.8822 | Val Loss: 0.2789 Acc: 0.8949                                               \n",
      "Epoch 087 | Train Loss: 0.3201 Acc: 0.8848 | Val Loss: 0.3015 Acc: 0.8803                                               \n",
      "Epoch 088 | Train Loss: 0.3026 Acc: 0.8934 | Val Loss: 0.3424 Acc: 0.8699                                               \n",
      "Epoch 089 | Train Loss: 0.3146 Acc: 0.8911 | Val Loss: 0.3104 Acc: 0.8737                                               \n",
      "Epoch 090 | Train Loss: 0.2995 Acc: 0.8918 | Val Loss: 0.2872 Acc: 0.8872                                               \n",
      "Epoch 091 | Train Loss: 0.2954 Acc: 0.8949 | Val Loss: 0.2538 Acc: 0.9045                                               \n",
      "Epoch 092 | Train Loss: 0.2611 Acc: 0.9075 | Val Loss: 0.2662 Acc: 0.8967                                               \n",
      "Epoch 093 | Train Loss: 0.2732 Acc: 0.9054 | Val Loss: 0.2712 Acc: 0.9000                                               \n",
      "Epoch 094 | Train Loss: 0.2592 Acc: 0.9098 | Val Loss: 0.2408 Acc: 0.9104                                               \n",
      "Epoch 095 | Train Loss: 0.2589 Acc: 0.9095 | Val Loss: 0.2782 Acc: 0.8967                                               \n",
      "Epoch 096 | Train Loss: 0.2563 Acc: 0.9066 | Val Loss: 0.2278 Acc: 0.9143                                               \n",
      "Epoch 097 | Train Loss: 0.2466 Acc: 0.9118 | Val Loss: 0.2178 Acc: 0.9176                                               \n",
      "Epoch 098 | Train Loss: 0.2356 Acc: 0.9186 | Val Loss: 0.2124 Acc: 0.9227                                               \n",
      "Epoch 099 | Train Loss: 0.2282 Acc: 0.9204 | Val Loss: 0.2329 Acc: 0.9158                                               \n",
      "Epoch 100 | Train Loss: 0.2268 Acc: 0.9198 | Val Loss: 0.2027 Acc: 0.9257                                               \n",
      "Epoch 101 | Train Loss: 0.2295 Acc: 0.9193 | Val Loss: 0.1906 Acc: 0.9284                                               \n",
      "Epoch 102 | Train Loss: 0.2249 Acc: 0.9210 | Val Loss: 0.1920 Acc: 0.9301                                               \n",
      "Epoch 103 | Train Loss: 0.2168 Acc: 0.9275 | Val Loss: 0.1906 Acc: 0.9301                                               \n",
      "Epoch 104 | Train Loss: 0.2048 Acc: 0.9287 | Val Loss: 0.2185 Acc: 0.9209                                               \n",
      "Epoch 105 | Train Loss: 0.2083 Acc: 0.9273 | Val Loss: 0.2589 Acc: 0.9060                                               \n",
      "Epoch 106 | Train Loss: 0.2002 Acc: 0.9308 | Val Loss: 0.1997 Acc: 0.9275                                               \n",
      "Epoch 107 | Train Loss: 0.1914 Acc: 0.9338 | Val Loss: 0.1830 Acc: 0.9352                                               \n",
      "Epoch 108 | Train Loss: 0.1951 Acc: 0.9309 | Val Loss: 0.1917 Acc: 0.9301                                               \n",
      "Epoch 109 | Train Loss: 0.1886 Acc: 0.9322 | Val Loss: 0.1796 Acc: 0.9391                                               \n",
      "Epoch 110 | Train Loss: 0.1827 Acc: 0.9356 | Val Loss: 0.1567 Acc: 0.9430                                               \n",
      "Epoch 111 | Train Loss: 0.1730 Acc: 0.9421 | Val Loss: 0.1838 Acc: 0.9304                                               \n",
      "Epoch 112 | Train Loss: 0.1819 Acc: 0.9379 | Val Loss: 0.1746 Acc: 0.9322                                               \n",
      "Epoch 113 | Train Loss: 0.1705 Acc: 0.9420 | Val Loss: 0.1856 Acc: 0.9340                                               \n",
      "Epoch 114 | Train Loss: 0.1602 Acc: 0.9448 | Val Loss: 0.1627 Acc: 0.9427                                               \n",
      "Epoch 115 | Train Loss: 0.1621 Acc: 0.9436 | Val Loss: 0.1810 Acc: 0.9358                                               \n",
      "Epoch 116 | Train Loss: 0.1605 Acc: 0.9456 | Val Loss: 0.1688 Acc: 0.9421                                               \n",
      "Epoch 117 | Train Loss: 0.1620 Acc: 0.9434 | Val Loss: 0.1523 Acc: 0.9481                                               \n",
      "Epoch 118 | Train Loss: 0.1510 Acc: 0.9475 | Val Loss: 0.1721 Acc: 0.9421                                               \n",
      "Epoch 119 | Train Loss: 0.1467 Acc: 0.9505 | Val Loss: 0.1564 Acc: 0.9469                                               \n",
      "Epoch 120 | Train Loss: 0.1473 Acc: 0.9492 | Val Loss: 0.1768 Acc: 0.9379                                               \n",
      "Epoch 121 | Train Loss: 0.1446 Acc: 0.9500 | Val Loss: 0.1798 Acc: 0.9370                                               \n",
      "Epoch 122 | Train Loss: 0.1409 Acc: 0.9515 | Val Loss: 0.1622 Acc: 0.9475                                               \n",
      "Epoch 123 | Train Loss: 0.1370 Acc: 0.9534 | Val Loss: 0.1558 Acc: 0.9478                                               \n",
      "Epoch 124 | Train Loss: 0.1366 Acc: 0.9554 | Val Loss: 0.1797 Acc: 0.9376                                               \n",
      "Epoch 125 | Train Loss: 0.1357 Acc: 0.9539 | Val Loss: 0.1523 Acc: 0.9487                                               \n",
      "Epoch 126 | Train Loss: 0.1343 Acc: 0.9548 | Val Loss: 0.1318 Acc: 0.9543                                               \n",
      "Epoch 127 | Train Loss: 0.1226 Acc: 0.9578 | Val Loss: 0.1341 Acc: 0.9546                                               \n",
      "Epoch 128 | Train Loss: 0.1331 Acc: 0.9539 | Val Loss: 0.1285 Acc: 0.9600                                               \n",
      "Epoch 129 | Train Loss: 0.1260 Acc: 0.9572 | Val Loss: 0.1378 Acc: 0.9549                                               \n",
      "Epoch 130 | Train Loss: 0.1213 Acc: 0.9574 | Val Loss: 0.1500 Acc: 0.9478                                               \n",
      "Epoch 131 | Train Loss: 0.1216 Acc: 0.9579 | Val Loss: 0.1372 Acc: 0.9564                                               \n",
      "Epoch 132 | Train Loss: 0.1221 Acc: 0.9593 | Val Loss: 0.1283 Acc: 0.9582                                               \n",
      "Epoch 133 | Train Loss: 0.1152 Acc: 0.9601 | Val Loss: 0.1259 Acc: 0.9576                                               \n",
      "Epoch 134 | Train Loss: 0.1154 Acc: 0.9621 | Val Loss: 0.1337 Acc: 0.9528                                               \n",
      "Epoch 135 | Train Loss: 0.1120 Acc: 0.9622 | Val Loss: 0.1347 Acc: 0.9552                                               \n",
      "Epoch 136 | Train Loss: 0.1165 Acc: 0.9607 | Val Loss: 0.1438 Acc: 0.9558                                               \n",
      "Epoch 137 | Train Loss: 0.1083 Acc: 0.9622 | Val Loss: 0.1306 Acc: 0.9591                                               \n",
      "Epoch 138 | Train Loss: 0.1085 Acc: 0.9632 | Val Loss: 0.1435 Acc: 0.9564                                               \n",
      "Epoch 139 | Train Loss: 0.1067 Acc: 0.9634 | Val Loss: 0.1365 Acc: 0.9549                                               \n",
      "Epoch 140 | Train Loss: 0.1019 Acc: 0.9644 | Val Loss: 0.1459 Acc: 0.9493                                               \n",
      "Epoch 141 | Train Loss: 0.1035 Acc: 0.9649 | Val Loss: 0.1301 Acc: 0.9564                                               \n",
      "Epoch 142 | Train Loss: 0.1005 Acc: 0.9662 | Val Loss: 0.1333 Acc: 0.9573                                               \n",
      "Epoch 143 | Train Loss: 0.0952 Acc: 0.9693 | Val Loss: 0.1300 Acc: 0.9576                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 256, 'cnn_dropout': 0.3176558445051534, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 32, 'cnn_kernels_2': 64, 'learning_rate': 4.31114211262436e-05, 'lstm_dense': 32, 'lstm_hidden_size': 128, 'lstm_layers': 3, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 59.2268 Acc: 0.3052 | Val Loss: 10.0308 Acc: 0.4042                                             \n",
      "Epoch 002 | Train Loss: 23.4257 Acc: 0.3598 | Val Loss: 6.2767 Acc: 0.4448                                              \n",
      "Epoch 003 | Train Loss: 15.7140 Acc: 0.3820 | Val Loss: 3.9785 Acc: 0.5364                                              \n",
      "Epoch 004 | Train Loss: 12.0146 Acc: 0.4037 | Val Loss: 3.2042 Acc: 0.5669                                              \n",
      "Epoch 005 | Train Loss: 10.0240 Acc: 0.4207 | Val Loss: 5.7635 Acc: 0.5439                                              \n",
      "Epoch 006 | Train Loss: 8.2306 Acc: 0.4326 | Val Loss: 3.4376 Acc: 0.5851                                               \n",
      "Epoch 007 | Train Loss: 7.3108 Acc: 0.4342 | Val Loss: 5.2321 Acc: 0.5833                                               \n",
      "Epoch 008 | Train Loss: 6.3086 Acc: 0.4506 | Val Loss: 2.4884 Acc: 0.4854                                               \n",
      "Epoch 009 | Train Loss: 5.6583 Acc: 0.4507 | Val Loss: 2.2248 Acc: 0.6218                                               \n",
      "Epoch 010 | Train Loss: 5.0299 Acc: 0.4613 | Val Loss: 2.3048 Acc: 0.6090                                               \n",
      "Epoch 011 | Train Loss: 4.4277 Acc: 0.4654 | Val Loss: 1.3990 Acc: 0.5773                                               \n",
      "Epoch 012 | Train Loss: 3.9099 Acc: 0.4774 | Val Loss: 2.3129 Acc: 0.4699                                               \n",
      "Epoch 013 | Train Loss: 3.7320 Acc: 0.4715 | Val Loss: 1.7438 Acc: 0.6469                                               \n",
      "Epoch 014 | Train Loss: 3.2836 Acc: 0.4934 | Val Loss: 1.5932 Acc: 0.6236                                               \n",
      "Epoch 015 | Train Loss: 3.0652 Acc: 0.4944 | Val Loss: 1.0387 Acc: 0.6072                                               \n",
      "Epoch 016 | Train Loss: 2.8578 Acc: 0.4968 | Val Loss: 2.4959 Acc: 0.6024                                               \n",
      "Epoch 017 | Train Loss: 2.5914 Acc: 0.5044 | Val Loss: 1.3615 Acc: 0.5528                                               \n",
      "Epoch 018 | Train Loss: 2.5373 Acc: 0.5022 | Val Loss: 0.9369 Acc: 0.5985                                               \n",
      "Epoch 019 | Train Loss: 2.3230 Acc: 0.5077 | Val Loss: 1.7209 Acc: 0.4794                                               \n",
      "Epoch 020 | Train Loss: 2.2372 Acc: 0.5113 | Val Loss: 0.9766 Acc: 0.5755                                               \n",
      "Epoch 021 | Train Loss: 2.1152 Acc: 0.5110 | Val Loss: 1.7198 Acc: 0.5170                                               \n",
      "Epoch 022 | Train Loss: 2.0193 Acc: 0.5077 | Val Loss: 0.8597 Acc: 0.6875                                               \n",
      "Epoch 023 | Train Loss: 1.9646 Acc: 0.5123 | Val Loss: 2.6825 Acc: 0.5454                                               \n",
      "Epoch 024 | Train Loss: 1.8394 Acc: 0.5182 | Val Loss: 1.7463 Acc: 0.6000                                               \n",
      "Epoch 025 | Train Loss: 1.7996 Acc: 0.5236 | Val Loss: 2.0199 Acc: 0.4334                                               \n",
      "Epoch 026 | Train Loss: 1.7242 Acc: 0.5190 | Val Loss: 1.0176 Acc: 0.5167                                               \n",
      "Epoch 027 | Train Loss: 1.6456 Acc: 0.5295 | Val Loss: 0.9344 Acc: 0.5567                                               \n",
      "Epoch 028 | Train Loss: 1.6426 Acc: 0.5238 | Val Loss: 0.7613 Acc: 0.6740                                               \n",
      "Epoch 029 | Train Loss: 1.5460 Acc: 0.5311 | Val Loss: 1.3832 Acc: 0.6060                                               \n",
      "Epoch 030 | Train Loss: 1.5265 Acc: 0.5278 | Val Loss: 2.0879 Acc: 0.4421                                               \n",
      "Epoch 031 | Train Loss: 1.4907 Acc: 0.5263 | Val Loss: 0.9000 Acc: 0.5782                                               \n",
      "Epoch 032 | Train Loss: 1.4567 Acc: 0.5354 | Val Loss: 0.9622 Acc: 0.5409                                               \n",
      "Epoch 033 | Train Loss: 1.3698 Acc: 0.5421 | Val Loss: 1.2732 Acc: 0.4740                                               \n",
      "Epoch 034 | Train Loss: 1.3752 Acc: 0.5441 | Val Loss: 0.9948 Acc: 0.6104                                               \n",
      "Epoch 035 | Train Loss: 1.3151 Acc: 0.5500 | Val Loss: 1.3761 Acc: 0.4767                                               \n",
      "Epoch 036 | Train Loss: 1.2770 Acc: 0.5536 | Val Loss: 0.8234 Acc: 0.6349                                               \n",
      "Epoch 037 | Train Loss: 1.2543 Acc: 0.5579 | Val Loss: 0.8531 Acc: 0.5681                                               \n",
      "Epoch 038 | Train Loss: 1.2493 Acc: 0.5623 | Val Loss: 1.0390 Acc: 0.5642                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 256, 'cnn_dropout': 0.17666458241317917, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 32, 'learning_rate': 0.0002618894066696093, 'lstm_dense': 64, 'lstm_hidden_size': 64, 'lstm_layers': 3, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 14.8173 Acc: 0.3871 | Val Loss: 7.7599 Acc: 0.4269                                              \n",
      "Epoch 002 | Train Loss: 5.4749 Acc: 0.4357 | Val Loss: 3.0984 Acc: 0.5310                                               \n",
      "Epoch 003 | Train Loss: 3.7893 Acc: 0.4596 | Val Loss: 1.9122 Acc: 0.5872                                               \n",
      "Epoch 004 | Train Loss: 2.8671 Acc: 0.4870 | Val Loss: 2.2302 Acc: 0.4869                                               \n",
      "Epoch 005 | Train Loss: 2.2657 Acc: 0.5147 | Val Loss: 2.8731 Acc: 0.5642                                               \n",
      "Epoch 006 | Train Loss: 1.9004 Acc: 0.5335 | Val Loss: 0.7316 Acc: 0.6955                                               \n",
      "Epoch 007 | Train Loss: 1.5745 Acc: 0.5733 | Val Loss: 1.5662 Acc: 0.5415                                               \n",
      "Epoch 008 | Train Loss: 1.4339 Acc: 0.5991 | Val Loss: 0.5341 Acc: 0.7728                                               \n",
      "Epoch 009 | Train Loss: 1.2160 Acc: 0.6409 | Val Loss: 0.5352 Acc: 0.7985                                               \n",
      "Epoch 010 | Train Loss: 1.0926 Acc: 0.6648 | Val Loss: 1.0899 Acc: 0.7230                                               \n",
      "Epoch 011 | Train Loss: 1.0043 Acc: 0.6821 | Val Loss: 0.5429 Acc: 0.7800                                               \n",
      "Epoch 012 | Train Loss: 0.9388 Acc: 0.6990 | Val Loss: 0.5024 Acc: 0.8009                                               \n",
      "Epoch 013 | Train Loss: 0.8092 Acc: 0.7253 | Val Loss: 1.2957 Acc: 0.6925                                               \n",
      "Epoch 014 | Train Loss: 0.7747 Acc: 0.7390 | Val Loss: 0.9841 Acc: 0.7176                                               \n",
      "Epoch 015 | Train Loss: 0.7115 Acc: 0.7620 | Val Loss: 0.7595 Acc: 0.7266                                               \n",
      "Epoch 016 | Train Loss: 0.6195 Acc: 0.7830 | Val Loss: 0.6121 Acc: 0.7887                                               \n",
      "Epoch 017 | Train Loss: 0.5908 Acc: 0.7968 | Val Loss: 0.6044 Acc: 0.7573                                               \n",
      "Epoch 018 | Train Loss: 0.5123 Acc: 0.8208 | Val Loss: 0.5908 Acc: 0.8030                                               \n",
      "Epoch 019 | Train Loss: 0.4510 Acc: 0.8419 | Val Loss: 0.5070 Acc: 0.8122                                               \n",
      "Epoch 020 | Train Loss: 0.4487 Acc: 0.8538 | Val Loss: 1.0870 Acc: 0.7236                                               \n",
      "Epoch 021 | Train Loss: 0.3503 Acc: 0.8808 | Val Loss: 0.2346 Acc: 0.9176                                               \n",
      "Epoch 022 | Train Loss: 0.3202 Acc: 0.8964 | Val Loss: 0.3446 Acc: 0.8693                                               \n",
      "Epoch 023 | Train Loss: 0.2914 Acc: 0.9076 | Val Loss: 0.9728 Acc: 0.7624                                               \n",
      "Epoch 024 | Train Loss: 0.2640 Acc: 0.9173 | Val Loss: 0.2283 Acc: 0.9299                                               \n",
      "Epoch 025 | Train Loss: 0.2201 Acc: 0.9307 | Val Loss: 0.2298 Acc: 0.9194                                               \n",
      "Epoch 026 | Train Loss: 0.2073 Acc: 0.9334 | Val Loss: 0.1873 Acc: 0.9316                                               \n",
      "Epoch 027 | Train Loss: 0.1800 Acc: 0.9426 | Val Loss: 0.3275 Acc: 0.8907                                               \n",
      "Epoch 028 | Train Loss: 0.1724 Acc: 0.9422 | Val Loss: 0.1514 Acc: 0.9463                                               \n",
      "Epoch 029 | Train Loss: 0.1692 Acc: 0.9457 | Val Loss: 0.1701 Acc: 0.9415                                               \n",
      "Epoch 030 | Train Loss: 0.1184 Acc: 0.9607 | Val Loss: 0.1384 Acc: 0.9585                                               \n",
      "Epoch 031 | Train Loss: 0.1294 Acc: 0.9569 | Val Loss: 0.1447 Acc: 0.9501                                               \n",
      "Epoch 032 | Train Loss: 0.1057 Acc: 0.9643 | Val Loss: 0.1656 Acc: 0.9478                                               \n",
      "Epoch 033 | Train Loss: 0.1007 Acc: 0.9683 | Val Loss: 0.1756 Acc: 0.9469                                               \n",
      "Epoch 034 | Train Loss: 0.0875 Acc: 0.9727 | Val Loss: 0.1106 Acc: 0.9669                                               \n",
      "Epoch 035 | Train Loss: 0.0975 Acc: 0.9701 | Val Loss: 0.1258 Acc: 0.9636                                               \n",
      "Epoch 036 | Train Loss: 0.0767 Acc: 0.9754 | Val Loss: 0.1409 Acc: 0.9549                                               \n",
      "Epoch 037 | Train Loss: 0.0707 Acc: 0.9775 | Val Loss: 0.1164 Acc: 0.9660                                               \n",
      "Epoch 038 | Train Loss: 0.0832 Acc: 0.9746 | Val Loss: 0.1021 Acc: 0.9687                                               \n",
      "Epoch 039 | Train Loss: 0.0606 Acc: 0.9796 | Val Loss: 0.0970 Acc: 0.9743                                               \n",
      "Epoch 040 | Train Loss: 0.0635 Acc: 0.9796 | Val Loss: 0.1505 Acc: 0.9579                                               \n",
      "Epoch 041 | Train Loss: 0.0590 Acc: 0.9821 | Val Loss: 0.1130 Acc: 0.9657                                               \n",
      "Epoch 042 | Train Loss: 0.0424 Acc: 0.9865 | Val Loss: 0.0931 Acc: 0.9719                                               \n",
      "Epoch 043 | Train Loss: 0.0584 Acc: 0.9819 | Val Loss: 0.0959 Acc: 0.9696                                               \n",
      "Epoch 044 | Train Loss: 0.0560 Acc: 0.9824 | Val Loss: 0.0860 Acc: 0.9761                                               \n",
      "Epoch 045 | Train Loss: 0.0433 Acc: 0.9869 | Val Loss: 0.1788 Acc: 0.9546                                               \n",
      "Epoch 046 | Train Loss: 0.0503 Acc: 0.9847 | Val Loss: 0.0896 Acc: 0.9755                                               \n",
      "Epoch 047 | Train Loss: 0.0392 Acc: 0.9881 | Val Loss: 0.1591 Acc: 0.9531                                               \n",
      "Epoch 048 | Train Loss: 0.0417 Acc: 0.9869 | Val Loss: 0.0858 Acc: 0.9758                                               \n",
      "Epoch 049 | Train Loss: 0.0357 Acc: 0.9881 | Val Loss: 0.0959 Acc: 0.9722                                               \n",
      "Epoch 050 | Train Loss: 0.0378 Acc: 0.9885 | Val Loss: 0.0585 Acc: 0.9812                                               \n",
      "Epoch 051 | Train Loss: 0.0319 Acc: 0.9889 | Val Loss: 0.0703 Acc: 0.9785                                               \n",
      "Epoch 052 | Train Loss: 0.0380 Acc: 0.9881 | Val Loss: 0.0720 Acc: 0.9806                                               \n",
      "Epoch 053 | Train Loss: 0.0321 Acc: 0.9893 | Val Loss: 0.0765 Acc: 0.9788                                               \n",
      "Epoch 054 | Train Loss: 0.0346 Acc: 0.9893 | Val Loss: 0.1351 Acc: 0.9675                                               \n",
      "Epoch 055 | Train Loss: 0.0332 Acc: 0.9886 | Val Loss: 0.0748 Acc: 0.9821                                               \n",
      "Epoch 056 | Train Loss: 0.0307 Acc: 0.9898 | Val Loss: 0.0830 Acc: 0.9770                                               \n",
      "Epoch 057 | Train Loss: 0.0311 Acc: 0.9906 | Val Loss: 0.0762 Acc: 0.9764                                               \n",
      "Epoch 058 | Train Loss: 0.0230 Acc: 0.9924 | Val Loss: 0.0743 Acc: 0.9782                                               \n",
      "Epoch 059 | Train Loss: 0.0378 Acc: 0.9893 | Val Loss: 0.1143 Acc: 0.9701                                               \n",
      "Epoch 060 | Train Loss: 0.0232 Acc: 0.9928 | Val Loss: 0.0882 Acc: 0.9785                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 32, 'cnn_dropout': 0.5695266730968924, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 16, 'cnn_kernels_2': 32, 'learning_rate': 0.0066241335759532035, 'lstm_dense': 64, 'lstm_hidden_size': 128, 'lstm_layers': 2, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 7.5361 Acc: 0.4533 | Val Loss: 0.8986 Acc: 0.5493                                               \n",
      "Epoch 002 | Train Loss: 0.8944 Acc: 0.5638 | Val Loss: 0.7705 Acc: 0.6224                                               \n",
      "Epoch 003 | Train Loss: 0.7928 Acc: 0.6156 | Val Loss: 0.6636 Acc: 0.7009                                               \n",
      "Epoch 004 | Train Loss: 0.7005 Acc: 0.6706 | Val Loss: 0.6277 Acc: 0.7558                                               \n",
      "Epoch 005 | Train Loss: 0.6782 Acc: 0.6790 | Val Loss: 0.6445 Acc: 0.6916                                               \n",
      "Epoch 006 | Train Loss: 0.6746 Acc: 0.6778 | Val Loss: 0.6151 Acc: 0.7427                                               \n",
      "Epoch 007 | Train Loss: 0.6787 Acc: 0.6879 | Val Loss: 0.6785 Acc: 0.6913                                               \n",
      "Epoch 008 | Train Loss: 0.6801 Acc: 0.6924 | Val Loss: 0.6361 Acc: 0.7349                                               \n",
      "Epoch 009 | Train Loss: 0.6883 Acc: 0.6867 | Val Loss: 0.6644 Acc: 0.6678                                               \n",
      "Epoch 010 | Train Loss: 0.7154 Acc: 0.6694 | Val Loss: 0.7029 Acc: 0.6693                                               \n",
      "Epoch 011 | Train Loss: 0.6955 Acc: 0.6847 | Val Loss: 0.6975 Acc: 0.6961                                               \n",
      "Epoch 012 | Train Loss: 0.6922 Acc: 0.6843 | Val Loss: 0.6297 Acc: 0.7093                                               \n",
      "Epoch 013 | Train Loss: 0.6987 Acc: 0.6888 | Val Loss: 0.8420 Acc: 0.5973                                               \n",
      "Epoch 014 | Train Loss: 0.7048 Acc: 0.6845 | Val Loss: 0.7162 Acc: 0.6624                                               \n",
      "Epoch 015 | Train Loss: 0.7391 Acc: 0.6742 | Val Loss: 0.8438 Acc: 0.5770                                               \n",
      "Epoch 016 | Train Loss: 0.7101 Acc: 0.6817 | Val Loss: 0.7295 Acc: 0.6197                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 36, 'cnn_dense': 128, 'cnn_dropout': 0.5664864379569511, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 48, 'cnn_kernels_2': 64, 'learning_rate': 0.009045994286165007, 'lstm_dense': 256, 'lstm_hidden_size': 64, 'lstm_layers': 5, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 752.4056 Acc: 0.4382 | Val Loss: 1.2418 Acc: 0.4421                                             \n",
      "Epoch 002 | Train Loss: 1.2423 Acc: 0.4422 | Val Loss: 1.2432 Acc: 0.4421                                               \n",
      "Epoch 003 | Train Loss: 1.2424 Acc: 0.4422 | Val Loss: 1.2424 Acc: 0.4421                                               \n",
      "Epoch 004 | Train Loss: 1.2424 Acc: 0.4422 | Val Loss: 1.2420 Acc: 0.4421                                               \n",
      "Epoch 005 | Train Loss: 1.2426 Acc: 0.4422 | Val Loss: 1.2422 Acc: 0.4421                                               \n",
      "Epoch 006 | Train Loss: 1.2425 Acc: 0.4422 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 007 | Train Loss: 1.2426 Acc: 0.4422 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 008 | Train Loss: 1.2424 Acc: 0.4422 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 009 | Train Loss: 1.2426 Acc: 0.4422 | Val Loss: 1.2421 Acc: 0.4421                                               \n",
      "Epoch 010 | Train Loss: 1.2425 Acc: 0.4422 | Val Loss: 1.2420 Acc: 0.4421                                               \n",
      "Epoch 011 | Train Loss: 1.2425 Acc: 0.4422 | Val Loss: 1.2420 Acc: 0.4421                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 32, 'cnn_dropout': 0.17733394990897966, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 32, 'cnn_kernels_2': 32, 'learning_rate': 0.000855751127131624, 'lstm_dense': 256, 'lstm_hidden_size': 96, 'lstm_layers': 2, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 7.3092 Acc: 0.4337 | Val Loss: 1.2573 Acc: 0.4421                                               \n",
      "Epoch 002 | Train Loss: 1.2429 Acc: 0.4410 | Val Loss: 1.2298 Acc: 0.4409                                               \n",
      "Epoch 003 | Train Loss: 1.2176 Acc: 0.4523 | Val Loss: 1.2069 Acc: 0.4722                                               \n",
      "Epoch 004 | Train Loss: 1.1833 Acc: 0.4875 | Val Loss: 1.1866 Acc: 0.4722                                               \n",
      "Epoch 005 | Train Loss: 1.1276 Acc: 0.5288 | Val Loss: 1.1046 Acc: 0.5433                                               \n",
      "Epoch 006 | Train Loss: 1.0898 Acc: 0.5337 | Val Loss: 1.0559 Acc: 0.5463                                               \n",
      "Epoch 007 | Train Loss: 1.0426 Acc: 0.5503 | Val Loss: 1.0203 Acc: 0.5555                                               \n",
      "Epoch 008 | Train Loss: 1.0096 Acc: 0.5633 | Val Loss: 0.9790 Acc: 0.5725                                               \n",
      "Epoch 009 | Train Loss: 0.9698 Acc: 0.5779 | Val Loss: 0.9364 Acc: 0.5928                                               \n",
      "Epoch 010 | Train Loss: 0.9331 Acc: 0.5927 | Val Loss: 0.9971 Acc: 0.5645                                               \n",
      "Epoch 011 | Train Loss: 0.9109 Acc: 0.5968 | Val Loss: 0.9984 Acc: 0.5600                                               \n",
      "Epoch 012 | Train Loss: 0.8581 Acc: 0.6236 | Val Loss: 0.9327 Acc: 0.6063                                               \n",
      "Epoch 013 | Train Loss: 0.8203 Acc: 0.6474 | Val Loss: 0.7878 Acc: 0.6854                                               \n",
      "Epoch 014 | Train Loss: 0.7668 Acc: 0.6910 | Val Loss: 0.7385 Acc: 0.7194                                               \n",
      "Epoch 015 | Train Loss: 0.7025 Acc: 0.7257 | Val Loss: 0.6887 Acc: 0.7263                                               \n",
      "Epoch 016 | Train Loss: 0.6329 Acc: 0.7592 | Val Loss: 0.6322 Acc: 0.7576                                               \n",
      "Epoch 017 | Train Loss: 0.5852 Acc: 0.7792 | Val Loss: 0.5501 Acc: 0.7949                                               \n",
      "Epoch 018 | Train Loss: 0.5439 Acc: 0.7933 | Val Loss: 0.5430 Acc: 0.7976                                               \n",
      "Epoch 019 | Train Loss: 0.5149 Acc: 0.8086 | Val Loss: 0.4765 Acc: 0.8149                                               \n",
      "Epoch 020 | Train Loss: 0.4638 Acc: 0.8263 | Val Loss: 0.4885 Acc: 0.8173                                               \n",
      "Epoch 021 | Train Loss: 0.4332 Acc: 0.8365 | Val Loss: 0.4505 Acc: 0.8313                                               \n",
      "Epoch 022 | Train Loss: 0.4006 Acc: 0.8523 | Val Loss: 0.4617 Acc: 0.8406                                               \n",
      "Epoch 023 | Train Loss: 0.3614 Acc: 0.8717 | Val Loss: 0.4105 Acc: 0.8591                                               \n",
      "Epoch 024 | Train Loss: 0.3415 Acc: 0.8806 | Val Loss: 0.4044 Acc: 0.8672                                               \n",
      "Epoch 025 | Train Loss: 0.2977 Acc: 0.8988 | Val Loss: 0.3908 Acc: 0.8681                                               \n",
      "Epoch 026 | Train Loss: 0.2866 Acc: 0.9031 | Val Loss: 0.4107 Acc: 0.8543                                               \n",
      "Epoch 027 | Train Loss: 0.2786 Acc: 0.9053 | Val Loss: 0.2983 Acc: 0.9006                                               \n",
      "Epoch 028 | Train Loss: 0.2500 Acc: 0.9159 | Val Loss: 0.2692 Acc: 0.9101                                               \n",
      "Epoch 029 | Train Loss: 0.2434 Acc: 0.9177 | Val Loss: 0.2973 Acc: 0.9090                                               \n",
      "Epoch 030 | Train Loss: 0.2302 Acc: 0.9254 | Val Loss: 0.2808 Acc: 0.9036                                               \n",
      "Epoch 031 | Train Loss: 0.2171 Acc: 0.9285 | Val Loss: 0.2798 Acc: 0.9075                                               \n",
      "Epoch 032 | Train Loss: 0.2343 Acc: 0.9219 | Val Loss: 0.3243 Acc: 0.8893                                               \n",
      "Epoch 033 | Train Loss: 0.2280 Acc: 0.9240 | Val Loss: 0.2306 Acc: 0.9239                                               \n",
      "Epoch 034 | Train Loss: 0.1986 Acc: 0.9325 | Val Loss: 0.2445 Acc: 0.9179                                               \n",
      "Epoch 035 | Train Loss: 0.1652 Acc: 0.9457 | Val Loss: 0.2699 Acc: 0.9119                                               \n",
      "Epoch 036 | Train Loss: 0.1614 Acc: 0.9482 | Val Loss: 0.2349 Acc: 0.9260                                               \n",
      "Epoch 037 | Train Loss: 0.1497 Acc: 0.9504 | Val Loss: 0.2002 Acc: 0.9343                                               \n",
      "Epoch 038 | Train Loss: 0.1497 Acc: 0.9513 | Val Loss: 0.1859 Acc: 0.9364                                               \n",
      "Epoch 039 | Train Loss: 0.1305 Acc: 0.9554 | Val Loss: 0.2279 Acc: 0.9266                                               \n",
      "Epoch 040 | Train Loss: 0.1436 Acc: 0.9521 | Val Loss: 0.2016 Acc: 0.9346                                               \n",
      "Epoch 041 | Train Loss: 0.1340 Acc: 0.9533 | Val Loss: 0.2127 Acc: 0.9337                                               \n",
      "Epoch 042 | Train Loss: 0.1362 Acc: 0.9528 | Val Loss: 0.2423 Acc: 0.9245                                               \n",
      "Epoch 043 | Train Loss: 0.1218 Acc: 0.9585 | Val Loss: 0.2123 Acc: 0.9334                                               \n",
      "Epoch 044 | Train Loss: 0.1133 Acc: 0.9622 | Val Loss: 0.1959 Acc: 0.9373                                               \n",
      "Epoch 045 | Train Loss: 0.1073 Acc: 0.9622 | Val Loss: 0.1949 Acc: 0.9361                                               \n",
      "Epoch 046 | Train Loss: 0.1133 Acc: 0.9614 | Val Loss: 0.2322 Acc: 0.9221                                               \n",
      "Epoch 047 | Train Loss: 0.0989 Acc: 0.9677 | Val Loss: 0.1801 Acc: 0.9430                                               \n",
      "Epoch 048 | Train Loss: 0.1107 Acc: 0.9631 | Val Loss: 0.1735 Acc: 0.9445                                               \n",
      "Epoch 049 | Train Loss: 0.0914 Acc: 0.9704 | Val Loss: 0.1887 Acc: 0.9418                                               \n",
      "Epoch 050 | Train Loss: 0.0803 Acc: 0.9730 | Val Loss: 0.1822 Acc: 0.9436                                               \n",
      "Epoch 051 | Train Loss: 0.0972 Acc: 0.9675 | Val Loss: 0.1583 Acc: 0.9493                                               \n",
      "Epoch 052 | Train Loss: 0.0756 Acc: 0.9745 | Val Loss: 0.1698 Acc: 0.9490                                               \n",
      "Epoch 053 | Train Loss: 0.0864 Acc: 0.9691 | Val Loss: 0.1940 Acc: 0.9448                                               \n",
      "Epoch 054 | Train Loss: 0.0886 Acc: 0.9710 | Val Loss: 0.1601 Acc: 0.9522                                               \n",
      "Epoch 055 | Train Loss: 0.0926 Acc: 0.9674 | Val Loss: 0.1563 Acc: 0.9504                                               \n",
      "Epoch 056 | Train Loss: 0.0842 Acc: 0.9714 | Val Loss: 0.1598 Acc: 0.9534                                               \n",
      "Epoch 057 | Train Loss: 0.0825 Acc: 0.9703 | Val Loss: 0.2040 Acc: 0.9319                                               \n",
      "Epoch 058 | Train Loss: 0.0756 Acc: 0.9728 | Val Loss: 0.1391 Acc: 0.9591                                               \n",
      "Epoch 059 | Train Loss: 0.0666 Acc: 0.9786 | Val Loss: 0.1538 Acc: 0.9561                                               \n",
      "Epoch 060 | Train Loss: 0.0683 Acc: 0.9770 | Val Loss: 0.1472 Acc: 0.9534                                               \n",
      "Epoch 061 | Train Loss: 0.0697 Acc: 0.9760 | Val Loss: 0.1862 Acc: 0.9424                                               \n",
      "Epoch 062 | Train Loss: 0.0663 Acc: 0.9778 | Val Loss: 0.1463 Acc: 0.9561                                               \n",
      "Epoch 063 | Train Loss: 0.0592 Acc: 0.9800 | Val Loss: 0.1418 Acc: 0.9549                                               \n",
      "Epoch 064 | Train Loss: 0.0509 Acc: 0.9824 | Val Loss: 0.1567 Acc: 0.9549                                               \n",
      "Epoch 065 | Train Loss: 0.0546 Acc: 0.9816 | Val Loss: 0.2370 Acc: 0.9293                                               \n",
      "Epoch 066 | Train Loss: 0.0832 Acc: 0.9718 | Val Loss: 0.1975 Acc: 0.9409                                               \n",
      "Epoch 067 | Train Loss: 0.0611 Acc: 0.9793 | Val Loss: 0.1954 Acc: 0.9421                                               \n",
      "Epoch 068 | Train Loss: 0.0699 Acc: 0.9757 | Val Loss: 0.1496 Acc: 0.9561                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 128, 'cnn_dropout': 0.20505170898012484, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 32, 'cnn_kernels_2': 96, 'learning_rate': 2.729767349974254e-05, 'lstm_dense': 256, 'lstm_hidden_size': 64, 'lstm_layers': 1, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 64.1228 Acc: 0.2938 | Val Loss: 12.8877 Acc: 0.3782                                             \n",
      "Epoch 002 | Train Loss: 29.8727 Acc: 0.3221 | Val Loss: 8.2715 Acc: 0.3904                                              \n",
      "Epoch 003 | Train Loss: 19.1354 Acc: 0.3367 | Val Loss: 6.2098 Acc: 0.3934                                              \n",
      "Epoch 004 | Train Loss: 13.7123 Acc: 0.3452 | Val Loss: 3.9752 Acc: 0.3872                                              \n",
      "Epoch 005 | Train Loss: 10.7366 Acc: 0.3572 | Val Loss: 2.4732 Acc: 0.3931                                              \n",
      "Epoch 006 | Train Loss: 8.6494 Acc: 0.3738 | Val Loss: 2.6567 Acc: 0.3424                                               \n",
      "Epoch 007 | Train Loss: 7.1694 Acc: 0.3796 | Val Loss: 2.1477 Acc: 0.3672                                               \n",
      "Epoch 008 | Train Loss: 6.2746 Acc: 0.3806 | Val Loss: 2.8231 Acc: 0.3749                                               \n",
      "Epoch 009 | Train Loss: 5.4676 Acc: 0.3893 | Val Loss: 2.4212 Acc: 0.4257                                               \n",
      "Epoch 010 | Train Loss: 4.7803 Acc: 0.3999 | Val Loss: 1.5320 Acc: 0.4215                                               \n",
      "Epoch 011 | Train Loss: 4.3373 Acc: 0.4085 | Val Loss: 1.7193 Acc: 0.4824                                               \n",
      "Epoch 012 | Train Loss: 3.8613 Acc: 0.4109 | Val Loss: 1.3104 Acc: 0.4872                                               \n",
      "Epoch 013 | Train Loss: 3.5410 Acc: 0.4177 | Val Loss: 1.3169 Acc: 0.5078                                               \n",
      "Epoch 014 | Train Loss: 3.1536 Acc: 0.4355 | Val Loss: 1.1106 Acc: 0.5776                                               \n",
      "Epoch 015 | Train Loss: 2.9177 Acc: 0.4347 | Val Loss: 1.2877 Acc: 0.4940                                               \n",
      "Epoch 016 | Train Loss: 2.7663 Acc: 0.4399 | Val Loss: 1.0175 Acc: 0.5051                                               \n",
      "Epoch 017 | Train Loss: 2.5958 Acc: 0.4409 | Val Loss: 1.1221 Acc: 0.5131                                               \n",
      "Epoch 018 | Train Loss: 2.4971 Acc: 0.4564 | Val Loss: 1.0899 Acc: 0.4910                                               \n",
      "Epoch 019 | Train Loss: 2.2899 Acc: 0.4590 | Val Loss: 1.0367 Acc: 0.5487                                               \n",
      "Epoch 020 | Train Loss: 2.1356 Acc: 0.4685 | Val Loss: 1.1343 Acc: 0.4979                                               \n",
      "Epoch 021 | Train Loss: 2.0650 Acc: 0.4747 | Val Loss: 1.0449 Acc: 0.5475                                               \n",
      "Epoch 022 | Train Loss: 1.9468 Acc: 0.4928 | Val Loss: 0.9960 Acc: 0.5567                                               \n",
      "Epoch 023 | Train Loss: 1.8931 Acc: 0.4883 | Val Loss: 0.9611 Acc: 0.5591                                               \n",
      "Epoch 024 | Train Loss: 1.7742 Acc: 0.5073 | Val Loss: 1.0479 Acc: 0.5391                                               \n",
      "Epoch 025 | Train Loss: 1.8027 Acc: 0.4960 | Val Loss: 1.0814 Acc: 0.5116                                               \n",
      "Epoch 026 | Train Loss: 1.7127 Acc: 0.5047 | Val Loss: 1.0285 Acc: 0.5570                                               \n",
      "Epoch 027 | Train Loss: 1.6516 Acc: 0.5189 | Val Loss: 1.0076 Acc: 0.5591                                               \n",
      "Epoch 028 | Train Loss: 1.5617 Acc: 0.5194 | Val Loss: 1.0047 Acc: 0.5967                                               \n",
      "Epoch 029 | Train Loss: 1.5458 Acc: 0.5289 | Val Loss: 0.9160 Acc: 0.6075                                               \n",
      "Epoch 030 | Train Loss: 1.4700 Acc: 0.5380 | Val Loss: 1.0661 Acc: 0.5561                                               \n",
      "Epoch 031 | Train Loss: 1.4577 Acc: 0.5469 | Val Loss: 0.9426 Acc: 0.5896                                               \n",
      "Epoch 032 | Train Loss: 1.3970 Acc: 0.5482 | Val Loss: 0.8543 Acc: 0.6215                                               \n",
      "Epoch 033 | Train Loss: 1.3938 Acc: 0.5474 | Val Loss: 0.9311 Acc: 0.5800                                               \n",
      "Epoch 034 | Train Loss: 1.3766 Acc: 0.5513 | Val Loss: 0.8670 Acc: 0.6424                                               \n",
      "Epoch 035 | Train Loss: 1.3573 Acc: 0.5559 | Val Loss: 0.8890 Acc: 0.5985                                               \n",
      "Epoch 036 | Train Loss: 1.2441 Acc: 0.5814 | Val Loss: 0.8173 Acc: 0.6179                                               \n",
      "Epoch 037 | Train Loss: 1.2524 Acc: 0.5803 | Val Loss: 0.8327 Acc: 0.6385                                               \n",
      "Epoch 038 | Train Loss: 1.2160 Acc: 0.5906 | Val Loss: 0.9465 Acc: 0.5943                                               \n",
      "Epoch 039 | Train Loss: 1.1726 Acc: 0.5844 | Val Loss: 0.9462 Acc: 0.5794                                               \n",
      "Epoch 040 | Train Loss: 1.1673 Acc: 0.5926 | Val Loss: 0.7765 Acc: 0.6567                                               \n",
      "Epoch 041 | Train Loss: 1.1477 Acc: 0.5973 | Val Loss: 0.8194 Acc: 0.6391                                               \n",
      "Epoch 042 | Train Loss: 1.1108 Acc: 0.6050 | Val Loss: 0.8674 Acc: 0.6152                                               \n",
      "Epoch 043 | Train Loss: 1.0824 Acc: 0.5954 | Val Loss: 0.8844 Acc: 0.5982                                               \n",
      "Epoch 044 | Train Loss: 1.0673 Acc: 0.6048 | Val Loss: 0.8844 Acc: 0.5955                                               \n",
      "Epoch 045 | Train Loss: 1.0602 Acc: 0.6089 | Val Loss: 0.8306 Acc: 0.6104                                               \n",
      "Epoch 046 | Train Loss: 1.0348 Acc: 0.6143 | Val Loss: 0.8393 Acc: 0.5872                                               \n",
      "Epoch 047 | Train Loss: 1.0031 Acc: 0.6211 | Val Loss: 0.7947 Acc: 0.6215                                               \n",
      "Epoch 048 | Train Loss: 0.9680 Acc: 0.6326 | Val Loss: 0.8058 Acc: 0.6266                                               \n",
      "Epoch 049 | Train Loss: 0.9691 Acc: 0.6183 | Val Loss: 0.8040 Acc: 0.5961                                               \n",
      "Epoch 050 | Train Loss: 0.9843 Acc: 0.6191 | Val Loss: 0.8636 Acc: 0.6021                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 32, 'cnn_dropout': 0.5463342698171637, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 16, 'cnn_kernels_2': 16, 'learning_rate': 0.00044319451790114265, 'lstm_dense': 32, 'lstm_hidden_size': 32, 'lstm_layers': 6, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 15.0846 Acc: 0.3843 | Val Loss: 5.8108 Acc: 0.3540                                              \n",
      "Epoch 002 | Train Loss: 5.0948 Acc: 0.4241 | Val Loss: 1.9320 Acc: 0.4448                                               \n",
      "Epoch 003 | Train Loss: 2.8009 Acc: 0.4635 | Val Loss: 1.1151 Acc: 0.4904                                               \n",
      "Epoch 004 | Train Loss: 1.7751 Acc: 0.5100 | Val Loss: 1.0312 Acc: 0.5093                                               \n",
      "Epoch 005 | Train Loss: 1.1718 Acc: 0.5743 | Val Loss: 1.0702 Acc: 0.6415                                               \n",
      "Epoch 006 | Train Loss: 0.8936 Acc: 0.6256 | Val Loss: 0.6161 Acc: 0.7152                                               \n",
      "Epoch 007 | Train Loss: 0.7917 Acc: 0.6579 | Val Loss: 0.6341 Acc: 0.7048                                               \n",
      "Epoch 008 | Train Loss: 0.7060 Acc: 0.6915 | Val Loss: 0.5419 Acc: 0.7707                                               \n",
      "Epoch 009 | Train Loss: 0.6471 Acc: 0.7220 | Val Loss: 0.5553 Acc: 0.7630                                               \n",
      "Epoch 010 | Train Loss: 0.6144 Acc: 0.7489 | Val Loss: 0.6006 Acc: 0.7149                                               \n",
      "Epoch 011 | Train Loss: 0.5706 Acc: 0.7666 | Val Loss: 0.4950 Acc: 0.7743                                               \n",
      "Epoch 012 | Train Loss: 0.5263 Acc: 0.7892 | Val Loss: 0.4251 Acc: 0.8134                                               \n",
      "Epoch 013 | Train Loss: 0.4927 Acc: 0.8085 | Val Loss: 0.3854 Acc: 0.8493                                               \n",
      "Epoch 014 | Train Loss: 0.4729 Acc: 0.8161 | Val Loss: 0.3317 Acc: 0.8719                                               \n",
      "Epoch 015 | Train Loss: 0.4449 Acc: 0.8321 | Val Loss: 0.4143 Acc: 0.8254                                               \n",
      "Epoch 016 | Train Loss: 0.4377 Acc: 0.8374 | Val Loss: 0.3106 Acc: 0.8824                                               \n",
      "Epoch 017 | Train Loss: 0.3996 Acc: 0.8513 | Val Loss: 0.3501 Acc: 0.8740                                               \n",
      "Epoch 018 | Train Loss: 0.3865 Acc: 0.8593 | Val Loss: 0.3085 Acc: 0.8773                                               \n",
      "Epoch 019 | Train Loss: 0.3721 Acc: 0.8650 | Val Loss: 0.2876 Acc: 0.8991                                               \n",
      "Epoch 020 | Train Loss: 0.3419 Acc: 0.8743 | Val Loss: 0.2373 Acc: 0.9039                                               \n",
      "Epoch 021 | Train Loss: 0.3374 Acc: 0.8807 | Val Loss: 0.6174 Acc: 0.7496                                               \n",
      "Epoch 022 | Train Loss: 0.3332 Acc: 0.8796 | Val Loss: 0.2297 Acc: 0.9078                                               \n",
      "Epoch 023 | Train Loss: 0.3054 Acc: 0.8917 | Val Loss: 0.2117 Acc: 0.9260                                               \n",
      "Epoch 024 | Train Loss: 0.3031 Acc: 0.8927 | Val Loss: 0.1751 Acc: 0.9364                                               \n",
      "Epoch 025 | Train Loss: 0.2868 Acc: 0.9001 | Val Loss: 0.2535 Acc: 0.8860                                               \n",
      "Epoch 026 | Train Loss: 0.2637 Acc: 0.9073 | Val Loss: 0.1788 Acc: 0.9370                                               \n",
      "Epoch 027 | Train Loss: 0.2632 Acc: 0.9078 | Val Loss: 0.2176 Acc: 0.9033                                               \n",
      "Epoch 028 | Train Loss: 0.2518 Acc: 0.9102 | Val Loss: 0.2228 Acc: 0.9099                                               \n",
      "Epoch 029 | Train Loss: 0.2530 Acc: 0.9117 | Val Loss: 0.1615 Acc: 0.9376                                               \n",
      "Epoch 030 | Train Loss: 0.2409 Acc: 0.9187 | Val Loss: 0.1808 Acc: 0.9293                                               \n",
      "Epoch 031 | Train Loss: 0.2298 Acc: 0.9193 | Val Loss: 0.1494 Acc: 0.9481                                               \n",
      "Epoch 032 | Train Loss: 0.2208 Acc: 0.9213 | Val Loss: 0.2211 Acc: 0.9164                                               \n",
      "Epoch 033 | Train Loss: 0.2252 Acc: 0.9203 | Val Loss: 0.1620 Acc: 0.9325                                               \n",
      "Epoch 034 | Train Loss: 0.1930 Acc: 0.9331 | Val Loss: 0.1413 Acc: 0.9490                                               \n",
      "Epoch 035 | Train Loss: 0.2038 Acc: 0.9283 | Val Loss: 0.1920 Acc: 0.9176                                               \n",
      "Epoch 036 | Train Loss: 0.1923 Acc: 0.9355 | Val Loss: 0.1648 Acc: 0.9209                                               \n",
      "Epoch 037 | Train Loss: 0.1857 Acc: 0.9372 | Val Loss: 0.1580 Acc: 0.9281                                               \n",
      "Epoch 038 | Train Loss: 0.1843 Acc: 0.9357 | Val Loss: 0.2485 Acc: 0.9048                                               \n",
      "Epoch 039 | Train Loss: 0.1743 Acc: 0.9413 | Val Loss: 0.1172 Acc: 0.9496                                               \n",
      "Epoch 040 | Train Loss: 0.1649 Acc: 0.9422 | Val Loss: 0.2130 Acc: 0.9158                                               \n",
      "Epoch 041 | Train Loss: 0.1614 Acc: 0.9448 | Val Loss: 0.1453 Acc: 0.9436                                               \n",
      "Epoch 042 | Train Loss: 0.1630 Acc: 0.9443 | Val Loss: 0.1848 Acc: 0.9319                                               \n",
      "Epoch 043 | Train Loss: 0.1468 Acc: 0.9498 | Val Loss: 0.0936 Acc: 0.9591                                               \n",
      "Epoch 044 | Train Loss: 0.1495 Acc: 0.9495 | Val Loss: 0.1472 Acc: 0.9304                                               \n",
      "Epoch 045 | Train Loss: 0.1427 Acc: 0.9501 | Val Loss: 0.3076 Acc: 0.8821                                               \n",
      "Epoch 046 | Train Loss: 0.1515 Acc: 0.9489 | Val Loss: 0.1420 Acc: 0.9469                                               \n",
      "Epoch 047 | Train Loss: 0.1316 Acc: 0.9556 | Val Loss: 0.1144 Acc: 0.9570                                               \n",
      "Epoch 048 | Train Loss: 0.1308 Acc: 0.9548 | Val Loss: 0.0938 Acc: 0.9624                                               \n",
      "Epoch 049 | Train Loss: 0.1351 Acc: 0.9554 | Val Loss: 0.1042 Acc: 0.9663                                               \n",
      "Epoch 050 | Train Loss: 0.1215 Acc: 0.9595 | Val Loss: 0.1414 Acc: 0.9463                                               \n",
      "Epoch 051 | Train Loss: 0.1312 Acc: 0.9564 | Val Loss: 0.1099 Acc: 0.9487                                               \n",
      "Epoch 052 | Train Loss: 0.1294 Acc: 0.9551 | Val Loss: 0.0933 Acc: 0.9612                                               \n",
      "Epoch 053 | Train Loss: 0.1120 Acc: 0.9622 | Val Loss: 0.1005 Acc: 0.9576                                               \n",
      "Epoch 054 | Train Loss: 0.1249 Acc: 0.9586 | Val Loss: 0.0920 Acc: 0.9710                                               \n",
      "Epoch 055 | Train Loss: 0.1173 Acc: 0.9619 | Val Loss: 0.1269 Acc: 0.9475                                               \n",
      "Epoch 056 | Train Loss: 0.1229 Acc: 0.9607 | Val Loss: 0.0984 Acc: 0.9615                                               \n",
      "Epoch 057 | Train Loss: 0.1101 Acc: 0.9640 | Val Loss: 0.1034 Acc: 0.9615                                               \n",
      "Epoch 058 | Train Loss: 0.1099 Acc: 0.9645 | Val Loss: 0.2204 Acc: 0.9182                                               \n",
      "Epoch 059 | Train Loss: 0.1109 Acc: 0.9657 | Val Loss: 0.1290 Acc: 0.9472                                               \n",
      "Epoch 060 | Train Loss: 0.1062 Acc: 0.9637 | Val Loss: 0.0861 Acc: 0.9716                                               \n",
      "Epoch 061 | Train Loss: 0.0994 Acc: 0.9707 | Val Loss: 0.0952 Acc: 0.9588                                               \n",
      "Epoch 062 | Train Loss: 0.0961 Acc: 0.9680 | Val Loss: 0.1044 Acc: 0.9618                                               \n",
      "Epoch 063 | Train Loss: 0.1013 Acc: 0.9683 | Val Loss: 0.0948 Acc: 0.9648                                               \n",
      "Epoch 064 | Train Loss: 0.0896 Acc: 0.9698 | Val Loss: 0.0985 Acc: 0.9606                                               \n",
      "Epoch 065 | Train Loss: 0.0847 Acc: 0.9705 | Val Loss: 0.1211 Acc: 0.9501                                               \n",
      "Epoch 066 | Train Loss: 0.0979 Acc: 0.9695 | Val Loss: 0.1060 Acc: 0.9570                                               \n",
      "Epoch 067 | Train Loss: 0.0875 Acc: 0.9719 | Val Loss: 0.1216 Acc: 0.9546                                               \n",
      "Epoch 068 | Train Loss: 0.0853 Acc: 0.9712 | Val Loss: 0.1231 Acc: 0.9534                                               \n",
      "Epoch 069 | Train Loss: 0.0825 Acc: 0.9734 | Val Loss: 0.1094 Acc: 0.9588                                               \n",
      "Epoch 070 | Train Loss: 0.0949 Acc: 0.9709 | Val Loss: 0.1304 Acc: 0.9510                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 32, 'cnn_dropout': 0.5179703892292838, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 32, 'learning_rate': 0.00011237319748261757, 'lstm_dense': 32, 'lstm_hidden_size': 128, 'lstm_layers': 3, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 14.1092 Acc: 0.3668 | Val Loss: 2.3097 Acc: 0.4531                                              \n",
      "Epoch 002 | Train Loss: 4.8158 Acc: 0.4262 | Val Loss: 1.4484 Acc: 0.5806                                               \n",
      "Epoch 003 | Train Loss: 3.0943 Acc: 0.4844 | Val Loss: 1.4628 Acc: 0.5352                                               \n",
      "Epoch 004 | Train Loss: 2.2341 Acc: 0.5253 | Val Loss: 1.0938 Acc: 0.5842                                               \n",
      "Epoch 005 | Train Loss: 1.7291 Acc: 0.5590 | Val Loss: 0.8282 Acc: 0.7000                                               \n",
      "Epoch 006 | Train Loss: 1.4703 Acc: 0.5923 | Val Loss: 0.8273 Acc: 0.6525                                               \n",
      "Epoch 007 | Train Loss: 1.2471 Acc: 0.6270 | Val Loss: 1.3240 Acc: 0.6558                                               \n",
      "Epoch 008 | Train Loss: 1.0551 Acc: 0.6637 | Val Loss: 0.9849 Acc: 0.7072                                               \n",
      "Epoch 009 | Train Loss: 0.9538 Acc: 0.6946 | Val Loss: 0.5638 Acc: 0.7746                                               \n",
      "Epoch 010 | Train Loss: 0.8348 Acc: 0.7311 | Val Loss: 0.5742 Acc: 0.7901                                               \n",
      "Epoch 011 | Train Loss: 0.7332 Acc: 0.7616 | Val Loss: 0.5679 Acc: 0.8045                                               \n",
      "Epoch 012 | Train Loss: 0.6542 Acc: 0.7810 | Val Loss: 0.5261 Acc: 0.8275                                               \n",
      "Epoch 013 | Train Loss: 0.5770 Acc: 0.8033 | Val Loss: 0.3073 Acc: 0.8812                                               \n",
      "Epoch 014 | Train Loss: 0.5255 Acc: 0.8195 | Val Loss: 0.4070 Acc: 0.8469                                               \n",
      "Epoch 015 | Train Loss: 0.4668 Acc: 0.8423 | Val Loss: 0.3436 Acc: 0.8740                                               \n",
      "Epoch 016 | Train Loss: 0.4106 Acc: 0.8607 | Val Loss: 0.3033 Acc: 0.8872                                               \n",
      "Epoch 017 | Train Loss: 0.3731 Acc: 0.8760 | Val Loss: 0.3590 Acc: 0.8752                                               \n",
      "Epoch 018 | Train Loss: 0.3425 Acc: 0.8836 | Val Loss: 0.2133 Acc: 0.9260                                               \n",
      "Epoch 019 | Train Loss: 0.3074 Acc: 0.8991 | Val Loss: 0.3343 Acc: 0.8916                                               \n",
      "Epoch 020 | Train Loss: 0.2783 Acc: 0.9089 | Val Loss: 0.2163 Acc: 0.9355                                               \n",
      "Epoch 021 | Train Loss: 0.2469 Acc: 0.9149 | Val Loss: 0.2254 Acc: 0.9209                                               \n",
      "Epoch 022 | Train Loss: 0.2374 Acc: 0.9253 | Val Loss: 0.2144 Acc: 0.9227                                               \n",
      "Epoch 023 | Train Loss: 0.2035 Acc: 0.9352 | Val Loss: 0.1656 Acc: 0.9478                                               \n",
      "Epoch 024 | Train Loss: 0.1949 Acc: 0.9369 | Val Loss: 0.2494 Acc: 0.9191                                               \n",
      "Epoch 025 | Train Loss: 0.1823 Acc: 0.9423 | Val Loss: 0.1657 Acc: 0.9478                                               \n",
      "Epoch 026 | Train Loss: 0.1737 Acc: 0.9440 | Val Loss: 0.1584 Acc: 0.9412                                               \n",
      "Epoch 027 | Train Loss: 0.1489 Acc: 0.9525 | Val Loss: 0.1213 Acc: 0.9591                                               \n",
      "Epoch 028 | Train Loss: 0.1381 Acc: 0.9575 | Val Loss: 0.1286 Acc: 0.9597                                               \n",
      "Epoch 029 | Train Loss: 0.1373 Acc: 0.9580 | Val Loss: 0.1190 Acc: 0.9621                                               \n",
      "Epoch 030 | Train Loss: 0.1280 Acc: 0.9594 | Val Loss: 0.1857 Acc: 0.9421                                               \n",
      "Epoch 031 | Train Loss: 0.1208 Acc: 0.9628 | Val Loss: 0.1522 Acc: 0.9513                                               \n",
      "Epoch 032 | Train Loss: 0.1151 Acc: 0.9652 | Val Loss: 0.1069 Acc: 0.9639                                               \n",
      "Epoch 033 | Train Loss: 0.1033 Acc: 0.9682 | Val Loss: 0.1152 Acc: 0.9612                                               \n",
      "Epoch 034 | Train Loss: 0.1029 Acc: 0.9684 | Val Loss: 0.1062 Acc: 0.9731                                               \n",
      "Epoch 035 | Train Loss: 0.0904 Acc: 0.9720 | Val Loss: 0.1150 Acc: 0.9675                                               \n",
      "Epoch 036 | Train Loss: 0.0939 Acc: 0.9703 | Val Loss: 0.0769 Acc: 0.9746                                               \n",
      "Epoch 037 | Train Loss: 0.0871 Acc: 0.9715 | Val Loss: 0.0697 Acc: 0.9785                                               \n",
      "Epoch 038 | Train Loss: 0.0860 Acc: 0.9734 | Val Loss: 0.0823 Acc: 0.9737                                               \n",
      "Epoch 039 | Train Loss: 0.0783 Acc: 0.9760 | Val Loss: 0.0797 Acc: 0.9752                                               \n",
      "Epoch 040 | Train Loss: 0.0709 Acc: 0.9775 | Val Loss: 0.1515 Acc: 0.9507                                               \n",
      "Epoch 041 | Train Loss: 0.0724 Acc: 0.9768 | Val Loss: 0.0745 Acc: 0.9782                                               \n",
      "Epoch 042 | Train Loss: 0.0684 Acc: 0.9783 | Val Loss: 0.0703 Acc: 0.9788                                               \n",
      "Epoch 043 | Train Loss: 0.0640 Acc: 0.9792 | Val Loss: 0.1356 Acc: 0.9648                                               \n",
      "Epoch 044 | Train Loss: 0.0594 Acc: 0.9834 | Val Loss: 0.1869 Acc: 0.9439                                               \n",
      "Epoch 045 | Train Loss: 0.0585 Acc: 0.9831 | Val Loss: 0.0786 Acc: 0.9776                                               \n",
      "Epoch 046 | Train Loss: 0.0578 Acc: 0.9824 | Val Loss: 0.0734 Acc: 0.9770                                               \n",
      "Epoch 047 | Train Loss: 0.0604 Acc: 0.9818 | Val Loss: 0.1152 Acc: 0.9624                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 256, 'cnn_dropout': 0.049506483379208044, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 32, 'learning_rate': 0.00019396409413325195, 'lstm_dense': 128, 'lstm_hidden_size': 64, 'lstm_layers': 1, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 29.3924 Acc: 0.3729 | Val Loss: 3.5051 Acc: 0.4916                                              \n",
      "Epoch 002 | Train Loss: 9.2760 Acc: 0.4094 | Val Loss: 3.0284 Acc: 0.5531                                               \n",
      "Epoch 003 | Train Loss: 5.6861 Acc: 0.4394 | Val Loss: 2.4426 Acc: 0.5949                                               \n",
      "Epoch 004 | Train Loss: 4.0753 Acc: 0.4623 | Val Loss: 1.8105 Acc: 0.5481                                               \n",
      "Epoch 005 | Train Loss: 3.2374 Acc: 0.4712 | Val Loss: 1.3789 Acc: 0.6039                                               \n",
      "Epoch 006 | Train Loss: 2.4767 Acc: 0.4992 | Val Loss: 1.2756 Acc: 0.5394                                               \n",
      "Epoch 007 | Train Loss: 2.2099 Acc: 0.5135 | Val Loss: 1.2218 Acc: 0.6364                                               \n",
      "Epoch 008 | Train Loss: 1.8841 Acc: 0.5227 | Val Loss: 1.3386 Acc: 0.5701                                               \n",
      "Epoch 009 | Train Loss: 1.7158 Acc: 0.5362 | Val Loss: 1.0642 Acc: 0.5937                                               \n",
      "Epoch 010 | Train Loss: 1.5571 Acc: 0.5453 | Val Loss: 0.8340 Acc: 0.6188                                               \n",
      "Epoch 011 | Train Loss: 1.3736 Acc: 0.5866 | Val Loss: 0.7367 Acc: 0.7188                                               \n",
      "Epoch 012 | Train Loss: 1.2260 Acc: 0.6070 | Val Loss: 0.7694 Acc: 0.6991                                               \n",
      "Epoch 013 | Train Loss: 1.1156 Acc: 0.6330 | Val Loss: 0.8671 Acc: 0.6352                                               \n",
      "Epoch 014 | Train Loss: 0.9709 Acc: 0.6622 | Val Loss: 0.6020 Acc: 0.7537                                               \n",
      "Epoch 015 | Train Loss: 0.9044 Acc: 0.6895 | Val Loss: 0.6806 Acc: 0.7451                                               \n",
      "Epoch 016 | Train Loss: 0.7959 Acc: 0.7233 | Val Loss: 0.5082 Acc: 0.7863                                               \n",
      "Epoch 017 | Train Loss: 0.6676 Acc: 0.7592 | Val Loss: 0.3997 Acc: 0.8430                                               \n",
      "Epoch 018 | Train Loss: 0.5783 Acc: 0.7989 | Val Loss: 0.4010 Acc: 0.8487                                               \n",
      "Epoch 019 | Train Loss: 0.5197 Acc: 0.8198 | Val Loss: 0.3786 Acc: 0.8573                                               \n",
      "Epoch 020 | Train Loss: 0.4501 Acc: 0.8419 | Val Loss: 0.3509 Acc: 0.8699                                               \n",
      "Epoch 021 | Train Loss: 0.3776 Acc: 0.8693 | Val Loss: 0.2800 Acc: 0.9003                                               \n",
      "Epoch 022 | Train Loss: 0.3215 Acc: 0.8902 | Val Loss: 0.3349 Acc: 0.8827                                               \n",
      "Epoch 023 | Train Loss: 0.3017 Acc: 0.8966 | Val Loss: 0.2518 Acc: 0.9134                                               \n",
      "Epoch 024 | Train Loss: 0.2701 Acc: 0.9090 | Val Loss: 0.2375 Acc: 0.9155                                               \n",
      "Epoch 025 | Train Loss: 0.2363 Acc: 0.9200 | Val Loss: 0.1898 Acc: 0.9334                                               \n",
      "Epoch 026 | Train Loss: 0.2103 Acc: 0.9304 | Val Loss: 0.4499 Acc: 0.8570                                               \n",
      "Epoch 027 | Train Loss: 0.1905 Acc: 0.9349 | Val Loss: 0.2299 Acc: 0.9206                                               \n",
      "Epoch 028 | Train Loss: 0.1870 Acc: 0.9343 | Val Loss: 0.1764 Acc: 0.9385                                               \n",
      "Epoch 029 | Train Loss: 0.1520 Acc: 0.9501 | Val Loss: 0.1667 Acc: 0.9454                                               \n",
      "Epoch 030 | Train Loss: 0.1370 Acc: 0.9539 | Val Loss: 0.1579 Acc: 0.9436                                               \n",
      "Epoch 031 | Train Loss: 0.1240 Acc: 0.9596 | Val Loss: 0.1844 Acc: 0.9367                                               \n",
      "Epoch 032 | Train Loss: 0.1139 Acc: 0.9618 | Val Loss: 0.1260 Acc: 0.9576                                               \n",
      "Epoch 033 | Train Loss: 0.1044 Acc: 0.9666 | Val Loss: 0.1510 Acc: 0.9531                                               \n",
      "Epoch 034 | Train Loss: 0.0963 Acc: 0.9687 | Val Loss: 0.1679 Acc: 0.9472                                               \n",
      "Epoch 035 | Train Loss: 0.1006 Acc: 0.9640 | Val Loss: 0.1594 Acc: 0.9466                                               \n",
      "Epoch 036 | Train Loss: 0.0933 Acc: 0.9701 | Val Loss: 0.1303 Acc: 0.9603                                               \n",
      "Epoch 037 | Train Loss: 0.0858 Acc: 0.9705 | Val Loss: 0.1279 Acc: 0.9594                                               \n",
      "Epoch 038 | Train Loss: 0.0760 Acc: 0.9737 | Val Loss: 0.1456 Acc: 0.9531                                               \n",
      "Epoch 039 | Train Loss: 0.0811 Acc: 0.9735 | Val Loss: 0.1503 Acc: 0.9549                                               \n",
      "Epoch 040 | Train Loss: 0.0653 Acc: 0.9782 | Val Loss: 0.1091 Acc: 0.9612                                               \n",
      "Epoch 041 | Train Loss: 0.0512 Acc: 0.9831 | Val Loss: 0.1138 Acc: 0.9654                                               \n",
      "Epoch 042 | Train Loss: 0.0488 Acc: 0.9848 | Val Loss: 0.1731 Acc: 0.9466                                               \n",
      "Epoch 043 | Train Loss: 0.0636 Acc: 0.9798 | Val Loss: 0.2468 Acc: 0.9296                                               \n",
      "Epoch 044 | Train Loss: 0.0721 Acc: 0.9748 | Val Loss: 0.1347 Acc: 0.9618                                               \n",
      "Epoch 045 | Train Loss: 0.0486 Acc: 0.9854 | Val Loss: 0.1249 Acc: 0.9618                                               \n",
      "Epoch 046 | Train Loss: 0.0717 Acc: 0.9754 | Val Loss: 0.1292 Acc: 0.9597                                               \n",
      "Epoch 047 | Train Loss: 0.0460 Acc: 0.9846 | Val Loss: 0.1191 Acc: 0.9660                                               \n",
      "Epoch 048 | Train Loss: 0.0470 Acc: 0.9838 | Val Loss: 0.1467 Acc: 0.9621                                               \n",
      "Epoch 049 | Train Loss: 0.0555 Acc: 0.9834 | Val Loss: 0.2043 Acc: 0.9475                                               \n",
      "Epoch 050 | Train Loss: 0.0405 Acc: 0.9858 | Val Loss: 0.1247 Acc: 0.9636                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 256, 'cnn_dropout': 0.12727790839787523, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 32, 'learning_rate': 0.0005848607858163696, 'lstm_dense': 64, 'lstm_hidden_size': 64, 'lstm_layers': 1, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 18.4810 Acc: 0.3814 | Val Loss: 2.7744 Acc: 0.5099                                              \n",
      "Epoch 002 | Train Loss: 5.4376 Acc: 0.4318 | Val Loss: 2.3244 Acc: 0.5266                                               \n",
      "Epoch 003 | Train Loss: 2.8975 Acc: 0.4752 | Val Loss: 1.2244 Acc: 0.6275                                               \n",
      "Epoch 004 | Train Loss: 2.0532 Acc: 0.5104 | Val Loss: 1.0078 Acc: 0.6301                                               \n",
      "Epoch 005 | Train Loss: 1.5984 Acc: 0.5369 | Val Loss: 0.9405 Acc: 0.6427                                               \n",
      "Epoch 006 | Train Loss: 1.3591 Acc: 0.5588 | Val Loss: 0.9992 Acc: 0.5525                                               \n",
      "Epoch 007 | Train Loss: 1.1676 Acc: 0.5690 | Val Loss: 0.9412 Acc: 0.5740                                               \n",
      "Epoch 008 | Train Loss: 1.0668 Acc: 0.5915 | Val Loss: 0.7575 Acc: 0.7009                                               \n",
      "Epoch 009 | Train Loss: 0.9840 Acc: 0.5956 | Val Loss: 0.8915 Acc: 0.5731                                               \n",
      "Epoch 010 | Train Loss: 0.9180 Acc: 0.6174 | Val Loss: 0.8046 Acc: 0.6507                                               \n",
      "Epoch 011 | Train Loss: 0.8761 Acc: 0.6231 | Val Loss: 0.6897 Acc: 0.6785                                               \n",
      "Epoch 012 | Train Loss: 0.8328 Acc: 0.6337 | Val Loss: 0.7047 Acc: 0.6815                                               \n",
      "Epoch 013 | Train Loss: 0.7991 Acc: 0.6433 | Val Loss: 0.6647 Acc: 0.7534                                               \n",
      "Epoch 014 | Train Loss: 0.7763 Acc: 0.6539 | Val Loss: 0.7099 Acc: 0.6707                                               \n",
      "Epoch 015 | Train Loss: 0.7561 Acc: 0.6621 | Val Loss: 0.6424 Acc: 0.7504                                               \n",
      "Epoch 016 | Train Loss: 0.7385 Acc: 0.6662 | Val Loss: 0.6973 Acc: 0.7191                                               \n",
      "Epoch 017 | Train Loss: 0.7141 Acc: 0.6798 | Val Loss: 0.6142 Acc: 0.7478                                               \n",
      "Epoch 018 | Train Loss: 0.7107 Acc: 0.6809 | Val Loss: 0.6713 Acc: 0.7218                                               \n",
      "Epoch 019 | Train Loss: 0.6861 Acc: 0.6928 | Val Loss: 0.5839 Acc: 0.7824                                               \n",
      "Epoch 020 | Train Loss: 0.6692 Acc: 0.6998 | Val Loss: 0.6359 Acc: 0.7537                                               \n",
      "Epoch 021 | Train Loss: 0.6554 Acc: 0.7068 | Val Loss: 0.6084 Acc: 0.7066                                               \n",
      "Epoch 022 | Train Loss: 0.6442 Acc: 0.7133 | Val Loss: 0.5531 Acc: 0.7893                                               \n",
      "Epoch 023 | Train Loss: 0.6295 Acc: 0.7207 | Val Loss: 0.6166 Acc: 0.7096                                               \n",
      "Epoch 024 | Train Loss: 0.6113 Acc: 0.7286 | Val Loss: 0.6309 Acc: 0.6627                                               \n",
      "Epoch 025 | Train Loss: 0.6119 Acc: 0.7262 | Val Loss: 0.6291 Acc: 0.7116                                               \n",
      "Epoch 026 | Train Loss: 0.5766 Acc: 0.7443 | Val Loss: 0.5133 Acc: 0.7982                                               \n",
      "Epoch 027 | Train Loss: 0.5750 Acc: 0.7478 | Val Loss: 0.5203 Acc: 0.7630                                               \n",
      "Epoch 028 | Train Loss: 0.5537 Acc: 0.7586 | Val Loss: 0.4969 Acc: 0.7812                                               \n",
      "Epoch 029 | Train Loss: 0.5312 Acc: 0.7705 | Val Loss: 0.5160 Acc: 0.7782                                               \n",
      "Epoch 030 | Train Loss: 0.5198 Acc: 0.7778 | Val Loss: 0.4748 Acc: 0.7755                                               \n",
      "Epoch 031 | Train Loss: 0.5084 Acc: 0.7805 | Val Loss: 0.4457 Acc: 0.8018                                               \n",
      "Epoch 032 | Train Loss: 0.4913 Acc: 0.7983 | Val Loss: 0.5155 Acc: 0.7639                                               \n",
      "Epoch 033 | Train Loss: 0.4646 Acc: 0.8057 | Val Loss: 0.4230 Acc: 0.8269                                               \n",
      "Epoch 034 | Train Loss: 0.4483 Acc: 0.8172 | Val Loss: 0.4427 Acc: 0.8182                                               \n",
      "Epoch 035 | Train Loss: 0.4439 Acc: 0.8190 | Val Loss: 0.4120 Acc: 0.8125                                               \n",
      "Epoch 036 | Train Loss: 0.4194 Acc: 0.8295 | Val Loss: 0.4048 Acc: 0.8346                                               \n",
      "Epoch 037 | Train Loss: 0.4026 Acc: 0.8421 | Val Loss: 0.3513 Acc: 0.8609                                               \n",
      "Epoch 038 | Train Loss: 0.3897 Acc: 0.8516 | Val Loss: 0.3514 Acc: 0.8531                                               \n",
      "Epoch 039 | Train Loss: 0.3721 Acc: 0.8515 | Val Loss: 0.4200 Acc: 0.8131                                               \n",
      "Epoch 040 | Train Loss: 0.3610 Acc: 0.8551 | Val Loss: 0.3212 Acc: 0.8713                                               \n",
      "Epoch 041 | Train Loss: 0.3324 Acc: 0.8735 | Val Loss: 0.3002 Acc: 0.8713                                               \n",
      "Epoch 042 | Train Loss: 0.3193 Acc: 0.8814 | Val Loss: 0.3436 Acc: 0.8597                                               \n",
      "Epoch 043 | Train Loss: 0.3026 Acc: 0.8883 | Val Loss: 0.4173 Acc: 0.8176                                               \n",
      "Epoch 044 | Train Loss: 0.2816 Acc: 0.8966 | Val Loss: 0.2414 Acc: 0.9155                                               \n",
      "Epoch 045 | Train Loss: 0.2610 Acc: 0.9047 | Val Loss: 0.3899 Acc: 0.8218                                               \n",
      "Epoch 046 | Train Loss: 0.2522 Acc: 0.9075 | Val Loss: 0.2324 Acc: 0.9137                                               \n",
      "Epoch 047 | Train Loss: 0.2300 Acc: 0.9176 | Val Loss: 0.1760 Acc: 0.9400                                               \n",
      "Epoch 048 | Train Loss: 0.2317 Acc: 0.9181 | Val Loss: 0.2207 Acc: 0.9140                                               \n",
      "Epoch 049 | Train Loss: 0.2217 Acc: 0.9202 | Val Loss: 0.2094 Acc: 0.9206                                               \n",
      "Epoch 050 | Train Loss: 0.1961 Acc: 0.9315 | Val Loss: 0.1845 Acc: 0.9358                                               \n",
      "Epoch 051 | Train Loss: 0.1757 Acc: 0.9381 | Val Loss: 0.1821 Acc: 0.9325                                               \n",
      "Epoch 052 | Train Loss: 0.1641 Acc: 0.9415 | Val Loss: 0.1612 Acc: 0.9412                                               \n",
      "Epoch 053 | Train Loss: 0.1591 Acc: 0.9428 | Val Loss: 0.1394 Acc: 0.9537                                               \n",
      "Epoch 054 | Train Loss: 0.1483 Acc: 0.9486 | Val Loss: 0.1811 Acc: 0.9287                                               \n",
      "Epoch 055 | Train Loss: 0.1311 Acc: 0.9563 | Val Loss: 0.1585 Acc: 0.9439                                               \n",
      "Epoch 056 | Train Loss: 0.1379 Acc: 0.9535 | Val Loss: 0.1549 Acc: 0.9421                                               \n",
      "Epoch 057 | Train Loss: 0.1170 Acc: 0.9598 | Val Loss: 0.0975 Acc: 0.9648                                               \n",
      "Epoch 058 | Train Loss: 0.1179 Acc: 0.9596 | Val Loss: 0.1007 Acc: 0.9657                                               \n",
      "Epoch 059 | Train Loss: 0.1036 Acc: 0.9643 | Val Loss: 0.1106 Acc: 0.9624                                               \n",
      "Epoch 060 | Train Loss: 0.0931 Acc: 0.9685 | Val Loss: 0.0918 Acc: 0.9719                                               \n",
      "Epoch 061 | Train Loss: 0.0948 Acc: 0.9692 | Val Loss: 0.1009 Acc: 0.9657                                               \n",
      "Epoch 062 | Train Loss: 0.0928 Acc: 0.9687 | Val Loss: 0.1111 Acc: 0.9606                                               \n",
      "Epoch 063 | Train Loss: 0.0908 Acc: 0.9706 | Val Loss: 0.0847 Acc: 0.9696                                               \n",
      "Epoch 064 | Train Loss: 0.0787 Acc: 0.9719 | Val Loss: 0.0870 Acc: 0.9678                                               \n",
      "Epoch 065 | Train Loss: 0.0658 Acc: 0.9784 | Val Loss: 0.0862 Acc: 0.9693                                               \n",
      "Epoch 066 | Train Loss: 0.0649 Acc: 0.9773 | Val Loss: 0.0871 Acc: 0.9678                                               \n",
      "Epoch 067 | Train Loss: 0.0816 Acc: 0.9716 | Val Loss: 0.0954 Acc: 0.9690                                               \n",
      "Epoch 068 | Train Loss: 0.0706 Acc: 0.9766 | Val Loss: 0.1028 Acc: 0.9612                                               \n",
      "Epoch 069 | Train Loss: 0.0621 Acc: 0.9790 | Val Loss: 0.0814 Acc: 0.9713                                               \n",
      "Epoch 070 | Train Loss: 0.0530 Acc: 0.9819 | Val Loss: 0.0481 Acc: 0.9833                                               \n",
      "Epoch 071 | Train Loss: 0.0548 Acc: 0.9806 | Val Loss: 0.0975 Acc: 0.9669                                               \n",
      "Epoch 072 | Train Loss: 0.0431 Acc: 0.9864 | Val Loss: 0.0412 Acc: 0.9869                                               \n",
      "Epoch 073 | Train Loss: 0.0521 Acc: 0.9833 | Val Loss: 0.0629 Acc: 0.9791                                               \n",
      "Epoch 074 | Train Loss: 0.0420 Acc: 0.9851 | Val Loss: 0.0515 Acc: 0.9827                                               \n",
      "Epoch 075 | Train Loss: 0.0482 Acc: 0.9841 | Val Loss: 0.0580 Acc: 0.9788                                               \n",
      "Epoch 076 | Train Loss: 0.0562 Acc: 0.9821 | Val Loss: 0.0546 Acc: 0.9806                                               \n",
      "Epoch 077 | Train Loss: 0.0498 Acc: 0.9826 | Val Loss: 0.0972 Acc: 0.9666                                               \n",
      "Epoch 078 | Train Loss: 0.0458 Acc: 0.9844 | Val Loss: 0.0573 Acc: 0.9785                                               \n",
      "Epoch 079 | Train Loss: 0.0440 Acc: 0.9851 | Val Loss: 0.0580 Acc: 0.9794                                               \n",
      "Epoch 080 | Train Loss: 0.0441 Acc: 0.9875 | Val Loss: 0.0596 Acc: 0.9794                                               \n",
      "Epoch 081 | Train Loss: 0.0452 Acc: 0.9855 | Val Loss: 0.0996 Acc: 0.9699                                               \n",
      "Epoch 082 | Train Loss: 0.0403 Acc: 0.9859 | Val Loss: 0.0798 Acc: 0.9728                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 256, 'cnn_dropout': 0.008514704282266194, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 96, 'learning_rate': 0.0006708965944506832, 'lstm_dense': 64, 'lstm_hidden_size': 64, 'lstm_layers': 1, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 15.3376 Acc: 0.3970 | Val Loss: 2.2796 Acc: 0.5293                                              \n",
      "Epoch 002 | Train Loss: 3.4563 Acc: 0.4326 | Val Loss: 1.2227 Acc: 0.5215                                               \n",
      "Epoch 003 | Train Loss: 2.0735 Acc: 0.4771 | Val Loss: 1.4607 Acc: 0.6036                                               \n",
      "Epoch 004 | Train Loss: 1.5869 Acc: 0.5075 | Val Loss: 1.0004 Acc: 0.6436                                               \n",
      "Epoch 005 | Train Loss: 1.3055 Acc: 0.5386 | Val Loss: 0.9251 Acc: 0.6815                                               \n",
      "Epoch 006 | Train Loss: 1.2116 Acc: 0.5556 | Val Loss: 1.0672 Acc: 0.5182                                               \n",
      "Epoch 007 | Train Loss: 1.0400 Acc: 0.5734 | Val Loss: 0.7633 Acc: 0.6096                                               \n",
      "Epoch 008 | Train Loss: 1.0364 Acc: 0.5739 | Val Loss: 0.8166 Acc: 0.5904                                               \n",
      "Epoch 009 | Train Loss: 1.0041 Acc: 0.5844 | Val Loss: 1.1367 Acc: 0.5349                                               \n",
      "Epoch 010 | Train Loss: 0.9324 Acc: 0.6033 | Val Loss: 1.0030 Acc: 0.5899                                               \n",
      "Epoch 011 | Train Loss: 0.8904 Acc: 0.6174 | Val Loss: 0.6340 Acc: 0.6872                                               \n",
      "Epoch 012 | Train Loss: 0.8504 Acc: 0.6200 | Val Loss: 0.6796 Acc: 0.7116                                               \n",
      "Epoch 013 | Train Loss: 0.8833 Acc: 0.6228 | Val Loss: 0.7156 Acc: 0.5982                                               \n",
      "Epoch 014 | Train Loss: 0.8276 Acc: 0.6319 | Val Loss: 0.7040 Acc: 0.6869                                               \n",
      "Epoch 015 | Train Loss: 0.8209 Acc: 0.6325 | Val Loss: 0.6633 Acc: 0.7006                                               \n",
      "Epoch 016 | Train Loss: 0.7597 Acc: 0.6602 | Val Loss: 0.5827 Acc: 0.7651                                               \n",
      "Epoch 017 | Train Loss: 0.7282 Acc: 0.6775 | Val Loss: 0.6750 Acc: 0.6713                                               \n",
      "Epoch 018 | Train Loss: 0.6974 Acc: 0.7013 | Val Loss: 0.7392 Acc: 0.6779                                               \n",
      "Epoch 019 | Train Loss: 0.6803 Acc: 0.7155 | Val Loss: 0.7051 Acc: 0.7188                                               \n",
      "Epoch 020 | Train Loss: 0.6401 Acc: 0.7368 | Val Loss: 0.4481 Acc: 0.8110                                               \n",
      "Epoch 021 | Train Loss: 0.5594 Acc: 0.7745 | Val Loss: 0.4290 Acc: 0.8101                                               \n",
      "Epoch 022 | Train Loss: 0.4444 Acc: 0.8234 | Val Loss: 0.3316 Acc: 0.8749                                               \n",
      "Epoch 023 | Train Loss: 0.3811 Acc: 0.8524 | Val Loss: 0.3328 Acc: 0.8848                                               \n",
      "Epoch 024 | Train Loss: 0.3048 Acc: 0.8869 | Val Loss: 0.2569 Acc: 0.8985                                               \n",
      "Epoch 025 | Train Loss: 0.2840 Acc: 0.8972 | Val Loss: 0.1940 Acc: 0.9263                                               \n",
      "Epoch 026 | Train Loss: 0.2157 Acc: 0.9241 | Val Loss: 0.1588 Acc: 0.9433                                               \n",
      "Epoch 027 | Train Loss: 0.1870 Acc: 0.9330 | Val Loss: 0.1582 Acc: 0.9406                                               \n",
      "Epoch 028 | Train Loss: 0.1581 Acc: 0.9443 | Val Loss: 0.1805 Acc: 0.9364                                               \n",
      "Epoch 029 | Train Loss: 0.1362 Acc: 0.9539 | Val Loss: 0.1293 Acc: 0.9552                                               \n",
      "Epoch 030 | Train Loss: 0.1209 Acc: 0.9584 | Val Loss: 0.2151 Acc: 0.9245                                               \n",
      "Epoch 031 | Train Loss: 0.1292 Acc: 0.9566 | Val Loss: 0.1171 Acc: 0.9600                                               \n",
      "Epoch 032 | Train Loss: 0.1134 Acc: 0.9648 | Val Loss: 0.1362 Acc: 0.9594                                               \n",
      "Epoch 033 | Train Loss: 0.0849 Acc: 0.9715 | Val Loss: 0.1170 Acc: 0.9636                                               \n",
      "Epoch 034 | Train Loss: 0.0657 Acc: 0.9788 | Val Loss: 0.0988 Acc: 0.9657                                               \n",
      "Epoch 035 | Train Loss: 0.0715 Acc: 0.9760 | Val Loss: 0.1538 Acc: 0.9522                                               \n",
      "Epoch 036 | Train Loss: 0.0726 Acc: 0.9760 | Val Loss: 0.1570 Acc: 0.9531                                               \n",
      "Epoch 037 | Train Loss: 0.0515 Acc: 0.9830 | Val Loss: 0.1195 Acc: 0.9633                                               \n",
      "Epoch 038 | Train Loss: 0.0499 Acc: 0.9834 | Val Loss: 0.1008 Acc: 0.9675                                               \n",
      "Epoch 039 | Train Loss: 0.0618 Acc: 0.9787 | Val Loss: 0.1046 Acc: 0.9681                                               \n",
      "Epoch 040 | Train Loss: 0.0539 Acc: 0.9829 | Val Loss: 0.1219 Acc: 0.9606                                               \n",
      "Epoch 041 | Train Loss: 0.0457 Acc: 0.9851 | Val Loss: 0.1024 Acc: 0.9693                                               \n",
      "Epoch 042 | Train Loss: 0.0521 Acc: 0.9828 | Val Loss: 0.1290 Acc: 0.9648                                               \n",
      "Epoch 043 | Train Loss: 0.0454 Acc: 0.9857 | Val Loss: 0.0929 Acc: 0.9734                                               \n",
      "Epoch 044 | Train Loss: 0.0369 Acc: 0.9875 | Val Loss: 0.1296 Acc: 0.9624                                               \n",
      "Epoch 022 | Train Loss: 0.6009 Acc: 0.7264 | Val Loss: 0.6399 Acc: 0.7257                                               \n",
      "Epoch 023 | Train Loss: 0.6253 Acc: 0.7181 | Val Loss: 0.5796 Acc: 0.7400                                               \n",
      "Epoch 024 | Train Loss: 0.6024 Acc: 0.7222 | Val Loss: 0.6115 Acc: 0.7236                                               \n",
      "Epoch 025 | Train Loss: 0.5840 Acc: 0.7366 | Val Loss: 0.5624 Acc: 0.7731                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 256, 'cnn_dropout': 0.38766654467685846, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 32, 'learning_rate': 0.0018333220390189416, 'lstm_dense': 64, 'lstm_hidden_size': 64, 'lstm_layers': 1, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 10.1095 Acc: 0.4409 | Val Loss: 1.2981 Acc: 0.5704                                              \n",
      "Epoch 002 | Train Loss: 1.7830 Acc: 0.5271 | Val Loss: 0.8894 Acc: 0.6940                                               \n",
      "Epoch 003 | Train Loss: 0.9791 Acc: 0.5933 | Val Loss: 0.8670 Acc: 0.6257                                               \n",
      "Epoch 004 | Train Loss: 0.8229 Acc: 0.6220 | Val Loss: 0.6684 Acc: 0.7012                                               \n",
      "Epoch 005 | Train Loss: 0.7481 Acc: 0.6500 | Val Loss: 0.7264 Acc: 0.6182                                               \n",
      "Epoch 006 | Train Loss: 0.7008 Acc: 0.6686 | Val Loss: 0.5354 Acc: 0.7788                                               \n",
      "Epoch 007 | Train Loss: 0.6760 Acc: 0.6872 | Val Loss: 0.5753 Acc: 0.7418                                               \n",
      "Epoch 008 | Train Loss: 0.6480 Acc: 0.7030 | Val Loss: 0.5302 Acc: 0.7893                                               \n",
      "Epoch 009 | Train Loss: 0.6322 Acc: 0.7121 | Val Loss: 0.5459 Acc: 0.7382                                               \n",
      "Epoch 010 | Train Loss: 0.5995 Acc: 0.7327 | Val Loss: 0.5524 Acc: 0.7555                                               \n",
      "Epoch 011 | Train Loss: 0.6035 Acc: 0.7316 | Val Loss: 0.5099 Acc: 0.7707                                               \n",
      "Epoch 012 | Train Loss: 0.5915 Acc: 0.7362 | Val Loss: 0.4901 Acc: 0.7552                                               \n",
      "Epoch 013 | Train Loss: 0.5678 Acc: 0.7538 | Val Loss: 0.4935 Acc: 0.7746                                               \n",
      "Epoch 014 | Train Loss: 0.5529 Acc: 0.7619 | Val Loss: 0.4314 Acc: 0.7890                                               \n",
      "Epoch 015 | Train Loss: 0.5842 Acc: 0.7489 | Val Loss: 0.4830 Acc: 0.8075                                               \n",
      "Epoch 016 | Train Loss: 0.5332 Acc: 0.7704 | Val Loss: 0.5260 Acc: 0.7946                                               \n",
      "Epoch 017 | Train Loss: 0.5459 Acc: 0.7636 | Val Loss: 0.5269 Acc: 0.7549                                               \n",
      "Epoch 018 | Train Loss: 0.5413 Acc: 0.7600 | Val Loss: 0.5393 Acc: 0.7788                                               \n",
      "Epoch 019 | Train Loss: 0.5402 Acc: 0.7639 | Val Loss: 0.5768 Acc: 0.7340                                               \n",
      "Epoch 020 | Train Loss: 0.5365 Acc: 0.7687 | Val Loss: 0.5008 Acc: 0.7600                                               \n",
      "Epoch 021 | Train Loss: 0.5127 Acc: 0.7801 | Val Loss: 0.5164 Acc: 0.7791                                               \n",
      "Epoch 022 | Train Loss: 0.5177 Acc: 0.7799 | Val Loss: 0.5331 Acc: 0.7696                                               \n",
      "Epoch 023 | Train Loss: 0.5154 Acc: 0.7798 | Val Loss: 0.4794 Acc: 0.7809                                               \n",
      "Epoch 024 | Train Loss: 0.5288 Acc: 0.7716 | Val Loss: 0.5224 Acc: 0.7406                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 64, 'cnn_dropout': 0.2340092390367473, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 96, 'learning_rate': 0.0005420484524609159, 'lstm_dense': 64, 'lstm_hidden_size': 64, 'lstm_layers': 1, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 17.5499 Acc: 0.4039 | Val Loss: 3.8107 Acc: 0.4591                                              \n",
      "Epoch 002 | Train Loss: 4.0854 Acc: 0.4483 | Val Loss: 1.5047 Acc: 0.4663                                               \n",
      "Epoch 003 | Train Loss: 2.1562 Acc: 0.4873 | Val Loss: 1.2581 Acc: 0.5182                                               \n",
      "Epoch 004 | Train Loss: 1.4473 Acc: 0.5283 | Val Loss: 0.7606 Acc: 0.6012                                               \n",
      "Epoch 005 | Train Loss: 1.0945 Acc: 0.5641 | Val Loss: 0.6771 Acc: 0.7018                                               \n",
      "Epoch 006 | Train Loss: 0.9642 Acc: 0.5927 | Val Loss: 0.7315 Acc: 0.6293                                               \n",
      "Epoch 007 | Train Loss: 0.8842 Acc: 0.6119 | Val Loss: 0.6833 Acc: 0.6475                                               \n",
      "Epoch 008 | Train Loss: 0.8001 Acc: 0.6409 | Val Loss: 0.6658 Acc: 0.6576                                               \n",
      "Epoch 009 | Train Loss: 0.7968 Acc: 0.6396 | Val Loss: 0.6575 Acc: 0.6851                                               \n",
      "Epoch 010 | Train Loss: 0.7602 Acc: 0.6571 | Val Loss: 0.6684 Acc: 0.6782                                               \n",
      "Epoch 011 | Train Loss: 0.7263 Acc: 0.6746 | Val Loss: 0.7778 Acc: 0.6107                                               \n",
      "Epoch 012 | Train Loss: 0.7274 Acc: 0.6733 | Val Loss: 0.6480 Acc: 0.6851                                               \n",
      "Epoch 013 | Train Loss: 0.7037 Acc: 0.6863 | Val Loss: 0.5857 Acc: 0.7116                                               \n",
      "Epoch 014 | Train Loss: 0.6799 Acc: 0.7001 | Val Loss: 0.5683 Acc: 0.7543                                               \n",
      "Epoch 015 | Train Loss: 0.6535 Acc: 0.7143 | Val Loss: 0.7006 Acc: 0.6206                                               \n",
      "Epoch 016 | Train Loss: 0.6914 Acc: 0.6960 | Val Loss: 0.5485 Acc: 0.8140                                               \n",
      "Epoch 017 | Train Loss: 0.6349 Acc: 0.7301 | Val Loss: 0.6727 Acc: 0.6331                                               \n",
      "Epoch 018 | Train Loss: 0.6252 Acc: 0.7317 | Val Loss: 0.5709 Acc: 0.7573                                               \n",
      "Epoch 019 | Train Loss: 0.5985 Acc: 0.7421 | Val Loss: 0.5220 Acc: 0.7824                                               \n",
      "Epoch 020 | Train Loss: 0.5914 Acc: 0.7474 | Val Loss: 0.5191 Acc: 0.7809                                               \n",
      "Epoch 021 | Train Loss: 0.5864 Acc: 0.7464 | Val Loss: 0.5866 Acc: 0.7678                                               \n",
      "Epoch 022 | Train Loss: 0.5631 Acc: 0.7589 | Val Loss: 0.5228 Acc: 0.7609                                               \n",
      "Epoch 023 | Train Loss: 0.5557 Acc: 0.7674 | Val Loss: 0.4971 Acc: 0.7812                                               \n",
      "Epoch 024 | Train Loss: 0.5452 Acc: 0.7658 | Val Loss: 0.4727 Acc: 0.7761                                               \n",
      "Epoch 025 | Train Loss: 0.5403 Acc: 0.7721 | Val Loss: 0.5372 Acc: 0.7254                                               \n",
      "Epoch 026 | Train Loss: 0.5332 Acc: 0.7742 | Val Loss: 0.5603 Acc: 0.7024                                               \n",
      "Epoch 027 | Train Loss: 0.5049 Acc: 0.7824 | Val Loss: 0.4258 Acc: 0.7943                                               \n",
      "Epoch 028 | Train Loss: 0.4955 Acc: 0.7898 | Val Loss: 0.4587 Acc: 0.7776                                               \n",
      "Epoch 029 | Train Loss: 0.4796 Acc: 0.7967 | Val Loss: 0.4138 Acc: 0.8182                                               \n",
      "Epoch 030 | Train Loss: 0.4655 Acc: 0.8030 | Val Loss: 0.4282 Acc: 0.7878                                               \n",
      "Epoch 031 | Train Loss: 0.4643 Acc: 0.8019 | Val Loss: 0.4499 Acc: 0.7531                                               \n",
      "Epoch 032 | Train Loss: 0.4400 Acc: 0.8256 | Val Loss: 0.3345 Acc: 0.8749                                               \n",
      "Epoch 033 | Train Loss: 0.4210 Acc: 0.8309 | Val Loss: 0.3557 Acc: 0.8663                                               \n",
      "Epoch 034 | Train Loss: 0.4266 Acc: 0.8323 | Val Loss: 0.4719 Acc: 0.8018                                               \n",
      "Epoch 035 | Train Loss: 0.4021 Acc: 0.8389 | Val Loss: 0.3296 Acc: 0.8737                                               \n",
      "Epoch 036 | Train Loss: 0.3767 Acc: 0.8501 | Val Loss: 0.4166 Acc: 0.8122                                               \n",
      "Epoch 037 | Train Loss: 0.3686 Acc: 0.8578 | Val Loss: 0.3440 Acc: 0.8731                                               \n",
      "Epoch 038 | Train Loss: 0.3692 Acc: 0.8579 | Val Loss: 0.3342 Acc: 0.8600                                               \n",
      "Epoch 039 | Train Loss: 0.3721 Acc: 0.8637 | Val Loss: 0.3259 Acc: 0.8967                                               \n",
      "Epoch 040 | Train Loss: 0.3364 Acc: 0.8707 | Val Loss: 0.3116 Acc: 0.8803                                               \n",
      "Epoch 041 | Train Loss: 0.3363 Acc: 0.8757 | Val Loss: 0.2869 Acc: 0.8794                                               \n",
      "Epoch 042 | Train Loss: 0.3336 Acc: 0.8801 | Val Loss: 0.2831 Acc: 0.9161                                               \n",
      "Epoch 043 | Train Loss: 0.3194 Acc: 0.8842 | Val Loss: 0.3161 Acc: 0.8719                                               \n",
      "Epoch 044 | Train Loss: 0.3003 Acc: 0.8936 | Val Loss: 0.3970 Acc: 0.8170                                               \n",
      "Epoch 045 | Train Loss: 0.2991 Acc: 0.8906 | Val Loss: 0.2372 Acc: 0.9042                                               \n",
      "Epoch 046 | Train Loss: 0.2893 Acc: 0.8954 | Val Loss: 0.2731 Acc: 0.8803                                               \n",
      "Epoch 047 | Train Loss: 0.2862 Acc: 0.9011 | Val Loss: 0.2584 Acc: 0.8899                                               \n",
      "Epoch 048 | Train Loss: 0.2924 Acc: 0.8972 | Val Loss: 0.3119 Acc: 0.8639                                               \n",
      "Epoch 049 | Train Loss: 0.2675 Acc: 0.9044 | Val Loss: 0.2323 Acc: 0.9400                                               \n",
      "Epoch 050 | Train Loss: 0.2639 Acc: 0.9055 | Val Loss: 0.2578 Acc: 0.8872                                               \n",
      "Epoch 051 | Train Loss: 0.2670 Acc: 0.9054 | Val Loss: 0.2417 Acc: 0.8866                                               \n",
      "Epoch 052 | Train Loss: 0.2722 Acc: 0.9050 | Val Loss: 0.3210 Acc: 0.8681                                               \n",
      "Epoch 053 | Train Loss: 0.2510 Acc: 0.9146 | Val Loss: 0.2512 Acc: 0.8806                                               \n",
      "Epoch 054 | Train Loss: 0.2584 Acc: 0.9097 | Val Loss: 0.2996 Acc: 0.8860                                               \n",
      "Epoch 055 | Train Loss: 0.2614 Acc: 0.9092 | Val Loss: 0.1986 Acc: 0.9349                                               \n",
      "Epoch 056 | Train Loss: 0.2494 Acc: 0.9116 | Val Loss: 0.1934 Acc: 0.9194                                               \n",
      "Epoch 057 | Train Loss: 0.2403 Acc: 0.9162 | Val Loss: 0.2836 Acc: 0.8818                                               \n",
      "Epoch 058 | Train Loss: 0.2451 Acc: 0.9173 | Val Loss: 0.2333 Acc: 0.9212                                               \n",
      "Epoch 059 | Train Loss: 0.2337 Acc: 0.9188 | Val Loss: 0.2690 Acc: 0.8737                                               \n",
      "Epoch 060 | Train Loss: 0.2201 Acc: 0.9230 | Val Loss: 0.3021 Acc: 0.8893                                               \n",
      "Epoch 061 | Train Loss: 0.2431 Acc: 0.9151 | Val Loss: 0.2306 Acc: 0.9107                                               \n",
      "Epoch 062 | Train Loss: 0.2146 Acc: 0.9284 | Val Loss: 0.2267 Acc: 0.9030                                               \n",
      "Epoch 063 | Train Loss: 0.1969 Acc: 0.9332 | Val Loss: 0.2157 Acc: 0.9119                                               \n",
      "Epoch 064 | Train Loss: 0.2076 Acc: 0.9315 | Val Loss: 0.2198 Acc: 0.9215                                               \n",
      "Epoch 065 | Train Loss: 0.1977 Acc: 0.9304 | Val Loss: 0.2036 Acc: 0.9075                                               \n",
      "Epoch 066 | Train Loss: 0.1961 Acc: 0.9337 | Val Loss: 0.1361 Acc: 0.9481                                               \n",
      "Epoch 067 | Train Loss: 0.1928 Acc: 0.9316 | Val Loss: 0.1504 Acc: 0.9537                                               \n",
      "Epoch 068 | Train Loss: 0.1910 Acc: 0.9386 | Val Loss: 0.2117 Acc: 0.9122                                               \n",
      "Epoch 069 | Train Loss: 0.1820 Acc: 0.9372 | Val Loss: 0.2369 Acc: 0.8949                                               \n",
      "Epoch 070 | Train Loss: 0.2032 Acc: 0.9335 | Val Loss: 0.2895 Acc: 0.8994                                               \n",
      "Epoch 071 | Train Loss: 0.1770 Acc: 0.9392 | Val Loss: 0.2673 Acc: 0.9027                                               \n",
      "Epoch 072 | Train Loss: 0.1988 Acc: 0.9345 | Val Loss: 0.2042 Acc: 0.9215                                               \n",
      "Epoch 073 | Train Loss: 0.1811 Acc: 0.9381 | Val Loss: 0.1801 Acc: 0.9230                                               \n",
      "Epoch 074 | Train Loss: 0.1724 Acc: 0.9431 | Val Loss: 0.1823 Acc: 0.9382                                               \n",
      "Epoch 075 | Train Loss: 0.1609 Acc: 0.9454 | Val Loss: 0.2155 Acc: 0.9006                                               \n",
      "Epoch 076 | Train Loss: 0.1756 Acc: 0.9414 | Val Loss: 0.1101 Acc: 0.9648                                               \n",
      "Epoch 077 | Train Loss: 0.1561 Acc: 0.9493 | Val Loss: 0.1906 Acc: 0.9313                                               \n",
      "Epoch 078 | Train Loss: 0.1555 Acc: 0.9469 | Val Loss: 0.1639 Acc: 0.9481                                               \n",
      "Epoch 079 | Train Loss: 0.1627 Acc: 0.9466 | Val Loss: 0.1373 Acc: 0.9493                                               \n",
      "Epoch 080 | Train Loss: 0.1654 Acc: 0.9454 | Val Loss: 0.1805 Acc: 0.9469                                               \n",
      "Epoch 081 | Train Loss: 0.1456 Acc: 0.9518 | Val Loss: 0.1621 Acc: 0.9442                                               \n",
      "Epoch 082 | Train Loss: 0.1538 Acc: 0.9514 | Val Loss: 0.1389 Acc: 0.9457                                               \n",
      "Epoch 083 | Train Loss: 0.1564 Acc: 0.9468 | Val Loss: 0.1810 Acc: 0.9206                                               \n",
      "Epoch 084 | Train Loss: 0.1437 Acc: 0.9512 | Val Loss: 0.1406 Acc: 0.9469                                               \n",
      "Epoch 085 | Train Loss: 0.1429 Acc: 0.9539 | Val Loss: 0.2027 Acc: 0.9403                                               \n",
      "Epoch 086 | Train Loss: 0.1354 Acc: 0.9560 | Val Loss: 0.1392 Acc: 0.9424                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 256, 'cnn_dropout': 0.11832840883873964, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 96, 'learning_rate': 0.0032821666051654867, 'lstm_dense': 64, 'lstm_hidden_size': 64, 'lstm_layers': 1, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 8.0553 Acc: 0.4659 | Val Loss: 0.8331 Acc: 0.5397                                               \n",
      "Epoch 002 | Train Loss: 1.0009 Acc: 0.5626 | Val Loss: 0.8543 Acc: 0.6075                                               \n",
      "Epoch 003 | Train Loss: 0.8193 Acc: 0.6136 | Val Loss: 0.6635 Acc: 0.6988                                               \n",
      "Epoch 004 | Train Loss: 0.7585 Acc: 0.6547 | Val Loss: 0.5766 Acc: 0.7078                                               \n",
      "Epoch 005 | Train Loss: 0.7176 Acc: 0.6798 | Val Loss: 0.6755 Acc: 0.6779                                               \n",
      "Epoch 006 | Train Loss: 0.6922 Acc: 0.6933 | Val Loss: 0.6614 Acc: 0.6519                                               \n",
      "Epoch 007 | Train Loss: 0.6624 Acc: 0.6981 | Val Loss: 0.5980 Acc: 0.7466                                               \n",
      "Epoch 008 | Train Loss: 0.6851 Acc: 0.6929 | Val Loss: 0.6665 Acc: 0.7140                                               \n",
      "Epoch 009 | Train Loss: 0.6829 Acc: 0.6905 | Val Loss: 0.6693 Acc: 0.6388                                               \n",
      "Epoch 010 | Train Loss: 0.6601 Acc: 0.7027 | Val Loss: 0.6112 Acc: 0.7242                                               \n",
      "Epoch 011 | Train Loss: 0.6488 Acc: 0.7042 | Val Loss: 0.6160 Acc: 0.6869                                               \n",
      "Epoch 012 | Train Loss: 0.6936 Acc: 0.6865 | Val Loss: 0.6066 Acc: 0.6866                                               \n",
      "Epoch 013 | Train Loss: 0.6586 Acc: 0.6995 | Val Loss: 0.6615 Acc: 0.6982                                               \n",
      "Epoch 014 | Train Loss: 0.6559 Acc: 0.7028 | Val Loss: 0.5762 Acc: 0.7093                                               \n",
      "Epoch 015 | Train Loss: 0.7358 Acc: 0.6693 | Val Loss: 0.7016 Acc: 0.6979                                               \n",
      "Epoch 016 | Train Loss: 0.7227 Acc: 0.6605 | Val Loss: 0.6699 Acc: 0.6690                                               \n",
      "Epoch 017 | Train Loss: 0.7022 Acc: 0.6736 | Val Loss: 0.6833 Acc: 0.6857                                               \n",
      "Epoch 018 | Train Loss: 0.6784 Acc: 0.6820 | Val Loss: 0.6906 Acc: 0.6857                                               \n",
      "Epoch 019 | Train Loss: 0.6700 Acc: 0.6815 | Val Loss: 0.6769 Acc: 0.6961                                               \n",
      "Epoch 020 | Train Loss: 0.6991 Acc: 0.6699 | Val Loss: 0.6984 Acc: 0.6287                                               \n",
      "Epoch 021 | Train Loss: 0.7016 Acc: 0.6656 | Val Loss: 0.6636 Acc: 0.6848                                               \n",
      "Epoch 022 | Train Loss: 0.6704 Acc: 0.6879 | Val Loss: 0.6210 Acc: 0.6881                                               \n",
      "Epoch 023 | Train Loss: 0.7157 Acc: 0.6669 | Val Loss: 0.7605 Acc: 0.6827                                               \n",
      "Epoch 024 | Train Loss: 0.7040 Acc: 0.6712 | Val Loss: 0.7051 Acc: 0.6699                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 36, 'cnn_dense': 256, 'cnn_dropout': 0.3433870060265756, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 32, 'learning_rate': 0.00015412282913917285, 'lstm_dense': 64, 'lstm_hidden_size': 64, 'lstm_layers': 1, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 32.1386 Acc: 0.3434 | Val Loss: 2.9647 Acc: 0.4749                                              \n",
      "Epoch 002 | Train Loss: 10.5044 Acc: 0.3894 | Val Loss: 2.7241 Acc: 0.5012                                              \n",
      "Epoch 003 | Train Loss: 6.2315 Acc: 0.4143 | Val Loss: 1.9600 Acc: 0.5027                                               \n",
      "Epoch 004 | Train Loss: 4.4345 Acc: 0.4293 | Val Loss: 1.6294 Acc: 0.5409                                               \n",
      "Epoch 005 | Train Loss: 3.3994 Acc: 0.4481 | Val Loss: 1.1642 Acc: 0.5869                                               \n",
      "Epoch 006 | Train Loss: 2.6821 Acc: 0.4638 | Val Loss: 1.3377 Acc: 0.5925                                               \n",
      "Epoch 007 | Train Loss: 2.2429 Acc: 0.4784 | Val Loss: 1.3483 Acc: 0.5752                                               \n",
      "Epoch 008 | Train Loss: 1.9561 Acc: 0.4890 | Val Loss: 1.2998 Acc: 0.5716                                               \n",
      "Epoch 009 | Train Loss: 1.7315 Acc: 0.4883 | Val Loss: 0.9698 Acc: 0.6591                                               \n",
      "Epoch 010 | Train Loss: 1.5177 Acc: 0.5135 | Val Loss: 0.9420 Acc: 0.6421                                               \n",
      "Epoch 011 | Train Loss: 1.3757 Acc: 0.5255 | Val Loss: 0.8234 Acc: 0.6866                                               \n",
      "Epoch 012 | Train Loss: 1.2271 Acc: 0.5365 | Val Loss: 0.7180 Acc: 0.7388                                               \n",
      "Epoch 013 | Train Loss: 1.1481 Acc: 0.5594 | Val Loss: 0.7285 Acc: 0.7272                                               \n",
      "Epoch 014 | Train Loss: 0.9918 Acc: 0.6178 | Val Loss: 0.7523 Acc: 0.6337                                               \n",
      "Epoch 015 | Train Loss: 0.9195 Acc: 0.6423 | Val Loss: 0.6662 Acc: 0.7370                                               \n",
      "Epoch 016 | Train Loss: 0.8463 Acc: 0.6654 | Val Loss: 0.6434 Acc: 0.7182                                               \n",
      "Epoch 017 | Train Loss: 0.7367 Acc: 0.7002 | Val Loss: 0.6108 Acc: 0.7403                                               \n",
      "Epoch 018 | Train Loss: 0.6322 Acc: 0.7490 | Val Loss: 0.5986 Acc: 0.7397                                               \n",
      "Epoch 019 | Train Loss: 0.5692 Acc: 0.7766 | Val Loss: 0.5865 Acc: 0.7528                                               \n",
      "Epoch 020 | Train Loss: 0.5007 Acc: 0.8009 | Val Loss: 0.3213 Acc: 0.8678                                               \n",
      "Epoch 021 | Train Loss: 0.4444 Acc: 0.8266 | Val Loss: 0.3217 Acc: 0.8752                                               \n",
      "Epoch 022 | Train Loss: 0.3924 Acc: 0.8509 | Val Loss: 0.3712 Acc: 0.8415                                               \n",
      "Epoch 023 | Train Loss: 0.3491 Acc: 0.8695 | Val Loss: 0.2934 Acc: 0.8922                                               \n",
      "Epoch 024 | Train Loss: 0.3177 Acc: 0.8818 | Val Loss: 0.3377 Acc: 0.8657                                               \n",
      "Epoch 025 | Train Loss: 0.2893 Acc: 0.8931 | Val Loss: 0.2023 Acc: 0.9272                                               \n",
      "Epoch 026 | Train Loss: 0.2641 Acc: 0.9025 | Val Loss: 0.2080 Acc: 0.9218                                               \n",
      "Epoch 027 | Train Loss: 0.2170 Acc: 0.9228 | Val Loss: 0.2337 Acc: 0.9104                                               \n",
      "Epoch 028 | Train Loss: 0.2221 Acc: 0.9194 | Val Loss: 0.2234 Acc: 0.9203                                               \n",
      "Epoch 029 | Train Loss: 0.1866 Acc: 0.9310 | Val Loss: 0.1864 Acc: 0.9340                                               \n",
      "Epoch 030 | Train Loss: 0.1807 Acc: 0.9350 | Val Loss: 0.2251 Acc: 0.9251                                               \n",
      "Epoch 031 | Train Loss: 0.1694 Acc: 0.9377 | Val Loss: 0.1692 Acc: 0.9424                                               \n",
      "Epoch 032 | Train Loss: 0.1612 Acc: 0.9441 | Val Loss: 0.1328 Acc: 0.9531                                               \n",
      "Epoch 033 | Train Loss: 0.1481 Acc: 0.9472 | Val Loss: 0.1775 Acc: 0.9379                                               \n",
      "Epoch 034 | Train Loss: 0.1336 Acc: 0.9512 | Val Loss: 0.1604 Acc: 0.9424                                               \n",
      "Epoch 035 | Train Loss: 0.1231 Acc: 0.9564 | Val Loss: 0.1791 Acc: 0.9331                                               \n",
      "Epoch 036 | Train Loss: 0.1118 Acc: 0.9595 | Val Loss: 0.1251 Acc: 0.9534                                               \n",
      "Epoch 037 | Train Loss: 0.1256 Acc: 0.9559 | Val Loss: 0.1230 Acc: 0.9555                                               \n",
      "Epoch 038 | Train Loss: 0.1076 Acc: 0.9616 | Val Loss: 0.1421 Acc: 0.9481                                               \n",
      "Epoch 039 | Train Loss: 0.1010 Acc: 0.9658 | Val Loss: 0.1544 Acc: 0.9478                                               \n",
      "Epoch 040 | Train Loss: 0.0982 Acc: 0.9653 | Val Loss: 0.1024 Acc: 0.9627                                               \n",
      "Epoch 041 | Train Loss: 0.0905 Acc: 0.9681 | Val Loss: 0.0999 Acc: 0.9654                                               \n",
      "Epoch 042 | Train Loss: 0.0837 Acc: 0.9699 | Val Loss: 0.0998 Acc: 0.9651                                               \n",
      "Epoch 043 | Train Loss: 0.0772 Acc: 0.9733 | Val Loss: 0.0845 Acc: 0.9701                                               \n",
      "Epoch 044 | Train Loss: 0.0819 Acc: 0.9725 | Val Loss: 0.0801 Acc: 0.9710                                               \n",
      "Epoch 045 | Train Loss: 0.0710 Acc: 0.9750 | Val Loss: 0.0649 Acc: 0.9758                                               \n",
      "Epoch 046 | Train Loss: 0.0686 Acc: 0.9780 | Val Loss: 0.0794 Acc: 0.9696                                               \n",
      "Epoch 047 | Train Loss: 0.0683 Acc: 0.9771 | Val Loss: 0.0712 Acc: 0.9770                                               \n",
      "Epoch 048 | Train Loss: 0.0635 Acc: 0.9793 | Val Loss: 0.1519 Acc: 0.9501                                               \n",
      "Epoch 049 | Train Loss: 0.0641 Acc: 0.9797 | Val Loss: 0.0682 Acc: 0.9776                                               \n",
      "Epoch 050 | Train Loss: 0.0537 Acc: 0.9807 | Val Loss: 0.0735 Acc: 0.9779                                               \n",
      "Epoch 051 | Train Loss: 0.0543 Acc: 0.9819 | Val Loss: 0.1591 Acc: 0.9451                                               \n",
      "Epoch 052 | Train Loss: 0.0586 Acc: 0.9810 | Val Loss: 0.1074 Acc: 0.9651                                               \n",
      "Epoch 053 | Train Loss: 0.0536 Acc: 0.9819 | Val Loss: 0.0862 Acc: 0.9707                                               \n",
      "Epoch 054 | Train Loss: 0.0467 Acc: 0.9837 | Val Loss: 0.0589 Acc: 0.9794                                               \n",
      "Epoch 055 | Train Loss: 0.0403 Acc: 0.9874 | Val Loss: 0.0689 Acc: 0.9785                                               \n",
      "Epoch 056 | Train Loss: 0.0594 Acc: 0.9812 | Val Loss: 0.0731 Acc: 0.9731                                               \n",
      "Epoch 057 | Train Loss: 0.0377 Acc: 0.9869 | Val Loss: 0.0578 Acc: 0.9800                                               \n",
      "Epoch 058 | Train Loss: 0.0391 Acc: 0.9876 | Val Loss: 0.0629 Acc: 0.9803                                               \n",
      "Epoch 059 | Train Loss: 0.0430 Acc: 0.9860 | Val Loss: 0.0671 Acc: 0.9791                                               \n",
      "Epoch 060 | Train Loss: 0.0451 Acc: 0.9855 | Val Loss: 0.0714 Acc: 0.9797                                               \n",
      "Epoch 061 | Train Loss: 0.0337 Acc: 0.9884 | Val Loss: 0.0759 Acc: 0.9767                                               \n",
      "Epoch 062 | Train Loss: 0.0371 Acc: 0.9880 | Val Loss: 0.0721 Acc: 0.9761                                               \n",
      "Epoch 063 | Train Loss: 0.0351 Acc: 0.9891 | Val Loss: 0.0556 Acc: 0.9806                                               \n",
      "Epoch 064 | Train Loss: 0.0328 Acc: 0.9897 | Val Loss: 0.0879 Acc: 0.9752                                               \n",
      "Epoch 065 | Train Loss: 0.0341 Acc: 0.9898 | Val Loss: 0.0853 Acc: 0.9725                                               \n",
      "Epoch 066 | Train Loss: 0.0386 Acc: 0.9878 | Val Loss: 0.0669 Acc: 0.9791                                               \n",
      "Epoch 067 | Train Loss: 0.0367 Acc: 0.9884 | Val Loss: 0.0803 Acc: 0.9767                                               \n",
      "Epoch 068 | Train Loss: 0.0313 Acc: 0.9896 | Val Loss: 0.0930 Acc: 0.9710                                               \n",
      "Epoch 069 | Train Loss: 0.0281 Acc: 0.9910 | Val Loss: 0.0472 Acc: 0.9842                                               \n",
      "Epoch 070 | Train Loss: 0.0297 Acc: 0.9910 | Val Loss: 0.0749 Acc: 0.9788                                               \n",
      "Epoch 071 | Train Loss: 0.0304 Acc: 0.9898 | Val Loss: 0.0593 Acc: 0.9857                                               \n",
      "Epoch 072 | Train Loss: 0.0236 Acc: 0.9921 | Val Loss: 0.1168 Acc: 0.9693                                               \n",
      "Epoch 073 | Train Loss: 0.0290 Acc: 0.9910 | Val Loss: 0.0553 Acc: 0.9854                                               \n",
      "Epoch 074 | Train Loss: 0.0257 Acc: 0.9901 | Val Loss: 0.0548 Acc: 0.9851                                               \n",
      "Epoch 075 | Train Loss: 0.0289 Acc: 0.9905 | Val Loss: 0.0634 Acc: 0.9806                                               \n",
      "Epoch 076 | Train Loss: 0.0254 Acc: 0.9916 | Val Loss: 0.0485 Acc: 0.9863                                               \n",
      "Epoch 077 | Train Loss: 0.0229 Acc: 0.9924 | Val Loss: 0.0467 Acc: 0.9857                                               \n",
      "Epoch 078 | Train Loss: 0.0218 Acc: 0.9922 | Val Loss: 0.0532 Acc: 0.9845                                               \n",
      "Epoch 079 | Train Loss: 0.0233 Acc: 0.9927 | Val Loss: 0.0552 Acc: 0.9836                                               \n",
      "Epoch 080 | Train Loss: 0.0240 Acc: 0.9925 | Val Loss: 0.0528 Acc: 0.9854                                               \n",
      "Epoch 081 | Train Loss: 0.0202 Acc: 0.9932 | Val Loss: 0.0653 Acc: 0.9818                                               \n",
      "Epoch 082 | Train Loss: 0.0170 Acc: 0.9950 | Val Loss: 0.0854 Acc: 0.9764                                               \n",
      "Epoch 083 | Train Loss: 0.0250 Acc: 0.9925 | Val Loss: 0.0607 Acc: 0.9824                                               \n",
      "Epoch 084 | Train Loss: 0.0259 Acc: 0.9920 | Val Loss: 0.0606 Acc: 0.9827                                               \n",
      "Epoch 085 | Train Loss: 0.0211 Acc: 0.9926 | Val Loss: 0.0892 Acc: 0.9755                                               \n",
      "Epoch 086 | Train Loss: 0.0153 Acc: 0.9957 | Val Loss: 0.0834 Acc: 0.9794                                               \n",
      "Epoch 087 | Train Loss: 0.0276 Acc: 0.9907 | Val Loss: 0.0610 Acc: 0.9851                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 36, 'cnn_dense': 256, 'cnn_dropout': 0.38661600603918106, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 32, 'learning_rate': 6.79715216090652e-05, 'lstm_dense': 128, 'lstm_hidden_size': 64, 'lstm_layers': 1, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 63.1679 Acc: 0.2949 | Val Loss: 16.8338 Acc: 0.3887                                             \n",
      "Epoch 002 | Train Loss: 23.8248 Acc: 0.3497 | Val Loss: 11.8002 Acc: 0.3887                                             \n",
      "Epoch 003 | Train Loss: 15.8747 Acc: 0.3806 | Val Loss: 7.7740 Acc: 0.4460                                              \n",
      "Epoch 004 | Train Loss: 12.3308 Acc: 0.3873 | Val Loss: 4.9407 Acc: 0.4579                                              \n",
      "Epoch 005 | Train Loss: 10.0029 Acc: 0.4000 | Val Loss: 7.9573 Acc: 0.4821                                              \n",
      "Epoch 006 | Train Loss: 8.4067 Acc: 0.4098 | Val Loss: 4.6959 Acc: 0.5015                                               \n",
      "Epoch 007 | Train Loss: 7.0419 Acc: 0.4278 | Val Loss: 3.0210 Acc: 0.5179                                               \n",
      "Epoch 008 | Train Loss: 6.2153 Acc: 0.4270 | Val Loss: 3.1224 Acc: 0.4797                                               \n",
      "Epoch 009 | Train Loss: 5.3244 Acc: 0.4329 | Val Loss: 3.4147 Acc: 0.4869                                               \n",
      "Epoch 010 | Train Loss: 4.6280 Acc: 0.4418 | Val Loss: 1.3434 Acc: 0.5534                                               \n",
      "Epoch 011 | Train Loss: 4.1848 Acc: 0.4462 | Val Loss: 1.5757 Acc: 0.5101                                               \n",
      "Epoch 012 | Train Loss: 3.6703 Acc: 0.4588 | Val Loss: 1.2490 Acc: 0.5296                                               \n",
      "Epoch 013 | Train Loss: 3.2622 Acc: 0.4619 | Val Loss: 1.0682 Acc: 0.5699                                               \n",
      "Epoch 014 | Train Loss: 2.9990 Acc: 0.4570 | Val Loss: 1.3859 Acc: 0.5800                                               \n",
      "Epoch 015 | Train Loss: 2.6025 Acc: 0.4750 | Val Loss: 1.1055 Acc: 0.5430                                               \n",
      "Epoch 016 | Train Loss: 2.3611 Acc: 0.4768 | Val Loss: 1.0865 Acc: 0.5800                                               \n",
      "Epoch 017 | Train Loss: 2.1809 Acc: 0.4832 | Val Loss: 0.9602 Acc: 0.6039                                               \n",
      "Epoch 018 | Train Loss: 2.0280 Acc: 0.4924 | Val Loss: 1.1412 Acc: 0.5904                                               \n",
      "Epoch 019 | Train Loss: 1.8365 Acc: 0.4980 | Val Loss: 0.8751 Acc: 0.6155                                               \n",
      "Epoch 020 | Train Loss: 1.6858 Acc: 0.5138 | Val Loss: 0.9004 Acc: 0.5254                                               \n",
      "Epoch 021 | Train Loss: 1.5940 Acc: 0.5090 | Val Loss: 0.9111 Acc: 0.6158                                               \n",
      "Epoch 022 | Train Loss: 1.5044 Acc: 0.5221 | Val Loss: 0.8369 Acc: 0.6081                                               \n",
      "Epoch 023 | Train Loss: 1.3874 Acc: 0.5319 | Val Loss: 0.7841 Acc: 0.6618                                               \n",
      "Epoch 024 | Train Loss: 1.3412 Acc: 0.5300 | Val Loss: 0.8605 Acc: 0.6081                                               \n",
      "Epoch 025 | Train Loss: 1.2491 Acc: 0.5356 | Val Loss: 0.7525 Acc: 0.6722                                               \n",
      "Epoch 026 | Train Loss: 1.2030 Acc: 0.5514 | Val Loss: 0.7545 Acc: 0.6719                                               \n",
      "Epoch 027 | Train Loss: 1.1185 Acc: 0.5727 | Val Loss: 0.7365 Acc: 0.6627                                               \n",
      "Epoch 028 | Train Loss: 1.0606 Acc: 0.5816 | Val Loss: 0.7290 Acc: 0.6669                                               \n",
      "Epoch 029 | Train Loss: 0.9977 Acc: 0.6016 | Val Loss: 0.7094 Acc: 0.6830                                               \n",
      "Epoch 030 | Train Loss: 0.9653 Acc: 0.6183 | Val Loss: 0.8247 Acc: 0.6549                                               \n",
      "Epoch 031 | Train Loss: 0.9419 Acc: 0.6169 | Val Loss: 0.6646 Acc: 0.7063                                               \n",
      "Epoch 032 | Train Loss: 0.8847 Acc: 0.6410 | Val Loss: 0.6978 Acc: 0.6791                                               \n",
      "Epoch 033 | Train Loss: 0.8497 Acc: 0.6503 | Val Loss: 0.6165 Acc: 0.7221                                               \n",
      "Epoch 034 | Train Loss: 0.7958 Acc: 0.6739 | Val Loss: 0.5960 Acc: 0.7072                                               \n",
      "Epoch 035 | Train Loss: 0.7506 Acc: 0.6915 | Val Loss: 0.5616 Acc: 0.7657                                               \n",
      "Epoch 036 | Train Loss: 0.7219 Acc: 0.7060 | Val Loss: 0.5919 Acc: 0.7173                                               \n",
      "Epoch 037 | Train Loss: 0.7065 Acc: 0.7139 | Val Loss: 0.6430 Acc: 0.7090                                               \n",
      "Epoch 038 | Train Loss: 0.6624 Acc: 0.7379 | Val Loss: 0.5232 Acc: 0.7797                                               \n",
      "Epoch 039 | Train Loss: 0.6260 Acc: 0.7531 | Val Loss: 0.5977 Acc: 0.7612                                               \n",
      "Epoch 040 | Train Loss: 0.5917 Acc: 0.7639 | Val Loss: 0.4466 Acc: 0.8257                                               \n",
      "Epoch 041 | Train Loss: 0.5527 Acc: 0.7842 | Val Loss: 0.4919 Acc: 0.8006                                               \n",
      "Epoch 042 | Train Loss: 0.4976 Acc: 0.8124 | Val Loss: 0.3704 Acc: 0.8594                                               \n",
      "Epoch 043 | Train Loss: 0.4662 Acc: 0.8263 | Val Loss: 0.3407 Acc: 0.8785                                               \n",
      "Epoch 044 | Train Loss: 0.4457 Acc: 0.8325 | Val Loss: 0.3034 Acc: 0.8881                                               \n",
      "Epoch 045 | Train Loss: 0.4094 Acc: 0.8517 | Val Loss: 0.3669 Acc: 0.8672                                               \n",
      "Epoch 046 | Train Loss: 0.3932 Acc: 0.8554 | Val Loss: 0.2771 Acc: 0.9006                                               \n",
      "Epoch 047 | Train Loss: 0.3631 Acc: 0.8695 | Val Loss: 0.3171 Acc: 0.8770                                               \n",
      "Epoch 048 | Train Loss: 0.3529 Acc: 0.8716 | Val Loss: 0.2826 Acc: 0.8946                                               \n",
      "Epoch 049 | Train Loss: 0.3218 Acc: 0.8865 | Val Loss: 0.2366 Acc: 0.9179                                               \n",
      "Epoch 050 | Train Loss: 0.2978 Acc: 0.8982 | Val Loss: 0.2363 Acc: 0.9218                                               \n",
      "Epoch 051 | Train Loss: 0.2819 Acc: 0.9002 | Val Loss: 0.2075 Acc: 0.9233                                               \n",
      "Epoch 052 | Train Loss: 0.2726 Acc: 0.9019 | Val Loss: 0.2385 Acc: 0.9146                                               \n",
      "Epoch 053 | Train Loss: 0.2627 Acc: 0.9070 | Val Loss: 0.1885 Acc: 0.9358                                               \n",
      "Epoch 054 | Train Loss: 0.2346 Acc: 0.9172 | Val Loss: 0.2049 Acc: 0.9316                                               \n",
      "Epoch 055 | Train Loss: 0.2339 Acc: 0.9197 | Val Loss: 0.1841 Acc: 0.9361                                               \n",
      "Epoch 056 | Train Loss: 0.2325 Acc: 0.9186 | Val Loss: 0.2744 Acc: 0.8979                                               \n",
      "Epoch 057 | Train Loss: 0.2110 Acc: 0.9241 | Val Loss: 0.1678 Acc: 0.9418                                               \n",
      "Epoch 058 | Train Loss: 0.2067 Acc: 0.9276 | Val Loss: 0.1887 Acc: 0.9346                                               \n",
      "Epoch 059 | Train Loss: 0.1841 Acc: 0.9381 | Val Loss: 0.1784 Acc: 0.9421                                               \n",
      "Epoch 060 | Train Loss: 0.1875 Acc: 0.9344 | Val Loss: 0.1829 Acc: 0.9367                                               \n",
      "Epoch 061 | Train Loss: 0.1865 Acc: 0.9387 | Val Loss: 0.1664 Acc: 0.9475                                               \n",
      "Epoch 062 | Train Loss: 0.1720 Acc: 0.9404 | Val Loss: 0.1618 Acc: 0.9463                                               \n",
      "Epoch 063 | Train Loss: 0.1521 Acc: 0.9466 | Val Loss: 0.1348 Acc: 0.9543                                               \n",
      "Epoch 064 | Train Loss: 0.1677 Acc: 0.9433 | Val Loss: 0.1664 Acc: 0.9460                                               \n",
      "Epoch 065 | Train Loss: 0.1566 Acc: 0.9466 | Val Loss: 0.1426 Acc: 0.9546                                               \n",
      "Epoch 066 | Train Loss: 0.1425 Acc: 0.9507 | Val Loss: 0.1533 Acc: 0.9487                                               \n",
      "Epoch 067 | Train Loss: 0.1368 Acc: 0.9545 | Val Loss: 0.1320 Acc: 0.9594                                               \n",
      "Epoch 068 | Train Loss: 0.1316 Acc: 0.9563 | Val Loss: 0.1695 Acc: 0.9385                                               \n",
      "Epoch 069 | Train Loss: 0.1332 Acc: 0.9554 | Val Loss: 0.1291 Acc: 0.9561                                               \n",
      "Epoch 070 | Train Loss: 0.1272 Acc: 0.9554 | Val Loss: 0.1119 Acc: 0.9615                                               \n",
      "Epoch 071 | Train Loss: 0.1298 Acc: 0.9540 | Val Loss: 0.1317 Acc: 0.9597                                               \n",
      "Epoch 072 | Train Loss: 0.1277 Acc: 0.9569 | Val Loss: 0.1445 Acc: 0.9510                                               \n",
      "Epoch 073 | Train Loss: 0.1112 Acc: 0.9620 | Val Loss: 0.1654 Acc: 0.9469                                               \n",
      "Epoch 074 | Train Loss: 0.1107 Acc: 0.9628 | Val Loss: 0.1274 Acc: 0.9582                                               \n",
      "Epoch 075 | Train Loss: 0.1141 Acc: 0.9604 | Val Loss: 0.1012 Acc: 0.9678                                               \n",
      "Epoch 076 | Train Loss: 0.1109 Acc: 0.9638 | Val Loss: 0.1019 Acc: 0.9690                                               \n",
      "Epoch 077 | Train Loss: 0.1078 Acc: 0.9640 | Val Loss: 0.1022 Acc: 0.9666                                               \n",
      "Epoch 078 | Train Loss: 0.0935 Acc: 0.9697 | Val Loss: 0.1368 Acc: 0.9528                                               \n",
      "Epoch 079 | Train Loss: 0.0978 Acc: 0.9660 | Val Loss: 0.1261 Acc: 0.9579                                               \n",
      "Epoch 080 | Train Loss: 0.0879 Acc: 0.9699 | Val Loss: 0.0944 Acc: 0.9675                                               \n",
      "Epoch 081 | Train Loss: 0.0877 Acc: 0.9707 | Val Loss: 0.1122 Acc: 0.9612                                               \n",
      "Epoch 082 | Train Loss: 0.0814 Acc: 0.9740 | Val Loss: 0.1055 Acc: 0.9675                                               \n",
      "Epoch 083 | Train Loss: 0.0873 Acc: 0.9719 | Val Loss: 0.1206 Acc: 0.9627                                               \n",
      "Epoch 084 | Train Loss: 0.0846 Acc: 0.9727 | Val Loss: 0.1299 Acc: 0.9558                                               \n",
      "Epoch 085 | Train Loss: 0.0839 Acc: 0.9729 | Val Loss: 0.1188 Acc: 0.9600                                               \n",
      "Epoch 086 | Train Loss: 0.0734 Acc: 0.9768 | Val Loss: 0.0925 Acc: 0.9704                                               \n",
      "Epoch 087 | Train Loss: 0.0729 Acc: 0.9755 | Val Loss: 0.1152 Acc: 0.9612                                               \n",
      "Epoch 088 | Train Loss: 0.0760 Acc: 0.9747 | Val Loss: 0.0855 Acc: 0.9743                                               \n",
      "Epoch 089 | Train Loss: 0.0756 Acc: 0.9745 | Val Loss: 0.1200 Acc: 0.9645                                               \n",
      "Epoch 090 | Train Loss: 0.0576 Acc: 0.9806 | Val Loss: 0.1157 Acc: 0.9648                                               \n",
      "Epoch 091 | Train Loss: 0.0647 Acc: 0.9796 | Val Loss: 0.0978 Acc: 0.9669                                               \n",
      "Epoch 092 | Train Loss: 0.0582 Acc: 0.9824 | Val Loss: 0.0873 Acc: 0.9767                                               \n",
      "Epoch 093 | Train Loss: 0.0656 Acc: 0.9781 | Val Loss: 0.0860 Acc: 0.9731                                               \n",
      "Epoch 094 | Train Loss: 0.0612 Acc: 0.9789 | Val Loss: 0.1106 Acc: 0.9672                                               \n",
      "Epoch 095 | Train Loss: 0.0571 Acc: 0.9807 | Val Loss: 0.0993 Acc: 0.9707                                               \n",
      "Epoch 096 | Train Loss: 0.0554 Acc: 0.9828 | Val Loss: 0.0824 Acc: 0.9743                                               \n",
      "Epoch 097 | Train Loss: 0.0652 Acc: 0.9780 | Val Loss: 0.0846 Acc: 0.9770                                               \n",
      "Epoch 098 | Train Loss: 0.0531 Acc: 0.9839 | Val Loss: 0.0963 Acc: 0.9710                                               \n",
      "Epoch 099 | Train Loss: 0.0580 Acc: 0.9803 | Val Loss: 0.0824 Acc: 0.9761                                               \n",
      "Epoch 100 | Train Loss: 0.0474 Acc: 0.9842 | Val Loss: 0.1029 Acc: 0.9707                                               \n",
      "Epoch 101 | Train Loss: 0.0463 Acc: 0.9857 | Val Loss: 0.1171 Acc: 0.9663                                               \n",
      "Epoch 102 | Train Loss: 0.0476 Acc: 0.9834 | Val Loss: 0.1045 Acc: 0.9707                                               \n",
      "Epoch 103 | Train Loss: 0.0461 Acc: 0.9854 | Val Loss: 0.0971 Acc: 0.9701                                               \n",
      "Epoch 104 | Train Loss: 0.0451 Acc: 0.9851 | Val Loss: 0.1319 Acc: 0.9618                                               \n",
      "Epoch 105 | Train Loss: 0.0475 Acc: 0.9848 | Val Loss: 0.0827 Acc: 0.9755                                               \n",
      "Epoch 106 | Train Loss: 0.0454 Acc: 0.9854 | Val Loss: 0.0950 Acc: 0.9734                                               \n",
      "Epoch 107 | Train Loss: 0.0397 Acc: 0.9878 | Val Loss: 0.0855 Acc: 0.9755                                               \n",
      "Epoch 108 | Train Loss: 0.0496 Acc: 0.9837 | Val Loss: 0.0642 Acc: 0.9794                                               \n",
      "Epoch 109 | Train Loss: 0.0372 Acc: 0.9878 | Val Loss: 0.0954 Acc: 0.9725                                               \n",
      "Epoch 110 | Train Loss: 0.0453 Acc: 0.9850 | Val Loss: 0.0841 Acc: 0.9728                                               \n",
      "Epoch 111 | Train Loss: 0.0395 Acc: 0.9877 | Val Loss: 0.0658 Acc: 0.9806                                               \n",
      "Epoch 112 | Train Loss: 0.0386 Acc: 0.9875 | Val Loss: 0.0761 Acc: 0.9749                                               \n",
      "Epoch 113 | Train Loss: 0.0406 Acc: 0.9866 | Val Loss: 0.0860 Acc: 0.9758                                               \n",
      "Epoch 114 | Train Loss: 0.0380 Acc: 0.9878 | Val Loss: 0.0783 Acc: 0.9773                                               \n",
      "Epoch 115 | Train Loss: 0.0344 Acc: 0.9888 | Val Loss: 0.0722 Acc: 0.9782                                               \n",
      "Epoch 116 | Train Loss: 0.0348 Acc: 0.9890 | Val Loss: 0.0711 Acc: 0.9785                                               \n",
      "Epoch 117 | Train Loss: 0.0316 Acc: 0.9904 | Val Loss: 0.0806 Acc: 0.9782                                               \n",
      "Epoch 118 | Train Loss: 0.0336 Acc: 0.9881 | Val Loss: 0.0865 Acc: 0.9761                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 36, 'cnn_dense': 64, 'cnn_dropout': 0.6977051371413565, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 32, 'learning_rate': 1.0325722997905299e-05, 'lstm_dense': 64, 'lstm_hidden_size': 64, 'lstm_layers': 1, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 97.8894 Acc: 0.3389 | Val Loss: 53.0926 Acc: 0.2710                                             \n",
      "Epoch 002 | Train Loss: 66.1932 Acc: 0.3290 | Val Loss: 28.2129 Acc: 0.4143                                             \n",
      "Epoch 003 | Train Loss: 51.5989 Acc: 0.3202 | Val Loss: 18.4420 Acc: 0.3893                                             \n",
      "Epoch 004 | Train Loss: 41.4385 Acc: 0.3120 | Val Loss: 13.8678 Acc: 0.4373                                             \n",
      "Epoch 005 | Train Loss: 33.4039 Acc: 0.3220 | Val Loss: 10.6144 Acc: 0.3522                                             \n",
      "Epoch 006 | Train Loss: 28.3325 Acc: 0.3246 | Val Loss: 7.6462 Acc: 0.4415                                              \n",
      "Epoch 007 | Train Loss: 23.8046 Acc: 0.3324 | Val Loss: 6.1744 Acc: 0.4146                                              \n",
      "Epoch 008 | Train Loss: 20.4095 Acc: 0.3430 | Val Loss: 4.2384 Acc: 0.4409                                              \n",
      "Epoch 009 | Train Loss: 18.4287 Acc: 0.3485 | Val Loss: 3.8097 Acc: 0.4063                                              \n",
      "Epoch 010 | Train Loss: 16.2097 Acc: 0.3547 | Val Loss: 4.1709 Acc: 0.4454                                              \n",
      "Epoch 011 | Train Loss: 15.1386 Acc: 0.3670 | Val Loss: 3.7071 Acc: 0.4872                                              \n",
      "Epoch 012 | Train Loss: 14.0821 Acc: 0.3655 | Val Loss: 3.5312 Acc: 0.4866                                              \n",
      "Epoch 013 | Train Loss: 13.1613 Acc: 0.3644 | Val Loss: 2.9937 Acc: 0.4812                                              \n",
      "Epoch 014 | Train Loss: 12.1690 Acc: 0.3711 | Val Loss: 2.6219 Acc: 0.5567                                              \n",
      "Epoch 015 | Train Loss: 11.4404 Acc: 0.3745 | Val Loss: 2.7476 Acc: 0.4893                                              \n",
      "Epoch 016 | Train Loss: 10.7235 Acc: 0.3798 | Val Loss: 2.7139 Acc: 0.4669                                              \n",
      "Epoch 017 | Train Loss: 10.1208 Acc: 0.3776 | Val Loss: 2.4331 Acc: 0.4719                                              \n",
      "Epoch 018 | Train Loss: 9.6010 Acc: 0.3849 | Val Loss: 2.0203 Acc: 0.5496                                               \n",
      "Epoch 019 | Train Loss: 9.1330 Acc: 0.3880 | Val Loss: 2.0021 Acc: 0.5904                                               \n",
      "Epoch 020 | Train Loss: 8.6673 Acc: 0.3921 | Val Loss: 1.9757 Acc: 0.5215                                               \n",
      "Epoch 021 | Train Loss: 8.1240 Acc: 0.3874 | Val Loss: 1.8321 Acc: 0.6003                                               \n",
      "Epoch 022 | Train Loss: 7.9651 Acc: 0.3873 | Val Loss: 1.8062 Acc: 0.5337                                               \n",
      "Epoch 023 | Train Loss: 7.4733 Acc: 0.3918 | Val Loss: 2.0028 Acc: 0.5397                                               \n",
      "Epoch 024 | Train Loss: 7.2263 Acc: 0.3928 | Val Loss: 1.7389 Acc: 0.5809                                               \n",
      "Epoch 025 | Train Loss: 7.0415 Acc: 0.3945 | Val Loss: 1.8757 Acc: 0.5522                                               \n",
      "Epoch 026 | Train Loss: 6.7613 Acc: 0.3957 | Val Loss: 1.9134 Acc: 0.5785                                               \n",
      "Epoch 027 | Train Loss: 6.3666 Acc: 0.4059 | Val Loss: 1.6185 Acc: 0.5122                                               \n",
      "Epoch 028 | Train Loss: 6.1145 Acc: 0.3978 | Val Loss: 1.6325 Acc: 0.5860                                               \n",
      "Epoch 029 | Train Loss: 5.9022 Acc: 0.4023 | Val Loss: 1.5960 Acc: 0.5627                                               \n",
      "Epoch 030 | Train Loss: 5.5343 Acc: 0.4077 | Val Loss: 1.6041 Acc: 0.5800                                               \n",
      "Epoch 031 | Train Loss: 5.5447 Acc: 0.3997 | Val Loss: 1.5246 Acc: 0.5854                                               \n",
      "Epoch 032 | Train Loss: 5.3474 Acc: 0.4004 | Val Loss: 1.5636 Acc: 0.6087                                               \n",
      "Epoch 033 | Train Loss: 5.0288 Acc: 0.4115 | Val Loss: 1.4737 Acc: 0.5684                                               \n",
      "Epoch 034 | Train Loss: 4.8528 Acc: 0.4181 | Val Loss: 1.5215 Acc: 0.5943                                               \n",
      "Epoch 035 | Train Loss: 4.6362 Acc: 0.4221 | Val Loss: 1.5403 Acc: 0.6006                                               \n",
      "Epoch 036 | Train Loss: 4.6847 Acc: 0.4150 | Val Loss: 1.4225 Acc: 0.5466                                               \n",
      "Epoch 037 | Train Loss: 4.4964 Acc: 0.4117 | Val Loss: 1.4294 Acc: 0.6233                                               \n",
      "Epoch 038 | Train Loss: 4.3312 Acc: 0.4268 | Val Loss: 1.3796 Acc: 0.5791                                               \n",
      "Epoch 039 | Train Loss: 4.2561 Acc: 0.4213 | Val Loss: 1.5374 Acc: 0.5251                                               \n",
      "Epoch 040 | Train Loss: 4.0860 Acc: 0.4220 | Val Loss: 1.4349 Acc: 0.5457                                               \n",
      "Epoch 041 | Train Loss: 3.9815 Acc: 0.4186 | Val Loss: 1.3897 Acc: 0.5460                                               \n",
      "Epoch 042 | Train Loss: 3.8471 Acc: 0.4294 | Val Loss: 1.3076 Acc: 0.5484                                               \n",
      "Epoch 043 | Train Loss: 3.7275 Acc: 0.4282 | Val Loss: 1.3423 Acc: 0.5304                                               \n",
      "Epoch 044 | Train Loss: 3.5200 Acc: 0.4364 | Val Loss: 1.2221 Acc: 0.5955                                               \n",
      "Epoch 045 | Train Loss: 3.4962 Acc: 0.4361 | Val Loss: 1.2246 Acc: 0.5818                                               \n",
      "Epoch 046 | Train Loss: 3.3562 Acc: 0.4440 | Val Loss: 1.4971 Acc: 0.5597                                               \n",
      "Epoch 047 | Train Loss: 3.2385 Acc: 0.4456 | Val Loss: 1.1966 Acc: 0.5779                                               \n",
      "Epoch 048 | Train Loss: 3.1762 Acc: 0.4437 | Val Loss: 1.1893 Acc: 0.5752                                               \n",
      "Epoch 049 | Train Loss: 3.0978 Acc: 0.4444 | Val Loss: 1.2711 Acc: 0.5734                                               \n",
      "Epoch 050 | Train Loss: 2.9842 Acc: 0.4581 | Val Loss: 1.3246 Acc: 0.5642                                               \n",
      "Epoch 051 | Train Loss: 2.9562 Acc: 0.4466 | Val Loss: 1.1862 Acc: 0.5597                                               \n",
      "Epoch 052 | Train Loss: 2.8032 Acc: 0.4568 | Val Loss: 1.2172 Acc: 0.5519                                               \n",
      "Epoch 053 | Train Loss: 2.7274 Acc: 0.4628 | Val Loss: 1.4022 Acc: 0.5624                                               \n",
      "Epoch 054 | Train Loss: 2.7255 Acc: 0.4551 | Val Loss: 1.2967 Acc: 0.5669                                               \n",
      "Epoch 055 | Train Loss: 2.7241 Acc: 0.4637 | Val Loss: 1.2022 Acc: 0.5561                                               \n",
      "Epoch 056 | Train Loss: 2.5254 Acc: 0.4660 | Val Loss: 1.1043 Acc: 0.5687                                               \n",
      "Epoch 057 | Train Loss: 2.4716 Acc: 0.4753 | Val Loss: 1.0083 Acc: 0.5946                                               \n",
      "Epoch 058 | Train Loss: 2.4377 Acc: 0.4682 | Val Loss: 1.1274 Acc: 0.5851                                               \n",
      "Epoch 059 | Train Loss: 2.3682 Acc: 0.4738 | Val Loss: 0.9971 Acc: 0.5982                                               \n",
      "Epoch 060 | Train Loss: 2.3054 Acc: 0.4762 | Val Loss: 1.1036 Acc: 0.5824                                               \n",
      "Epoch 061 | Train Loss: 2.2410 Acc: 0.4836 | Val Loss: 0.9725 Acc: 0.6084                                               \n",
      "Epoch 062 | Train Loss: 2.2361 Acc: 0.4881 | Val Loss: 1.1785 Acc: 0.5764                                               \n",
      "Epoch 063 | Train Loss: 2.2077 Acc: 0.4900 | Val Loss: 1.0495 Acc: 0.5824                                               \n",
      "Epoch 064 | Train Loss: 2.1103 Acc: 0.4876 | Val Loss: 1.0167 Acc: 0.5770                                               \n",
      "Epoch 065 | Train Loss: 2.0931 Acc: 0.4863 | Val Loss: 0.9975 Acc: 0.6054                                               \n",
      "Epoch 066 | Train Loss: 2.0622 Acc: 0.4950 | Val Loss: 1.0404 Acc: 0.5821                                               \n",
      "Epoch 067 | Train Loss: 2.0091 Acc: 0.4939 | Val Loss: 0.9531 Acc: 0.5913                                               \n",
      "Epoch 068 | Train Loss: 1.9697 Acc: 0.4995 | Val Loss: 0.9347 Acc: 0.6048                                               \n",
      "Epoch 069 | Train Loss: 1.9003 Acc: 0.5063 | Val Loss: 0.9357 Acc: 0.6024                                               \n",
      "Epoch 070 | Train Loss: 1.8626 Acc: 0.5047 | Val Loss: 0.9405 Acc: 0.5890                                               \n",
      "Epoch 071 | Train Loss: 1.8226 Acc: 0.5053 | Val Loss: 0.9892 Acc: 0.5821                                               \n",
      "Epoch 072 | Train Loss: 1.7788 Acc: 0.5166 | Val Loss: 0.9797 Acc: 0.6063                                               \n",
      "Epoch 073 | Train Loss: 1.7539 Acc: 0.5124 | Val Loss: 1.0087 Acc: 0.5863                                               \n",
      "Epoch 074 | Train Loss: 1.7426 Acc: 0.5140 | Val Loss: 0.9440 Acc: 0.6000                                               \n",
      "Epoch 075 | Train Loss: 1.6939 Acc: 0.5188 | Val Loss: 0.9104 Acc: 0.6036                                               \n",
      "Epoch 076 | Train Loss: 1.6590 Acc: 0.5245 | Val Loss: 0.9931 Acc: 0.6042                                               \n",
      "Epoch 077 | Train Loss: 1.6041 Acc: 0.5277 | Val Loss: 0.8847 Acc: 0.5925                                               \n",
      "Epoch 078 | Train Loss: 1.5673 Acc: 0.5283 | Val Loss: 1.0233 Acc: 0.5773                                               \n",
      "Epoch 079 | Train Loss: 1.5561 Acc: 0.5346 | Val Loss: 0.8920 Acc: 0.5961                                               \n",
      "Epoch 080 | Train Loss: 1.5429 Acc: 0.5341 | Val Loss: 0.9486 Acc: 0.6045                                               \n",
      "Epoch 081 | Train Loss: 1.5343 Acc: 0.5368 | Val Loss: 0.9127 Acc: 0.6009                                               \n",
      "Epoch 082 | Train Loss: 1.5113 Acc: 0.5378 | Val Loss: 0.8834 Acc: 0.6024                                               \n",
      "Epoch 083 | Train Loss: 1.4808 Acc: 0.5342 | Val Loss: 0.8417 Acc: 0.6039                                               \n",
      "Epoch 084 | Train Loss: 1.4633 Acc: 0.5377 | Val Loss: 0.8994 Acc: 0.5937                                               \n",
      "Epoch 085 | Train Loss: 1.4185 Acc: 0.5453 | Val Loss: 0.9429 Acc: 0.5952                                               \n",
      "Epoch 086 | Train Loss: 1.4171 Acc: 0.5412 | Val Loss: 0.8916 Acc: 0.6045                                               \n",
      "Epoch 087 | Train Loss: 1.3761 Acc: 0.5447 | Val Loss: 0.8007 Acc: 0.6066                                               \n",
      "Epoch 088 | Train Loss: 1.3730 Acc: 0.5521 | Val Loss: 0.8516 Acc: 0.6170                                               \n",
      "Epoch 089 | Train Loss: 1.3434 Acc: 0.5598 | Val Loss: 0.8234 Acc: 0.5994                                               \n",
      "Epoch 090 | Train Loss: 1.3180 Acc: 0.5561 | Val Loss: 0.8158 Acc: 0.6119                                               \n",
      "Epoch 091 | Train Loss: 1.3138 Acc: 0.5624 | Val Loss: 0.8356 Acc: 0.6081                                               \n",
      "Epoch 092 | Train Loss: 1.3114 Acc: 0.5615 | Val Loss: 0.8217 Acc: 0.6182                                               \n",
      "Epoch 093 | Train Loss: 1.3034 Acc: 0.5643 | Val Loss: 0.8245 Acc: 0.6066                                               \n",
      "Epoch 094 | Train Loss: 1.2328 Acc: 0.5697 | Val Loss: 0.8121 Acc: 0.6209                                               \n",
      "Epoch 095 | Train Loss: 1.2528 Acc: 0.5614 | Val Loss: 0.8452 Acc: 0.6164                                               \n",
      "Epoch 096 | Train Loss: 1.2378 Acc: 0.5641 | Val Loss: 0.8129 Acc: 0.6066                                               \n",
      "Epoch 097 | Train Loss: 1.2301 Acc: 0.5697 | Val Loss: 0.8382 Acc: 0.6382                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 36, 'cnn_dense': 256, 'cnn_dropout': 0.3438735331880601, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 32, 'learning_rate': 0.00014974892071901113, 'lstm_dense': 64, 'lstm_hidden_size': 64, 'lstm_layers': 1, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 30.0262 Acc: 0.3641 | Val Loss: 2.8430 Acc: 0.4170                                              \n",
      "Epoch 002 | Train Loss: 8.7354 Acc: 0.4238 | Val Loss: 2.2717 Acc: 0.5167                                               \n",
      "Epoch 003 | Train Loss: 5.0788 Acc: 0.4444 | Val Loss: 2.9429 Acc: 0.4669                                               \n",
      "Epoch 004 | Train Loss: 3.6110 Acc: 0.4757 | Val Loss: 1.7632 Acc: 0.5352                                               \n",
      "Epoch 005 | Train Loss: 2.9349 Acc: 0.4882 | Val Loss: 1.4121 Acc: 0.5845                                               \n",
      "Epoch 006 | Train Loss: 2.3224 Acc: 0.5065 | Val Loss: 1.3464 Acc: 0.5525                                               \n",
      "Epoch 007 | Train Loss: 2.0182 Acc: 0.5177 | Val Loss: 1.2271 Acc: 0.5591                                               \n",
      "Epoch 008 | Train Loss: 1.7397 Acc: 0.5294 | Val Loss: 1.3745 Acc: 0.5648                                               \n",
      "Epoch 009 | Train Loss: 1.5558 Acc: 0.5448 | Val Loss: 0.8598 Acc: 0.6896                                               \n",
      "Epoch 010 | Train Loss: 1.4263 Acc: 0.5535 | Val Loss: 0.9581 Acc: 0.6484                                               \n",
      "Epoch 011 | Train Loss: 1.3069 Acc: 0.5613 | Val Loss: 0.8106 Acc: 0.6588                                               \n",
      "Epoch 012 | Train Loss: 1.1787 Acc: 0.5805 | Val Loss: 0.7501 Acc: 0.6633                                               \n",
      "Epoch 013 | Train Loss: 1.0932 Acc: 0.5784 | Val Loss: 0.6995 Acc: 0.6851                                               \n",
      "Epoch 014 | Train Loss: 1.0716 Acc: 0.5895 | Val Loss: 0.8315 Acc: 0.6185                                               \n",
      "Epoch 015 | Train Loss: 0.9835 Acc: 0.6091 | Val Loss: 0.9291 Acc: 0.6385                                               \n",
      "Epoch 016 | Train Loss: 0.9027 Acc: 0.6367 | Val Loss: 0.5714 Acc: 0.7710                                               \n",
      "Epoch 017 | Train Loss: 0.8020 Acc: 0.6829 | Val Loss: 0.5736 Acc: 0.7669                                               \n",
      "Epoch 018 | Train Loss: 0.7357 Acc: 0.7075 | Val Loss: 0.5467 Acc: 0.7928                                               \n",
      "Epoch 019 | Train Loss: 0.6457 Acc: 0.7498 | Val Loss: 0.5834 Acc: 0.7552                                               \n",
      "Epoch 020 | Train Loss: 0.5721 Acc: 0.7749 | Val Loss: 0.3785 Acc: 0.8743                                               \n",
      "Epoch 021 | Train Loss: 0.4944 Acc: 0.8100 | Val Loss: 0.3763 Acc: 0.8448                                               \n",
      "Epoch 022 | Train Loss: 0.4266 Acc: 0.8424 | Val Loss: 0.2849 Acc: 0.8940                                               \n",
      "Epoch 023 | Train Loss: 0.3805 Acc: 0.8528 | Val Loss: 0.2496 Acc: 0.9069                                               \n",
      "Epoch 024 | Train Loss: 0.3237 Acc: 0.8803 | Val Loss: 0.2309 Acc: 0.9167                                               \n",
      "Epoch 025 | Train Loss: 0.2978 Acc: 0.8907 | Val Loss: 0.2194 Acc: 0.9182                                               \n",
      "Epoch 026 | Train Loss: 0.2714 Acc: 0.9000 | Val Loss: 0.1692 Acc: 0.9388                                               \n",
      "Epoch 027 | Train Loss: 0.2435 Acc: 0.9113 | Val Loss: 0.1729 Acc: 0.9376                                               \n",
      "Epoch 028 | Train Loss: 0.2228 Acc: 0.9169 | Val Loss: 0.1583 Acc: 0.9430                                               \n",
      "Epoch 029 | Train Loss: 0.1973 Acc: 0.9287 | Val Loss: 0.1468 Acc: 0.9501                                               \n",
      "Epoch 030 | Train Loss: 0.1890 Acc: 0.9316 | Val Loss: 0.1662 Acc: 0.9382                                               \n",
      "Epoch 031 | Train Loss: 0.1676 Acc: 0.9413 | Val Loss: 0.1248 Acc: 0.9576                                               \n",
      "Epoch 032 | Train Loss: 0.1497 Acc: 0.9487 | Val Loss: 0.1360 Acc: 0.9555                                               \n",
      "Epoch 033 | Train Loss: 0.1614 Acc: 0.9436 | Val Loss: 0.1057 Acc: 0.9663                                               \n",
      "Epoch 034 | Train Loss: 0.1347 Acc: 0.9522 | Val Loss: 0.1175 Acc: 0.9597                                               \n",
      "Epoch 035 | Train Loss: 0.1210 Acc: 0.9580 | Val Loss: 0.1039 Acc: 0.9660                                               \n",
      "Epoch 036 | Train Loss: 0.1076 Acc: 0.9643 | Val Loss: 0.1073 Acc: 0.9648                                               \n",
      "Epoch 037 | Train Loss: 0.1117 Acc: 0.9621 | Val Loss: 0.1139 Acc: 0.9561                                               \n",
      "Epoch 038 | Train Loss: 0.1005 Acc: 0.9656 | Val Loss: 0.1443 Acc: 0.9496                                               \n",
      "Epoch 039 | Train Loss: 0.0882 Acc: 0.9694 | Val Loss: 0.0809 Acc: 0.9719                                               \n",
      "Epoch 040 | Train Loss: 0.0962 Acc: 0.9678 | Val Loss: 0.0815 Acc: 0.9719                                               \n",
      "Epoch 041 | Train Loss: 0.0791 Acc: 0.9731 | Val Loss: 0.0635 Acc: 0.9758                                               \n",
      "Epoch 042 | Train Loss: 0.0751 Acc: 0.9756 | Val Loss: 0.0613 Acc: 0.9785                                               \n",
      "Epoch 043 | Train Loss: 0.0736 Acc: 0.9765 | Val Loss: 0.0651 Acc: 0.9794                                               \n",
      "Epoch 044 | Train Loss: 0.0652 Acc: 0.9784 | Val Loss: 0.0680 Acc: 0.9764                                               \n",
      "Epoch 045 | Train Loss: 0.0663 Acc: 0.9769 | Val Loss: 0.0619 Acc: 0.9782                                               \n",
      "Epoch 046 | Train Loss: 0.0570 Acc: 0.9800 | Val Loss: 0.0747 Acc: 0.9743                                               \n",
      "Epoch 047 | Train Loss: 0.0689 Acc: 0.9781 | Val Loss: 0.0562 Acc: 0.9830                                               \n",
      "Epoch 048 | Train Loss: 0.0554 Acc: 0.9808 | Val Loss: 0.0773 Acc: 0.9752                                               \n",
      "Epoch 049 | Train Loss: 0.0586 Acc: 0.9796 | Val Loss: 0.0669 Acc: 0.9767                                               \n",
      "Epoch 050 | Train Loss: 0.0457 Acc: 0.9840 | Val Loss: 0.0621 Acc: 0.9812                                               \n",
      "Epoch 051 | Train Loss: 0.0548 Acc: 0.9820 | Val Loss: 0.0732 Acc: 0.9764                                               \n",
      "Epoch 052 | Train Loss: 0.0548 Acc: 0.9829 | Val Loss: 0.0599 Acc: 0.9821                                               \n",
      "Epoch 053 | Train Loss: 0.0423 Acc: 0.9859 | Val Loss: 0.0614 Acc: 0.9788                                               \n",
      "Epoch 054 | Train Loss: 0.0433 Acc: 0.9853 | Val Loss: 0.1137 Acc: 0.9636                                               \n",
      "Epoch 055 | Train Loss: 0.0423 Acc: 0.9844 | Val Loss: 0.0603 Acc: 0.9815                                               \n",
      "Epoch 056 | Train Loss: 0.0369 Acc: 0.9872 | Val Loss: 0.0482 Acc: 0.9860                                               \n",
      "Epoch 057 | Train Loss: 0.0424 Acc: 0.9860 | Val Loss: 0.0526 Acc: 0.9836                                               \n",
      "Epoch 058 | Train Loss: 0.0357 Acc: 0.9877 | Val Loss: 0.0461 Acc: 0.9830                                               \n",
      "Epoch 059 | Train Loss: 0.0385 Acc: 0.9862 | Val Loss: 0.0667 Acc: 0.9803                                               \n",
      "Epoch 060 | Train Loss: 0.0341 Acc: 0.9885 | Val Loss: 0.0374 Acc: 0.9884                                               \n",
      "Epoch 061 | Train Loss: 0.0335 Acc: 0.9872 | Val Loss: 0.0371 Acc: 0.9896                                               \n",
      "Epoch 062 | Train Loss: 0.0296 Acc: 0.9904 | Val Loss: 0.0533 Acc: 0.9830                                               \n",
      "Epoch 063 | Train Loss: 0.0293 Acc: 0.9893 | Val Loss: 0.0594 Acc: 0.9824                                               \n",
      "Epoch 064 | Train Loss: 0.0333 Acc: 0.9891 | Val Loss: 0.0623 Acc: 0.9812                                               \n",
      "Epoch 065 | Train Loss: 0.0272 Acc: 0.9905 | Val Loss: 0.0511 Acc: 0.9857                                               \n",
      "Epoch 066 | Train Loss: 0.0255 Acc: 0.9916 | Val Loss: 0.0531 Acc: 0.9857                                               \n",
      "Epoch 067 | Train Loss: 0.0298 Acc: 0.9898 | Val Loss: 0.0499 Acc: 0.9839                                               \n",
      "Epoch 068 | Train Loss: 0.0317 Acc: 0.9903 | Val Loss: 0.0626 Acc: 0.9827                                               \n",
      "Epoch 069 | Train Loss: 0.0310 Acc: 0.9905 | Val Loss: 0.0541 Acc: 0.9848                                               \n",
      "Epoch 070 | Train Loss: 0.0245 Acc: 0.9913 | Val Loss: 0.0506 Acc: 0.9824                                               \n",
      "Epoch 071 | Train Loss: 0.0205 Acc: 0.9927 | Val Loss: 0.0727 Acc: 0.9776                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 36, 'cnn_dense': 256, 'cnn_dropout': 0.02937850336978859, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 32, 'learning_rate': 0.00036213105276746457, 'lstm_dense': 64, 'lstm_hidden_size': 96, 'lstm_layers': 4, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 19.4151 Acc: 0.3820 | Val Loss: 2.0570 Acc: 0.5331                                              \n",
      "Epoch 002 | Train Loss: 4.0472 Acc: 0.4515 | Val Loss: 2.0121 Acc: 0.5609                                               \n",
      "Epoch 003 | Train Loss: 2.7134 Acc: 0.4760 | Val Loss: 1.2689 Acc: 0.6361                                               \n",
      "Epoch 004 | Train Loss: 2.0579 Acc: 0.5037 | Val Loss: 1.1693 Acc: 0.6048                                               \n",
      "Epoch 005 | Train Loss: 1.6391 Acc: 0.5200 | Val Loss: 1.2860 Acc: 0.5684                                               \n",
      "Epoch 006 | Train Loss: 1.4724 Acc: 0.5330 | Val Loss: 0.9161 Acc: 0.6069                                               \n",
      "Epoch 007 | Train Loss: 1.2435 Acc: 0.5531 | Val Loss: 0.9815 Acc: 0.6427                                               \n",
      "Epoch 008 | Train Loss: 1.0688 Acc: 0.5717 | Val Loss: 1.1076 Acc: 0.5839                                               \n",
      "Epoch 009 | Train Loss: 0.9867 Acc: 0.5836 | Val Loss: 0.6674 Acc: 0.7045                                               \n",
      "Epoch 010 | Train Loss: 0.9409 Acc: 0.5933 | Val Loss: 1.1370 Acc: 0.5582                                               \n",
      "Epoch 011 | Train Loss: 0.8994 Acc: 0.6016 | Val Loss: 0.7811 Acc: 0.6155                                               \n",
      "Epoch 012 | Train Loss: 0.8086 Acc: 0.6434 | Val Loss: 0.5589 Acc: 0.7484                                               \n",
      "Epoch 013 | Train Loss: 0.7157 Acc: 0.6839 | Val Loss: 0.6092 Acc: 0.7227                                               \n",
      "Epoch 014 | Train Loss: 0.6504 Acc: 0.7209 | Val Loss: 0.6198 Acc: 0.7454                                               \n",
      "Epoch 015 | Train Loss: 0.5865 Acc: 0.7533 | Val Loss: 0.4693 Acc: 0.8060                                               \n",
      "Epoch 016 | Train Loss: 0.5099 Acc: 0.7839 | Val Loss: 0.4322 Acc: 0.8251                                               \n",
      "Epoch 017 | Train Loss: 0.4538 Acc: 0.8207 | Val Loss: 0.4698 Acc: 0.8134                                               \n",
      "Epoch 018 | Train Loss: 0.3819 Acc: 0.8484 | Val Loss: 0.3108 Acc: 0.8770                                               \n",
      "Epoch 019 | Train Loss: 0.3422 Acc: 0.8732 | Val Loss: 0.2675 Acc: 0.9015                                               \n",
      "Epoch 020 | Train Loss: 0.2822 Acc: 0.8990 | Val Loss: 0.2736 Acc: 0.8985                                               \n",
      "Epoch 021 | Train Loss: 0.2292 Acc: 0.9189 | Val Loss: 0.2156 Acc: 0.9218                                               \n",
      "Epoch 022 | Train Loss: 0.2143 Acc: 0.9261 | Val Loss: 0.2312 Acc: 0.9075                                               \n",
      "Epoch 023 | Train Loss: 0.1722 Acc: 0.9388 | Val Loss: 0.1612 Acc: 0.9448                                               \n",
      "Epoch 024 | Train Loss: 0.1604 Acc: 0.9455 | Val Loss: 0.1537 Acc: 0.9442                                               \n",
      "Epoch 025 | Train Loss: 0.1488 Acc: 0.9484 | Val Loss: 0.1458 Acc: 0.9507                                               \n",
      "Epoch 026 | Train Loss: 0.1352 Acc: 0.9546 | Val Loss: 0.1566 Acc: 0.9445                                               \n",
      "Epoch 027 | Train Loss: 0.1385 Acc: 0.9543 | Val Loss: 0.1857 Acc: 0.9340                                               \n",
      "Epoch 028 | Train Loss: 0.1042 Acc: 0.9649 | Val Loss: 0.1157 Acc: 0.9594                                               \n",
      "Epoch 029 | Train Loss: 0.1021 Acc: 0.9659 | Val Loss: 0.1773 Acc: 0.9457                                               \n",
      "Epoch 030 | Train Loss: 0.0904 Acc: 0.9692 | Val Loss: 0.1479 Acc: 0.9501                                               \n",
      "Epoch 031 | Train Loss: 0.0833 Acc: 0.9733 | Val Loss: 0.0926 Acc: 0.9690                                               \n",
      "Epoch 032 | Train Loss: 0.0848 Acc: 0.9727 | Val Loss: 0.1317 Acc: 0.9549                                               \n",
      "Epoch 033 | Train Loss: 0.0701 Acc: 0.9756 | Val Loss: 0.1248 Acc: 0.9684                                               \n",
      "Epoch 034 | Train Loss: 0.0672 Acc: 0.9773 | Val Loss: 0.2195 Acc: 0.9266                                               \n",
      "Epoch 035 | Train Loss: 0.0700 Acc: 0.9758 | Val Loss: 0.1383 Acc: 0.9570                                               \n",
      "Epoch 036 | Train Loss: 0.0532 Acc: 0.9831 | Val Loss: 0.1128 Acc: 0.9657                                               \n",
      "Epoch 037 | Train Loss: 0.0490 Acc: 0.9831 | Val Loss: 0.0813 Acc: 0.9755                                               \n",
      "Epoch 038 | Train Loss: 0.0563 Acc: 0.9814 | Val Loss: 0.1059 Acc: 0.9690                                               \n",
      "Epoch 039 | Train Loss: 0.0494 Acc: 0.9832 | Val Loss: 0.1152 Acc: 0.9687                                               \n",
      "Epoch 040 | Train Loss: 0.0683 Acc: 0.9776 | Val Loss: 0.1573 Acc: 0.9561                                               \n",
      "Epoch 041 | Train Loss: 0.0545 Acc: 0.9824 | Val Loss: 0.1767 Acc: 0.9460                                               \n",
      "Epoch 042 | Train Loss: 0.0476 Acc: 0.9846 | Val Loss: 0.1005 Acc: 0.9707                                               \n",
      "Epoch 043 | Train Loss: 0.0318 Acc: 0.9903 | Val Loss: 0.0903 Acc: 0.9722                                               \n",
      "Epoch 044 | Train Loss: 0.0339 Acc: 0.9889 | Val Loss: 0.1200 Acc: 0.9669                                               \n",
      "Epoch 045 | Train Loss: 0.0410 Acc: 0.9874 | Val Loss: 0.1574 Acc: 0.9582                                               \n",
      "Epoch 046 | Train Loss: 0.0380 Acc: 0.9878 | Val Loss: 0.1422 Acc: 0.9639                                               \n",
      "Epoch 047 | Train Loss: 0.0421 Acc: 0.9875 | Val Loss: 0.0949 Acc: 0.9725                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 128, 'cnn_dropout': 0.3796915160492759, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 32, 'learning_rate': 0.0010334605530322371, 'lstm_dense': 128, 'lstm_hidden_size': 64, 'lstm_layers': 6, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 18.4648 Acc: 0.4114 | Val Loss: 1.3612 Acc: 0.6296                                              \n",
      "Epoch 002 | Train Loss: 3.5943 Acc: 0.4877 | Val Loss: 1.3233 Acc: 0.5478                                               \n",
      "Epoch 003 | Train Loss: 2.0895 Acc: 0.5319 | Val Loss: 1.5766 Acc: 0.5842                                               \n",
      "Epoch 004 | Train Loss: 1.5279 Acc: 0.5575 | Val Loss: 1.0076 Acc: 0.6343                                               \n",
      "Epoch 005 | Train Loss: 1.1430 Acc: 0.5855 | Val Loss: 0.7660 Acc: 0.6630                                               \n",
      "Epoch 006 | Train Loss: 1.0042 Acc: 0.5994 | Val Loss: 0.6335 Acc: 0.6991                                               \n",
      "Epoch 007 | Train Loss: 0.8808 Acc: 0.6229 | Val Loss: 0.9584 Acc: 0.5764                                               \n",
      "Epoch 008 | Train Loss: 0.8369 Acc: 0.6387 | Val Loss: 0.6581 Acc: 0.7090                                               \n",
      "Epoch 009 | Train Loss: 0.7520 Acc: 0.6629 | Val Loss: 0.7862 Acc: 0.6003                                               \n",
      "Epoch 010 | Train Loss: 0.7395 Acc: 0.6730 | Val Loss: 0.7054 Acc: 0.6448                                               \n",
      "Epoch 011 | Train Loss: 0.7177 Acc: 0.6774 | Val Loss: 0.6497 Acc: 0.6764                                               \n",
      "Epoch 012 | Train Loss: 0.7029 Acc: 0.6783 | Val Loss: 0.7552 Acc: 0.5961                                               \n",
      "Epoch 013 | Train Loss: 0.7001 Acc: 0.6848 | Val Loss: 0.6668 Acc: 0.6463                                               \n",
      "Epoch 014 | Train Loss: 0.6607 Acc: 0.7049 | Val Loss: 0.6629 Acc: 0.6457                                               \n",
      "Epoch 015 | Train Loss: 0.6734 Acc: 0.7066 | Val Loss: 0.5740 Acc: 0.7113                                               \n",
      "Epoch 016 | Train Loss: 0.6319 Acc: 0.7219 | Val Loss: 0.7293 Acc: 0.6549                                               \n",
      "Epoch 017 | Train Loss: 0.6317 Acc: 0.7250 | Val Loss: 0.7141 Acc: 0.6848                                               \n",
      "Epoch 012 | Train Loss: 0.6185 Acc: 0.7168 | Val Loss: 0.5734 Acc: 0.7496                                               \n",
      "Epoch 013 | Train Loss: 0.5923 Acc: 0.7339 | Val Loss: 0.6617 Acc: 0.7161                                               \n",
      "Epoch 014 | Train Loss: 0.6521 Acc: 0.7109 | Val Loss: 0.5850 Acc: 0.7373                                               \n",
      "Epoch 015 | Train Loss: 0.6258 Acc: 0.7286 | Val Loss: 0.6278 Acc: 0.7445                                               \n",
      "Epoch 016 | Train Loss: 0.6018 Acc: 0.7299 | Val Loss: 0.5165 Acc: 0.7991                                               \n",
      "Epoch 017 | Train Loss: 0.6149 Acc: 0.7252 | Val Loss: 0.5595 Acc: 0.7913                                               \n",
      "Epoch 018 | Train Loss: 0.6615 Acc: 0.7081 | Val Loss: 0.5921 Acc: 0.7119                                               \n",
      "Epoch 019 | Train Loss: 0.5996 Acc: 0.7310 | Val Loss: 0.6577 Acc: 0.7355                                               \n",
      "Epoch 020 | Train Loss: 0.5891 Acc: 0.7354 | Val Loss: 0.4724 Acc: 0.7818                                               \n",
      "Epoch 021 | Train Loss: 0.5896 Acc: 0.7400 | Val Loss: 0.4597 Acc: 0.7863                                               \n",
      "Epoch 022 | Train Loss: 0.6145 Acc: 0.7300 | Val Loss: 0.4130 Acc: 0.8510                                               \n",
      "Epoch 023 | Train Loss: 0.5947 Acc: 0.7316 | Val Loss: 0.6207 Acc: 0.7319                                               \n",
      "Epoch 024 | Train Loss: 0.5914 Acc: 0.7411 | Val Loss: 0.5844 Acc: 0.7400                                               \n",
      "Epoch 025 | Train Loss: 0.6077 Acc: 0.7270 | Val Loss: 0.6449 Acc: 0.7585                                               \n",
      "Epoch 026 | Train Loss: 0.5916 Acc: 0.7339 | Val Loss: 0.5520 Acc: 0.7648                                               \n",
      "Epoch 027 | Train Loss: 0.5738 Acc: 0.7401 | Val Loss: 0.5322 Acc: 0.8039                                               \n",
      "Epoch 028 | Train Loss: 0.5876 Acc: 0.7408 | Val Loss: 0.4576 Acc: 0.8194                                               \n",
      "Epoch 029 | Train Loss: 0.5640 Acc: 0.7449 | Val Loss: 0.4851 Acc: 0.8003                                               \n",
      "Epoch 030 | Train Loss: 0.5458 Acc: 0.7581 | Val Loss: 0.5814 Acc: 0.7546                                               \n",
      "Epoch 031 | Train Loss: 0.5825 Acc: 0.7444 | Val Loss: 0.6664 Acc: 0.7325                                               \n",
      "Epoch 032 | Train Loss: 0.5969 Acc: 0.7348 | Val Loss: 0.5029 Acc: 0.7758                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 36, 'cnn_dense': 64, 'cnn_dropout': 0.06981494560611974, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 32, 'learning_rate': 7.370574188436634e-05, 'lstm_dense': 64, 'lstm_hidden_size': 96, 'lstm_layers': 1, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 79.9525 Acc: 0.3144 | Val Loss: 18.7592 Acc: 0.4263                                             \n",
      "Epoch 002 | Train Loss: 28.1116 Acc: 0.3742 | Val Loss: 6.0376 Acc: 0.4484                                              \n",
      "Epoch 003 | Train Loss: 16.7335 Acc: 0.3926 | Val Loss: 5.0648 Acc: 0.4445                                              \n",
      "Epoch 004 | Train Loss: 12.3403 Acc: 0.4001 | Val Loss: 2.9413 Acc: 0.5313                                              \n",
      "Epoch 005 | Train Loss: 9.1083 Acc: 0.4222 | Val Loss: 4.2475 Acc: 0.4869                                               \n",
      "Epoch 006 | Train Loss: 7.5288 Acc: 0.4294 | Val Loss: 2.1613 Acc: 0.5716                                               \n",
      "Epoch 007 | Train Loss: 6.1338 Acc: 0.4270 | Val Loss: 2.8255 Acc: 0.5513                                               \n",
      "Epoch 008 | Train Loss: 4.9184 Acc: 0.4458 | Val Loss: 1.4779 Acc: 0.5707                                               \n",
      "Epoch 009 | Train Loss: 4.3016 Acc: 0.4453 | Val Loss: 2.2761 Acc: 0.5072                                               \n",
      "Epoch 010 | Train Loss: 3.6004 Acc: 0.4666 | Val Loss: 1.2510 Acc: 0.6018                                               \n",
      "Epoch 011 | Train Loss: 2.9577 Acc: 0.4661 | Val Loss: 1.5646 Acc: 0.5296                                               \n",
      "Epoch 012 | Train Loss: 2.7140 Acc: 0.4663 | Val Loss: 1.2514 Acc: 0.5409                                               \n",
      "Epoch 013 | Train Loss: 2.4131 Acc: 0.4697 | Val Loss: 1.1207 Acc: 0.5531                                               \n",
      "Epoch 014 | Train Loss: 2.1635 Acc: 0.4750 | Val Loss: 0.9382 Acc: 0.5388                                               \n",
      "Epoch 015 | Train Loss: 1.9669 Acc: 0.4798 | Val Loss: 1.0859 Acc: 0.5301                                               \n",
      "Epoch 016 | Train Loss: 1.7993 Acc: 0.4841 | Val Loss: 1.0007 Acc: 0.5830                                               \n",
      "Epoch 017 | Train Loss: 1.6778 Acc: 0.4818 | Val Loss: 1.1996 Acc: 0.5227                                               \n",
      "Epoch 018 | Train Loss: 1.5587 Acc: 0.4975 | Val Loss: 0.9199 Acc: 0.5636                                               \n",
      "Epoch 019 | Train Loss: 1.5196 Acc: 0.5124 | Val Loss: 0.8165 Acc: 0.5606                                               \n",
      "Epoch 020 | Train Loss: 1.3562 Acc: 0.5403 | Val Loss: 0.8596 Acc: 0.6066                                               \n",
      "Epoch 021 | Train Loss: 1.2656 Acc: 0.5800 | Val Loss: 0.8745 Acc: 0.6478                                               \n",
      "Epoch 022 | Train Loss: 1.2082 Acc: 0.5982 | Val Loss: 0.8782 Acc: 0.6310                                               \n",
      "Epoch 023 | Train Loss: 1.0947 Acc: 0.6264 | Val Loss: 0.7919 Acc: 0.6158                                               \n",
      "Epoch 024 | Train Loss: 1.0064 Acc: 0.6548 | Val Loss: 0.5694 Acc: 0.7496                                               \n",
      "Epoch 025 | Train Loss: 0.9629 Acc: 0.6895 | Val Loss: 0.5427 Acc: 0.7594                                               \n",
      "Epoch 026 | Train Loss: 0.8392 Acc: 0.7264 | Val Loss: 0.5525 Acc: 0.7901                                               \n",
      "Epoch 027 | Train Loss: 0.7537 Acc: 0.7556 | Val Loss: 0.3947 Acc: 0.8385                                               \n",
      "Epoch 028 | Train Loss: 0.6614 Acc: 0.7868 | Val Loss: 0.3830 Acc: 0.8504                                               \n",
      "Epoch 029 | Train Loss: 0.5923 Acc: 0.8181 | Val Loss: 0.5211 Acc: 0.8146                                               \n",
      "Epoch 030 | Train Loss: 0.5526 Acc: 0.8326 | Val Loss: 0.3124 Acc: 0.8887                                               \n",
      "Epoch 031 | Train Loss: 0.4704 Acc: 0.8446 | Val Loss: 0.2646 Acc: 0.9021                                               \n",
      "Epoch 032 | Train Loss: 0.4360 Acc: 0.8620 | Val Loss: 0.4468 Acc: 0.8346                                               \n",
      "Epoch 033 | Train Loss: 0.3920 Acc: 0.8717 | Val Loss: 0.2553 Acc: 0.9018                                               \n",
      "Epoch 034 | Train Loss: 0.3774 Acc: 0.8814 | Val Loss: 0.2430 Acc: 0.9128                                               \n",
      "Epoch 035 | Train Loss: 0.3488 Acc: 0.8912 | Val Loss: 0.2273 Acc: 0.9164                                               \n",
      "Epoch 036 | Train Loss: 0.3204 Acc: 0.9021 | Val Loss: 0.2374 Acc: 0.9137                                               \n",
      "Epoch 037 | Train Loss: 0.3239 Acc: 0.9022 | Val Loss: 0.2468 Acc: 0.9170                                               \n",
      "Epoch 038 | Train Loss: 0.3057 Acc: 0.9074 | Val Loss: 0.2674 Acc: 0.9054                                               \n",
      "Epoch 039 | Train Loss: 0.2948 Acc: 0.9084 | Val Loss: 0.2076 Acc: 0.9245                                               \n",
      "Epoch 040 | Train Loss: 0.2795 Acc: 0.9106 | Val Loss: 0.1734 Acc: 0.9403                                               \n",
      "Epoch 041 | Train Loss: 0.2479 Acc: 0.9242 | Val Loss: 0.2139 Acc: 0.9272                                               \n",
      "Epoch 042 | Train Loss: 0.2291 Acc: 0.9275 | Val Loss: 0.1785 Acc: 0.9382                                               \n",
      "Epoch 043 | Train Loss: 0.2334 Acc: 0.9267 | Val Loss: 0.1730 Acc: 0.9400                                               \n",
      "Epoch 044 | Train Loss: 0.1938 Acc: 0.9376 | Val Loss: 0.1990 Acc: 0.9337                                               \n",
      "Epoch 045 | Train Loss: 0.1874 Acc: 0.9401 | Val Loss: 0.2059 Acc: 0.9275                                               \n",
      "Epoch 046 | Train Loss: 0.1725 Acc: 0.9440 | Val Loss: 0.1586 Acc: 0.9460                                               \n",
      "Epoch 047 | Train Loss: 0.1594 Acc: 0.9495 | Val Loss: 0.1811 Acc: 0.9382                                               \n",
      "Epoch 048 | Train Loss: 0.1522 Acc: 0.9489 | Val Loss: 0.1604 Acc: 0.9478                                               \n",
      "Epoch 049 | Train Loss: 0.1506 Acc: 0.9517 | Val Loss: 0.1571 Acc: 0.9478                                               \n",
      "Epoch 050 | Train Loss: 0.1405 Acc: 0.9551 | Val Loss: 0.1442 Acc: 0.9519                                               \n",
      "Epoch 051 | Train Loss: 0.1477 Acc: 0.9536 | Val Loss: 0.1660 Acc: 0.9430                                               \n",
      "Epoch 052 | Train Loss: 0.1306 Acc: 0.9566 | Val Loss: 0.1520 Acc: 0.9463                                               \n",
      "Epoch 053 | Train Loss: 0.1183 Acc: 0.9599 | Val Loss: 0.1581 Acc: 0.9507                                               \n",
      "Epoch 054 | Train Loss: 0.1392 Acc: 0.9575 | Val Loss: 0.2242 Acc: 0.9379                                               \n",
      "Epoch 055 | Train Loss: 0.1251 Acc: 0.9604 | Val Loss: 0.1397 Acc: 0.9552                                               \n",
      "Epoch 056 | Train Loss: 0.1030 Acc: 0.9670 | Val Loss: 0.1495 Acc: 0.9513                                               \n",
      "Epoch 057 | Train Loss: 0.1033 Acc: 0.9669 | Val Loss: 0.1434 Acc: 0.9507                                               \n",
      "Epoch 058 | Train Loss: 0.0941 Acc: 0.9686 | Val Loss: 0.2166 Acc: 0.9412                                               \n",
      "Epoch 059 | Train Loss: 0.0839 Acc: 0.9716 | Val Loss: 0.1696 Acc: 0.9463                                               \n",
      "Epoch 060 | Train Loss: 0.0904 Acc: 0.9687 | Val Loss: 0.1091 Acc: 0.9660                                               \n",
      "Epoch 061 | Train Loss: 0.0786 Acc: 0.9734 | Val Loss: 0.1352 Acc: 0.9627                                               \n",
      "Epoch 062 | Train Loss: 0.0866 Acc: 0.9738 | Val Loss: 0.1428 Acc: 0.9573                                               \n",
      "Epoch 063 | Train Loss: 0.0711 Acc: 0.9757 | Val Loss: 0.1468 Acc: 0.9606                                               \n",
      "Epoch 064 | Train Loss: 0.0677 Acc: 0.9786 | Val Loss: 0.1233 Acc: 0.9642                                               \n",
      "Epoch 065 | Train Loss: 0.0655 Acc: 0.9787 | Val Loss: 0.1317 Acc: 0.9603                                               \n",
      "Epoch 066 | Train Loss: 0.0691 Acc: 0.9765 | Val Loss: 0.1984 Acc: 0.9493                                               \n",
      "Epoch 067 | Train Loss: 0.0796 Acc: 0.9747 | Val Loss: 0.2519 Acc: 0.9299                                               \n",
      "Epoch 068 | Train Loss: 0.0959 Acc: 0.9678 | Val Loss: 0.1395 Acc: 0.9603                                               \n",
      "Epoch 069 | Train Loss: 0.0591 Acc: 0.9807 | Val Loss: 0.1447 Acc: 0.9561                                               \n",
      "Epoch 070 | Train Loss: 0.0520 Acc: 0.9825 | Val Loss: 0.1518 Acc: 0.9531                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 256, 'cnn_dropout': 0.30925543458417937, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 16, 'learning_rate': 0.00014474080895970372, 'lstm_dense': 64, 'lstm_hidden_size': 64, 'lstm_layers': 3, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 6.9722 Acc: 0.3248 | Val Loss: 1.1420 Acc: 0.3039                                               \n",
      "Epoch 002 | Train Loss: 1.0849 Acc: 0.3930 | Val Loss: 0.9824 Acc: 0.4421                                               \n",
      "Epoch 003 | Train Loss: 1.0334 Acc: 0.4156 | Val Loss: 0.9173 Acc: 0.4504                                               \n",
      "Epoch 004 | Train Loss: 1.0031 Acc: 0.4263 | Val Loss: 0.8962 Acc: 0.4406                                               \n",
      "Epoch 005 | Train Loss: 0.9797 Acc: 0.4297 | Val Loss: 0.9199 Acc: 0.4421                                               \n",
      "Epoch 006 | Train Loss: 0.9636 Acc: 0.4282 | Val Loss: 0.9161 Acc: 0.4654                                               \n",
      "Epoch 007 | Train Loss: 0.9453 Acc: 0.4463 | Val Loss: 0.8780 Acc: 0.4896                                               \n",
      "Epoch 008 | Train Loss: 0.9462 Acc: 0.4573 | Val Loss: 0.8824 Acc: 0.4146                                               \n",
      "Epoch 009 | Train Loss: 0.9372 Acc: 0.4773 | Val Loss: 0.8520 Acc: 0.5493                                               \n",
      "Epoch 010 | Train Loss: 0.9054 Acc: 0.5023 | Val Loss: 0.8464 Acc: 0.5663                                               \n",
      "Epoch 011 | Train Loss: 0.9269 Acc: 0.5012 | Val Loss: 0.8636 Acc: 0.5899                                               \n",
      "Epoch 012 | Train Loss: 0.9073 Acc: 0.5137 | Val Loss: 0.8023 Acc: 0.5421                                               \n",
      "Epoch 013 | Train Loss: 0.8946 Acc: 0.5247 | Val Loss: 0.8348 Acc: 0.5215                                               \n",
      "Epoch 014 | Train Loss: 0.9033 Acc: 0.5287 | Val Loss: 0.8004 Acc: 0.6499                                               \n",
      "Epoch 015 | Train Loss: 0.8888 Acc: 0.5358 | Val Loss: 0.7917 Acc: 0.6125                                               \n",
      "Epoch 016 | Train Loss: 0.8925 Acc: 0.5294 | Val Loss: 0.8236 Acc: 0.5672                                               \n",
      "Epoch 017 | Train Loss: 0.8944 Acc: 0.5341 | Val Loss: 0.7810 Acc: 0.5985                                               \n",
      "Epoch 018 | Train Loss: 0.8887 Acc: 0.5354 | Val Loss: 0.8096 Acc: 0.5964                                               \n",
      "Epoch 019 | Train Loss: 0.8778 Acc: 0.5503 | Val Loss: 0.7873 Acc: 0.5857                                               \n",
      "Epoch 020 | Train Loss: 0.8877 Acc: 0.5365 | Val Loss: 0.8311 Acc: 0.6167                                               \n",
      "Epoch 021 | Train Loss: 0.8991 Acc: 0.5352 | Val Loss: 0.7957 Acc: 0.6140                                               \n",
      "Epoch 022 | Train Loss: 0.8836 Acc: 0.5378 | Val Loss: 0.8181 Acc: 0.5955                                               \n",
      "Epoch 023 | Train Loss: 0.8748 Acc: 0.5505 | Val Loss: 0.7719 Acc: 0.5794                                               \n",
      "Epoch 024 | Train Loss: 0.8787 Acc: 0.5397 | Val Loss: 0.8218 Acc: 0.5901                                               \n",
      "Epoch 025 | Train Loss: 0.8756 Acc: 0.5528 | Val Loss: 0.8090 Acc: 0.5785                                               \n",
      "Epoch 026 | Train Loss: 0.8769 Acc: 0.5404 | Val Loss: 0.7828 Acc: 0.6045                                               \n",
      "Epoch 027 | Train Loss: 0.8847 Acc: 0.5450 | Val Loss: 0.7796 Acc: 0.6373                                               \n",
      "Epoch 028 | Train Loss: 0.8741 Acc: 0.5471 | Val Loss: 0.7740 Acc: 0.6248                                               \n",
      "Epoch 029 | Train Loss: 0.8583 Acc: 0.5574 | Val Loss: 0.7610 Acc: 0.6296                                               \n",
      "Epoch 030 | Train Loss: 0.8618 Acc: 0.5580 | Val Loss: 0.7569 Acc: 0.6358                                               \n",
      "Epoch 031 | Train Loss: 0.8681 Acc: 0.5533 | Val Loss: 0.7635 Acc: 0.6176                                               \n",
      "Epoch 032 | Train Loss: 0.8620 Acc: 0.5503 | Val Loss: 0.7630 Acc: 0.6122                                               \n",
      "Epoch 033 | Train Loss: 0.8656 Acc: 0.5555 | Val Loss: 0.7874 Acc: 0.5848                                               \n",
      "Epoch 034 | Train Loss: 0.8606 Acc: 0.5585 | Val Loss: 0.7748 Acc: 0.6042                                               \n",
      "Epoch 035 | Train Loss: 0.8741 Acc: 0.5462 | Val Loss: 0.7901 Acc: 0.5946                                               \n",
      "Epoch 036 | Train Loss: 0.8680 Acc: 0.5532 | Val Loss: 0.7609 Acc: 0.6460                                               \n",
      "Epoch 037 | Train Loss: 0.8672 Acc: 0.5613 | Val Loss: 0.7644 Acc: 0.6242                                               \n",
      "Epoch 038 | Train Loss: 0.8918 Acc: 0.5409 | Val Loss: 0.8008 Acc: 0.6242                                               \n",
      "Epoch 039 | Train Loss: 0.8707 Acc: 0.5517 | Val Loss: 0.7842 Acc: 0.6167                                               \n",
      "Epoch 040 | Train Loss: 0.8792 Acc: 0.5480 | Val Loss: 0.7724 Acc: 0.6045                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 36, 'cnn_dense': 128, 'cnn_dropout': 0.6273065297959043, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 32, 'learning_rate': 1.0293901619270713e-05, 'lstm_dense': 128, 'lstm_hidden_size': 64, 'lstm_layers': 5, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 114.4262 Acc: 0.2834 | Val Loss: 33.3959 Acc: 0.4310                                            \n",
      "Epoch 002 | Train Loss: 88.1263 Acc: 0.3232 | Val Loss: 27.8800 Acc: 0.4570                                             \n",
      "Epoch 003 | Train Loss: 71.9764 Acc: 0.3352 | Val Loss: 22.8670 Acc: 0.4657                                             \n",
      "Epoch 004 | Train Loss: 59.5716 Acc: 0.3453 | Val Loss: 17.7477 Acc: 0.3713                                             \n",
      "Epoch 005 | Train Loss: 50.2589 Acc: 0.3471 | Val Loss: 15.1937 Acc: 0.3818                                             \n",
      "Epoch 006 | Train Loss: 41.1523 Acc: 0.3658 | Val Loss: 13.1793 Acc: 0.4143                                             \n",
      "Epoch 007 | Train Loss: 35.1505 Acc: 0.3653 | Val Loss: 11.7992 Acc: 0.4146                                             \n",
      "Epoch 008 | Train Loss: 30.9873 Acc: 0.3660 | Val Loss: 9.7473 Acc: 0.4143                                              \n",
      "Epoch 009 | Train Loss: 27.4812 Acc: 0.3714 | Val Loss: 8.1055 Acc: 0.4510                                              \n",
      "Epoch 010 | Train Loss: 24.6422 Acc: 0.3697 | Val Loss: 5.9321 Acc: 0.4200                                              \n",
      "Epoch 011 | Train Loss: 22.5178 Acc: 0.3782 | Val Loss: 5.1329 Acc: 0.3907                                              \n",
      "Epoch 012 | Train Loss: 20.9667 Acc: 0.3700 | Val Loss: 4.3757 Acc: 0.3988                                              \n",
      "Epoch 013 | Train Loss: 19.4160 Acc: 0.3775 | Val Loss: 4.0426 Acc: 0.4021                                              \n",
      "Epoch 014 | Train Loss: 18.2864 Acc: 0.3866 | Val Loss: 4.2739 Acc: 0.5015                                              \n",
      "Epoch 015 | Train Loss: 17.2058 Acc: 0.3855 | Val Loss: 3.5552 Acc: 0.5081                                              \n",
      "Epoch 016 | Train Loss: 15.5433 Acc: 0.3893 | Val Loss: 3.8979 Acc: 0.5069                                              \n",
      "Epoch 017 | Train Loss: 14.7120 Acc: 0.3934 | Val Loss: 4.2596 Acc: 0.4985                                              \n",
      "Epoch 018 | Train Loss: 13.9963 Acc: 0.4009 | Val Loss: 3.6640 Acc: 0.4985                                              \n",
      "Epoch 019 | Train Loss: 13.1455 Acc: 0.3966 | Val Loss: 3.9498 Acc: 0.5051                                              \n",
      "Epoch 020 | Train Loss: 12.6669 Acc: 0.4009 | Val Loss: 3.7456 Acc: 0.5164                                              \n",
      "Epoch 021 | Train Loss: 12.1310 Acc: 0.4029 | Val Loss: 3.4474 Acc: 0.5113                                              \n",
      "Epoch 022 | Train Loss: 11.8610 Acc: 0.4053 | Val Loss: 3.8934 Acc: 0.4988                                              \n",
      "Epoch 023 | Train Loss: 11.4210 Acc: 0.4020 | Val Loss: 3.1309 Acc: 0.4991                                              \n",
      "Epoch 024 | Train Loss: 10.5456 Acc: 0.4131 | Val Loss: 3.2118 Acc: 0.5149                                              \n",
      "Epoch 025 | Train Loss: 10.2726 Acc: 0.4255 | Val Loss: 2.8922 Acc: 0.5140                                              \n",
      "Epoch 026 | Train Loss: 9.8572 Acc: 0.4200 | Val Loss: 3.1015 Acc: 0.5227                                               \n",
      "Epoch 027 | Train Loss: 9.0999 Acc: 0.4372 | Val Loss: 2.6545 Acc: 0.5161                                               \n",
      "Epoch 028 | Train Loss: 8.8417 Acc: 0.4359 | Val Loss: 2.8348 Acc: 0.5099                                               \n",
      "Epoch 029 | Train Loss: 8.6569 Acc: 0.4354 | Val Loss: 2.4360 Acc: 0.4976                                               \n",
      "Epoch 030 | Train Loss: 8.6163 Acc: 0.4329 | Val Loss: 2.4822 Acc: 0.4893                                               \n",
      "Epoch 031 | Train Loss: 7.9170 Acc: 0.4416 | Val Loss: 2.1162 Acc: 0.5442                                               \n",
      "Epoch 032 | Train Loss: 7.6104 Acc: 0.4485 | Val Loss: 2.0867 Acc: 0.5194                                               \n",
      "Epoch 033 | Train Loss: 7.5383 Acc: 0.4451 | Val Loss: 1.9435 Acc: 0.5690                                               \n",
      "Epoch 034 | Train Loss: 7.1599 Acc: 0.4482 | Val Loss: 1.9412 Acc: 0.5487                                               \n",
      "Epoch 035 | Train Loss: 6.9337 Acc: 0.4583 | Val Loss: 1.8954 Acc: 0.5919                                               \n",
      "Epoch 036 | Train Loss: 6.6200 Acc: 0.4620 | Val Loss: 2.0859 Acc: 0.5534                                               \n",
      "Epoch 037 | Train Loss: 6.3908 Acc: 0.4676 | Val Loss: 1.6839 Acc: 0.5937                                               \n",
      "Epoch 038 | Train Loss: 6.1769 Acc: 0.4722 | Val Loss: 1.7121 Acc: 0.5797                                               \n",
      "Epoch 039 | Train Loss: 5.8240 Acc: 0.4776 | Val Loss: 1.6758 Acc: 0.5764                                               \n",
      "Epoch 040 | Train Loss: 5.7448 Acc: 0.4772 | Val Loss: 1.6439 Acc: 0.5761                                               \n",
      "Epoch 041 | Train Loss: 5.5119 Acc: 0.4787 | Val Loss: 1.5773 Acc: 0.5794                                               \n",
      "Epoch 042 | Train Loss: 5.1752 Acc: 0.4920 | Val Loss: 1.4888 Acc: 0.6358                                               \n",
      "Epoch 043 | Train Loss: 5.2173 Acc: 0.4932 | Val Loss: 1.6185 Acc: 0.6412                                               \n",
      "Epoch 044 | Train Loss: 5.0008 Acc: 0.4980 | Val Loss: 1.5856 Acc: 0.6310                                               \n",
      "Epoch 045 | Train Loss: 4.8270 Acc: 0.5054 | Val Loss: 1.6993 Acc: 0.6143                                               \n",
      "Epoch 046 | Train Loss: 4.7131 Acc: 0.5084 | Val Loss: 1.3055 Acc: 0.6561                                               \n",
      "Epoch 047 | Train Loss: 4.4914 Acc: 0.5235 | Val Loss: 1.4710 Acc: 0.6510                                               \n",
      "Epoch 048 | Train Loss: 4.2952 Acc: 0.5161 | Val Loss: 1.6789 Acc: 0.6278                                               \n",
      "Epoch 049 | Train Loss: 4.3468 Acc: 0.5263 | Val Loss: 1.2909 Acc: 0.6621                                               \n",
      "Epoch 050 | Train Loss: 4.0593 Acc: 0.5418 | Val Loss: 1.4916 Acc: 0.6507                                               \n",
      "Epoch 051 | Train Loss: 3.9357 Acc: 0.5453 | Val Loss: 1.2150 Acc: 0.7119                                               \n",
      "Epoch 052 | Train Loss: 3.8504 Acc: 0.5501 | Val Loss: 1.3159 Acc: 0.6764                                               \n",
      "Epoch 053 | Train Loss: 3.7578 Acc: 0.5497 | Val Loss: 1.3460 Acc: 0.6994                                               \n",
      "Epoch 054 | Train Loss: 3.7315 Acc: 0.5591 | Val Loss: 1.5613 Acc: 0.6934                                               \n",
      "Epoch 055 | Train Loss: 3.6218 Acc: 0.5539 | Val Loss: 1.2286 Acc: 0.7227                                               \n",
      "Epoch 056 | Train Loss: 3.5626 Acc: 0.5613 | Val Loss: 1.4153 Acc: 0.6907                                               \n",
      "Epoch 057 | Train Loss: 3.3296 Acc: 0.5656 | Val Loss: 1.2533 Acc: 0.7215                                               \n",
      "Epoch 058 | Train Loss: 3.3767 Acc: 0.5713 | Val Loss: 1.3473 Acc: 0.6773                                               \n",
      "Epoch 059 | Train Loss: 3.2980 Acc: 0.5709 | Val Loss: 1.2933 Acc: 0.7313                                               \n",
      "Epoch 060 | Train Loss: 3.1097 Acc: 0.5788 | Val Loss: 1.2107 Acc: 0.7251                                               \n",
      "Epoch 061 | Train Loss: 3.1636 Acc: 0.5779 | Val Loss: 1.2175 Acc: 0.7251                                               \n",
      "Epoch 062 | Train Loss: 3.0827 Acc: 0.5864 | Val Loss: 1.0910 Acc: 0.7236                                               \n",
      "Epoch 063 | Train Loss: 2.8595 Acc: 0.5935 | Val Loss: 1.0407 Acc: 0.7287                                               \n",
      "Epoch 064 | Train Loss: 2.8946 Acc: 0.5929 | Val Loss: 1.2822 Acc: 0.6890                                               \n",
      "Epoch 065 | Train Loss: 2.8267 Acc: 0.5965 | Val Loss: 1.1007 Acc: 0.7051                                               \n",
      "Epoch 066 | Train Loss: 2.6852 Acc: 0.6024 | Val Loss: 1.0307 Acc: 0.7367                                               \n",
      "Epoch 067 | Train Loss: 2.7208 Acc: 0.6093 | Val Loss: 1.0005 Acc: 0.7409                                               \n",
      "Epoch 068 | Train Loss: 2.5954 Acc: 0.6094 | Val Loss: 1.0440 Acc: 0.7158                                               \n",
      "Epoch 069 | Train Loss: 2.4639 Acc: 0.6129 | Val Loss: 1.0218 Acc: 0.7149                                               \n",
      "Epoch 070 | Train Loss: 2.5162 Acc: 0.6194 | Val Loss: 1.1428 Acc: 0.6964                                               \n",
      "Epoch 071 | Train Loss: 2.5351 Acc: 0.6147 | Val Loss: 1.0257 Acc: 0.7248                                               \n",
      "Epoch 072 | Train Loss: 2.4255 Acc: 0.6188 | Val Loss: 1.0786 Acc: 0.7158                                               \n",
      "Epoch 073 | Train Loss: 2.4146 Acc: 0.6236 | Val Loss: 0.9303 Acc: 0.7406                                               \n",
      "Epoch 074 | Train Loss: 2.3693 Acc: 0.6241 | Val Loss: 0.9052 Acc: 0.7266                                               \n",
      "Epoch 075 | Train Loss: 2.2553 Acc: 0.6332 | Val Loss: 1.0191 Acc: 0.7263                                               \n",
      "Epoch 076 | Train Loss: 2.1972 Acc: 0.6371 | Val Loss: 0.9428 Acc: 0.7421                                               \n",
      "Epoch 077 | Train Loss: 2.2709 Acc: 0.6304 | Val Loss: 1.0061 Acc: 0.7233                                               \n",
      "Epoch 078 | Train Loss: 2.1580 Acc: 0.6397 | Val Loss: 0.8752 Acc: 0.7516                                               \n",
      "Epoch 079 | Train Loss: 2.0981 Acc: 0.6400 | Val Loss: 0.9476 Acc: 0.7340                                               \n",
      "Epoch 080 | Train Loss: 2.0389 Acc: 0.6461 | Val Loss: 0.8987 Acc: 0.7325                                               \n",
      "Epoch 081 | Train Loss: 2.0812 Acc: 0.6446 | Val Loss: 0.9151 Acc: 0.7406                                               \n",
      "Epoch 082 | Train Loss: 1.9567 Acc: 0.6525 | Val Loss: 0.8898 Acc: 0.7466                                               \n",
      "Epoch 083 | Train Loss: 1.9745 Acc: 0.6512 | Val Loss: 0.9248 Acc: 0.7394                                               \n",
      "Epoch 084 | Train Loss: 1.9103 Acc: 0.6583 | Val Loss: 0.8704 Acc: 0.7528                                               \n",
      "Epoch 085 | Train Loss: 1.9441 Acc: 0.6588 | Val Loss: 0.8604 Acc: 0.7376                                               \n",
      "Epoch 086 | Train Loss: 1.8694 Acc: 0.6628 | Val Loss: 0.7596 Acc: 0.7699                                               \n",
      "Epoch 087 | Train Loss: 1.8389 Acc: 0.6691 | Val Loss: 0.8105 Acc: 0.7564                                               \n",
      "Epoch 088 | Train Loss: 1.8330 Acc: 0.6656 | Val Loss: 0.8311 Acc: 0.7585                                               \n",
      "Epoch 089 | Train Loss: 1.8363 Acc: 0.6686 | Val Loss: 0.7033 Acc: 0.7815                                               \n",
      "Epoch 090 | Train Loss: 1.7571 Acc: 0.6733 | Val Loss: 0.7349 Acc: 0.7848                                               \n",
      "Epoch 091 | Train Loss: 1.7985 Acc: 0.6730 | Val Loss: 0.7911 Acc: 0.7618                                               \n",
      "Epoch 092 | Train Loss: 1.7548 Acc: 0.6716 | Val Loss: 0.7038 Acc: 0.7803                                               \n",
      "Epoch 093 | Train Loss: 1.6363 Acc: 0.6830 | Val Loss: 0.7610 Acc: 0.7713                                               \n",
      "Epoch 094 | Train Loss: 1.6417 Acc: 0.6803 | Val Loss: 0.7364 Acc: 0.7743                                               \n",
      "Epoch 095 | Train Loss: 1.6111 Acc: 0.6865 | Val Loss: 0.7093 Acc: 0.7818                                               \n",
      "Epoch 096 | Train Loss: 1.6243 Acc: 0.6949 | Val Loss: 0.7101 Acc: 0.7707                                               \n",
      "Epoch 097 | Train Loss: 1.5377 Acc: 0.6979 | Val Loss: 0.7028 Acc: 0.7788                                               \n",
      "Epoch 098 | Train Loss: 1.5587 Acc: 0.6963 | Val Loss: 0.7174 Acc: 0.7764                                               \n",
      "Epoch 099 | Train Loss: 1.5054 Acc: 0.6973 | Val Loss: 0.6930 Acc: 0.7752                                               \n",
      "Epoch 100 | Train Loss: 1.5385 Acc: 0.6979 | Val Loss: 0.7643 Acc: 0.7773                                               \n",
      "Epoch 101 | Train Loss: 1.4949 Acc: 0.7022 | Val Loss: 0.8349 Acc: 0.7457                                               \n",
      "Epoch 102 | Train Loss: 1.4442 Acc: 0.7069 | Val Loss: 0.7407 Acc: 0.7800                                               \n",
      "Epoch 103 | Train Loss: 1.4452 Acc: 0.7071 | Val Loss: 0.8149 Acc: 0.7406                                               \n",
      "Epoch 104 | Train Loss: 1.3883 Acc: 0.7106 | Val Loss: 0.8849 Acc: 0.7642                                               \n",
      "Epoch 105 | Train Loss: 1.3178 Acc: 0.7207 | Val Loss: 0.7221 Acc: 0.7776                                               \n",
      "Epoch 106 | Train Loss: 1.3555 Acc: 0.7204 | Val Loss: 0.7308 Acc: 0.7830                                               \n",
      "Epoch 107 | Train Loss: 1.3202 Acc: 0.7270 | Val Loss: 0.6285 Acc: 0.7925                                               \n",
      "Epoch 108 | Train Loss: 1.3657 Acc: 0.7226 | Val Loss: 0.7176 Acc: 0.7964                                               \n",
      "Epoch 109 | Train Loss: 1.3251 Acc: 0.7282 | Val Loss: 0.8079 Acc: 0.7707                                               \n",
      "Epoch 110 | Train Loss: 1.2846 Acc: 0.7331 | Val Loss: 0.7449 Acc: 0.7803                                               \n",
      "Epoch 111 | Train Loss: 1.2287 Acc: 0.7402 | Val Loss: 0.6001 Acc: 0.8146                                               \n",
      "Epoch 112 | Train Loss: 1.1576 Acc: 0.7511 | Val Loss: 0.5192 Acc: 0.8182                                               \n",
      "Epoch 113 | Train Loss: 1.2252 Acc: 0.7460 | Val Loss: 0.6795 Acc: 0.7925                                               \n",
      "Epoch 114 | Train Loss: 1.1901 Acc: 0.7515 | Val Loss: 0.6468 Acc: 0.7970                                               \n",
      "Epoch 115 | Train Loss: 1.2006 Acc: 0.7496 | Val Loss: 0.5762 Acc: 0.8143                                               \n",
      "Epoch 116 | Train Loss: 1.1666 Acc: 0.7539 | Val Loss: 0.7044 Acc: 0.7800                                               \n",
      "Epoch 117 | Train Loss: 1.1112 Acc: 0.7600 | Val Loss: 0.5586 Acc: 0.8164                                               \n",
      "Epoch 118 | Train Loss: 1.1237 Acc: 0.7507 | Val Loss: 0.6051 Acc: 0.8146                                               \n",
      "Epoch 119 | Train Loss: 1.1403 Acc: 0.7577 | Val Loss: 0.6495 Acc: 0.7904                                               \n",
      "Epoch 120 | Train Loss: 1.1116 Acc: 0.7639 | Val Loss: 0.6383 Acc: 0.8006                                               \n",
      "Epoch 121 | Train Loss: 1.0877 Acc: 0.7666 | Val Loss: 0.6121 Acc: 0.7988                                               \n",
      "Epoch 122 | Train Loss: 1.0350 Acc: 0.7706 | Val Loss: 0.5779 Acc: 0.8218                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 256, 'cnn_dropout': 0.22424134742962193, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 32, 'learning_rate': 0.00032008974062776497, 'lstm_dense': 64, 'lstm_hidden_size': 96, 'lstm_layers': 1, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 5.9552 Acc: 0.4296 | Val Loss: 1.2743 Acc: 0.4421                                               \n",
      "Epoch 002 | Train Loss: 1.2579 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 003 | Train Loss: 1.2390 Acc: 0.4422 | Val Loss: 1.2312 Acc: 0.4421                                               \n",
      "Epoch 004 | Train Loss: 1.2293 Acc: 0.4422 | Val Loss: 1.2232 Acc: 0.4421                                               \n",
      "Epoch 005 | Train Loss: 1.2212 Acc: 0.4422 | Val Loss: 1.2117 Acc: 0.4421                                               \n",
      "Epoch 006 | Train Loss: 1.2127 Acc: 0.4422 | Val Loss: 1.2132 Acc: 0.4421                                               \n",
      "Epoch 007 | Train Loss: 1.2033 Acc: 0.4422 | Val Loss: 1.1915 Acc: 0.4421                                               \n",
      "Epoch 008 | Train Loss: 1.1895 Acc: 0.4427 | Val Loss: 1.1787 Acc: 0.4606                                               \n",
      "Epoch 009 | Train Loss: 1.1734 Acc: 0.4686 | Val Loss: 1.1590 Acc: 0.5036                                               \n",
      "Epoch 010 | Train Loss: 1.1590 Acc: 0.4925 | Val Loss: 1.1434 Acc: 0.5185                                               \n",
      "Epoch 011 | Train Loss: 1.1385 Acc: 0.5151 | Val Loss: 1.1343 Acc: 0.5301                                               \n",
      "Epoch 012 | Train Loss: 1.1205 Acc: 0.5336 | Val Loss: 1.1014 Acc: 0.5490                                               \n",
      "Epoch 013 | Train Loss: 1.0989 Acc: 0.5340 | Val Loss: 1.0780 Acc: 0.5490                                               \n",
      "Epoch 014 | Train Loss: 1.0817 Acc: 0.5426 | Val Loss: 1.0578 Acc: 0.5573                                               \n",
      "Epoch 015 | Train Loss: 1.0577 Acc: 0.5496 | Val Loss: 1.0478 Acc: 0.5445                                               \n",
      "Epoch 016 | Train Loss: 1.0340 Acc: 0.5568 | Val Loss: 1.0066 Acc: 0.5645                                               \n",
      "Epoch 017 | Train Loss: 1.0211 Acc: 0.5540 | Val Loss: 1.0132 Acc: 0.5466                                               \n",
      "Epoch 018 | Train Loss: 1.0138 Acc: 0.5570 | Val Loss: 1.0161 Acc: 0.5687                                               \n",
      "Epoch 019 | Train Loss: 0.9664 Acc: 0.5724 | Val Loss: 0.9499 Acc: 0.5731                                               \n",
      "Epoch 020 | Train Loss: 0.9361 Acc: 0.5910 | Val Loss: 0.9170 Acc: 0.6269                                               \n",
      "Epoch 021 | Train Loss: 0.9097 Acc: 0.6259 | Val Loss: 0.9153 Acc: 0.6251                                               \n",
      "Epoch 022 | Train Loss: 0.8804 Acc: 0.6441 | Val Loss: 0.8428 Acc: 0.6713                                               \n",
      "Epoch 023 | Train Loss: 0.8404 Acc: 0.6736 | Val Loss: 0.8102 Acc: 0.6904                                               \n",
      "Epoch 024 | Train Loss: 0.8253 Acc: 0.6736 | Val Loss: 0.8001 Acc: 0.6961                                               \n",
      "Epoch 025 | Train Loss: 0.7897 Acc: 0.6985 | Val Loss: 0.7822 Acc: 0.6991                                               \n",
      "Epoch 026 | Train Loss: 0.7617 Acc: 0.7130 | Val Loss: 0.7928 Acc: 0.7000                                               \n",
      "Epoch 027 | Train Loss: 0.7379 Acc: 0.7198 | Val Loss: 0.6963 Acc: 0.7424                                               \n",
      "Epoch 028 | Train Loss: 0.7090 Acc: 0.7289 | Val Loss: 0.6977 Acc: 0.7364                                               \n",
      "Epoch 029 | Train Loss: 0.6864 Acc: 0.7385 | Val Loss: 0.6705 Acc: 0.7478                                               \n",
      "Epoch 030 | Train Loss: 0.6735 Acc: 0.7430 | Val Loss: 0.6333 Acc: 0.7657                                               \n",
      "Epoch 031 | Train Loss: 0.6466 Acc: 0.7538 | Val Loss: 0.5840 Acc: 0.7755                                               \n",
      "Epoch 032 | Train Loss: 0.6023 Acc: 0.7688 | Val Loss: 0.5985 Acc: 0.7812                                               \n",
      "Epoch 033 | Train Loss: 0.5734 Acc: 0.7810 | Val Loss: 0.6677 Acc: 0.7275                                               \n",
      "Epoch 034 | Train Loss: 0.5717 Acc: 0.7813 | Val Loss: 0.5215 Acc: 0.7976                                               \n",
      "Epoch 035 | Train Loss: 0.5396 Acc: 0.7934 | Val Loss: 0.5354 Acc: 0.7994                                               \n",
      "Epoch 036 | Train Loss: 0.5143 Acc: 0.8090 | Val Loss: 0.5089 Acc: 0.8158                                               \n",
      "Epoch 037 | Train Loss: 0.5133 Acc: 0.8065 | Val Loss: 0.5896 Acc: 0.7678                                               \n",
      "Epoch 038 | Train Loss: 0.4986 Acc: 0.8216 | Val Loss: 0.4652 Acc: 0.8343                                               \n",
      "Epoch 039 | Train Loss: 0.4702 Acc: 0.8314 | Val Loss: 0.4599 Acc: 0.8352                                               \n",
      "Epoch 040 | Train Loss: 0.4351 Acc: 0.8483 | Val Loss: 0.4568 Acc: 0.8346                                               \n",
      "Epoch 041 | Train Loss: 0.4263 Acc: 0.8497 | Val Loss: 0.4555 Acc: 0.8481                                               \n",
      "Epoch 042 | Train Loss: 0.4181 Acc: 0.8563 | Val Loss: 0.4286 Acc: 0.8546                                               \n",
      "Epoch 043 | Train Loss: 0.3977 Acc: 0.8639 | Val Loss: 0.3702 Acc: 0.8743                                               \n",
      "Epoch 044 | Train Loss: 0.3694 Acc: 0.8743 | Val Loss: 0.3815 Acc: 0.8719                                               \n",
      "Epoch 045 | Train Loss: 0.3658 Acc: 0.8746 | Val Loss: 0.3452 Acc: 0.8848                                               \n",
      "Epoch 046 | Train Loss: 0.3444 Acc: 0.8839 | Val Loss: 0.3579 Acc: 0.8755                                               \n",
      "Epoch 047 | Train Loss: 0.3416 Acc: 0.8812 | Val Loss: 0.3111 Acc: 0.8997                                               \n",
      "Epoch 048 | Train Loss: 0.3487 Acc: 0.8808 | Val Loss: 0.3366 Acc: 0.8857                                               \n",
      "Epoch 049 | Train Loss: 0.3166 Acc: 0.8966 | Val Loss: 0.3051 Acc: 0.8991                                               \n",
      "Epoch 050 | Train Loss: 0.2942 Acc: 0.9033 | Val Loss: 0.3097 Acc: 0.8970                                               \n",
      "Epoch 051 | Train Loss: 0.2846 Acc: 0.9059 | Val Loss: 0.2641 Acc: 0.9104                                               \n",
      "Epoch 052 | Train Loss: 0.2796 Acc: 0.9072 | Val Loss: 0.3120 Acc: 0.8979                                               \n",
      "Epoch 053 | Train Loss: 0.2743 Acc: 0.9096 | Val Loss: 0.2587 Acc: 0.9164                                               \n",
      "Epoch 054 | Train Loss: 0.2636 Acc: 0.9117 | Val Loss: 0.2994 Acc: 0.9090                                               \n",
      "Epoch 055 | Train Loss: 0.2487 Acc: 0.9184 | Val Loss: 0.2627 Acc: 0.9161                                               \n",
      "Epoch 056 | Train Loss: 0.2368 Acc: 0.9231 | Val Loss: 0.2926 Acc: 0.9063                                               \n",
      "Epoch 057 | Train Loss: 0.2393 Acc: 0.9202 | Val Loss: 0.2862 Acc: 0.8991                                               \n",
      "Epoch 058 | Train Loss: 0.2067 Acc: 0.9343 | Val Loss: 0.2636 Acc: 0.9125                                               \n",
      "Epoch 059 | Train Loss: 0.2139 Acc: 0.9298 | Val Loss: 0.2864 Acc: 0.9009                                               \n",
      "Epoch 060 | Train Loss: 0.1959 Acc: 0.9356 | Val Loss: 0.2680 Acc: 0.9131                                               \n",
      "Epoch 061 | Train Loss: 0.2032 Acc: 0.9331 | Val Loss: 0.2334 Acc: 0.9263                                               \n",
      "Epoch 062 | Train Loss: 0.1831 Acc: 0.9421 | Val Loss: 0.2230 Acc: 0.9248                                               \n",
      "Epoch 063 | Train Loss: 0.1799 Acc: 0.9422 | Val Loss: 0.2229 Acc: 0.9254                                               \n",
      "Epoch 064 | Train Loss: 0.1684 Acc: 0.9444 | Val Loss: 0.2151 Acc: 0.9301                                               \n",
      "Epoch 065 | Train Loss: 0.1808 Acc: 0.9404 | Val Loss: 0.2274 Acc: 0.9296                                               \n",
      "Epoch 066 | Train Loss: 0.1533 Acc: 0.9506 | Val Loss: 0.1796 Acc: 0.9448                                               \n",
      "Epoch 067 | Train Loss: 0.1668 Acc: 0.9437 | Val Loss: 0.1989 Acc: 0.9346                                               \n",
      "Epoch 068 | Train Loss: 0.1578 Acc: 0.9484 | Val Loss: 0.1854 Acc: 0.9406                                               \n",
      "Epoch 069 | Train Loss: 0.1424 Acc: 0.9524 | Val Loss: 0.2114 Acc: 0.9313                                               \n",
      "Epoch 070 | Train Loss: 0.1711 Acc: 0.9417 | Val Loss: 0.1852 Acc: 0.9391                                               \n",
      "Epoch 071 | Train Loss: 0.1430 Acc: 0.9527 | Val Loss: 0.1780 Acc: 0.9367                                               \n",
      "Epoch 072 | Train Loss: 0.1307 Acc: 0.9601 | Val Loss: 0.1973 Acc: 0.9337                                               \n",
      "Epoch 073 | Train Loss: 0.1302 Acc: 0.9591 | Val Loss: 0.2138 Acc: 0.9281                                               \n",
      "Epoch 074 | Train Loss: 0.1209 Acc: 0.9619 | Val Loss: 0.1677 Acc: 0.9424                                               \n",
      "Epoch 075 | Train Loss: 0.1255 Acc: 0.9595 | Val Loss: 0.1584 Acc: 0.9487                                               \n",
      "Epoch 076 | Train Loss: 0.1107 Acc: 0.9627 | Val Loss: 0.1490 Acc: 0.9507                                               \n",
      "Epoch 077 | Train Loss: 0.1103 Acc: 0.9653 | Val Loss: 0.1766 Acc: 0.9496                                               \n",
      "Epoch 078 | Train Loss: 0.1019 Acc: 0.9653 | Val Loss: 0.1466 Acc: 0.9543                                               \n",
      "Epoch 079 | Train Loss: 0.0988 Acc: 0.9688 | Val Loss: 0.1941 Acc: 0.9406                                               \n",
      "Epoch 080 | Train Loss: 0.0982 Acc: 0.9693 | Val Loss: 0.1985 Acc: 0.9427                                               \n",
      "Epoch 081 | Train Loss: 0.1117 Acc: 0.9617 | Val Loss: 0.1341 Acc: 0.9570                                               \n",
      "Epoch 082 | Train Loss: 0.1008 Acc: 0.9662 | Val Loss: 0.1594 Acc: 0.9481                                               \n",
      "Epoch 083 | Train Loss: 0.0907 Acc: 0.9702 | Val Loss: 0.1292 Acc: 0.9552                                               \n",
      "Epoch 084 | Train Loss: 0.0840 Acc: 0.9728 | Val Loss: 0.1628 Acc: 0.9501                                               \n",
      "Epoch 085 | Train Loss: 0.1233 Acc: 0.9587 | Val Loss: 0.2173 Acc: 0.9284                                               \n",
      "Epoch 086 | Train Loss: 0.1160 Acc: 0.9619 | Val Loss: 0.1326 Acc: 0.9591                                               \n",
      "Epoch 087 | Train Loss: 0.0872 Acc: 0.9707 | Val Loss: 0.1512 Acc: 0.9510                                               \n",
      "Epoch 088 | Train Loss: 0.0816 Acc: 0.9725 | Val Loss: 0.1394 Acc: 0.9561                                               \n",
      "Epoch 089 | Train Loss: 0.0768 Acc: 0.9754 | Val Loss: 0.1261 Acc: 0.9561                                               \n",
      "Epoch 090 | Train Loss: 0.0792 Acc: 0.9746 | Val Loss: 0.1406 Acc: 0.9558                                               \n",
      "Epoch 091 | Train Loss: 0.0768 Acc: 0.9757 | Val Loss: 0.1352 Acc: 0.9576                                               \n",
      "Epoch 092 | Train Loss: 0.0721 Acc: 0.9763 | Val Loss: 0.1346 Acc: 0.9600                                               \n",
      "Epoch 093 | Train Loss: 0.0680 Acc: 0.9775 | Val Loss: 0.1735 Acc: 0.9484                                               \n",
      "Epoch 094 | Train Loss: 0.0716 Acc: 0.9768 | Val Loss: 0.1320 Acc: 0.9582                                               \n",
      "Epoch 095 | Train Loss: 0.0669 Acc: 0.9780 | Val Loss: 0.1248 Acc: 0.9588                                               \n",
      "Epoch 096 | Train Loss: 0.0655 Acc: 0.9799 | Val Loss: 0.1290 Acc: 0.9594                                               \n",
      "Epoch 097 | Train Loss: 0.0667 Acc: 0.9790 | Val Loss: 0.1183 Acc: 0.9648                                               \n",
      "Epoch 098 | Train Loss: 0.0575 Acc: 0.9818 | Val Loss: 0.1482 Acc: 0.9525                                               \n",
      "Epoch 099 | Train Loss: 0.0645 Acc: 0.9799 | Val Loss: 0.1298 Acc: 0.9582                                               \n",
      "Epoch 100 | Train Loss: 0.0550 Acc: 0.9822 | Val Loss: 0.1275 Acc: 0.9609                                               \n",
      "Epoch 101 | Train Loss: 0.0542 Acc: 0.9815 | Val Loss: 0.1254 Acc: 0.9588                                               \n",
      "Epoch 102 | Train Loss: 0.0480 Acc: 0.9857 | Val Loss: 0.1041 Acc: 0.9663                                               \n",
      "Epoch 103 | Train Loss: 0.0504 Acc: 0.9838 | Val Loss: 0.1311 Acc: 0.9576                                               \n",
      "Epoch 104 | Train Loss: 0.0470 Acc: 0.9847 | Val Loss: 0.1190 Acc: 0.9642                                               \n",
      "Epoch 105 | Train Loss: 0.0501 Acc: 0.9841 | Val Loss: 0.1995 Acc: 0.9430                                               \n",
      "Epoch 106 | Train Loss: 0.0534 Acc: 0.9824 | Val Loss: 0.1748 Acc: 0.9496                                               \n",
      "Epoch 107 | Train Loss: 0.0504 Acc: 0.9838 | Val Loss: 0.1141 Acc: 0.9618                                               \n",
      "Epoch 108 | Train Loss: 0.0436 Acc: 0.9863 | Val Loss: 0.1020 Acc: 0.9699                                               \n",
      "Epoch 109 | Train Loss: 0.0393 Acc: 0.9878 | Val Loss: 0.1531 Acc: 0.9555                                               \n",
      "Epoch 110 | Train Loss: 0.0479 Acc: 0.9838 | Val Loss: 0.1173 Acc: 0.9678                                               \n",
      "Epoch 111 | Train Loss: 0.0426 Acc: 0.9876 | Val Loss: 0.1269 Acc: 0.9612                                               \n",
      "Epoch 112 | Train Loss: 0.0446 Acc: 0.9859 | Val Loss: 0.1327 Acc: 0.9585                                               \n",
      "Epoch 113 | Train Loss: 0.0366 Acc: 0.9879 | Val Loss: 0.1127 Acc: 0.9663                                               \n",
      "Epoch 114 | Train Loss: 0.0418 Acc: 0.9859 | Val Loss: 0.1221 Acc: 0.9600                                               \n",
      "Epoch 115 | Train Loss: 0.0392 Acc: 0.9875 | Val Loss: 0.0990 Acc: 0.9690                                               \n",
      "Epoch 116 | Train Loss: 0.0408 Acc: 0.9875 | Val Loss: 0.1154 Acc: 0.9660                                               \n",
      "Epoch 117 | Train Loss: 0.0450 Acc: 0.9851 | Val Loss: 0.1032 Acc: 0.9651                                               \n",
      "Epoch 118 | Train Loss: 0.0484 Acc: 0.9846 | Val Loss: 0.1091 Acc: 0.9681                                               \n",
      "Epoch 119 | Train Loss: 0.0374 Acc: 0.9881 | Val Loss: 0.1245 Acc: 0.9642                                               \n",
      "Epoch 120 | Train Loss: 0.0359 Acc: 0.9890 | Val Loss: 0.1347 Acc: 0.9591                                               \n",
      "Epoch 121 | Train Loss: 0.0377 Acc: 0.9877 | Val Loss: 0.1242 Acc: 0.9663                                               \n",
      "Epoch 122 | Train Loss: 0.0381 Acc: 0.9885 | Val Loss: 0.1045 Acc: 0.9704                                               \n",
      "Epoch 123 | Train Loss: 0.0441 Acc: 0.9852 | Val Loss: 0.1249 Acc: 0.9660                                               \n",
      "Epoch 124 | Train Loss: 0.0386 Acc: 0.9884 | Val Loss: 0.1392 Acc: 0.9606                                               \n",
      "Epoch 125 | Train Loss: 0.0338 Acc: 0.9896 | Val Loss: 0.1518 Acc: 0.9576                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 256, 'cnn_dropout': 0.10429377676003188, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 16, 'learning_rate': 0.0020535757704218476, 'lstm_dense': 64, 'lstm_hidden_size': 32, 'lstm_layers': 4, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 31.9959 Acc: 0.4065 | Val Loss: 2.5167 Acc: 0.4779                                              \n",
      "Epoch 002 | Train Loss: 2.7126 Acc: 0.4570 | Val Loss: 1.4097 Acc: 0.4827                                               \n",
      "Epoch 003 | Train Loss: 2.0716 Acc: 0.4811 | Val Loss: 1.9055 Acc: 0.5203                                               \n",
      "Epoch 004 | Train Loss: 1.4967 Acc: 0.5171 | Val Loss: 1.2022 Acc: 0.5642                                               \n",
      "Epoch 005 | Train Loss: 1.3799 Acc: 0.5424 | Val Loss: 1.4235 Acc: 0.5090                                               \n",
      "Epoch 006 | Train Loss: 1.3276 Acc: 0.5609 | Val Loss: 1.8390 Acc: 0.5606                                               \n",
      "Epoch 007 | Train Loss: 1.0853 Acc: 0.6070 | Val Loss: 1.1296 Acc: 0.6549                                               \n",
      "Epoch 008 | Train Loss: 0.9799 Acc: 0.6250 | Val Loss: 0.8043 Acc: 0.6597                                               \n",
      "Epoch 009 | Train Loss: 0.8228 Acc: 0.6564 | Val Loss: 0.7871 Acc: 0.6445                                               \n",
      "Epoch 010 | Train Loss: 0.8527 Acc: 0.6561 | Val Loss: 0.5762 Acc: 0.7343                                               \n",
      "Epoch 011 | Train Loss: 0.7344 Acc: 0.6900 | Val Loss: 0.7041 Acc: 0.6704                                               \n",
      "Epoch 012 | Train Loss: 0.6856 Acc: 0.7124 | Val Loss: 0.6601 Acc: 0.7397                                               \n",
      "Epoch 013 | Train Loss: 0.6343 Acc: 0.7375 | Val Loss: 0.5101 Acc: 0.7964                                               \n",
      "Epoch 014 | Train Loss: 0.6160 Acc: 0.7497 | Val Loss: 0.4629 Acc: 0.8066                                               \n",
      "Epoch 015 | Train Loss: 0.5998 Acc: 0.7565 | Val Loss: 0.5950 Acc: 0.7552                                               \n",
      "Epoch 016 | Train Loss: 0.5687 Acc: 0.7696 | Val Loss: 0.6025 Acc: 0.7466                                               \n",
      "Epoch 017 | Train Loss: 0.5357 Acc: 0.7856 | Val Loss: 0.4840 Acc: 0.7997                                               \n",
      "Epoch 018 | Train Loss: 0.4557 Acc: 0.8181 | Val Loss: 0.4316 Acc: 0.8328                                               \n",
      "Epoch 019 | Train Loss: 0.4111 Acc: 0.8387 | Val Loss: 0.5116 Acc: 0.7961                                               \n",
      "Epoch 020 | Train Loss: 0.4030 Acc: 0.8417 | Val Loss: 0.3599 Acc: 0.8513                                               \n",
      "Epoch 021 | Train Loss: 0.3447 Acc: 0.8625 | Val Loss: 0.3596 Acc: 0.8525                                               \n",
      "Epoch 022 | Train Loss: 0.3293 Acc: 0.8766 | Val Loss: 0.2808 Acc: 0.8848                                               \n",
      "Epoch 023 | Train Loss: 0.3259 Acc: 0.8748 | Val Loss: 0.3278 Acc: 0.8728                                               \n",
      "Epoch 024 | Train Loss: 0.2761 Acc: 0.8946 | Val Loss: 0.2797 Acc: 0.8866                                               \n",
      "Epoch 025 | Train Loss: 0.2696 Acc: 0.8985 | Val Loss: 0.2337 Acc: 0.9048                                               \n",
      "Epoch 026 | Train Loss: 0.2375 Acc: 0.9124 | Val Loss: 0.2430 Acc: 0.9024                                               \n",
      "Epoch 027 | Train Loss: 0.2523 Acc: 0.9073 | Val Loss: 0.2200 Acc: 0.9176                                               \n",
      "Epoch 028 | Train Loss: 0.2049 Acc: 0.9256 | Val Loss: 0.1797 Acc: 0.9346                                               \n",
      "Epoch 029 | Train Loss: 0.2036 Acc: 0.9256 | Val Loss: 0.1874 Acc: 0.9278                                               \n",
      "Epoch 030 | Train Loss: 0.1980 Acc: 0.9298 | Val Loss: 0.1857 Acc: 0.9352                                               \n",
      "Epoch 031 | Train Loss: 0.1944 Acc: 0.9309 | Val Loss: 0.1964 Acc: 0.9331                                               \n",
      "Epoch 032 | Train Loss: 0.2115 Acc: 0.9261 | Val Loss: 0.1753 Acc: 0.9364                                               \n",
      "Epoch 033 | Train Loss: 0.2070 Acc: 0.9269 | Val Loss: 0.2103 Acc: 0.9200                                               \n",
      "Epoch 034 | Train Loss: 0.1815 Acc: 0.9354 | Val Loss: 0.2094 Acc: 0.9230                                               \n",
      "Epoch 035 | Train Loss: 0.1823 Acc: 0.9379 | Val Loss: 0.3695 Acc: 0.8725                                               \n",
      "Epoch 036 | Train Loss: 0.1575 Acc: 0.9451 | Val Loss: 0.1567 Acc: 0.9466                                               \n",
      "Epoch 037 | Train Loss: 0.1523 Acc: 0.9471 | Val Loss: 0.1842 Acc: 0.9406                                               \n",
      "Epoch 038 | Train Loss: 0.1453 Acc: 0.9502 | Val Loss: 0.1795 Acc: 0.9388                                               \n",
      "Epoch 039 | Train Loss: 0.1498 Acc: 0.9482 | Val Loss: 0.1753 Acc: 0.9373                                               \n",
      "Epoch 040 | Train Loss: 0.1237 Acc: 0.9600 | Val Loss: 0.1595 Acc: 0.9448                                               \n",
      "Epoch 041 | Train Loss: 0.1412 Acc: 0.9510 | Val Loss: 0.1550 Acc: 0.9433                                               \n",
      "Epoch 042 | Train Loss: 0.1231 Acc: 0.9587 | Val Loss: 0.1628 Acc: 0.9463                                               \n",
      "Epoch 043 | Train Loss: 0.1574 Acc: 0.9436 | Val Loss: 0.1713 Acc: 0.9376                                               \n",
      "Epoch 044 | Train Loss: 0.1463 Acc: 0.9501 | Val Loss: 0.1381 Acc: 0.9528                                               \n",
      "Epoch 045 | Train Loss: 0.1367 Acc: 0.9521 | Val Loss: 0.1578 Acc: 0.9481                                               \n",
      "Epoch 046 | Train Loss: 0.1174 Acc: 0.9591 | Val Loss: 0.1379 Acc: 0.9513                                               \n",
      "Epoch 047 | Train Loss: 0.1183 Acc: 0.9580 | Val Loss: 0.1581 Acc: 0.9481                                               \n",
      "Epoch 048 | Train Loss: 0.1102 Acc: 0.9624 | Val Loss: 0.1793 Acc: 0.9397                                               \n",
      "Epoch 049 | Train Loss: 0.1261 Acc: 0.9551 | Val Loss: 0.1751 Acc: 0.9436                                               \n",
      "Epoch 050 | Train Loss: 0.1063 Acc: 0.9635 | Val Loss: 0.1383 Acc: 0.9531                                               \n",
      "Epoch 051 | Train Loss: 0.1241 Acc: 0.9578 | Val Loss: 0.1609 Acc: 0.9510                                               \n",
      "Epoch 052 | Train Loss: 0.1197 Acc: 0.9579 | Val Loss: 0.1482 Acc: 0.9540                                               \n",
      "Epoch 053 | Train Loss: 0.1074 Acc: 0.9639 | Val Loss: 0.1447 Acc: 0.9555                                               \n",
      "Epoch 054 | Train Loss: 0.1332 Acc: 0.9556 | Val Loss: 0.1290 Acc: 0.9588                                               \n",
      "Epoch 055 | Train Loss: 0.1162 Acc: 0.9609 | Val Loss: 0.1361 Acc: 0.9576                                               \n",
      "Epoch 056 | Train Loss: 0.0987 Acc: 0.9671 | Val Loss: 0.1290 Acc: 0.9588                                               \n",
      "Epoch 057 | Train Loss: 0.1049 Acc: 0.9664 | Val Loss: 0.1307 Acc: 0.9600                                               \n",
      "Epoch 058 | Train Loss: 0.1175 Acc: 0.9598 | Val Loss: 0.2018 Acc: 0.9385                                               \n",
      "Epoch 059 | Train Loss: 0.1210 Acc: 0.9580 | Val Loss: 0.1507 Acc: 0.9445                                               \n",
      "Epoch 060 | Train Loss: 0.0899 Acc: 0.9697 | Val Loss: 0.1404 Acc: 0.9567                                               \n",
      "Epoch 061 | Train Loss: 0.1092 Acc: 0.9658 | Val Loss: 0.1513 Acc: 0.9493                                               \n",
      "Epoch 062 | Train Loss: 0.1198 Acc: 0.9593 | Val Loss: 0.1927 Acc: 0.9430                                               \n",
      "Epoch 063 | Train Loss: 0.0987 Acc: 0.9675 | Val Loss: 0.1667 Acc: 0.9516                                               \n",
      "Epoch 064 | Train Loss: 0.1009 Acc: 0.9673 | Val Loss: 0.2046 Acc: 0.9376                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 128, 'cnn_dropout': 0.40620545768177385, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 64, 'learning_rate': 0.0007007268395862725, 'lstm_dense': 64, 'lstm_hidden_size': 64, 'lstm_layers': 6, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 14.1047 Acc: 0.4250 | Val Loss: 3.6410 Acc: 0.4499                                              \n",
      "Epoch 002 | Train Loss: 3.0809 Acc: 0.4966 | Val Loss: 2.3307 Acc: 0.5328                                               \n",
      "Epoch 003 | Train Loss: 1.7389 Acc: 0.5444 | Val Loss: 1.2093 Acc: 0.5976                                               \n",
      "Epoch 004 | Train Loss: 1.1956 Acc: 0.5770 | Val Loss: 0.7319 Acc: 0.6893                                               \n",
      "Epoch 005 | Train Loss: 0.9843 Acc: 0.5978 | Val Loss: 0.7436 Acc: 0.6051                                               \n",
      "Epoch 006 | Train Loss: 0.8331 Acc: 0.6315 | Val Loss: 0.8369 Acc: 0.6337                                               \n",
      "Epoch 007 | Train Loss: 0.7845 Acc: 0.6527 | Val Loss: 0.5834 Acc: 0.7391                                               \n",
      "Epoch 008 | Train Loss: 0.7031 Acc: 0.6681 | Val Loss: 0.5978 Acc: 0.7281                                               \n",
      "Epoch 009 | Train Loss: 0.6809 Acc: 0.6862 | Val Loss: 0.5352 Acc: 0.7782                                               \n",
      "Epoch 010 | Train Loss: 0.6467 Acc: 0.7065 | Val Loss: 0.4999 Acc: 0.7919                                               \n",
      "Epoch 011 | Train Loss: 0.6089 Acc: 0.7175 | Val Loss: 0.5272 Acc: 0.7475                                               \n",
      "Epoch 012 | Train Loss: 0.5846 Acc: 0.7268 | Val Loss: 0.4849 Acc: 0.8018                                               \n",
      "Epoch 013 | Train Loss: 0.5639 Acc: 0.7352 | Val Loss: 0.4659 Acc: 0.7693                                               \n",
      "Epoch 014 | Train Loss: 0.5332 Acc: 0.7550 | Val Loss: 0.4387 Acc: 0.8263                                               \n",
      "Epoch 015 | Train Loss: 0.5393 Acc: 0.7539 | Val Loss: 0.4821 Acc: 0.7904                                               \n",
      "Epoch 016 | Train Loss: 0.5167 Acc: 0.7638 | Val Loss: 0.4072 Acc: 0.8316                                               \n",
      "Epoch 017 | Train Loss: 0.4966 Acc: 0.7733 | Val Loss: 0.3898 Acc: 0.7997                                               \n",
      "Epoch 018 | Train Loss: 0.5136 Acc: 0.7686 | Val Loss: 0.4411 Acc: 0.7910                                               \n",
      "Epoch 019 | Train Loss: 0.5013 Acc: 0.7713 | Val Loss: 0.4306 Acc: 0.8060                                               \n",
      "Epoch 020 | Train Loss: 0.4645 Acc: 0.7901 | Val Loss: 0.4302 Acc: 0.7701                                               \n",
      "Epoch 021 | Train Loss: 0.4469 Acc: 0.7980 | Val Loss: 0.4215 Acc: 0.8027                                               \n",
      "Epoch 022 | Train Loss: 0.4288 Acc: 0.8051 | Val Loss: 0.4180 Acc: 0.8110                                               \n",
      "Epoch 023 | Train Loss: 0.4139 Acc: 0.8146 | Val Loss: 0.3995 Acc: 0.7904                                               \n",
      "Epoch 024 | Train Loss: 0.4236 Acc: 0.8098 | Val Loss: 0.3123 Acc: 0.8612                                               \n",
      "Epoch 025 | Train Loss: 0.4050 Acc: 0.8235 | Val Loss: 0.4188 Acc: 0.7713                                               \n",
      "Epoch 026 | Train Loss: 0.3865 Acc: 0.8301 | Val Loss: 0.4086 Acc: 0.8218                                               \n",
      "Epoch 027 | Train Loss: 0.3883 Acc: 0.8310 | Val Loss: 0.3603 Acc: 0.8439                                               \n",
      "Epoch 028 | Train Loss: 0.3887 Acc: 0.8325 | Val Loss: 0.4274 Acc: 0.8266                                               \n",
      "Epoch 029 | Train Loss: 0.3453 Acc: 0.8478 | Val Loss: 0.3727 Acc: 0.8388                                               \n",
      "Epoch 030 | Train Loss: 0.3593 Acc: 0.8471 | Val Loss: 0.3217 Acc: 0.8412                                               \n",
      "Epoch 031 | Train Loss: 0.3432 Acc: 0.8522 | Val Loss: 0.4060 Acc: 0.8272                                               \n",
      "Epoch 032 | Train Loss: 0.3392 Acc: 0.8542 | Val Loss: 0.3530 Acc: 0.8672                                               \n",
      "Epoch 033 | Train Loss: 0.3627 Acc: 0.8477 | Val Loss: 0.3832 Acc: 0.7934                                               \n",
      "Epoch 034 | Train Loss: 0.3240 Acc: 0.8686 | Val Loss: 0.3788 Acc: 0.8451                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 64, 'cnn_dropout': 0.2730961143078874, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 16, 'cnn_kernels_2': 32, 'learning_rate': 1.672444548723697e-05, 'lstm_dense': 256, 'lstm_hidden_size': 64, 'lstm_layers': 4, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 13.1187 Acc: 0.3323 | Val Loss: 1.3958 Acc: 0.2743                                              \n",
      "Epoch 002 | Train Loss: 1.6608 Acc: 0.3385 | Val Loss: 1.3924 Acc: 0.2791                                               \n",
      "Epoch 003 | Train Loss: 1.5088 Acc: 0.3597 | Val Loss: 1.3798 Acc: 0.3048                                               \n",
      "Epoch 004 | Train Loss: 1.4114 Acc: 0.3832 | Val Loss: 1.2851 Acc: 0.4030                                               \n",
      "Epoch 005 | Train Loss: 1.3285 Acc: 0.4167 | Val Loss: 1.1596 Acc: 0.4699                                               \n",
      "Epoch 006 | Train Loss: 1.2593 Acc: 0.4312 | Val Loss: 1.1098 Acc: 0.4710                                               \n",
      "Epoch 007 | Train Loss: 1.1971 Acc: 0.4403 | Val Loss: 1.1108 Acc: 0.4779                                               \n",
      "Epoch 008 | Train Loss: 1.1925 Acc: 0.4322 | Val Loss: 1.1065 Acc: 0.4490                                               \n",
      "Epoch 009 | Train Loss: 1.1598 Acc: 0.4279 | Val Loss: 1.0970 Acc: 0.4525                                               \n",
      "Epoch 010 | Train Loss: 1.1545 Acc: 0.4426 | Val Loss: 1.1382 Acc: 0.3806                                               \n",
      "Epoch 011 | Train Loss: 1.1401 Acc: 0.4445 | Val Loss: 1.0855 Acc: 0.4564                                               \n",
      "Epoch 012 | Train Loss: 1.1163 Acc: 0.4447 | Val Loss: 1.0806 Acc: 0.4806                                               \n",
      "Epoch 013 | Train Loss: 1.1131 Acc: 0.4502 | Val Loss: 1.0923 Acc: 0.4322                                               \n",
      "Epoch 014 | Train Loss: 1.1055 Acc: 0.4446 | Val Loss: 1.0813 Acc: 0.3737                                               \n",
      "Epoch 015 | Train Loss: 1.0898 Acc: 0.4400 | Val Loss: 1.0540 Acc: 0.3767                                               \n",
      "Epoch 016 | Train Loss: 1.0731 Acc: 0.4373 | Val Loss: 1.0054 Acc: 0.4525                                               \n",
      "Epoch 017 | Train Loss: 1.0552 Acc: 0.4450 | Val Loss: 0.9924 Acc: 0.4564                                               \n",
      "Epoch 018 | Train Loss: 1.0368 Acc: 0.4432 | Val Loss: 0.9949 Acc: 0.3964                                               \n",
      "Epoch 019 | Train Loss: 1.0372 Acc: 0.4444 | Val Loss: 0.9697 Acc: 0.4460                                               \n",
      "Epoch 020 | Train Loss: 1.0253 Acc: 0.4420 | Val Loss: 0.9639 Acc: 0.4394                                               \n",
      "Epoch 021 | Train Loss: 1.0103 Acc: 0.4463 | Val Loss: 0.9558 Acc: 0.4803                                               \n",
      "Epoch 022 | Train Loss: 1.0000 Acc: 0.4451 | Val Loss: 0.9394 Acc: 0.4749                                               \n",
      "Epoch 023 | Train Loss: 0.9969 Acc: 0.4498 | Val Loss: 0.9385 Acc: 0.4916                                               \n",
      "Epoch 024 | Train Loss: 0.9956 Acc: 0.4508 | Val Loss: 0.9342 Acc: 0.4707                                               \n",
      "Epoch 025 | Train Loss: 0.9877 Acc: 0.4420 | Val Loss: 0.9344 Acc: 0.4633                                               \n",
      "Epoch 026 | Train Loss: 0.9825 Acc: 0.4472 | Val Loss: 0.9081 Acc: 0.4699                                               \n",
      "Epoch 027 | Train Loss: 0.9744 Acc: 0.4489 | Val Loss: 0.9114 Acc: 0.4513                                               \n",
      "Epoch 028 | Train Loss: 0.9715 Acc: 0.4463 | Val Loss: 0.9129 Acc: 0.4848                                               \n",
      "Epoch 029 | Train Loss: 0.9735 Acc: 0.4489 | Val Loss: 0.9132 Acc: 0.4394                                               \n",
      "Epoch 030 | Train Loss: 0.9654 Acc: 0.4460 | Val Loss: 0.9004 Acc: 0.4579                                               \n",
      "Epoch 031 | Train Loss: 0.9688 Acc: 0.4531 | Val Loss: 0.9029 Acc: 0.4660                                               \n",
      "Epoch 032 | Train Loss: 0.9558 Acc: 0.4449 | Val Loss: 0.8953 Acc: 0.4794                                               \n",
      "Epoch 033 | Train Loss: 0.9552 Acc: 0.4529 | Val Loss: 0.8943 Acc: 0.4513                                               \n",
      "Epoch 034 | Train Loss: 0.9535 Acc: 0.4503 | Val Loss: 0.9182 Acc: 0.4319                                               \n",
      "Epoch 035 | Train Loss: 0.9592 Acc: 0.4491 | Val Loss: 0.9024 Acc: 0.4872                                               \n",
      "Epoch 036 | Train Loss: 0.9451 Acc: 0.4514 | Val Loss: 0.8875 Acc: 0.4684                                               \n",
      "Epoch 037 | Train Loss: 0.9462 Acc: 0.4538 | Val Loss: 0.9005 Acc: 0.4472                                               \n",
      "Epoch 038 | Train Loss: 0.9472 Acc: 0.4512 | Val Loss: 0.8923 Acc: 0.4555                                               \n",
      "Epoch 039 | Train Loss: 0.9475 Acc: 0.4471 | Val Loss: 0.8837 Acc: 0.4690                                               \n",
      "Epoch 040 | Train Loss: 0.9376 Acc: 0.4496 | Val Loss: 0.8910 Acc: 0.5149                                               \n",
      "Epoch 041 | Train Loss: 0.9390 Acc: 0.4512 | Val Loss: 0.9048 Acc: 0.4370                                               \n",
      "Epoch 042 | Train Loss: 0.9325 Acc: 0.4530 | Val Loss: 0.9020 Acc: 0.4218                                               \n",
      "Epoch 043 | Train Loss: 0.9331 Acc: 0.4520 | Val Loss: 0.8883 Acc: 0.4185                                               \n",
      "Epoch 044 | Train Loss: 0.9273 Acc: 0.4546 | Val Loss: 0.8782 Acc: 0.4570                                               \n",
      "Epoch 045 | Train Loss: 0.9304 Acc: 0.4524 | Val Loss: 0.8826 Acc: 0.4409                                               \n",
      "Epoch 046 | Train Loss: 0.9270 Acc: 0.4540 | Val Loss: 0.8708 Acc: 0.4827                                               \n",
      "Epoch 047 | Train Loss: 0.9255 Acc: 0.4573 | Val Loss: 0.9051 Acc: 0.4128                                               \n",
      "Epoch 048 | Train Loss: 0.9229 Acc: 0.4528 | Val Loss: 0.8867 Acc: 0.4484                                               \n",
      "Epoch 049 | Train Loss: 0.9256 Acc: 0.4529 | Val Loss: 0.8774 Acc: 0.4484                                               \n",
      "Epoch 050 | Train Loss: 0.9216 Acc: 0.4494 | Val Loss: 0.8763 Acc: 0.4940                                               \n",
      "Epoch 051 | Train Loss: 0.9174 Acc: 0.4547 | Val Loss: 0.8765 Acc: 0.4409                                               \n",
      "Epoch 052 | Train Loss: 0.9155 Acc: 0.4562 | Val Loss: 0.8695 Acc: 0.4490                                               \n",
      "Epoch 053 | Train Loss: 0.9171 Acc: 0.4543 | Val Loss: 0.8516 Acc: 0.4693                                               \n",
      "Epoch 054 | Train Loss: 0.9175 Acc: 0.4555 | Val Loss: 0.8532 Acc: 0.4693                                               \n",
      "Epoch 055 | Train Loss: 0.9143 Acc: 0.4629 | Val Loss: 0.8730 Acc: 0.5039                                               \n",
      "Epoch 056 | Train Loss: 0.9144 Acc: 0.4654 | Val Loss: 0.8668 Acc: 0.4561                                               \n",
      "Epoch 057 | Train Loss: 0.9154 Acc: 0.4605 | Val Loss: 0.8626 Acc: 0.4812                                               \n",
      "Epoch 058 | Train Loss: 0.9071 Acc: 0.4629 | Val Loss: 0.8614 Acc: 0.4925                                               \n",
      "Epoch 059 | Train Loss: 0.9076 Acc: 0.4571 | Val Loss: 0.8552 Acc: 0.4531                                               \n",
      "Epoch 060 | Train Loss: 0.8994 Acc: 0.4650 | Val Loss: 0.8608 Acc: 0.4540                                               \n",
      "Epoch 061 | Train Loss: 0.9042 Acc: 0.4596 | Val Loss: 0.8603 Acc: 0.4693                                               \n",
      "Epoch 062 | Train Loss: 0.9046 Acc: 0.4650 | Val Loss: 0.8650 Acc: 0.3982                                               \n",
      "Epoch 063 | Train Loss: 0.9043 Acc: 0.4647 | Val Loss: 0.8380 Acc: 0.4490                                               \n",
      "Epoch 064 | Train Loss: 0.9005 Acc: 0.4643 | Val Loss: 0.8488 Acc: 0.4836                                               \n",
      "Epoch 065 | Train Loss: 0.9044 Acc: 0.4612 | Val Loss: 0.8456 Acc: 0.4749                                               \n",
      "Epoch 066 | Train Loss: 0.9007 Acc: 0.4643 | Val Loss: 0.8369 Acc: 0.4901                                               \n",
      "Epoch 067 | Train Loss: 0.9033 Acc: 0.4634 | Val Loss: 0.8390 Acc: 0.4946                                               \n",
      "Epoch 068 | Train Loss: 0.8912 Acc: 0.4670 | Val Loss: 0.8376 Acc: 0.4439                                               \n",
      "Epoch 069 | Train Loss: 0.8970 Acc: 0.4612 | Val Loss: 0.8430 Acc: 0.4388                                               \n",
      "Epoch 070 | Train Loss: 0.8994 Acc: 0.4646 | Val Loss: 0.8351 Acc: 0.5349                                               \n",
      "Epoch 071 | Train Loss: 0.8955 Acc: 0.4618 | Val Loss: 0.8443 Acc: 0.5206                                               \n",
      "Epoch 072 | Train Loss: 0.8902 Acc: 0.4709 | Val Loss: 0.8638 Acc: 0.4406                                               \n",
      "Epoch 073 | Train Loss: 0.8907 Acc: 0.4707 | Val Loss: 0.8414 Acc: 0.5090                                               \n",
      "Epoch 074 | Train Loss: 0.8905 Acc: 0.4694 | Val Loss: 0.8280 Acc: 0.4406                                               \n",
      "Epoch 075 | Train Loss: 0.8867 Acc: 0.4760 | Val Loss: 0.8418 Acc: 0.5149                                               \n",
      "Epoch 076 | Train Loss: 0.8923 Acc: 0.4763 | Val Loss: 0.8378 Acc: 0.5430                                               \n",
      "Epoch 077 | Train Loss: 0.8887 Acc: 0.4767 | Val Loss: 0.8370 Acc: 0.5263                                               \n",
      "Epoch 078 | Train Loss: 0.8866 Acc: 0.4723 | Val Loss: 0.8314 Acc: 0.5293                                               \n",
      "Epoch 079 | Train Loss: 0.8760 Acc: 0.4796 | Val Loss: 0.8556 Acc: 0.5096                                               \n",
      "Epoch 080 | Train Loss: 0.8930 Acc: 0.4663 | Val Loss: 0.8347 Acc: 0.5448                                               \n",
      "Epoch 081 | Train Loss: 0.8861 Acc: 0.4720 | Val Loss: 0.8773 Acc: 0.5767                                               \n",
      "Epoch 082 | Train Loss: 0.8865 Acc: 0.4712 | Val Loss: 0.8339 Acc: 0.4663                                               \n",
      "Epoch 083 | Train Loss: 0.8897 Acc: 0.4713 | Val Loss: 0.8265 Acc: 0.4884                                               \n",
      "Epoch 084 | Train Loss: 0.8802 Acc: 0.4826 | Val Loss: 0.8246 Acc: 0.4809                                               \n",
      "Epoch 085 | Train Loss: 0.8835 Acc: 0.4764 | Val Loss: 0.8247 Acc: 0.5152                                               \n",
      "Epoch 086 | Train Loss: 0.8797 Acc: 0.4762 | Val Loss: 0.8232 Acc: 0.4487                                               \n",
      "Epoch 087 | Train Loss: 0.8759 Acc: 0.4841 | Val Loss: 0.8192 Acc: 0.5496                                               \n",
      "Epoch 088 | Train Loss: 0.8815 Acc: 0.4859 | Val Loss: 0.8193 Acc: 0.5310                                               \n",
      "Epoch 089 | Train Loss: 0.8789 Acc: 0.4778 | Val Loss: 0.8605 Acc: 0.4839                                               \n",
      "Epoch 090 | Train Loss: 0.8782 Acc: 0.4819 | Val Loss: 0.8256 Acc: 0.5325                                               \n",
      "Epoch 091 | Train Loss: 0.8783 Acc: 0.4801 | Val Loss: 0.8137 Acc: 0.5343                                               \n",
      "Epoch 092 | Train Loss: 0.8820 Acc: 0.4862 | Val Loss: 0.8134 Acc: 0.5101                                               \n",
      "Epoch 093 | Train Loss: 0.8806 Acc: 0.4822 | Val Loss: 0.8237 Acc: 0.4713                                               \n",
      "Epoch 094 | Train Loss: 0.8763 Acc: 0.4948 | Val Loss: 0.8204 Acc: 0.4615                                               \n",
      "Epoch 095 | Train Loss: 0.8756 Acc: 0.4834 | Val Loss: 0.8090 Acc: 0.4361                                               \n",
      "Epoch 096 | Train Loss: 0.8727 Acc: 0.4814 | Val Loss: 0.8166 Acc: 0.4681                                               \n",
      "Epoch 097 | Train Loss: 0.8684 Acc: 0.4929 | Val Loss: 0.8198 Acc: 0.5585                                               \n",
      "Epoch 098 | Train Loss: 0.8766 Acc: 0.4898 | Val Loss: 0.8100 Acc: 0.5194                                               \n",
      "Epoch 099 | Train Loss: 0.8743 Acc: 0.4872 | Val Loss: 0.8187 Acc: 0.5415                                               \n",
      "Epoch 100 | Train Loss: 0.8682 Acc: 0.4900 | Val Loss: 0.8056 Acc: 0.5439                                               \n",
      "Epoch 101 | Train Loss: 0.8737 Acc: 0.4825 | Val Loss: 0.8226 Acc: 0.4290                                               \n",
      "Epoch 102 | Train Loss: 0.8712 Acc: 0.4871 | Val Loss: 0.8218 Acc: 0.5084                                               \n",
      "Epoch 103 | Train Loss: 0.8624 Acc: 0.4867 | Val Loss: 0.8063 Acc: 0.5546                                               \n",
      "Epoch 104 | Train Loss: 0.8657 Acc: 0.4922 | Val Loss: 0.7958 Acc: 0.5325                                               \n",
      "Epoch 105 | Train Loss: 0.8527 Acc: 0.4938 | Val Loss: 0.7890 Acc: 0.4740                                               \n",
      "Epoch 106 | Train Loss: 0.8618 Acc: 0.4949 | Val Loss: 0.8127 Acc: 0.5409                                               \n",
      "Epoch 107 | Train Loss: 0.8575 Acc: 0.4944 | Val Loss: 0.7975 Acc: 0.5307                                               \n",
      "Epoch 108 | Train Loss: 0.8637 Acc: 0.4898 | Val Loss: 0.7988 Acc: 0.5516                                               \n",
      "Epoch 109 | Train Loss: 0.8523 Acc: 0.4965 | Val Loss: 0.7937 Acc: 0.5740                                               \n",
      "Epoch 110 | Train Loss: 0.8649 Acc: 0.4944 | Val Loss: 0.8228 Acc: 0.5719                                               \n",
      "Epoch 111 | Train Loss: 0.8564 Acc: 0.5079 | Val Loss: 0.8312 Acc: 0.5373                                               \n",
      "Epoch 112 | Train Loss: 0.8542 Acc: 0.4982 | Val Loss: 0.7957 Acc: 0.4430                                               \n",
      "Epoch 113 | Train Loss: 0.8563 Acc: 0.5021 | Val Loss: 0.8061 Acc: 0.5555                                               \n",
      "Epoch 114 | Train Loss: 0.8500 Acc: 0.5090 | Val Loss: 0.7769 Acc: 0.6009                                               \n",
      "Epoch 115 | Train Loss: 0.8455 Acc: 0.5177 | Val Loss: 0.7836 Acc: 0.5334                                               \n",
      "Epoch 116 | Train Loss: 0.8380 Acc: 0.5295 | Val Loss: 0.7646 Acc: 0.6182                                               \n",
      "Epoch 117 | Train Loss: 0.8393 Acc: 0.5235 | Val Loss: 0.7570 Acc: 0.6167                                               \n",
      "Epoch 118 | Train Loss: 0.8319 Acc: 0.5302 | Val Loss: 0.7826 Acc: 0.5654                                               \n",
      "Epoch 119 | Train Loss: 0.8257 Acc: 0.5331 | Val Loss: 0.7637 Acc: 0.5666                                               \n",
      "Epoch 120 | Train Loss: 0.8360 Acc: 0.5300 | Val Loss: 0.7698 Acc: 0.5716                                               \n",
      "Epoch 121 | Train Loss: 0.8286 Acc: 0.5332 | Val Loss: 0.7515 Acc: 0.6066                                               \n",
      "Epoch 122 | Train Loss: 0.8210 Acc: 0.5310 | Val Loss: 0.7516 Acc: 0.5907                                               \n",
      "Epoch 123 | Train Loss: 0.8282 Acc: 0.5366 | Val Loss: 0.7702 Acc: 0.5943                                               \n",
      "Epoch 124 | Train Loss: 0.8172 Acc: 0.5468 | Val Loss: 0.7573 Acc: 0.5967                                               \n",
      "Epoch 125 | Train Loss: 0.8222 Acc: 0.5400 | Val Loss: 0.7599 Acc: 0.5749                                               \n",
      "Epoch 126 | Train Loss: 0.8288 Acc: 0.5388 | Val Loss: 0.7575 Acc: 0.5955                                               \n",
      "Epoch 127 | Train Loss: 0.8153 Acc: 0.5447 | Val Loss: 0.7359 Acc: 0.5546                                               \n",
      "Epoch 128 | Train Loss: 0.8181 Acc: 0.5408 | Val Loss: 0.7497 Acc: 0.5758                                               \n",
      "Epoch 129 | Train Loss: 0.8088 Acc: 0.5439 | Val Loss: 0.7294 Acc: 0.5827                                               \n",
      "Epoch 130 | Train Loss: 0.8093 Acc: 0.5504 | Val Loss: 0.7258 Acc: 0.6063                                               \n",
      "Epoch 131 | Train Loss: 0.8018 Acc: 0.5445 | Val Loss: 0.7323 Acc: 0.5725                                               \n",
      "Epoch 132 | Train Loss: 0.8128 Acc: 0.5439 | Val Loss: 0.7137 Acc: 0.5997                                               \n",
      "Epoch 133 | Train Loss: 0.8035 Acc: 0.5449 | Val Loss: 0.7152 Acc: 0.6349                                               \n",
      "Epoch 134 | Train Loss: 0.8068 Acc: 0.5509 | Val Loss: 0.7388 Acc: 0.6090                                               \n",
      "Epoch 135 | Train Loss: 0.8040 Acc: 0.5484 | Val Loss: 0.7121 Acc: 0.6409                                               \n",
      "Epoch 136 | Train Loss: 0.7992 Acc: 0.5422 | Val Loss: 0.7374 Acc: 0.6319                                               \n",
      "Epoch 137 | Train Loss: 0.7966 Acc: 0.5463 | Val Loss: 0.7104 Acc: 0.5890                                               \n",
      "Epoch 138 | Train Loss: 0.7952 Acc: 0.5485 | Val Loss: 0.7291 Acc: 0.6191                                               \n",
      "Epoch 139 | Train Loss: 0.7998 Acc: 0.5552 | Val Loss: 0.7270 Acc: 0.6600                                               \n",
      "Epoch 140 | Train Loss: 0.7908 Acc: 0.5540 | Val Loss: 0.6947 Acc: 0.6134                                               \n",
      "Epoch 141 | Train Loss: 0.7934 Acc: 0.5514 | Val Loss: 0.7095 Acc: 0.6278                                               \n",
      "Epoch 142 | Train Loss: 0.7944 Acc: 0.5530 | Val Loss: 0.7132 Acc: 0.5612                                               \n",
      "Epoch 143 | Train Loss: 0.8002 Acc: 0.5497 | Val Loss: 0.6993 Acc: 0.6472                                               \n",
      "Epoch 144 | Train Loss: 0.7895 Acc: 0.5533 | Val Loss: 0.6983 Acc: 0.6125                                               \n",
      "Epoch 145 | Train Loss: 0.7851 Acc: 0.5510 | Val Loss: 0.6965 Acc: 0.6445                                               \n",
      "Epoch 146 | Train Loss: 0.7889 Acc: 0.5550 | Val Loss: 0.6953 Acc: 0.5970                                               \n",
      "Epoch 147 | Train Loss: 0.7834 Acc: 0.5588 | Val Loss: 0.7126 Acc: 0.6388                                               \n",
      "Epoch 148 | Train Loss: 0.7838 Acc: 0.5536 | Val Loss: 0.7060 Acc: 0.6519                                               \n",
      "Epoch 149 | Train Loss: 0.7937 Acc: 0.5530 | Val Loss: 0.7227 Acc: 0.5857                                               \n",
      "Epoch 150 | Train Loss: 0.7808 Acc: 0.5552 | Val Loss: 0.7118 Acc: 0.6597                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 36, 'cnn_dense': 256, 'cnn_dropout': 0.48057080633773397, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 32, 'cnn_kernels_2': 16, 'learning_rate': 0.005039250781834631, 'lstm_dense': 64, 'lstm_hidden_size': 32, 'lstm_layers': 5, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 10.7374 Acc: 0.4681 | Val Loss: 0.7629 Acc: 0.6591                                              \n",
      "Epoch 002 | Train Loss: 0.8574 Acc: 0.6101 | Val Loss: 0.6707 Acc: 0.7307                                               \n",
      "Epoch 003 | Train Loss: 0.7527 Acc: 0.6511 | Val Loss: 0.6029 Acc: 0.7445                                               \n",
      "Epoch 004 | Train Loss: 0.6964 Acc: 0.6726 | Val Loss: 0.6143 Acc: 0.7152                                               \n",
      "Epoch 005 | Train Loss: 0.7099 Acc: 0.6718 | Val Loss: 0.6309 Acc: 0.7119                                               \n",
      "Epoch 006 | Train Loss: 0.6978 Acc: 0.6719 | Val Loss: 0.6192 Acc: 0.6806                                               \n",
      "Epoch 007 | Train Loss: 0.7207 Acc: 0.6606 | Val Loss: 0.7444 Acc: 0.6576                                               \n",
      "Epoch 008 | Train Loss: 0.7458 Acc: 0.6577 | Val Loss: 0.6371 Acc: 0.7588                                               \n",
      "Epoch 009 | Train Loss: 0.7500 Acc: 0.6464 | Val Loss: 0.6746 Acc: 0.6821                                               \n",
      "Epoch 010 | Train Loss: 0.7554 Acc: 0.6421 | Val Loss: 0.7203 Acc: 0.6761                                               \n",
      "Epoch 011 | Train Loss: 0.7725 Acc: 0.6433 | Val Loss: 0.6313 Acc: 0.7206                                               \n",
      "Epoch 012 | Train Loss: 0.7904 Acc: 0.6349 | Val Loss: 0.7628 Acc: 0.6334                                               \n",
      "Epoch 013 | Train Loss: 0.8636 Acc: 0.5883 | Val Loss: 0.9008 Acc: 0.4991                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 256, 'cnn_dropout': 0.6761456655671014, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 64, 'cnn_kernels_2': 64, 'learning_rate': 6.92819055432657e-05, 'lstm_dense': 128, 'lstm_hidden_size': 128, 'lstm_layers': 1, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 45.8993 Acc: 0.3071 | Val Loss: 12.1972 Acc: 0.4307                                             \n",
      "Epoch 002 | Train Loss: 18.9943 Acc: 0.3550 | Val Loss: 5.8752 Acc: 0.4012                                              \n",
      "Epoch 003 | Train Loss: 12.8420 Acc: 0.3856 | Val Loss: 2.9865 Acc: 0.4651                                              \n",
      "Epoch 004 | Train Loss: 9.6353 Acc: 0.3923 | Val Loss: 1.6991 Acc: 0.5021                                               \n",
      "Epoch 005 | Train Loss: 7.5732 Acc: 0.4067 | Val Loss: 1.4911 Acc: 0.4955                                               \n",
      "Epoch 006 | Train Loss: 6.4468 Acc: 0.4038 | Val Loss: 1.7592 Acc: 0.4507                                               \n",
      "Epoch 007 | Train Loss: 5.5174 Acc: 0.4232 | Val Loss: 1.7730 Acc: 0.5925                                               \n",
      "Epoch 008 | Train Loss: 4.6503 Acc: 0.4482 | Val Loss: 3.6135 Acc: 0.4445                                               \n",
      "Epoch 009 | Train Loss: 4.2077 Acc: 0.4493 | Val Loss: 1.3743 Acc: 0.5469                                               \n",
      "Epoch 010 | Train Loss: 3.7940 Acc: 0.4614 | Val Loss: 1.3765 Acc: 0.5770                                               \n",
      "Epoch 011 | Train Loss: 3.3442 Acc: 0.4753 | Val Loss: 1.5454 Acc: 0.6242                                               \n",
      "Epoch 012 | Train Loss: 3.1911 Acc: 0.4712 | Val Loss: 1.0664 Acc: 0.5370                                               \n",
      "Epoch 013 | Train Loss: 2.9738 Acc: 0.4740 | Val Loss: 1.2502 Acc: 0.5722                                               \n",
      "Epoch 014 | Train Loss: 2.7577 Acc: 0.4794 | Val Loss: 0.9709 Acc: 0.6116                                               \n",
      "Epoch 015 | Train Loss: 2.5452 Acc: 0.4806 | Val Loss: 1.1793 Acc: 0.5433                                               \n",
      "Epoch 016 | Train Loss: 2.4250 Acc: 0.4889 | Val Loss: 1.6548 Acc: 0.6128                                               \n",
      "Epoch 017 | Train Loss: 2.3670 Acc: 0.4805 | Val Loss: 1.3874 Acc: 0.5896                                               \n",
      "Epoch 018 | Train Loss: 2.2682 Acc: 0.4866 | Val Loss: 0.8518 Acc: 0.6525                                               \n",
      "Epoch 019 | Train Loss: 2.1519 Acc: 0.4892 | Val Loss: 1.0832 Acc: 0.5266                                               \n",
      "Epoch 020 | Train Loss: 2.0419 Acc: 0.4962 | Val Loss: 0.8337 Acc: 0.6472                                               \n",
      "Epoch 021 | Train Loss: 1.8972 Acc: 0.5041 | Val Loss: 0.9389 Acc: 0.6352                                               \n",
      "Epoch 022 | Train Loss: 1.7988 Acc: 0.5104 | Val Loss: 1.0751 Acc: 0.5481                                               \n",
      "Epoch 023 | Train Loss: 1.8011 Acc: 0.5071 | Val Loss: 1.2370 Acc: 0.5839                                               \n",
      "Epoch 024 | Train Loss: 1.7514 Acc: 0.5069 | Val Loss: 1.4573 Acc: 0.5719                                               \n",
      "Epoch 025 | Train Loss: 1.6970 Acc: 0.5102 | Val Loss: 1.3300 Acc: 0.4678                                               \n",
      "Epoch 026 | Train Loss: 1.6133 Acc: 0.5138 | Val Loss: 1.0741 Acc: 0.4648                                               \n",
      "Epoch 027 | Train Loss: 1.5723 Acc: 0.5150 | Val Loss: 0.7574 Acc: 0.6779                                               \n",
      "Epoch 028 | Train Loss: 1.4871 Acc: 0.5232 | Val Loss: 0.7966 Acc: 0.6618                                               \n",
      "Epoch 029 | Train Loss: 1.4785 Acc: 0.5204 | Val Loss: 1.1057 Acc: 0.5540                                               \n",
      "Epoch 030 | Train Loss: 1.3963 Acc: 0.5352 | Val Loss: 1.2927 Acc: 0.5296                                               \n",
      "Epoch 031 | Train Loss: 1.4339 Acc: 0.5271 | Val Loss: 1.4554 Acc: 0.5776                                               \n",
      "Epoch 032 | Train Loss: 1.3531 Acc: 0.5397 | Val Loss: 1.0407 Acc: 0.5245                                               \n",
      "Epoch 033 | Train Loss: 1.3161 Acc: 0.5417 | Val Loss: 0.7418 Acc: 0.6454                                               \n",
      "Epoch 034 | Train Loss: 1.3260 Acc: 0.5406 | Val Loss: 0.8282 Acc: 0.6346                                               \n",
      "Epoch 035 | Train Loss: 1.2631 Acc: 0.5487 | Val Loss: 1.1299 Acc: 0.5116                                               \n",
      "Epoch 036 | Train Loss: 1.2029 Acc: 0.5556 | Val Loss: 0.9088 Acc: 0.5979                                               \n",
      "Epoch 037 | Train Loss: 1.2360 Acc: 0.5462 | Val Loss: 1.3641 Acc: 0.5269                                               \n",
      "Epoch 038 | Train Loss: 1.1928 Acc: 0.5529 | Val Loss: 0.6717 Acc: 0.6800                                               \n",
      "Epoch 039 | Train Loss: 1.1621 Acc: 0.5588 | Val Loss: 0.7176 Acc: 0.6994                                               \n",
      "Epoch 040 | Train Loss: 1.1448 Acc: 0.5625 | Val Loss: 0.7145 Acc: 0.6463                                               \n",
      "Epoch 041 | Train Loss: 1.1144 Acc: 0.5721 | Val Loss: 1.6982 Acc: 0.4481                                               \n",
      "Epoch 042 | Train Loss: 1.1159 Acc: 0.5704 | Val Loss: 0.7825 Acc: 0.6260                                               \n",
      "Epoch 043 | Train Loss: 1.0674 Acc: 0.5763 | Val Loss: 0.8315 Acc: 0.5409                                               \n",
      "Epoch 044 | Train Loss: 1.0768 Acc: 0.5836 | Val Loss: 1.3063 Acc: 0.5206                                               \n",
      "Epoch 045 | Train Loss: 1.0589 Acc: 0.5824 | Val Loss: 1.0800 Acc: 0.5042                                               \n",
      "Epoch 046 | Train Loss: 1.0253 Acc: 0.6018 | Val Loss: 0.6665 Acc: 0.7134                                               \n",
      "Epoch 047 | Train Loss: 1.0052 Acc: 0.6127 | Val Loss: 0.8155 Acc: 0.6191                                               \n",
      "Epoch 048 | Train Loss: 0.9707 Acc: 0.6150 | Val Loss: 0.8413 Acc: 0.5890                                               \n",
      "Epoch 049 | Train Loss: 0.9519 Acc: 0.6272 | Val Loss: 0.6576 Acc: 0.7122                                               \n",
      "Epoch 050 | Train Loss: 0.9408 Acc: 0.6403 | Val Loss: 0.8917 Acc: 0.6528                                               \n",
      "Epoch 051 | Train Loss: 0.9278 Acc: 0.6381 | Val Loss: 0.8706 Acc: 0.6657                                               \n",
      "Epoch 052 | Train Loss: 0.8750 Acc: 0.6589 | Val Loss: 0.9478 Acc: 0.5866                                               \n",
      "Epoch 053 | Train Loss: 0.8479 Acc: 0.6713 | Val Loss: 0.7687 Acc: 0.6221                                               \n",
      "Epoch 054 | Train Loss: 0.8496 Acc: 0.6669 | Val Loss: 0.7528 Acc: 0.6985                                               \n",
      "Epoch 055 | Train Loss: 0.8282 Acc: 0.6800 | Val Loss: 0.7651 Acc: 0.6475                                               \n",
      "Epoch 056 | Train Loss: 0.8015 Acc: 0.6878 | Val Loss: 0.7358 Acc: 0.6349                                               \n",
      "Epoch 057 | Train Loss: 0.7640 Acc: 0.6931 | Val Loss: 0.6952 Acc: 0.6788                                               \n",
      "Epoch 058 | Train Loss: 0.7142 Acc: 0.7133 | Val Loss: 0.6801 Acc: 0.7227                                               \n",
      "Epoch 059 | Train Loss: 0.7137 Acc: 0.7228 | Val Loss: 0.7018 Acc: 0.7155                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 32, 'cnn_dropout': 0.4224776297184629, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 32, 'learning_rate': 0.0002378044365815219, 'lstm_dense': 32, 'lstm_hidden_size': 96, 'lstm_layers': 2, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 30.7821 Acc: 0.3261 | Val Loss: 2.9525 Acc: 0.3696                                              \n",
      "Epoch 002 | Train Loss: 6.4552 Acc: 0.4069 | Val Loss: 1.3578 Acc: 0.5579                                               \n",
      "Epoch 003 | Train Loss: 3.5399 Acc: 0.4588 | Val Loss: 1.1821 Acc: 0.5433                                               \n",
      "Epoch 004 | Train Loss: 2.5339 Acc: 0.5168 | Val Loss: 1.1391 Acc: 0.6245                                               \n",
      "Epoch 005 | Train Loss: 2.0089 Acc: 0.5626 | Val Loss: 0.9677 Acc: 0.6624                                               \n",
      "Epoch 006 | Train Loss: 1.6372 Acc: 0.6083 | Val Loss: 1.0701 Acc: 0.6863                                               \n",
      "Epoch 007 | Train Loss: 1.3786 Acc: 0.6421 | Val Loss: 0.7824 Acc: 0.7206                                               \n",
      "Epoch 008 | Train Loss: 1.1514 Acc: 0.6727 | Val Loss: 0.7898 Acc: 0.7316                                               \n",
      "Epoch 009 | Train Loss: 0.9986 Acc: 0.7109 | Val Loss: 0.6574 Acc: 0.7767                                               \n",
      "Epoch 010 | Train Loss: 0.8700 Acc: 0.7333 | Val Loss: 0.5541 Acc: 0.7899                                               \n",
      "Epoch 011 | Train Loss: 0.8123 Acc: 0.7460 | Val Loss: 0.5005 Acc: 0.8149                                               \n",
      "Epoch 012 | Train Loss: 0.6870 Acc: 0.7828 | Val Loss: 0.5604 Acc: 0.7928                                               \n",
      "Epoch 013 | Train Loss: 0.6582 Acc: 0.7895 | Val Loss: 0.4558 Acc: 0.8376                                               \n",
      "Epoch 014 | Train Loss: 0.5467 Acc: 0.8227 | Val Loss: 0.3638 Acc: 0.8669                                               \n",
      "Epoch 015 | Train Loss: 0.4767 Acc: 0.8418 | Val Loss: 0.3458 Acc: 0.8770                                               \n",
      "Epoch 016 | Train Loss: 0.4492 Acc: 0.8528 | Val Loss: 0.3213 Acc: 0.8901                                               \n",
      "Epoch 017 | Train Loss: 0.4598 Acc: 0.8571 | Val Loss: 0.4348 Acc: 0.8537                                               \n",
      "Epoch 018 | Train Loss: 0.3692 Acc: 0.8806 | Val Loss: 0.2960 Acc: 0.8952                                               \n",
      "Epoch 019 | Train Loss: 0.3411 Acc: 0.8885 | Val Loss: 0.2554 Acc: 0.9149                                               \n",
      "Epoch 020 | Train Loss: 0.3476 Acc: 0.8898 | Val Loss: 0.2979 Acc: 0.8904                                               \n",
      "Epoch 021 | Train Loss: 0.2830 Acc: 0.9063 | Val Loss: 0.2388 Acc: 0.9179                                               \n",
      "Epoch 022 | Train Loss: 0.2817 Acc: 0.9073 | Val Loss: 0.2158 Acc: 0.9233                                               \n",
      "Epoch 023 | Train Loss: 0.2504 Acc: 0.9202 | Val Loss: 0.1943 Acc: 0.9307                                               \n",
      "Epoch 024 | Train Loss: 0.2735 Acc: 0.9152 | Val Loss: 0.1952 Acc: 0.9269                                               \n",
      "Epoch 025 | Train Loss: 0.2338 Acc: 0.9264 | Val Loss: 0.2570 Acc: 0.9104                                               \n",
      "Epoch 026 | Train Loss: 0.2331 Acc: 0.9248 | Val Loss: 0.1872 Acc: 0.9349                                               \n",
      "Epoch 027 | Train Loss: 0.2205 Acc: 0.9325 | Val Loss: 0.1651 Acc: 0.9394                                               \n",
      "Epoch 028 | Train Loss: 0.2213 Acc: 0.9284 | Val Loss: 0.2526 Acc: 0.9194                                               \n",
      "Epoch 029 | Train Loss: 0.2023 Acc: 0.9376 | Val Loss: 0.3350 Acc: 0.9039                                               \n",
      "Epoch 030 | Train Loss: 0.1887 Acc: 0.9428 | Val Loss: 0.1241 Acc: 0.9561                                               \n",
      "Epoch 031 | Train Loss: 0.1662 Acc: 0.9434 | Val Loss: 0.1493 Acc: 0.9519                                               \n",
      "Epoch 032 | Train Loss: 0.1606 Acc: 0.9494 | Val Loss: 0.1619 Acc: 0.9513                                               \n",
      "Epoch 033 | Train Loss: 0.1633 Acc: 0.9493 | Val Loss: 0.1952 Acc: 0.9457                                               \n",
      "Epoch 034 | Train Loss: 0.1704 Acc: 0.9466 | Val Loss: 0.1280 Acc: 0.9621                                               \n",
      "Epoch 035 | Train Loss: 0.1616 Acc: 0.9481 | Val Loss: 0.1399 Acc: 0.9549                                               \n",
      "Epoch 036 | Train Loss: 0.1670 Acc: 0.9512 | Val Loss: 0.1550 Acc: 0.9579                                               \n",
      "Epoch 037 | Train Loss: 0.1352 Acc: 0.9581 | Val Loss: 0.1069 Acc: 0.9672                                               \n",
      "Epoch 038 | Train Loss: 0.1260 Acc: 0.9617 | Val Loss: 0.1606 Acc: 0.9463                                               \n",
      "Epoch 039 | Train Loss: 0.1194 Acc: 0.9613 | Val Loss: 0.1071 Acc: 0.9693                                               \n",
      "Epoch 040 | Train Loss: 0.1188 Acc: 0.9616 | Val Loss: 0.1219 Acc: 0.9642                                               \n",
      "Epoch 041 | Train Loss: 0.1455 Acc: 0.9552 | Val Loss: 0.1227 Acc: 0.9624                                               \n",
      "Epoch 042 | Train Loss: 0.1160 Acc: 0.9630 | Val Loss: 0.1610 Acc: 0.9546                                               \n",
      "Epoch 043 | Train Loss: 0.1036 Acc: 0.9659 | Val Loss: 0.1358 Acc: 0.9600                                               \n",
      "Epoch 044 | Train Loss: 0.1039 Acc: 0.9672 | Val Loss: 0.1085 Acc: 0.9716                                               \n",
      "Epoch 045 | Train Loss: 0.1166 Acc: 0.9640 | Val Loss: 0.1065 Acc: 0.9696                                               \n",
      "Epoch 046 | Train Loss: 0.0963 Acc: 0.9694 | Val Loss: 0.1187 Acc: 0.9633                                               \n",
      "Epoch 047 | Train Loss: 0.1011 Acc: 0.9684 | Val Loss: 0.1397 Acc: 0.9582                                               \n",
      "Epoch 048 | Train Loss: 0.0890 Acc: 0.9714 | Val Loss: 0.1075 Acc: 0.9690                                               \n",
      "Epoch 049 | Train Loss: 0.0819 Acc: 0.9749 | Val Loss: 0.1040 Acc: 0.9722                                               \n",
      "Epoch 050 | Train Loss: 0.1058 Acc: 0.9663 | Val Loss: 0.1075 Acc: 0.9696                                               \n",
      "Epoch 051 | Train Loss: 0.0830 Acc: 0.9746 | Val Loss: 0.1131 Acc: 0.9696                                               \n",
      "Epoch 052 | Train Loss: 0.0819 Acc: 0.9722 | Val Loss: 0.1268 Acc: 0.9627                                               \n",
      "Epoch 053 | Train Loss: 0.0839 Acc: 0.9746 | Val Loss: 0.1431 Acc: 0.9552                                               \n",
      "Epoch 054 | Train Loss: 0.0813 Acc: 0.9732 | Val Loss: 0.1257 Acc: 0.9618                                               \n",
      "Epoch 055 | Train Loss: 0.0685 Acc: 0.9779 | Val Loss: 0.1627 Acc: 0.9496                                               \n",
      "Epoch 056 | Train Loss: 0.0643 Acc: 0.9799 | Val Loss: 0.1061 Acc: 0.9693                                               \n",
      "Epoch 057 | Train Loss: 0.0623 Acc: 0.9797 | Val Loss: 0.0799 Acc: 0.9800                                               \n",
      "Epoch 058 | Train Loss: 0.0650 Acc: 0.9789 | Val Loss: 0.0991 Acc: 0.9713                                               \n",
      "Epoch 059 | Train Loss: 0.0883 Acc: 0.9732 | Val Loss: 0.1134 Acc: 0.9725                                               \n",
      "Epoch 060 | Train Loss: 0.0612 Acc: 0.9804 | Val Loss: 0.1358 Acc: 0.9636                                               \n",
      "Epoch 061 | Train Loss: 0.0653 Acc: 0.9787 | Val Loss: 0.1078 Acc: 0.9716                                               \n",
      "Epoch 062 | Train Loss: 0.0597 Acc: 0.9828 | Val Loss: 0.0872 Acc: 0.9785                                               \n",
      "Epoch 063 | Train Loss: 0.0580 Acc: 0.9828 | Val Loss: 0.0830 Acc: 0.9794                                               \n",
      "Epoch 064 | Train Loss: 0.0566 Acc: 0.9834 | Val Loss: 0.0948 Acc: 0.9758                                               \n",
      "Epoch 065 | Train Loss: 0.0556 Acc: 0.9842 | Val Loss: 0.1318 Acc: 0.9660                                               \n",
      "Epoch 066 | Train Loss: 0.0457 Acc: 0.9867 | Val Loss: 0.1122 Acc: 0.9746                                               \n",
      "Epoch 067 | Train Loss: 0.0551 Acc: 0.9840 | Val Loss: 0.0799 Acc: 0.9779                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 256, 'cnn_dropout': 0.3348238757846756, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 64, 'cnn_kernels_2': 96, 'learning_rate': 0.0004475653262863987, 'lstm_dense': 64, 'lstm_hidden_size': 64, 'lstm_layers': 3, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 4.1889 Acc: 0.4189 | Val Loss: 1.2842 Acc: 0.4421                                               \n",
      "Epoch 002 | Train Loss: 1.2702 Acc: 0.4422 | Val Loss: 1.2591 Acc: 0.4421                                               \n",
      "Epoch 003 | Train Loss: 1.2540 Acc: 0.4422 | Val Loss: 1.2484 Acc: 0.4421                                               \n",
      "Epoch 004 | Train Loss: 1.2462 Acc: 0.4422 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 005 | Train Loss: 1.2409 Acc: 0.4422 | Val Loss: 1.2368 Acc: 0.4421                                               \n",
      "Epoch 006 | Train Loss: 1.2359 Acc: 0.4422 | Val Loss: 1.2291 Acc: 0.4421                                               \n",
      "Epoch 007 | Train Loss: 1.2290 Acc: 0.4422 | Val Loss: 1.2204 Acc: 0.4421                                               \n",
      "Epoch 008 | Train Loss: 1.2179 Acc: 0.4421 | Val Loss: 1.2073 Acc: 0.4424                                               \n",
      "Epoch 009 | Train Loss: 1.1944 Acc: 0.4557 | Val Loss: 1.1800 Acc: 0.5128                                               \n",
      "Epoch 010 | Train Loss: 1.1721 Acc: 0.4942 | Val Loss: 1.1519 Acc: 0.5212                                               \n",
      "Epoch 011 | Train Loss: 1.1424 Acc: 0.5169 | Val Loss: 1.1293 Acc: 0.5263                                               \n",
      "Epoch 012 | Train Loss: 1.1100 Acc: 0.5341 | Val Loss: 1.1087 Acc: 0.5251                                               \n",
      "Epoch 013 | Train Loss: 1.0904 Acc: 0.5400 | Val Loss: 1.0819 Acc: 0.5352                                               \n",
      "Epoch 014 | Train Loss: 1.0678 Acc: 0.5530 | Val Loss: 1.0350 Acc: 0.5722                                               \n",
      "Epoch 015 | Train Loss: 1.0434 Acc: 0.5640 | Val Loss: 1.0221 Acc: 0.5839                                               \n",
      "Epoch 016 | Train Loss: 1.0140 Acc: 0.5773 | Val Loss: 0.9974 Acc: 0.5839                                               \n",
      "Epoch 017 | Train Loss: 1.0080 Acc: 0.5779 | Val Loss: 1.0010 Acc: 0.5701                                               \n",
      "Epoch 018 | Train Loss: 0.9933 Acc: 0.5790 | Val Loss: 1.0653 Acc: 0.5394                                               \n",
      "Epoch 019 | Train Loss: 0.9971 Acc: 0.5774 | Val Loss: 1.1169 Acc: 0.5015                                               \n",
      "Epoch 020 | Train Loss: 0.9827 Acc: 0.5848 | Val Loss: 0.9248 Acc: 0.6125                                               \n",
      "Epoch 021 | Train Loss: 0.9285 Acc: 0.6048 | Val Loss: 0.9269 Acc: 0.6036                                               \n",
      "Epoch 022 | Train Loss: 0.9021 Acc: 0.6167 | Val Loss: 0.8591 Acc: 0.6310                                               \n",
      "Epoch 023 | Train Loss: 0.8804 Acc: 0.6244 | Val Loss: 0.8755 Acc: 0.6200                                               \n",
      "Epoch 024 | Train Loss: 0.8545 Acc: 0.6300 | Val Loss: 0.8227 Acc: 0.6358                                               \n",
      "Epoch 025 | Train Loss: 0.8995 Acc: 0.6103 | Val Loss: 0.8281 Acc: 0.6412                                               \n",
      "Epoch 026 | Train Loss: 0.8083 Acc: 0.6590 | Val Loss: 0.7793 Acc: 0.6818                                               \n",
      "Epoch 027 | Train Loss: 0.7739 Acc: 0.6883 | Val Loss: 0.7141 Acc: 0.7185                                               \n",
      "Epoch 028 | Train Loss: 0.7227 Acc: 0.7212 | Val Loss: 0.6611 Acc: 0.7552                                               \n",
      "Epoch 029 | Train Loss: 0.6499 Acc: 0.7598 | Val Loss: 0.5991 Acc: 0.7824                                               \n",
      "Epoch 030 | Train Loss: 0.6075 Acc: 0.7813 | Val Loss: 0.5729 Acc: 0.7910                                               \n",
      "Epoch 031 | Train Loss: 0.5573 Acc: 0.7980 | Val Loss: 0.5363 Acc: 0.8128                                               \n",
      "Epoch 032 | Train Loss: 0.5023 Acc: 0.8199 | Val Loss: 0.4744 Acc: 0.8322                                               \n",
      "Epoch 033 | Train Loss: 0.4804 Acc: 0.8230 | Val Loss: 0.4835 Acc: 0.8236                                               \n",
      "Epoch 034 | Train Loss: 0.4527 Acc: 0.8327 | Val Loss: 0.5134 Acc: 0.8027                                               \n",
      "Epoch 035 | Train Loss: 0.4342 Acc: 0.8375 | Val Loss: 0.4221 Acc: 0.8322                                               \n",
      "Epoch 036 | Train Loss: 0.3887 Acc: 0.8548 | Val Loss: 0.3905 Acc: 0.8615                                               \n",
      "Epoch 037 | Train Loss: 0.3610 Acc: 0.8690 | Val Loss: 0.3526 Acc: 0.8675                                               \n",
      "Epoch 038 | Train Loss: 0.3371 Acc: 0.8788 | Val Loss: 0.3561 Acc: 0.8746                                               \n",
      "Epoch 039 | Train Loss: 0.3282 Acc: 0.8832 | Val Loss: 0.3291 Acc: 0.8878                                               \n",
      "Epoch 040 | Train Loss: 0.3028 Acc: 0.8890 | Val Loss: 0.3072 Acc: 0.8931                                               \n",
      "Epoch 041 | Train Loss: 0.2880 Acc: 0.9022 | Val Loss: 0.3182 Acc: 0.8845                                               \n",
      "Epoch 042 | Train Loss: 0.2783 Acc: 0.9017 | Val Loss: 0.2630 Acc: 0.9107                                               \n",
      "Epoch 043 | Train Loss: 0.2478 Acc: 0.9194 | Val Loss: 0.2580 Acc: 0.9218                                               \n",
      "Epoch 044 | Train Loss: 0.2768 Acc: 0.9084 | Val Loss: 0.3629 Acc: 0.8824                                               \n",
      "Epoch 045 | Train Loss: 0.2825 Acc: 0.9027 | Val Loss: 0.2792 Acc: 0.9069                                               \n",
      "Epoch 046 | Train Loss: 0.2422 Acc: 0.9222 | Val Loss: 0.3252 Acc: 0.8928                                               \n",
      "Epoch 047 | Train Loss: 0.2521 Acc: 0.9145 | Val Loss: 0.2529 Acc: 0.9209                                               \n",
      "Epoch 048 | Train Loss: 0.2198 Acc: 0.9291 | Val Loss: 0.2093 Acc: 0.9304                                               \n",
      "Epoch 049 | Train Loss: 0.1873 Acc: 0.9424 | Val Loss: 0.2257 Acc: 0.9358                                               \n",
      "Epoch 050 | Train Loss: 0.1766 Acc: 0.9428 | Val Loss: 0.2007 Acc: 0.9316                                               \n",
      "Epoch 051 | Train Loss: 0.1620 Acc: 0.9501 | Val Loss: 0.1871 Acc: 0.9436                                               \n",
      "Epoch 052 | Train Loss: 0.1750 Acc: 0.9436 | Val Loss: 0.2085 Acc: 0.9349                                               \n",
      "Epoch 053 | Train Loss: 0.1602 Acc: 0.9498 | Val Loss: 0.2291 Acc: 0.9248                                               \n",
      "Epoch 054 | Train Loss: 0.1569 Acc: 0.9496 | Val Loss: 0.1926 Acc: 0.9358                                               \n",
      "Epoch 055 | Train Loss: 0.1396 Acc: 0.9544 | Val Loss: 0.1864 Acc: 0.9403                                               \n",
      "Epoch 056 | Train Loss: 0.1212 Acc: 0.9622 | Val Loss: 0.1671 Acc: 0.9490                                               \n",
      "Epoch 057 | Train Loss: 0.1383 Acc: 0.9554 | Val Loss: 0.1611 Acc: 0.9519                                               \n",
      "Epoch 058 | Train Loss: 0.1225 Acc: 0.9610 | Val Loss: 0.1722 Acc: 0.9525                                               \n",
      "Epoch 059 | Train Loss: 0.1111 Acc: 0.9629 | Val Loss: 0.1690 Acc: 0.9493                                               \n",
      "Epoch 060 | Train Loss: 0.0977 Acc: 0.9686 | Val Loss: 0.1619 Acc: 0.9564                                               \n",
      "Epoch 061 | Train Loss: 0.1028 Acc: 0.9671 | Val Loss: 0.1418 Acc: 0.9588                                               \n",
      "Epoch 062 | Train Loss: 0.0928 Acc: 0.9716 | Val Loss: 0.1602 Acc: 0.9510                                               \n",
      "Epoch 063 | Train Loss: 0.0875 Acc: 0.9712 | Val Loss: 0.1611 Acc: 0.9490                                               \n",
      "Epoch 064 | Train Loss: 0.0903 Acc: 0.9713 | Val Loss: 0.1414 Acc: 0.9561                                               \n",
      "Epoch 065 | Train Loss: 0.0791 Acc: 0.9742 | Val Loss: 0.1383 Acc: 0.9585                                               \n",
      "Epoch 066 | Train Loss: 0.0800 Acc: 0.9734 | Val Loss: 0.1692 Acc: 0.9525                                               \n",
      "Epoch 067 | Train Loss: 0.0790 Acc: 0.9734 | Val Loss: 0.1694 Acc: 0.9469                                               \n",
      "Epoch 068 | Train Loss: 0.0718 Acc: 0.9771 | Val Loss: 0.1452 Acc: 0.9585                                               \n",
      "Epoch 069 | Train Loss: 0.0829 Acc: 0.9731 | Val Loss: 0.1584 Acc: 0.9558                                               \n",
      "Epoch 070 | Train Loss: 0.0678 Acc: 0.9775 | Val Loss: 0.1374 Acc: 0.9576                                               \n",
      "Epoch 071 | Train Loss: 0.0731 Acc: 0.9764 | Val Loss: 0.1479 Acc: 0.9540                                               \n",
      "Epoch 072 | Train Loss: 0.0714 Acc: 0.9766 | Val Loss: 0.1679 Acc: 0.9504                                               \n",
      "Epoch 073 | Train Loss: 0.0570 Acc: 0.9825 | Val Loss: 0.1274 Acc: 0.9621                                               \n",
      "Epoch 074 | Train Loss: 0.0574 Acc: 0.9817 | Val Loss: 0.1608 Acc: 0.9573                                               \n",
      "Epoch 075 | Train Loss: 0.0749 Acc: 0.9760 | Val Loss: 0.1645 Acc: 0.9516                                               \n",
      "Epoch 076 | Train Loss: 0.0635 Acc: 0.9797 | Val Loss: 0.1355 Acc: 0.9639                                               \n",
      "Epoch 077 | Train Loss: 0.0716 Acc: 0.9756 | Val Loss: 0.1405 Acc: 0.9597                                               \n",
      "Epoch 078 | Train Loss: 0.0532 Acc: 0.9848 | Val Loss: 0.1442 Acc: 0.9603                                               \n",
      "Epoch 079 | Train Loss: 0.0582 Acc: 0.9813 | Val Loss: 0.1514 Acc: 0.9573                                               \n",
      "Epoch 080 | Train Loss: 0.0727 Acc: 0.9763 | Val Loss: 0.1427 Acc: 0.9594                                               \n",
      "Epoch 081 | Train Loss: 0.0564 Acc: 0.9811 | Val Loss: 0.1317 Acc: 0.9627                                               \n",
      "Epoch 082 | Train Loss: 0.0410 Acc: 0.9872 | Val Loss: 0.1287 Acc: 0.9633                                               \n",
      "Epoch 083 | Train Loss: 0.0387 Acc: 0.9869 | Val Loss: 0.1426 Acc: 0.9621                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 36, 'cnn_dense': 64, 'cnn_dropout': 0.00031974095810999303, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 32, 'cnn_kernels_2': 32, 'learning_rate': 0.00017852044166101214, 'lstm_dense': 256, 'lstm_hidden_size': 128, 'lstm_layers': 1, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 5.5060 Acc: 0.4388 | Val Loss: 3.5848 Acc: 0.5221                                               \n",
      "Epoch 002 | Train Loss: 2.1304 Acc: 0.5148 | Val Loss: 2.1502 Acc: 0.5116                                               \n",
      "Epoch 003 | Train Loss: 1.4929 Acc: 0.5481 | Val Loss: 0.8441 Acc: 0.6104                                               \n",
      "Epoch 004 | Train Loss: 1.1562 Acc: 0.5791 | Val Loss: 0.7968 Acc: 0.6054                                               \n",
      "Epoch 005 | Train Loss: 0.8908 Acc: 0.6384 | Val Loss: 0.8745 Acc: 0.5970                                               \n",
      "Epoch 006 | Train Loss: 0.7319 Acc: 0.7062 | Val Loss: 0.6814 Acc: 0.6994                                               \n",
      "Epoch 007 | Train Loss: 0.5522 Acc: 0.7792 | Val Loss: 0.6109 Acc: 0.7722                                               \n",
      "Epoch 008 | Train Loss: 0.4523 Acc: 0.8282 | Val Loss: 0.7472 Acc: 0.7128                                               \n",
      "Epoch 009 | Train Loss: 0.3651 Acc: 0.8642 | Val Loss: 0.2906 Acc: 0.8833                                               \n",
      "Epoch 010 | Train Loss: 0.3036 Acc: 0.8876 | Val Loss: 0.2371 Acc: 0.9018                                               \n",
      "Epoch 011 | Train Loss: 0.2761 Acc: 0.9013 | Val Loss: 0.2569 Acc: 0.9039                                               \n",
      "Epoch 012 | Train Loss: 0.2364 Acc: 0.9149 | Val Loss: 0.1425 Acc: 0.9490                                               \n",
      "Epoch 013 | Train Loss: 0.2159 Acc: 0.9287 | Val Loss: 0.1409 Acc: 0.9507                                               \n",
      "Epoch 014 | Train Loss: 0.1892 Acc: 0.9311 | Val Loss: 0.1073 Acc: 0.9603                                               \n",
      "Epoch 015 | Train Loss: 0.1672 Acc: 0.9410 | Val Loss: 0.1236 Acc: 0.9531                                               \n",
      "Epoch 016 | Train Loss: 0.1473 Acc: 0.9492 | Val Loss: 0.1110 Acc: 0.9645                                               \n",
      "Epoch 017 | Train Loss: 0.1354 Acc: 0.9525 | Val Loss: 1.2919 Acc: 0.7063                                               \n",
      "Epoch 018 | Train Loss: 0.1233 Acc: 0.9599 | Val Loss: 0.1003 Acc: 0.9642                                               \n",
      "Epoch 019 | Train Loss: 0.1123 Acc: 0.9633 | Val Loss: 0.0958 Acc: 0.9654                                               \n",
      "Epoch 020 | Train Loss: 0.0990 Acc: 0.9671 | Val Loss: 0.1442 Acc: 0.9484                                               \n",
      "Epoch 021 | Train Loss: 0.0944 Acc: 0.9695 | Val Loss: 0.1245 Acc: 0.9576                                               \n",
      "Epoch 022 | Train Loss: 0.0892 Acc: 0.9692 | Val Loss: 2.1109 Acc: 0.6328                                               \n",
      "Epoch 023 | Train Loss: 0.0816 Acc: 0.9748 | Val Loss: 0.1112 Acc: 0.9600                                               \n",
      "Epoch 024 | Train Loss: 0.0754 Acc: 0.9751 | Val Loss: 0.1019 Acc: 0.9639                                               \n",
      "Epoch 025 | Train Loss: 0.0698 Acc: 0.9757 | Val Loss: 0.0795 Acc: 0.9737                                               \n",
      "Epoch 026 | Train Loss: 0.0677 Acc: 0.9773 | Val Loss: 0.0863 Acc: 0.9725                                               \n",
      "Epoch 027 | Train Loss: 0.0633 Acc: 0.9790 | Val Loss: 0.0851 Acc: 0.9696                                               \n",
      "Epoch 028 | Train Loss: 0.0560 Acc: 0.9824 | Val Loss: 0.0634 Acc: 0.9767                                               \n",
      "Epoch 029 | Train Loss: 0.0485 Acc: 0.9839 | Val Loss: 0.0810 Acc: 0.9707                                               \n",
      "Epoch 030 | Train Loss: 0.0518 Acc: 0.9827 | Val Loss: 0.2371 Acc: 0.9290                                               \n",
      "Epoch 031 | Train Loss: 0.0545 Acc: 0.9833 | Val Loss: 0.0716 Acc: 0.9770                                               \n",
      "Epoch 032 | Train Loss: 0.0459 Acc: 0.9848 | Val Loss: 0.0949 Acc: 0.9687                                               \n",
      "Epoch 033 | Train Loss: 0.0434 Acc: 0.9859 | Val Loss: 0.1441 Acc: 0.9522                                               \n",
      "Epoch 034 | Train Loss: 0.0394 Acc: 0.9880 | Val Loss: 0.0747 Acc: 0.9770                                               \n",
      "Epoch 035 | Train Loss: 0.0367 Acc: 0.9890 | Val Loss: 0.0889 Acc: 0.9749                                               \n",
      "Epoch 036 | Train Loss: 0.0420 Acc: 0.9865 | Val Loss: 0.0857 Acc: 0.9722                                               \n",
      "Epoch 037 | Train Loss: 0.0344 Acc: 0.9883 | Val Loss: 0.8413 Acc: 0.8624                                               \n",
      "Epoch 038 | Train Loss: 0.0356 Acc: 0.9888 | Val Loss: 0.0591 Acc: 0.9815                                               \n",
      "Epoch 039 | Train Loss: 0.0373 Acc: 0.9877 | Val Loss: 0.0653 Acc: 0.9788                                               \n",
      "Epoch 040 | Train Loss: 0.0280 Acc: 0.9903 | Val Loss: 0.0806 Acc: 0.9773                                               \n",
      "Epoch 041 | Train Loss: 0.0379 Acc: 0.9886 | Val Loss: 0.0896 Acc: 0.9746                                               \n",
      "Epoch 042 | Train Loss: 0.0234 Acc: 0.9919 | Val Loss: 0.0702 Acc: 0.9761                                               \n",
      "Epoch 043 | Train Loss: 0.0284 Acc: 0.9913 | Val Loss: 0.0834 Acc: 0.9746                                               \n",
      "Epoch 044 | Train Loss: 0.0286 Acc: 0.9907 | Val Loss: 0.1203 Acc: 0.9630                                               \n",
      "Epoch 045 | Train Loss: 0.0266 Acc: 0.9919 | Val Loss: 0.0565 Acc: 0.9827                                               \n",
      "Epoch 046 | Train Loss: 0.0297 Acc: 0.9908 | Val Loss: 0.0633 Acc: 0.9812                                               \n",
      "Epoch 047 | Train Loss: 0.0252 Acc: 0.9922 | Val Loss: 0.0725 Acc: 0.9800                                               \n",
      "Epoch 048 | Train Loss: 0.0274 Acc: 0.9907 | Val Loss: 0.0565 Acc: 0.9830                                               \n",
      "Epoch 049 | Train Loss: 0.0258 Acc: 0.9911 | Val Loss: 0.0524 Acc: 0.9866                                               \n",
      "Epoch 050 | Train Loss: 0.0230 Acc: 0.9924 | Val Loss: 0.0983 Acc: 0.9761                                               \n",
      "Epoch 051 | Train Loss: 0.0215 Acc: 0.9939 | Val Loss: 0.0545 Acc: 0.9860                                               \n",
      "Epoch 052 | Train Loss: 0.0229 Acc: 0.9926 | Val Loss: 0.0675 Acc: 0.9794                                               \n",
      "Epoch 053 | Train Loss: 0.0196 Acc: 0.9933 | Val Loss: 0.0617 Acc: 0.9833                                               \n",
      "Epoch 054 | Train Loss: 0.0172 Acc: 0.9946 | Val Loss: 0.0649 Acc: 0.9818                                               \n",
      "Epoch 055 | Train Loss: 0.0243 Acc: 0.9924 | Val Loss: 0.1086 Acc: 0.9722                                               \n",
      "Epoch 056 | Train Loss: 0.0134 Acc: 0.9953 | Val Loss: 0.0632 Acc: 0.9842                                               \n",
      "Epoch 057 | Train Loss: 0.0172 Acc: 0.9943 | Val Loss: 0.1493 Acc: 0.9615                                               \n",
      "Epoch 058 | Train Loss: 0.0177 Acc: 0.9951 | Val Loss: 0.0644 Acc: 0.9827                                               \n",
      "Epoch 059 | Train Loss: 0.0165 Acc: 0.9952 | Val Loss: 0.0534 Acc: 0.9845                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 128, 'cnn_dropout': 0.15881729612211898, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 64, 'learning_rate': 9.614833279082735e-05, 'lstm_dense': 64, 'lstm_hidden_size': 32, 'lstm_layers': 6, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 75.7605 Acc: 0.3459 | Val Loss: 10.7530 Acc: 0.3675                                             \n",
      "Epoch 002 | Train Loss: 20.3047 Acc: 0.3717 | Val Loss: 4.1755 Acc: 0.4209                                              \n",
      "Epoch 003 | Train Loss: 12.1932 Acc: 0.3844 | Val Loss: 3.2120 Acc: 0.4415                                              \n",
      "Epoch 004 | Train Loss: 9.0021 Acc: 0.3900 | Val Loss: 1.9776 Acc: 0.4746                                               \n",
      "Epoch 005 | Train Loss: 6.7044 Acc: 0.4189 | Val Loss: 1.6399 Acc: 0.5794                                               \n",
      "Epoch 006 | Train Loss: 5.4549 Acc: 0.4346 | Val Loss: 1.5408 Acc: 0.5815                                               \n",
      "Epoch 007 | Train Loss: 4.4617 Acc: 0.4529 | Val Loss: 1.4808 Acc: 0.6373                                               \n",
      "Epoch 008 | Train Loss: 3.7993 Acc: 0.4902 | Val Loss: 1.1334 Acc: 0.6663                                               \n",
      "Epoch 009 | Train Loss: 3.2253 Acc: 0.5185 | Val Loss: 1.0044 Acc: 0.7107                                               \n",
      "Epoch 010 | Train Loss: 2.7407 Acc: 0.5480 | Val Loss: 1.1684 Acc: 0.7299                                               \n",
      "Epoch 011 | Train Loss: 2.5208 Acc: 0.5693 | Val Loss: 0.8064 Acc: 0.7439                                               \n",
      "Epoch 012 | Train Loss: 2.1890 Acc: 0.5994 | Val Loss: 0.8589 Acc: 0.7815                                               \n",
      "Epoch 013 | Train Loss: 1.8964 Acc: 0.6236 | Val Loss: 0.7029 Acc: 0.7830                                               \n",
      "Epoch 014 | Train Loss: 1.7555 Acc: 0.6492 | Val Loss: 0.6550 Acc: 0.8137                                               \n",
      "Epoch 015 | Train Loss: 1.5423 Acc: 0.6698 | Val Loss: 0.6291 Acc: 0.8266                                               \n",
      "Epoch 016 | Train Loss: 1.4619 Acc: 0.6926 | Val Loss: 0.6649 Acc: 0.7949                                               \n",
      "Epoch 017 | Train Loss: 1.2737 Acc: 0.7095 | Val Loss: 0.6473 Acc: 0.8245                                               \n",
      "Epoch 018 | Train Loss: 1.1705 Acc: 0.7245 | Val Loss: 0.7575 Acc: 0.8093                                               \n",
      "Epoch 019 | Train Loss: 1.0530 Acc: 0.7446 | Val Loss: 0.6455 Acc: 0.8358                                               \n",
      "Epoch 020 | Train Loss: 0.9944 Acc: 0.7580 | Val Loss: 0.5746 Acc: 0.8537                                               \n",
      "Epoch 021 | Train Loss: 0.9045 Acc: 0.7751 | Val Loss: 0.5654 Acc: 0.8272                                               \n",
      "Epoch 022 | Train Loss: 0.8199 Acc: 0.7828 | Val Loss: 0.5653 Acc: 0.8439                                               \n",
      "Epoch 023 | Train Loss: 0.7215 Acc: 0.8034 | Val Loss: 0.6144 Acc: 0.8355                                               \n",
      "Epoch 024 | Train Loss: 0.6850 Acc: 0.8122 | Val Loss: 0.4206 Acc: 0.8869                                               \n",
      "Epoch 025 | Train Loss: 0.6292 Acc: 0.8248 | Val Loss: 0.4239 Acc: 0.9000                                               \n",
      "Epoch 026 | Train Loss: 0.6066 Acc: 0.8313 | Val Loss: 0.3742 Acc: 0.9101                                               \n",
      "Epoch 027 | Train Loss: 0.5662 Acc: 0.8456 | Val Loss: 0.3841 Acc: 0.8928                                               \n",
      "Epoch 028 | Train Loss: 0.5289 Acc: 0.8501 | Val Loss: 0.4877 Acc: 0.8758                                               \n",
      "Epoch 029 | Train Loss: 0.4971 Acc: 0.8626 | Val Loss: 0.4827 Acc: 0.8594                                               \n",
      "Epoch 030 | Train Loss: 0.4797 Acc: 0.8652 | Val Loss: 0.4184 Acc: 0.8806                                               \n",
      "Epoch 031 | Train Loss: 0.4418 Acc: 0.8728 | Val Loss: 0.3816 Acc: 0.8707                                               \n",
      "Epoch 032 | Train Loss: 0.4187 Acc: 0.8792 | Val Loss: 0.4220 Acc: 0.8740                                               \n",
      "Epoch 033 | Train Loss: 0.4026 Acc: 0.8846 | Val Loss: 0.3333 Acc: 0.9042                                               \n",
      "Epoch 034 | Train Loss: 0.4005 Acc: 0.8911 | Val Loss: 0.4656 Acc: 0.8666                                               \n",
      "Epoch 035 | Train Loss: 0.3655 Acc: 0.8970 | Val Loss: 0.3207 Acc: 0.9075                                               \n",
      "Epoch 036 | Train Loss: 0.3408 Acc: 0.8975 | Val Loss: 0.3718 Acc: 0.8907                                               \n",
      "Epoch 037 | Train Loss: 0.3062 Acc: 0.9081 | Val Loss: 0.2881 Acc: 0.9090                                               \n",
      "Epoch 038 | Train Loss: 0.2892 Acc: 0.9122 | Val Loss: 0.3113 Acc: 0.9060                                               \n",
      "Epoch 039 | Train Loss: 0.2596 Acc: 0.9197 | Val Loss: 0.2565 Acc: 0.9257                                               \n",
      "Epoch 040 | Train Loss: 0.2463 Acc: 0.9222 | Val Loss: 0.2653 Acc: 0.9263                                               \n",
      "Epoch 041 | Train Loss: 0.2481 Acc: 0.9263 | Val Loss: 0.3195 Acc: 0.9063                                               \n",
      "Epoch 042 | Train Loss: 0.2184 Acc: 0.9349 | Val Loss: 0.2185 Acc: 0.9355                                               \n",
      "Epoch 043 | Train Loss: 0.2119 Acc: 0.9371 | Val Loss: 0.2593 Acc: 0.9018                                               \n",
      "Epoch 044 | Train Loss: 0.2319 Acc: 0.9356 | Val Loss: 0.2347 Acc: 0.9412                                               \n",
      "Epoch 045 | Train Loss: 0.2192 Acc: 0.9331 | Val Loss: 0.2514 Acc: 0.9116                                               \n",
      "Epoch 046 | Train Loss: 0.1939 Acc: 0.9423 | Val Loss: 0.3137 Acc: 0.8907                                               \n",
      "Epoch 047 | Train Loss: 0.1820 Acc: 0.9456 | Val Loss: 0.2709 Acc: 0.9125                                               \n",
      "Epoch 048 | Train Loss: 0.1871 Acc: 0.9449 | Val Loss: 0.2873 Acc: 0.9063                                               \n",
      "Epoch 049 | Train Loss: 0.1701 Acc: 0.9535 | Val Loss: 0.3942 Acc: 0.8863                                               \n",
      "Epoch 050 | Train Loss: 0.1696 Acc: 0.9500 | Val Loss: 0.3654 Acc: 0.8842                                               \n",
      "Epoch 051 | Train Loss: 0.1480 Acc: 0.9566 | Val Loss: 0.2678 Acc: 0.9101                                               \n",
      "Epoch 052 | Train Loss: 0.1511 Acc: 0.9557 | Val Loss: 0.3580 Acc: 0.8896                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 256, 'cnn_dropout': 0.4734894108387411, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 16, 'cnn_kernels_2': 16, 'learning_rate': 0.0022940821749405484, 'lstm_dense': 32, 'lstm_hidden_size': 64, 'lstm_layers': 5, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 13.0689 Acc: 0.4191 | Val Loss: 1.5997 Acc: 0.5496                                              \n",
      "Epoch 002 | Train Loss: 1.6027 Acc: 0.5206 | Val Loss: 0.9526 Acc: 0.6051                                               \n",
      "Epoch 003 | Train Loss: 0.9749 Acc: 0.6015 | Val Loss: 0.8003 Acc: 0.6027                                               \n",
      "Epoch 004 | Train Loss: 0.8067 Acc: 0.6396 | Val Loss: 0.6685 Acc: 0.6854                                               \n",
      "Epoch 005 | Train Loss: 0.7309 Acc: 0.6733 | Val Loss: 0.6056 Acc: 0.7287                                               \n",
      "Epoch 006 | Train Loss: 0.6749 Acc: 0.6978 | Val Loss: 0.6555 Acc: 0.7084                                               \n",
      "Epoch 007 | Train Loss: 0.6535 Acc: 0.7067 | Val Loss: 0.5531 Acc: 0.7361                                               \n",
      "Epoch 008 | Train Loss: 0.6135 Acc: 0.7324 | Val Loss: 0.4825 Acc: 0.8149                                               \n",
      "Epoch 009 | Train Loss: 0.5975 Acc: 0.7437 | Val Loss: 0.7316 Acc: 0.6433                                               \n",
      "Epoch 010 | Train Loss: 0.5867 Acc: 0.7464 | Val Loss: 0.5820 Acc: 0.6893                                               \n",
      "Epoch 011 | Train Loss: 0.5652 Acc: 0.7523 | Val Loss: 0.5223 Acc: 0.7782                                               \n",
      "Epoch 012 | Train Loss: 0.6340 Acc: 0.7280 | Val Loss: 0.6343 Acc: 0.6961                                               \n",
      "Epoch 013 | Train Loss: 0.6228 Acc: 0.7266 | Val Loss: 0.6982 Acc: 0.6504                                               \n",
      "Epoch 014 | Train Loss: 0.6016 Acc: 0.7368 | Val Loss: 0.5345 Acc: 0.7716                                               \n",
      "Epoch 015 | Train Loss: 0.5751 Acc: 0.7439 | Val Loss: 0.6081 Acc: 0.7287                                               \n",
      "Epoch 016 | Train Loss: 0.5429 Acc: 0.7586 | Val Loss: 0.5880 Acc: 0.7179                                               \n",
      "Epoch 017 | Train Loss: 0.5432 Acc: 0.7526 | Val Loss: 0.6685 Acc: 0.6609                                               \n",
      "Epoch 018 | Train Loss: 0.5451 Acc: 0.7587 | Val Loss: 0.6506 Acc: 0.7543                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 32, 'cnn_dropout': 0.5967583695606333, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 32, 'cnn_kernels_2': 32, 'learning_rate': 0.0012255862088161656, 'lstm_dense': 64, 'lstm_hidden_size': 96, 'lstm_layers': 4, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 9.1673 Acc: 0.4223 | Val Loss: 1.7789 Acc: 0.4319                                               \n",
      "Epoch 002 | Train Loss: 1.3784 Acc: 0.4851 | Val Loss: 1.1183 Acc: 0.4672                                               \n",
      "Epoch 003 | Train Loss: 0.9398 Acc: 0.5310 | Val Loss: 0.7955 Acc: 0.5812                                               \n",
      "Epoch 004 | Train Loss: 0.7774 Acc: 0.6177 | Val Loss: 0.8317 Acc: 0.5863                                               \n",
      "Epoch 005 | Train Loss: 0.6809 Acc: 0.6746 | Val Loss: 0.6776 Acc: 0.6648                                               \n",
      "Epoch 006 | Train Loss: 0.6215 Acc: 0.7084 | Val Loss: 0.6245 Acc: 0.6866                                               \n",
      "Epoch 007 | Train Loss: 0.5827 Acc: 0.7217 | Val Loss: 0.6122 Acc: 0.6466                                               \n",
      "Epoch 008 | Train Loss: 0.5584 Acc: 0.7452 | Val Loss: 0.6800 Acc: 0.7045                                               \n",
      "Epoch 009 | Train Loss: 0.5413 Acc: 0.7521 | Val Loss: 0.6770 Acc: 0.6442                                               \n",
      "Epoch 010 | Train Loss: 0.5151 Acc: 0.7682 | Val Loss: 0.6788 Acc: 0.7209                                               \n",
      "Epoch 011 | Train Loss: 0.4972 Acc: 0.7771 | Val Loss: 0.7144 Acc: 0.6976                                               \n",
      "Epoch 012 | Train Loss: 0.4890 Acc: 0.7801 | Val Loss: 0.4954 Acc: 0.7669                                               \n",
      "Epoch 013 | Train Loss: 0.4557 Acc: 0.7980 | Val Loss: 0.5264 Acc: 0.7734                                               \n",
      "Epoch 014 | Train Loss: 0.4582 Acc: 0.7977 | Val Loss: 0.6257 Acc: 0.7579                                               \n",
      "Epoch 015 | Train Loss: 0.4468 Acc: 0.8042 | Val Loss: 0.5469 Acc: 0.7690                                               \n",
      "Epoch 016 | Train Loss: 0.4411 Acc: 0.8082 | Val Loss: 0.7728 Acc: 0.6519                                               \n",
      "Epoch 017 | Train Loss: 0.4277 Acc: 0.8130 | Val Loss: 0.5425 Acc: 0.7800                                               \n",
      "Epoch 018 | Train Loss: 0.4269 Acc: 0.8148 | Val Loss: 0.6646 Acc: 0.7155                                               \n",
      "Epoch 019 | Train Loss: 0.4260 Acc: 0.8130 | Val Loss: 0.6421 Acc: 0.7349                                               \n",
      "Epoch 020 | Train Loss: 0.4124 Acc: 0.8166 | Val Loss: 0.7389 Acc: 0.6597                                               \n",
      "Epoch 021 | Train Loss: 0.4053 Acc: 0.8266 | Val Loss: 0.4822 Acc: 0.8191                                               \n",
      "Epoch 022 | Train Loss: 0.4035 Acc: 0.8254 | Val Loss: 0.4676 Acc: 0.8290                                               \n",
      "Epoch 023 | Train Loss: 0.4534 Acc: 0.8169 | Val Loss: 1.0318 Acc: 0.6454                                               \n",
      "Epoch 024 | Train Loss: 0.3989 Acc: 0.8338 | Val Loss: 0.4923 Acc: 0.8188                                               \n",
      "Epoch 025 | Train Loss: 0.3829 Acc: 0.8395 | Val Loss: 0.5315 Acc: 0.7913                                               \n",
      "Epoch 026 | Train Loss: 0.3985 Acc: 0.8308 | Val Loss: 0.6407 Acc: 0.7573                                               \n",
      "Epoch 027 | Train Loss: 0.3910 Acc: 0.8413 | Val Loss: 0.5796 Acc: 0.8036                                               \n",
      "Epoch 028 | Train Loss: 0.3690 Acc: 0.8447 | Val Loss: 0.5436 Acc: 0.7364                                               \n",
      "Epoch 029 | Train Loss: 0.4085 Acc: 0.8377 | Val Loss: 0.4941 Acc: 0.8633                                               \n",
      "Epoch 030 | Train Loss: 0.3805 Acc: 0.8407 | Val Loss: 0.4468 Acc: 0.7958                                               \n",
      "Epoch 031 | Train Loss: 0.3717 Acc: 0.8484 | Val Loss: 0.8437 Acc: 0.6651                                               \n",
      "Epoch 032 | Train Loss: 0.3711 Acc: 0.8457 | Val Loss: 0.6848 Acc: 0.7633                                               \n",
      "Epoch 033 | Train Loss: 0.3519 Acc: 0.8575 | Val Loss: 1.0090 Acc: 0.6182                                               \n",
      "Epoch 034 | Train Loss: 0.3660 Acc: 0.8503 | Val Loss: 0.6684 Acc: 0.7116                                               \n",
      "Epoch 035 | Train Loss: 0.3573 Acc: 0.8539 | Val Loss: 1.7090 Acc: 0.4979                                               \n",
      "Epoch 036 | Train Loss: 0.4155 Acc: 0.8294 | Val Loss: 0.5173 Acc: 0.7528                                               \n",
      "Epoch 037 | Train Loss: 0.3726 Acc: 0.8503 | Val Loss: 0.6394 Acc: 0.7863                                               \n",
      "Epoch 038 | Train Loss: 0.3719 Acc: 0.8510 | Val Loss: 0.6032 Acc: 0.8131                                               \n",
      "Epoch 039 | Train Loss: 0.3451 Acc: 0.8545 | Val Loss: 0.8801 Acc: 0.7149                                               \n",
      "Epoch 040 | Train Loss: 0.3458 Acc: 0.8619 | Val Loss: 0.4196 Acc: 0.8149                                               \n",
      "Epoch 041 | Train Loss: 0.4008 Acc: 0.8334 | Val Loss: 0.5791 Acc: 0.7994                                               \n",
      "Epoch 042 | Train Loss: 0.4081 Acc: 0.8293 | Val Loss: 0.6421 Acc: 0.7734                                               \n",
      "Epoch 043 | Train Loss: 0.3695 Acc: 0.8462 | Val Loss: 0.8622 Acc: 0.6582                                               \n",
      "Epoch 044 | Train Loss: 0.3498 Acc: 0.8569 | Val Loss: 0.7706 Acc: 0.6794                                               \n",
      "Epoch 045 | Train Loss: 0.3562 Acc: 0.8557 | Val Loss: 0.6928 Acc: 0.7122                                               \n",
      "Epoch 046 | Train Loss: 0.3462 Acc: 0.8592 | Val Loss: 0.5767 Acc: 0.8048                                               \n",
      "Epoch 047 | Train Loss: 0.4457 Acc: 0.8387 | Val Loss: 0.5564 Acc: 0.7269                                               \n",
      "Epoch 048 | Train Loss: 0.3633 Acc: 0.8577 | Val Loss: 0.7070 Acc: 0.7648                                               \n",
      "Epoch 049 | Train Loss: 0.3514 Acc: 0.8581 | Val Loss: 0.6318 Acc: 0.6979                                               \n",
      "Epoch 050 | Train Loss: 0.3516 Acc: 0.8584 | Val Loss: 0.6082 Acc: 0.7516                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 256, 'cnn_dropout': 0.31462347084395964, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 64, 'learning_rate': 4.835214487794516e-05, 'lstm_dense': 256, 'lstm_hidden_size': 128, 'lstm_layers': 2, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 8.0320 Acc: 0.2843 | Val Loss: 1.1973 Acc: 0.2740                                               \n",
      "Epoch 002 | Train Loss: 1.1904 Acc: 0.3129 | Val Loss: 1.1068 Acc: 0.3296                                               \n",
      "Epoch 003 | Train Loss: 1.0948 Acc: 0.3826 | Val Loss: 1.0090 Acc: 0.4101                                               \n",
      "Epoch 004 | Train Loss: 1.0405 Acc: 0.4125 | Val Loss: 0.9798 Acc: 0.4633                                               \n",
      "Epoch 005 | Train Loss: 1.0118 Acc: 0.4257 | Val Loss: 0.9113 Acc: 0.4839                                               \n",
      "Epoch 006 | Train Loss: 0.9927 Acc: 0.4400 | Val Loss: 0.9321 Acc: 0.4561                                               \n",
      "Epoch 007 | Train Loss: 0.9886 Acc: 0.4543 | Val Loss: 0.8913 Acc: 0.5322                                               \n",
      "Epoch 008 | Train Loss: 0.9692 Acc: 0.4653 | Val Loss: 0.8935 Acc: 0.4701                                               \n",
      "Epoch 009 | Train Loss: 0.9550 Acc: 0.4747 | Val Loss: 0.8781 Acc: 0.5128                                               \n",
      "Epoch 010 | Train Loss: 0.9442 Acc: 0.4866 | Val Loss: 0.8767 Acc: 0.5269                                               \n",
      "Epoch 011 | Train Loss: 0.9430 Acc: 0.4829 | Val Loss: 0.8850 Acc: 0.5036                                               \n",
      "Epoch 012 | Train Loss: 0.9312 Acc: 0.4858 | Val Loss: 0.8785 Acc: 0.5078                                               \n",
      "Epoch 013 | Train Loss: 0.9196 Acc: 0.4904 | Val Loss: 0.8783 Acc: 0.5152                                               \n",
      "Epoch 014 | Train Loss: 0.9217 Acc: 0.4948 | Val Loss: 0.8978 Acc: 0.4785                                               \n",
      "Epoch 015 | Train Loss: 0.9151 Acc: 0.4986 | Val Loss: 0.8419 Acc: 0.5161                                               \n",
      "Epoch 016 | Train Loss: 0.9147 Acc: 0.4943 | Val Loss: 0.8423 Acc: 0.5119                                               \n",
      "Epoch 017 | Train Loss: 0.9135 Acc: 0.4930 | Val Loss: 0.8605 Acc: 0.5012                                               \n",
      "Epoch 018 | Train Loss: 0.9034 Acc: 0.4991 | Val Loss: 0.8616 Acc: 0.5594                                               \n",
      "Epoch 019 | Train Loss: 0.9001 Acc: 0.5001 | Val Loss: 0.8382 Acc: 0.4991                                               \n",
      "Epoch 020 | Train Loss: 0.8936 Acc: 0.5032 | Val Loss: 0.8257 Acc: 0.5845                                               \n",
      "Epoch 021 | Train Loss: 0.9003 Acc: 0.4976 | Val Loss: 0.8286 Acc: 0.4991                                               \n",
      "Epoch 022 | Train Loss: 0.8936 Acc: 0.5059 | Val Loss: 0.8460 Acc: 0.5125                                               \n",
      "Epoch 023 | Train Loss: 0.9039 Acc: 0.5047 | Val Loss: 0.8459 Acc: 0.5690                                               \n",
      "Epoch 024 | Train Loss: 0.8859 Acc: 0.5084 | Val Loss: 0.8300 Acc: 0.5045                                               \n",
      "Epoch 025 | Train Loss: 0.8860 Acc: 0.5075 | Val Loss: 0.8108 Acc: 0.5287                                               \n",
      "Epoch 026 | Train Loss: 0.8883 Acc: 0.5095 | Val Loss: 0.8065 Acc: 0.5161                                               \n",
      "Epoch 027 | Train Loss: 0.8824 Acc: 0.5071 | Val Loss: 0.8272 Acc: 0.5955                                               \n",
      "Epoch 028 | Train Loss: 0.8772 Acc: 0.5103 | Val Loss: 0.8178 Acc: 0.5248                                               \n",
      "Epoch 029 | Train Loss: 0.8854 Acc: 0.5083 | Val Loss: 0.8023 Acc: 0.5496                                               \n",
      "Epoch 030 | Train Loss: 0.8751 Acc: 0.5067 | Val Loss: 0.7870 Acc: 0.5597                                               \n",
      "Epoch 031 | Train Loss: 0.8779 Acc: 0.5079 | Val Loss: 0.8236 Acc: 0.5045                                               \n",
      "Epoch 032 | Train Loss: 0.8629 Acc: 0.5191 | Val Loss: 0.7997 Acc: 0.5433                                               \n",
      "Epoch 033 | Train Loss: 0.8729 Acc: 0.5211 | Val Loss: 0.8238 Acc: 0.4928                                               \n",
      "Epoch 034 | Train Loss: 0.8633 Acc: 0.5302 | Val Loss: 0.7977 Acc: 0.5478                                               \n",
      "Epoch 035 | Train Loss: 0.8566 Acc: 0.5216 | Val Loss: 0.8024 Acc: 0.5761                                               \n",
      "Epoch 036 | Train Loss: 0.8492 Acc: 0.5324 | Val Loss: 0.7729 Acc: 0.5284                                               \n",
      "Epoch 037 | Train Loss: 0.8520 Acc: 0.5308 | Val Loss: 0.8108 Acc: 0.5582                                               \n",
      "Epoch 038 | Train Loss: 0.8380 Acc: 0.5438 | Val Loss: 0.7821 Acc: 0.6057                                               \n",
      "Epoch 039 | Train Loss: 0.8489 Acc: 0.5491 | Val Loss: 0.7660 Acc: 0.6221                                               \n",
      "Epoch 040 | Train Loss: 0.8283 Acc: 0.5576 | Val Loss: 0.7426 Acc: 0.5884                                               \n",
      "Epoch 041 | Train Loss: 0.8246 Acc: 0.5680 | Val Loss: 0.7251 Acc: 0.6460                                               \n",
      "Epoch 042 | Train Loss: 0.8236 Acc: 0.5652 | Val Loss: 0.7480 Acc: 0.6376                                               \n",
      "Epoch 043 | Train Loss: 0.8125 Acc: 0.5670 | Val Loss: 0.7517 Acc: 0.6045                                               \n",
      "Epoch 044 | Train Loss: 0.8064 Acc: 0.5753 | Val Loss: 0.7241 Acc: 0.6642                                               \n",
      "Epoch 045 | Train Loss: 0.8125 Acc: 0.5757 | Val Loss: 0.7534 Acc: 0.5546                                               \n",
      "Epoch 046 | Train Loss: 0.8123 Acc: 0.5827 | Val Loss: 0.6986 Acc: 0.6546                                               \n",
      "Epoch 047 | Train Loss: 0.7997 Acc: 0.5777 | Val Loss: 0.7231 Acc: 0.6418                                               \n",
      "Epoch 048 | Train Loss: 0.7922 Acc: 0.5850 | Val Loss: 0.7259 Acc: 0.6424                                               \n",
      "Epoch 049 | Train Loss: 0.7927 Acc: 0.5787 | Val Loss: 0.7541 Acc: 0.5884                                               \n",
      "Epoch 050 | Train Loss: 0.7836 Acc: 0.5896 | Val Loss: 0.7112 Acc: 0.6090                                               \n",
      "Epoch 051 | Train Loss: 0.7915 Acc: 0.5883 | Val Loss: 0.7141 Acc: 0.6510                                               \n",
      "Epoch 052 | Train Loss: 0.7795 Acc: 0.5947 | Val Loss: 0.7076 Acc: 0.6675                                               \n",
      "Epoch 053 | Train Loss: 0.7829 Acc: 0.6015 | Val Loss: 0.7141 Acc: 0.6382                                               \n",
      "Epoch 054 | Train Loss: 0.7763 Acc: 0.6073 | Val Loss: 0.7067 Acc: 0.6600                                               \n",
      "Epoch 055 | Train Loss: 0.7737 Acc: 0.6041 | Val Loss: 0.6815 Acc: 0.6519                                               \n",
      "Epoch 056 | Train Loss: 0.7734 Acc: 0.6057 | Val Loss: 0.6986 Acc: 0.6493                                               \n",
      "Epoch 057 | Train Loss: 0.7646 Acc: 0.6085 | Val Loss: 0.6814 Acc: 0.6863                                               \n",
      "Epoch 058 | Train Loss: 0.7650 Acc: 0.6142 | Val Loss: 0.6880 Acc: 0.6782                                               \n",
      "Epoch 059 | Train Loss: 0.7523 Acc: 0.6197 | Val Loss: 0.7126 Acc: 0.6654                                               \n",
      "Epoch 060 | Train Loss: 0.7594 Acc: 0.6159 | Val Loss: 0.6617 Acc: 0.6791                                               \n",
      "Epoch 061 | Train Loss: 0.7549 Acc: 0.6148 | Val Loss: 0.6539 Acc: 0.6651                                               \n",
      "Epoch 062 | Train Loss: 0.7541 Acc: 0.6142 | Val Loss: 0.6938 Acc: 0.6561                                               \n",
      "Epoch 063 | Train Loss: 0.7439 Acc: 0.6190 | Val Loss: 0.6721 Acc: 0.6612                                               \n",
      "Epoch 064 | Train Loss: 0.7547 Acc: 0.6211 | Val Loss: 0.6566 Acc: 0.7266                                               \n",
      "Epoch 065 | Train Loss: 0.7436 Acc: 0.6186 | Val Loss: 0.6824 Acc: 0.6785                                               \n",
      "Epoch 066 | Train Loss: 0.7469 Acc: 0.6230 | Val Loss: 0.6800 Acc: 0.6937                                               \n",
      "Epoch 067 | Train Loss: 0.7410 Acc: 0.6221 | Val Loss: 0.6493 Acc: 0.7164                                               \n",
      "Epoch 068 | Train Loss: 0.7413 Acc: 0.6224 | Val Loss: 0.6509 Acc: 0.6824                                               \n",
      "Epoch 069 | Train Loss: 0.7356 Acc: 0.6266 | Val Loss: 0.6455 Acc: 0.7155                                               \n",
      "Epoch 070 | Train Loss: 0.7263 Acc: 0.6350 | Val Loss: 0.6719 Acc: 0.6481                                               \n",
      "Epoch 071 | Train Loss: 0.7261 Acc: 0.6324 | Val Loss: 0.6570 Acc: 0.6561                                               \n",
      "Epoch 072 | Train Loss: 0.7268 Acc: 0.6366 | Val Loss: 0.6897 Acc: 0.6657                                               \n",
      "Epoch 073 | Train Loss: 0.7250 Acc: 0.6403 | Val Loss: 0.6332 Acc: 0.7496                                               \n",
      "Epoch 074 | Train Loss: 0.7127 Acc: 0.6459 | Val Loss: 0.6338 Acc: 0.6964                                               \n",
      "Epoch 075 | Train Loss: 0.7176 Acc: 0.6361 | Val Loss: 0.6361 Acc: 0.7182                                               \n",
      "Epoch 076 | Train Loss: 0.7127 Acc: 0.6456 | Val Loss: 0.6275 Acc: 0.6710                                               \n",
      "Epoch 077 | Train Loss: 0.7141 Acc: 0.6474 | Val Loss: 0.6270 Acc: 0.6919                                               \n",
      "Epoch 078 | Train Loss: 0.7223 Acc: 0.6410 | Val Loss: 0.6166 Acc: 0.7045                                               \n",
      "Epoch 079 | Train Loss: 0.7092 Acc: 0.6454 | Val Loss: 0.6209 Acc: 0.6878                                               \n",
      "Epoch 080 | Train Loss: 0.7019 Acc: 0.6536 | Val Loss: 0.6076 Acc: 0.7472                                               \n",
      "Epoch 081 | Train Loss: 0.7050 Acc: 0.6552 | Val Loss: 0.6246 Acc: 0.7167                                               \n",
      "Epoch 082 | Train Loss: 0.7026 Acc: 0.6502 | Val Loss: 0.6024 Acc: 0.7379                                               \n",
      "Epoch 083 | Train Loss: 0.6889 Acc: 0.6603 | Val Loss: 0.5925 Acc: 0.7221                                               \n",
      "Epoch 084 | Train Loss: 0.7004 Acc: 0.6518 | Val Loss: 0.6032 Acc: 0.7075                                               \n",
      "Epoch 085 | Train Loss: 0.6952 Acc: 0.6542 | Val Loss: 0.6064 Acc: 0.7182                                               \n",
      "Epoch 086 | Train Loss: 0.6894 Acc: 0.6586 | Val Loss: 0.5854 Acc: 0.7081                                               \n",
      "Epoch 087 | Train Loss: 0.6870 Acc: 0.6590 | Val Loss: 0.6465 Acc: 0.6884                                               \n",
      "Epoch 088 | Train Loss: 0.6873 Acc: 0.6636 | Val Loss: 0.5841 Acc: 0.7185                                               \n",
      "Epoch 089 | Train Loss: 0.6794 Acc: 0.6596 | Val Loss: 0.5899 Acc: 0.7122                                               \n",
      "Epoch 090 | Train Loss: 0.6863 Acc: 0.6628 | Val Loss: 0.6098 Acc: 0.7039                                               \n",
      "Epoch 091 | Train Loss: 0.6791 Acc: 0.6668 | Val Loss: 0.5766 Acc: 0.7122                                               \n",
      "Epoch 092 | Train Loss: 0.6699 Acc: 0.6725 | Val Loss: 0.5706 Acc: 0.7158                                               \n",
      "Epoch 093 | Train Loss: 0.6746 Acc: 0.6713 | Val Loss: 0.5793 Acc: 0.7128                                               \n",
      "Epoch 094 | Train Loss: 0.6668 Acc: 0.6742 | Val Loss: 0.5688 Acc: 0.7057                                               \n",
      "Epoch 095 | Train Loss: 0.6780 Acc: 0.6668 | Val Loss: 0.5612 Acc: 0.7436                                               \n",
      "Epoch 096 | Train Loss: 0.6661 Acc: 0.6641 | Val Loss: 0.6018 Acc: 0.7355                                               \n",
      "Epoch 097 | Train Loss: 0.6687 Acc: 0.6753 | Val Loss: 0.5571 Acc: 0.7299                                               \n",
      "Epoch 098 | Train Loss: 0.6697 Acc: 0.6698 | Val Loss: 0.5685 Acc: 0.7260                                               \n",
      "Epoch 099 | Train Loss: 0.6692 Acc: 0.6689 | Val Loss: 0.5598 Acc: 0.7179                                               \n",
      "Epoch 100 | Train Loss: 0.6625 Acc: 0.6748 | Val Loss: 0.5464 Acc: 0.7349                                               \n",
      "Epoch 101 | Train Loss: 0.6608 Acc: 0.6730 | Val Loss: 0.5630 Acc: 0.7567                                               \n",
      "Epoch 102 | Train Loss: 0.6644 Acc: 0.6732 | Val Loss: 0.5729 Acc: 0.7331                                               \n",
      "Epoch 103 | Train Loss: 0.6557 Acc: 0.6778 | Val Loss: 0.5461 Acc: 0.7275                                               \n",
      "Epoch 104 | Train Loss: 0.6448 Acc: 0.6810 | Val Loss: 0.5420 Acc: 0.7367                                               \n",
      "Epoch 105 | Train Loss: 0.6583 Acc: 0.6773 | Val Loss: 0.5410 Acc: 0.7179                                               \n",
      "Epoch 106 | Train Loss: 0.6512 Acc: 0.6790 | Val Loss: 0.5543 Acc: 0.7493                                               \n",
      "Epoch 107 | Train Loss: 0.6472 Acc: 0.6828 | Val Loss: 0.5496 Acc: 0.7406                                               \n",
      "Epoch 108 | Train Loss: 0.6517 Acc: 0.6829 | Val Loss: 0.5513 Acc: 0.7131                                               \n",
      "Epoch 109 | Train Loss: 0.6516 Acc: 0.6777 | Val Loss: 0.5490 Acc: 0.7639                                               \n",
      "Epoch 110 | Train Loss: 0.6424 Acc: 0.6857 | Val Loss: 0.5285 Acc: 0.7257                                               \n",
      "Epoch 111 | Train Loss: 0.6473 Acc: 0.6836 | Val Loss: 0.5335 Acc: 0.7254                                               \n",
      "Epoch 112 | Train Loss: 0.6439 Acc: 0.6886 | Val Loss: 0.5450 Acc: 0.7119                                               \n",
      "Epoch 113 | Train Loss: 0.6454 Acc: 0.6866 | Val Loss: 0.5432 Acc: 0.7427                                               \n",
      "Epoch 114 | Train Loss: 0.6462 Acc: 0.6808 | Val Loss: 0.5320 Acc: 0.7110                                               \n",
      "Epoch 115 | Train Loss: 0.6467 Acc: 0.6826 | Val Loss: 0.5188 Acc: 0.7582                                               \n",
      "Epoch 116 | Train Loss: 0.6407 Acc: 0.6880 | Val Loss: 0.5340 Acc: 0.7382                                               \n",
      "Epoch 117 | Train Loss: 0.6314 Acc: 0.6993 | Val Loss: 0.5241 Acc: 0.7510                                               \n",
      "Epoch 118 | Train Loss: 0.6404 Acc: 0.6856 | Val Loss: 0.5136 Acc: 0.7776                                               \n",
      "Epoch 119 | Train Loss: 0.6234 Acc: 0.6960 | Val Loss: 0.5276 Acc: 0.7245                                               \n",
      "Epoch 120 | Train Loss: 0.6210 Acc: 0.7005 | Val Loss: 0.5460 Acc: 0.6973                                               \n",
      "Epoch 121 | Train Loss: 0.6229 Acc: 0.7020 | Val Loss: 0.5159 Acc: 0.7278                                               \n",
      "Epoch 122 | Train Loss: 0.6338 Acc: 0.6966 | Val Loss: 0.5611 Acc: 0.7406                                               \n",
      "Epoch 123 | Train Loss: 0.6251 Acc: 0.6968 | Val Loss: 0.5077 Acc: 0.7618                                               \n",
      "Epoch 124 | Train Loss: 0.6310 Acc: 0.6957 | Val Loss: 0.5225 Acc: 0.7475                                               \n",
      "Epoch 125 | Train Loss: 0.6304 Acc: 0.6976 | Val Loss: 0.5049 Acc: 0.7409                                               \n",
      "Epoch 126 | Train Loss: 0.6180 Acc: 0.7043 | Val Loss: 0.5138 Acc: 0.7734                                               \n",
      "Epoch 127 | Train Loss: 0.6170 Acc: 0.6991 | Val Loss: 0.5153 Acc: 0.7525                                               \n",
      "Epoch 128 | Train Loss: 0.6172 Acc: 0.6997 | Val Loss: 0.5012 Acc: 0.7684                                               \n",
      "Epoch 129 | Train Loss: 0.6146 Acc: 0.7030 | Val Loss: 0.4914 Acc: 0.7710                                               \n",
      "Epoch 130 | Train Loss: 0.6173 Acc: 0.7025 | Val Loss: 0.5113 Acc: 0.7322                                               \n",
      "Epoch 131 | Train Loss: 0.6194 Acc: 0.7045 | Val Loss: 0.5020 Acc: 0.7167                                               \n",
      "Epoch 132 | Train Loss: 0.6018 Acc: 0.7082 | Val Loss: 0.5154 Acc: 0.7400                                               \n",
      "Epoch 133 | Train Loss: 0.6243 Acc: 0.7043 | Val Loss: 0.5014 Acc: 0.7663                                               \n",
      "Epoch 134 | Train Loss: 0.6070 Acc: 0.7076 | Val Loss: 0.5142 Acc: 0.7445                                               \n",
      "Epoch 135 | Train Loss: 0.6127 Acc: 0.7040 | Val Loss: 0.5157 Acc: 0.7069                                               \n",
      "Epoch 136 | Train Loss: 0.6066 Acc: 0.7071 | Val Loss: 0.4976 Acc: 0.7507                                               \n",
      "Epoch 137 | Train Loss: 0.6134 Acc: 0.7074 | Val Loss: 0.4941 Acc: 0.7761                                               \n",
      "Epoch 138 | Train Loss: 0.5999 Acc: 0.7123 | Val Loss: 0.4979 Acc: 0.7555                                               \n",
      "Epoch 139 | Train Loss: 0.6007 Acc: 0.7060 | Val Loss: 0.4881 Acc: 0.7516                                               \n",
      "Epoch 140 | Train Loss: 0.6038 Acc: 0.7115 | Val Loss: 0.4820 Acc: 0.7696                                               \n",
      "Epoch 141 | Train Loss: 0.5919 Acc: 0.7169 | Val Loss: 0.4817 Acc: 0.7961                                               \n",
      "Epoch 142 | Train Loss: 0.5948 Acc: 0.7112 | Val Loss: 0.4803 Acc: 0.7725                                               \n",
      "Epoch 143 | Train Loss: 0.6014 Acc: 0.7138 | Val Loss: 0.4905 Acc: 0.7609                                               \n",
      "Epoch 144 | Train Loss: 0.5884 Acc: 0.7208 | Val Loss: 0.4781 Acc: 0.7806                                               \n",
      "Epoch 145 | Train Loss: 0.5889 Acc: 0.7180 | Val Loss: 0.4814 Acc: 0.7600                                               \n",
      "Epoch 146 | Train Loss: 0.6006 Acc: 0.7101 | Val Loss: 0.4870 Acc: 0.7663                                               \n",
      "Epoch 147 | Train Loss: 0.5963 Acc: 0.7097 | Val Loss: 0.5020 Acc: 0.7251                                               \n",
      "Epoch 148 | Train Loss: 0.5979 Acc: 0.7168 | Val Loss: 0.5135 Acc: 0.7901                                               \n",
      "Epoch 149 | Train Loss: 0.6021 Acc: 0.7157 | Val Loss: 0.4650 Acc: 0.7519                                               \n",
      "Epoch 150 | Train Loss: 0.5869 Acc: 0.7175 | Val Loss: 0.4844 Acc: 0.7364                                               \n",
      "100%|████████████████████████████████████████████████| 50/50 [56:52<00:00, 68.24s/trial, best loss: 0.03706981659818218]\n",
      "Best hyperparameters: {'batch_size': np.int64(1), 'cnn_dense': np.int64(3), 'cnn_dropout': np.float64(0.3438735331880601), 'cnn_kernel_size_1': np.int64(1), 'cnn_kernel_size_2': np.int64(1), 'cnn_kernels_1': np.int64(3), 'cnn_kernels_2': np.int64(1), 'learning_rate': np.float64(0.00014974892071901113), 'lstm_dense': np.int64(1), 'lstm_hidden_size': np.int64(1), 'lstm_layers': np.int64(0), 'optimizer': np.int64(0)}\n",
      "TPE search finished in 3412.03 seconds\n",
      "Best (raw indices): {'batch_size': np.int64(1), 'cnn_dense': np.int64(3), 'cnn_dropout': np.float64(0.3438735331880601), 'cnn_kernel_size_1': np.int64(1), 'cnn_kernel_size_2': np.int64(1), 'cnn_kernels_1': np.int64(3), 'cnn_kernels_2': np.int64(1), 'learning_rate': np.float64(0.00014974892071901113), 'lstm_dense': np.int64(1), 'lstm_hidden_size': np.int64(1), 'lstm_layers': np.int64(0), 'optimizer': np.int64(0)}\n",
      "Best (interpreted): {'batch_size': 36, 'cnn_dense': 256, 'cnn_dropout': np.float64(0.3438735331880601), 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 32, 'learning_rate': np.float64(0.00014974892071901113), 'lstm_dense': 64, 'lstm_hidden_size': 64, 'lstm_layers': 1, 'optimizer': 'adam'}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m trials, params = hyperparameter_search()\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mbest_parameters.json\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mw+\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[43mjson\u001b[49m.dump(params, f, indent=\u001b[32m4\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "trials, params = hyperparameter_search()\n",
    "with open(\"best_parameters.json\", \"w+\") as f:\n",
    "    import json\n",
    "    json.dump(params, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b0526fe-c2b6-44b6-801c-b85489073f88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting TPE search...\n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 64, 'cnn_dropout': 0.1793512885568253, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 32, 'learning_rate': 2.849957138409511e-05, 'lstm_dense': 128, 'lstm_hidden_size': 96, 'lstm_layers': 2, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 102.2467 Acc: 0.3460 | Val Loss: 33.2389 Acc: 0.4469                                            \n",
      "Epoch 002 | Train Loss: 45.8676 Acc: 0.3631 | Val Loss: 12.7050 Acc: 0.4155                                             \n",
      "Epoch 003 | Train Loss: 30.8543 Acc: 0.3900 | Val Loss: 9.5081 Acc: 0.3501                                              \n",
      "Epoch 004 | Train Loss: 21.3121 Acc: 0.4073 | Val Loss: 7.0657 Acc: 0.3755                                              \n",
      "Epoch 005 | Train Loss: 16.4363 Acc: 0.4153 | Val Loss: 4.9482 Acc: 0.3860                                              \n",
      "Epoch 006 | Train Loss: 14.0938 Acc: 0.4153 | Val Loss: 3.6100 Acc: 0.4364                                              \n",
      "Epoch 007 | Train Loss: 11.9552 Acc: 0.4267 | Val Loss: 2.9635 Acc: 0.4203                                              \n",
      "Epoch 008 | Train Loss: 10.3400 Acc: 0.4259 | Val Loss: 2.8208 Acc: 0.4654                                              \n",
      "Epoch 009 | Train Loss: 9.1210 Acc: 0.4297 | Val Loss: 2.9832 Acc: 0.4484                                               \n",
      "Epoch 010 | Train Loss: 8.2164 Acc: 0.4367 | Val Loss: 2.4798 Acc: 0.4606                                               \n",
      "Epoch 011 | Train Loss: 7.4617 Acc: 0.4421 | Val Loss: 2.4552 Acc: 0.5224                                               \n",
      "Epoch 012 | Train Loss: 6.7623 Acc: 0.4447 | Val Loss: 2.5327 Acc: 0.5507                                               \n",
      "Epoch 013 | Train Loss: 6.1077 Acc: 0.4520 | Val Loss: 2.4528 Acc: 0.5585                                               \n",
      "Epoch 014 | Train Loss: 5.5244 Acc: 0.4745 | Val Loss: 1.7524 Acc: 0.5940                                               \n",
      "Epoch 015 | Train Loss: 5.0212 Acc: 0.4734 | Val Loss: 1.9016 Acc: 0.6122                                               \n",
      "Epoch 016 | Train Loss: 4.6494 Acc: 0.4823 | Val Loss: 1.6305 Acc: 0.6239                                               \n",
      "Epoch 017 | Train Loss: 4.3849 Acc: 0.4882 | Val Loss: 1.6274 Acc: 0.5531                                               \n",
      "Epoch 018 | Train Loss: 3.8774 Acc: 0.4965 | Val Loss: 1.4722 Acc: 0.6239                                               \n",
      "Epoch 019 | Train Loss: 3.6800 Acc: 0.5039 | Val Loss: 1.4410 Acc: 0.6633                                               \n",
      "Epoch 020 | Train Loss: 3.4915 Acc: 0.5124 | Val Loss: 1.5731 Acc: 0.6212                                               \n",
      "Epoch 021 | Train Loss: 3.1831 Acc: 0.5169 | Val Loss: 1.3838 Acc: 0.6501                                               \n",
      "Epoch 022 | Train Loss: 3.0343 Acc: 0.5221 | Val Loss: 1.3399 Acc: 0.6564                                               \n",
      "Epoch 023 | Train Loss: 2.7749 Acc: 0.5368 | Val Loss: 1.2142 Acc: 0.6579                                               \n",
      "Epoch 024 | Train Loss: 2.5913 Acc: 0.5378 | Val Loss: 1.4364 Acc: 0.6564                                               \n",
      "Epoch 025 | Train Loss: 2.5243 Acc: 0.5364 | Val Loss: 1.3391 Acc: 0.6657                                               \n",
      "Epoch 026 | Train Loss: 2.3576 Acc: 0.5471 | Val Loss: 1.3081 Acc: 0.6755                                               \n",
      "Epoch 027 | Train Loss: 2.2610 Acc: 0.5515 | Val Loss: 1.3161 Acc: 0.6567                                               \n",
      "Epoch 028 | Train Loss: 2.1449 Acc: 0.5642 | Val Loss: 1.1095 Acc: 0.6952                                               \n",
      "Epoch 029 | Train Loss: 2.0117 Acc: 0.5627 | Val Loss: 1.0979 Acc: 0.6922                                               \n",
      "Epoch 030 | Train Loss: 1.8655 Acc: 0.5763 | Val Loss: 0.9691 Acc: 0.6842                                               \n",
      "Epoch 031 | Train Loss: 1.7812 Acc: 0.5822 | Val Loss: 1.0967 Acc: 0.6943                                               \n",
      "Epoch 032 | Train Loss: 1.7546 Acc: 0.5851 | Val Loss: 1.0736 Acc: 0.6973                                               \n",
      "Epoch 033 | Train Loss: 1.6674 Acc: 0.5877 | Val Loss: 1.0736 Acc: 0.7104                                               \n",
      "Epoch 034 | Train Loss: 1.5968 Acc: 0.5977 | Val Loss: 1.0647 Acc: 0.6991                                               \n",
      "Epoch 035 | Train Loss: 1.4889 Acc: 0.6081 | Val Loss: 0.9852 Acc: 0.6976                                               \n",
      "Epoch 036 | Train Loss: 1.4755 Acc: 0.6100 | Val Loss: 0.8559 Acc: 0.7245                                               \n",
      "Epoch 037 | Train Loss: 1.4125 Acc: 0.6079 | Val Loss: 0.8176 Acc: 0.7364                                               \n",
      "Epoch 038 | Train Loss: 1.3184 Acc: 0.6280 | Val Loss: 0.9066 Acc: 0.7278                                               \n",
      "Epoch 039 | Train Loss: 1.2967 Acc: 0.6321 | Val Loss: 1.0068 Acc: 0.6830                                               \n",
      "Epoch 040 | Train Loss: 1.2136 Acc: 0.6478 | Val Loss: 0.8009 Acc: 0.7209                                               \n",
      "Epoch 041 | Train Loss: 1.1728 Acc: 0.6437 | Val Loss: 0.8473 Acc: 0.7382                                               \n",
      "Epoch 042 | Train Loss: 1.1074 Acc: 0.6612 | Val Loss: 0.7527 Acc: 0.7525                                               \n",
      "Epoch 043 | Train Loss: 1.0699 Acc: 0.6642 | Val Loss: 0.7831 Acc: 0.7478                                               \n",
      "Epoch 044 | Train Loss: 1.0567 Acc: 0.6776 | Val Loss: 0.7085 Acc: 0.7573                                               \n",
      "Epoch 045 | Train Loss: 1.0066 Acc: 0.6777 | Val Loss: 0.7027 Acc: 0.7725                                               \n",
      "Epoch 046 | Train Loss: 0.9617 Acc: 0.6913 | Val Loss: 0.7301 Acc: 0.7579                                               \n",
      "Epoch 047 | Train Loss: 0.9393 Acc: 0.6949 | Val Loss: 0.7922 Acc: 0.7591                                               \n",
      "Epoch 048 | Train Loss: 0.9309 Acc: 0.7003 | Val Loss: 0.6781 Acc: 0.7716                                               \n",
      "Epoch 049 | Train Loss: 0.8924 Acc: 0.7157 | Val Loss: 0.7759 Acc: 0.7561                                               \n",
      "Epoch 050 | Train Loss: 0.8454 Acc: 0.7186 | Val Loss: 0.7649 Acc: 0.7612                                               \n",
      "Epoch 051 | Train Loss: 0.7990 Acc: 0.7342 | Val Loss: 0.5996 Acc: 0.7943                                               \n",
      "Epoch 052 | Train Loss: 0.7954 Acc: 0.7362 | Val Loss: 0.6349 Acc: 0.8066                                               \n",
      "Epoch 053 | Train Loss: 0.7907 Acc: 0.7377 | Val Loss: 0.7296 Acc: 0.7636                                               \n",
      "Epoch 054 | Train Loss: 0.7387 Acc: 0.7480 | Val Loss: 0.5879 Acc: 0.8137                                               \n",
      "Epoch 055 | Train Loss: 0.7276 Acc: 0.7531 | Val Loss: 0.5877 Acc: 0.7854                                               \n",
      "Epoch 056 | Train Loss: 0.6914 Acc: 0.7628 | Val Loss: 0.5871 Acc: 0.8033                                               \n",
      "Epoch 057 | Train Loss: 0.6712 Acc: 0.7699 | Val Loss: 0.6505 Acc: 0.7824                                               \n",
      "Epoch 058 | Train Loss: 0.6653 Acc: 0.7728 | Val Loss: 0.5343 Acc: 0.8167                                               \n",
      "Epoch 059 | Train Loss: 0.6388 Acc: 0.7804 | Val Loss: 0.5565 Acc: 0.8170                                               \n",
      "Epoch 060 | Train Loss: 0.6199 Acc: 0.7895 | Val Loss: 0.5250 Acc: 0.8224                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 32, 'cnn_dropout': 0.6972369650619279, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 16, 'learning_rate': 0.00017103116844246206, 'lstm_dense': 32, 'lstm_hidden_size': 128, 'lstm_layers': 2, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 30.6697 Acc: 0.3634 | Val Loss: 5.1578 Acc: 0.4269                                              \n",
      "Epoch 002 | Train Loss: 8.5971 Acc: 0.4264 | Val Loss: 2.2145 Acc: 0.5743                                               \n",
      "Epoch 003 | Train Loss: 5.2104 Acc: 0.4668 | Val Loss: 2.4579 Acc: 0.6018                                               \n",
      "Epoch 004 | Train Loss: 3.6903 Acc: 0.5047 | Val Loss: 1.9499 Acc: 0.6131                                               \n",
      "Epoch 005 | Train Loss: 2.7805 Acc: 0.5447 | Val Loss: 1.5782 Acc: 0.5916                                               \n",
      "Epoch 006 | Train Loss: 2.3347 Acc: 0.5712 | Val Loss: 1.7977 Acc: 0.6230                                               \n",
      "Epoch 007 | Train Loss: 2.0241 Acc: 0.5865 | Val Loss: 0.9362 Acc: 0.6624                                               \n",
      "Epoch 008 | Train Loss: 1.6858 Acc: 0.6103 | Val Loss: 1.1646 Acc: 0.6078                                               \n",
      "Epoch 009 | Train Loss: 1.5337 Acc: 0.6286 | Val Loss: 0.7825 Acc: 0.7346                                               \n",
      "Epoch 010 | Train Loss: 1.3975 Acc: 0.6401 | Val Loss: 1.0792 Acc: 0.6325                                               \n",
      "Epoch 011 | Train Loss: 1.1814 Acc: 0.6610 | Val Loss: 0.6987 Acc: 0.7319                                               \n",
      "Epoch 012 | Train Loss: 1.0894 Acc: 0.6765 | Val Loss: 0.5860 Acc: 0.7785                                               \n",
      "Epoch 013 | Train Loss: 0.9686 Acc: 0.7001 | Val Loss: 0.7232 Acc: 0.7534                                               \n",
      "Epoch 014 | Train Loss: 0.8913 Acc: 0.7082 | Val Loss: 0.5906 Acc: 0.7675                                               \n",
      "Epoch 015 | Train Loss: 0.8416 Acc: 0.7209 | Val Loss: 0.9878 Acc: 0.6528                                               \n",
      "Epoch 016 | Train Loss: 0.7896 Acc: 0.7343 | Val Loss: 0.5066 Acc: 0.8015                                               \n",
      "Epoch 017 | Train Loss: 0.7113 Acc: 0.7540 | Val Loss: 0.5557 Acc: 0.8012                                               \n",
      "Epoch 018 | Train Loss: 0.6797 Acc: 0.7633 | Val Loss: 0.4204 Acc: 0.8293                                               \n",
      "Epoch 019 | Train Loss: 0.6203 Acc: 0.7759 | Val Loss: 0.4130 Acc: 0.8457                                               \n",
      "Epoch 020 | Train Loss: 0.5722 Acc: 0.7966 | Val Loss: 0.5249 Acc: 0.8128                                               \n",
      "Epoch 021 | Train Loss: 0.5468 Acc: 0.8026 | Val Loss: 0.3410 Acc: 0.8719                                               \n",
      "Epoch 022 | Train Loss: 0.4972 Acc: 0.8197 | Val Loss: 0.3475 Acc: 0.8776                                               \n",
      "Epoch 023 | Train Loss: 0.4715 Acc: 0.8357 | Val Loss: 0.3708 Acc: 0.8645                                               \n",
      "Epoch 024 | Train Loss: 0.4764 Acc: 0.8295 | Val Loss: 0.3237 Acc: 0.8863                                               \n",
      "Epoch 025 | Train Loss: 0.4258 Acc: 0.8444 | Val Loss: 0.3549 Acc: 0.8731                                               \n",
      "Epoch 026 | Train Loss: 0.3808 Acc: 0.8592 | Val Loss: 0.3545 Acc: 0.8797                                               \n",
      "Epoch 027 | Train Loss: 0.3803 Acc: 0.8630 | Val Loss: 0.2789 Acc: 0.8991                                               \n",
      "Epoch 028 | Train Loss: 0.3494 Acc: 0.8752 | Val Loss: 0.2810 Acc: 0.9042                                               \n",
      "Epoch 029 | Train Loss: 0.3157 Acc: 0.8861 | Val Loss: 0.2762 Acc: 0.9045                                               \n",
      "Epoch 030 | Train Loss: 0.3095 Acc: 0.8869 | Val Loss: 0.2197 Acc: 0.9185                                               \n",
      "Epoch 031 | Train Loss: 0.3029 Acc: 0.8889 | Val Loss: 0.2839 Acc: 0.8982                                               \n",
      "Epoch 032 | Train Loss: 0.2931 Acc: 0.8959 | Val Loss: 0.2277 Acc: 0.9161                                               \n",
      "Epoch 033 | Train Loss: 0.2734 Acc: 0.9034 | Val Loss: 0.2991 Acc: 0.9000                                               \n",
      "Epoch 034 | Train Loss: 0.2721 Acc: 0.9025 | Val Loss: 0.2394 Acc: 0.9113                                               \n",
      "Epoch 035 | Train Loss: 0.2441 Acc: 0.9130 | Val Loss: 0.2650 Acc: 0.9093                                               \n",
      "Epoch 036 | Train Loss: 0.2361 Acc: 0.9169 | Val Loss: 0.2148 Acc: 0.9233                                               \n",
      "Epoch 037 | Train Loss: 0.2241 Acc: 0.9194 | Val Loss: 0.3062 Acc: 0.8946                                               \n",
      "Epoch 038 | Train Loss: 0.2137 Acc: 0.9257 | Val Loss: 0.1929 Acc: 0.9290                                               \n",
      "Epoch 039 | Train Loss: 0.1986 Acc: 0.9295 | Val Loss: 0.1800 Acc: 0.9331                                               \n",
      "Epoch 040 | Train Loss: 0.1925 Acc: 0.9329 | Val Loss: 0.1485 Acc: 0.9490                                               \n",
      "Epoch 041 | Train Loss: 0.2142 Acc: 0.9237 | Val Loss: 0.1431 Acc: 0.9543                                               \n",
      "Epoch 042 | Train Loss: 0.1759 Acc: 0.9375 | Val Loss: 0.1989 Acc: 0.9287                                               \n",
      "Epoch 043 | Train Loss: 0.1691 Acc: 0.9413 | Val Loss: 0.1615 Acc: 0.9475                                               \n",
      "Epoch 044 | Train Loss: 0.1710 Acc: 0.9398 | Val Loss: 0.1696 Acc: 0.9328                                               \n",
      "Epoch 045 | Train Loss: 0.1641 Acc: 0.9437 | Val Loss: 0.1240 Acc: 0.9591                                               \n",
      "Epoch 046 | Train Loss: 0.1815 Acc: 0.9394 | Val Loss: 0.1160 Acc: 0.9609                                               \n",
      "Epoch 047 | Train Loss: 0.1439 Acc: 0.9501 | Val Loss: 0.1402 Acc: 0.9472                                               \n",
      "Epoch 048 | Train Loss: 0.1420 Acc: 0.9514 | Val Loss: 0.1182 Acc: 0.9570                                               \n",
      "Epoch 049 | Train Loss: 0.1370 Acc: 0.9533 | Val Loss: 0.1259 Acc: 0.9573                                               \n",
      "Epoch 050 | Train Loss: 0.1328 Acc: 0.9540 | Val Loss: 0.1297 Acc: 0.9528                                               \n",
      "Epoch 051 | Train Loss: 0.1338 Acc: 0.9541 | Val Loss: 0.1218 Acc: 0.9552                                               \n",
      "Epoch 052 | Train Loss: 0.1216 Acc: 0.9577 | Val Loss: 0.1043 Acc: 0.9675                                               \n",
      "Epoch 053 | Train Loss: 0.1212 Acc: 0.9580 | Val Loss: 0.1129 Acc: 0.9615                                               \n",
      "Epoch 054 | Train Loss: 0.1214 Acc: 0.9583 | Val Loss: 0.1115 Acc: 0.9630                                               \n",
      "Epoch 055 | Train Loss: 0.1085 Acc: 0.9634 | Val Loss: 0.1014 Acc: 0.9699                                               \n",
      "Epoch 056 | Train Loss: 0.1182 Acc: 0.9604 | Val Loss: 0.0963 Acc: 0.9675                                               \n",
      "Epoch 057 | Train Loss: 0.0999 Acc: 0.9663 | Val Loss: 0.0815 Acc: 0.9761                                               \n",
      "Epoch 058 | Train Loss: 0.0995 Acc: 0.9646 | Val Loss: 0.0850 Acc: 0.9719                                               \n",
      "Epoch 059 | Train Loss: 0.1018 Acc: 0.9643 | Val Loss: 0.0842 Acc: 0.9731                                               \n",
      "Epoch 060 | Train Loss: 0.0978 Acc: 0.9674 | Val Loss: 0.1201 Acc: 0.9600                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 32, 'cnn_dropout': 0.5838934775471082, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 32, 'learning_rate': 0.007493931852027543, 'lstm_dense': 64, 'lstm_hidden_size': 64, 'lstm_layers': 4, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 15.5957 Acc: 0.4582 | Val Loss: 1.2492 Acc: 0.5137                                              \n",
      "Epoch 002 | Train Loss: 1.0138 Acc: 0.5364 | Val Loss: 1.2948 Acc: 0.4961                                               \n",
      "Epoch 003 | Train Loss: 1.1409 Acc: 0.5129 | Val Loss: 0.9581 Acc: 0.5657                                               \n",
      "Epoch 004 | Train Loss: 0.9943 Acc: 0.5088 | Val Loss: 0.8538 Acc: 0.5684                                               \n",
      "Epoch 005 | Train Loss: 1.0087 Acc: 0.4763 | Val Loss: 0.9278 Acc: 0.4901                                               \n",
      "Epoch 006 | Train Loss: 1.0782 Acc: 0.4619 | Val Loss: 0.9295 Acc: 0.4845                                               \n",
      "Epoch 007 | Train Loss: 1.0707 Acc: 0.4582 | Val Loss: 1.2047 Acc: 0.4475                                               \n",
      "Epoch 008 | Train Loss: 1.0649 Acc: 0.4707 | Val Loss: 1.0001 Acc: 0.4803                                               \n",
      "Epoch 009 | Train Loss: 1.0733 Acc: 0.4675 | Val Loss: 1.1378 Acc: 0.4543                                               \n",
      "Epoch 010 | Train Loss: 1.3062 Acc: 0.4406 | Val Loss: 1.2454 Acc: 0.4421                                               \n",
      "Epoch 011 | Train Loss: 1.2468 Acc: 0.4389 | Val Loss: 1.2511 Acc: 0.4373                                               \n",
      "Epoch 012 | Train Loss: 1.2514 Acc: 0.4387 | Val Loss: 1.2388 Acc: 0.4421                                               \n",
      "Epoch 013 | Train Loss: 1.2407 Acc: 0.4421 | Val Loss: 1.2394 Acc: 0.4421                                               \n",
      "Epoch 014 | Train Loss: 1.2667 Acc: 0.4420 | Val Loss: 1.2396 Acc: 0.4421                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 256, 'cnn_dropout': 0.10645755494528952, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 64, 'learning_rate': 0.0002269292511272618, 'lstm_dense': 128, 'lstm_hidden_size': 96, 'lstm_layers': 1, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 14.4122 Acc: 0.3945 | Val Loss: 2.5209 Acc: 0.5063                                              \n",
      "Epoch 002 | Train Loss: 5.0774 Acc: 0.4500 | Val Loss: 2.4336 Acc: 0.5075                                               \n",
      "Epoch 003 | Train Loss: 3.5253 Acc: 0.4617 | Val Loss: 3.2351 Acc: 0.4916                                               \n",
      "Epoch 004 | Train Loss: 2.5975 Acc: 0.4794 | Val Loss: 2.1502 Acc: 0.5430                                               \n",
      "Epoch 005 | Train Loss: 1.9382 Acc: 0.5133 | Val Loss: 1.0392 Acc: 0.5657                                               \n",
      "Epoch 006 | Train Loss: 1.5432 Acc: 0.5330 | Val Loss: 0.8895 Acc: 0.5239                                               \n",
      "Epoch 007 | Train Loss: 1.3082 Acc: 0.5473 | Val Loss: 1.0954 Acc: 0.5206                                               \n",
      "Epoch 008 | Train Loss: 1.1069 Acc: 0.5687 | Val Loss: 0.9384 Acc: 0.5937                                               \n",
      "Epoch 009 | Train Loss: 0.9651 Acc: 0.6086 | Val Loss: 0.7509 Acc: 0.6475                                               \n",
      "Epoch 010 | Train Loss: 0.8950 Acc: 0.6165 | Val Loss: 0.6708 Acc: 0.6991                                               \n",
      "Epoch 011 | Train Loss: 0.8052 Acc: 0.6452 | Val Loss: 0.6932 Acc: 0.6457                                               \n",
      "Epoch 012 | Train Loss: 0.7534 Acc: 0.6709 | Val Loss: 0.5443 Acc: 0.7812                                               \n",
      "Epoch 013 | Train Loss: 0.7201 Acc: 0.6821 | Val Loss: 0.6420 Acc: 0.6824                                               \n",
      "Epoch 014 | Train Loss: 0.6647 Acc: 0.7233 | Val Loss: 0.6020 Acc: 0.7057                                               \n",
      "Epoch 015 | Train Loss: 0.6183 Acc: 0.7440 | Val Loss: 0.5353 Acc: 0.7579                                               \n",
      "Epoch 016 | Train Loss: 0.5873 Acc: 0.7626 | Val Loss: 0.4197 Acc: 0.8418                                               \n",
      "Epoch 017 | Train Loss: 0.5261 Acc: 0.7880 | Val Loss: 0.3570 Acc: 0.8651                                               \n",
      "Epoch 018 | Train Loss: 0.4725 Acc: 0.8166 | Val Loss: 0.3217 Acc: 0.8884                                               \n",
      "Epoch 019 | Train Loss: 0.4119 Acc: 0.8408 | Val Loss: 0.2571 Acc: 0.9188                                               \n",
      "Epoch 020 | Train Loss: 0.3545 Acc: 0.8687 | Val Loss: 0.2319 Acc: 0.9224                                               \n",
      "Epoch 021 | Train Loss: 0.3017 Acc: 0.8916 | Val Loss: 0.1831 Acc: 0.9406                                               \n",
      "Epoch 022 | Train Loss: 0.2783 Acc: 0.8992 | Val Loss: 0.3476 Acc: 0.8510                                               \n",
      "Epoch 023 | Train Loss: 0.2563 Acc: 0.9054 | Val Loss: 0.1682 Acc: 0.9430                                               \n",
      "Epoch 024 | Train Loss: 0.2014 Acc: 0.9281 | Val Loss: 0.2989 Acc: 0.8800                                               \n",
      "Epoch 025 | Train Loss: 0.1849 Acc: 0.9342 | Val Loss: 0.1357 Acc: 0.9472                                               \n",
      "Epoch 026 | Train Loss: 0.1588 Acc: 0.9452 | Val Loss: 0.1584 Acc: 0.9391                                               \n",
      "Epoch 027 | Train Loss: 0.1508 Acc: 0.9489 | Val Loss: 0.1190 Acc: 0.9561                                               \n",
      "Epoch 028 | Train Loss: 0.1329 Acc: 0.9606 | Val Loss: 0.1141 Acc: 0.9576                                               \n",
      "Epoch 029 | Train Loss: 0.1189 Acc: 0.9607 | Val Loss: 0.0885 Acc: 0.9660                                               \n",
      "Epoch 030 | Train Loss: 0.1071 Acc: 0.9636 | Val Loss: 0.1084 Acc: 0.9606                                               \n",
      "Epoch 031 | Train Loss: 0.1005 Acc: 0.9665 | Val Loss: 0.0617 Acc: 0.9785                                               \n",
      "Epoch 032 | Train Loss: 0.0810 Acc: 0.9734 | Val Loss: 0.1127 Acc: 0.9579                                               \n",
      "Epoch 033 | Train Loss: 0.0786 Acc: 0.9751 | Val Loss: 0.0883 Acc: 0.9681                                               \n",
      "Epoch 034 | Train Loss: 0.0731 Acc: 0.9765 | Val Loss: 0.0547 Acc: 0.9806                                               \n",
      "Epoch 035 | Train Loss: 0.0703 Acc: 0.9754 | Val Loss: 0.0679 Acc: 0.9767                                               \n",
      "Epoch 036 | Train Loss: 0.0810 Acc: 0.9781 | Val Loss: 0.0653 Acc: 0.9791                                               \n",
      "Epoch 037 | Train Loss: 0.0524 Acc: 0.9831 | Val Loss: 0.0632 Acc: 0.9773                                               \n",
      "Epoch 038 | Train Loss: 0.0593 Acc: 0.9813 | Val Loss: 0.1023 Acc: 0.9612                                               \n",
      "Epoch 039 | Train Loss: 0.0863 Acc: 0.9773 | Val Loss: 0.0945 Acc: 0.9669                                               \n",
      "Epoch 040 | Train Loss: 0.0326 Acc: 0.9898 | Val Loss: 0.0558 Acc: 0.9806                                               \n",
      "Epoch 041 | Train Loss: 0.0511 Acc: 0.9843 | Val Loss: 0.0692 Acc: 0.9758                                               \n",
      "Epoch 042 | Train Loss: 0.0350 Acc: 0.9883 | Val Loss: 0.0970 Acc: 0.9684                                               \n",
      "Epoch 043 | Train Loss: 0.0459 Acc: 0.9863 | Val Loss: 0.0506 Acc: 0.9827                                               \n",
      "Epoch 044 | Train Loss: 0.0416 Acc: 0.9873 | Val Loss: 0.1060 Acc: 0.9675                                               \n",
      "Epoch 045 | Train Loss: 0.0234 Acc: 0.9919 | Val Loss: 0.0530 Acc: 0.9833                                               \n",
      "Epoch 046 | Train Loss: 0.0360 Acc: 0.9896 | Val Loss: 0.0596 Acc: 0.9806                                               \n",
      "Epoch 047 | Train Loss: 0.0314 Acc: 0.9904 | Val Loss: 0.0669 Acc: 0.9755                                               \n",
      "Epoch 048 | Train Loss: 0.0235 Acc: 0.9927 | Val Loss: 0.0921 Acc: 0.9690                                               \n",
      "Epoch 049 | Train Loss: 0.0532 Acc: 0.9857 | Val Loss: 0.0572 Acc: 0.9797                                               \n",
      "Epoch 050 | Train Loss: 0.0193 Acc: 0.9943 | Val Loss: 0.0790 Acc: 0.9752                                               \n",
      "Epoch 051 | Train Loss: 0.0251 Acc: 0.9926 | Val Loss: 0.0407 Acc: 0.9875                                               \n",
      "Epoch 052 | Train Loss: 0.0238 Acc: 0.9927 | Val Loss: 0.1296 Acc: 0.9606                                               \n",
      "Epoch 053 | Train Loss: 0.0305 Acc: 0.9911 | Val Loss: 0.0586 Acc: 0.9785                                               \n",
      "Epoch 054 | Train Loss: 0.0215 Acc: 0.9933 | Val Loss: 0.1197 Acc: 0.9672                                               \n",
      "Epoch 055 | Train Loss: 0.0222 Acc: 0.9931 | Val Loss: 0.1292 Acc: 0.9582                                               \n",
      "Epoch 056 | Train Loss: 0.0248 Acc: 0.9934 | Val Loss: 0.0488 Acc: 0.9851                                               \n",
      "Epoch 057 | Train Loss: 0.0178 Acc: 0.9943 | Val Loss: 0.0530 Acc: 0.9830                                               \n",
      "Epoch 058 | Train Loss: 0.0139 Acc: 0.9952 | Val Loss: 0.0452 Acc: 0.9863                                               \n",
      "Epoch 059 | Train Loss: 0.0198 Acc: 0.9926 | Val Loss: 0.0633 Acc: 0.9833                                               \n",
      "Epoch 060 | Train Loss: 0.0268 Acc: 0.9925 | Val Loss: 0.1689 Acc: 0.9525                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 32, 'cnn_dropout': 0.0266675568445911, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 16, 'cnn_kernels_2': 16, 'learning_rate': 0.0003792984749733401, 'lstm_dense': 256, 'lstm_hidden_size': 32, 'lstm_layers': 5, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 24.5613 Acc: 0.3822 | Val Loss: 2.8883 Acc: 0.4973                                              \n",
      "Epoch 002 | Train Loss: 6.1172 Acc: 0.4418 | Val Loss: 1.8873 Acc: 0.5230                                               \n",
      "Epoch 003 | Train Loss: 3.6805 Acc: 0.4600 | Val Loss: 1.5041 Acc: 0.5304                                               \n",
      "Epoch 004 | Train Loss: 2.5127 Acc: 0.4987 | Val Loss: 1.0323 Acc: 0.6325                                               \n",
      "Epoch 005 | Train Loss: 1.9060 Acc: 0.5309 | Val Loss: 1.0186 Acc: 0.5976                                               \n",
      "Epoch 006 | Train Loss: 1.6451 Acc: 0.5653 | Val Loss: 0.7112 Acc: 0.7319                                               \n",
      "Epoch 007 | Train Loss: 1.2999 Acc: 0.6070 | Val Loss: 0.8638 Acc: 0.7012                                               \n",
      "Epoch 008 | Train Loss: 1.1228 Acc: 0.6418 | Val Loss: 0.8558 Acc: 0.6800                                               \n",
      "Epoch 009 | Train Loss: 1.0021 Acc: 0.6718 | Val Loss: 0.8564 Acc: 0.7063                                               \n",
      "Epoch 010 | Train Loss: 0.8525 Acc: 0.7248 | Val Loss: 0.6625 Acc: 0.7263                                               \n",
      "Epoch 011 | Train Loss: 0.7595 Acc: 0.7504 | Val Loss: 0.5025 Acc: 0.8203                                               \n",
      "Epoch 012 | Train Loss: 0.6669 Acc: 0.7717 | Val Loss: 0.6476 Acc: 0.7860                                               \n",
      "Epoch 013 | Train Loss: 0.6023 Acc: 0.7967 | Val Loss: 0.4854 Acc: 0.8245                                               \n",
      "Epoch 014 | Train Loss: 0.5584 Acc: 0.8116 | Val Loss: 0.5684 Acc: 0.7946                                               \n",
      "Epoch 015 | Train Loss: 0.4913 Acc: 0.8314 | Val Loss: 0.4907 Acc: 0.8331                                               \n",
      "Epoch 016 | Train Loss: 0.4566 Acc: 0.8421 | Val Loss: 0.3739 Acc: 0.8687                                               \n",
      "Epoch 017 | Train Loss: 0.4040 Acc: 0.8610 | Val Loss: 0.3922 Acc: 0.8693                                               \n",
      "Epoch 018 | Train Loss: 0.3601 Acc: 0.8754 | Val Loss: 0.4929 Acc: 0.8430                                               \n",
      "Epoch 019 | Train Loss: 0.3577 Acc: 0.8766 | Val Loss: 0.2763 Acc: 0.8896                                               \n",
      "Epoch 020 | Train Loss: 0.3133 Acc: 0.8918 | Val Loss: 0.3143 Acc: 0.8818                                               \n",
      "Epoch 021 | Train Loss: 0.2694 Acc: 0.9093 | Val Loss: 0.4747 Acc: 0.8519                                               \n",
      "Epoch 022 | Train Loss: 0.2789 Acc: 0.9060 | Val Loss: 0.2482 Acc: 0.8985                                               \n",
      "Epoch 023 | Train Loss: 0.2527 Acc: 0.9163 | Val Loss: 0.2244 Acc: 0.9164                                               \n",
      "Epoch 024 | Train Loss: 0.2353 Acc: 0.9229 | Val Loss: 0.2466 Acc: 0.9206                                               \n",
      "Epoch 025 | Train Loss: 0.2337 Acc: 0.9266 | Val Loss: 0.2721 Acc: 0.9054                                               \n",
      "Epoch 026 | Train Loss: 0.2004 Acc: 0.9377 | Val Loss: 0.3190 Acc: 0.9015                                               \n",
      "Epoch 027 | Train Loss: 0.1960 Acc: 0.9398 | Val Loss: 0.2216 Acc: 0.9128                                               \n",
      "Epoch 028 | Train Loss: 0.1967 Acc: 0.9383 | Val Loss: 0.2062 Acc: 0.9290                                               \n",
      "Epoch 029 | Train Loss: 0.1694 Acc: 0.9463 | Val Loss: 0.1903 Acc: 0.9284                                               \n",
      "Epoch 030 | Train Loss: 0.1785 Acc: 0.9428 | Val Loss: 0.2218 Acc: 0.9242                                               \n",
      "Epoch 031 | Train Loss: 0.1474 Acc: 0.9548 | Val Loss: 0.2470 Acc: 0.9188                                               \n",
      "Epoch 032 | Train Loss: 0.1462 Acc: 0.9556 | Val Loss: 0.2191 Acc: 0.9203                                               \n",
      "Epoch 033 | Train Loss: 0.1394 Acc: 0.9576 | Val Loss: 0.2278 Acc: 0.9287                                               \n",
      "Epoch 034 | Train Loss: 0.1358 Acc: 0.9594 | Val Loss: 0.1789 Acc: 0.9391                                               \n",
      "Epoch 035 | Train Loss: 0.1302 Acc: 0.9584 | Val Loss: 0.2358 Acc: 0.9269                                               \n",
      "Epoch 036 | Train Loss: 0.1152 Acc: 0.9650 | Val Loss: 0.2980 Acc: 0.9197                                               \n",
      "Epoch 037 | Train Loss: 0.1001 Acc: 0.9686 | Val Loss: 0.2590 Acc: 0.9161                                               \n",
      "Epoch 038 | Train Loss: 0.1073 Acc: 0.9691 | Val Loss: 0.1794 Acc: 0.9451                                               \n",
      "Epoch 039 | Train Loss: 0.0934 Acc: 0.9710 | Val Loss: 0.1545 Acc: 0.9478                                               \n",
      "Epoch 040 | Train Loss: 0.1154 Acc: 0.9651 | Val Loss: 0.2376 Acc: 0.9325                                               \n",
      "Epoch 041 | Train Loss: 0.0899 Acc: 0.9725 | Val Loss: 0.1830 Acc: 0.9391                                               \n",
      "Epoch 042 | Train Loss: 0.0827 Acc: 0.9755 | Val Loss: 0.2942 Acc: 0.9119                                               \n",
      "Epoch 043 | Train Loss: 0.0785 Acc: 0.9766 | Val Loss: 0.1379 Acc: 0.9633                                               \n",
      "Epoch 044 | Train Loss: 0.0757 Acc: 0.9769 | Val Loss: 0.1687 Acc: 0.9475                                               \n",
      "Epoch 045 | Train Loss: 0.0764 Acc: 0.9775 | Val Loss: 0.2022 Acc: 0.9337                                               \n",
      "Epoch 046 | Train Loss: 0.0755 Acc: 0.9769 | Val Loss: 0.1294 Acc: 0.9624                                               \n",
      "Epoch 047 | Train Loss: 0.0699 Acc: 0.9771 | Val Loss: 0.1336 Acc: 0.9678                                               \n",
      "Epoch 048 | Train Loss: 0.0672 Acc: 0.9783 | Val Loss: 0.1362 Acc: 0.9660                                               \n",
      "Epoch 049 | Train Loss: 0.0634 Acc: 0.9798 | Val Loss: 0.1061 Acc: 0.9725                                               \n",
      "Epoch 050 | Train Loss: 0.0664 Acc: 0.9799 | Val Loss: 0.1164 Acc: 0.9701                                               \n",
      "Epoch 051 | Train Loss: 0.0792 Acc: 0.9775 | Val Loss: 0.2160 Acc: 0.9412                                               \n",
      "Epoch 052 | Train Loss: 0.0855 Acc: 0.9737 | Val Loss: 0.1622 Acc: 0.9487                                               \n",
      "Epoch 053 | Train Loss: 0.0583 Acc: 0.9813 | Val Loss: 0.1454 Acc: 0.9639                                               \n",
      "Epoch 054 | Train Loss: 0.0637 Acc: 0.9810 | Val Loss: 0.2500 Acc: 0.9346                                               \n",
      "Epoch 055 | Train Loss: 0.0694 Acc: 0.9789 | Val Loss: 0.1398 Acc: 0.9543                                               \n",
      "Epoch 056 | Train Loss: 0.0553 Acc: 0.9843 | Val Loss: 0.1342 Acc: 0.9687                                               \n",
      "Epoch 057 | Train Loss: 0.0540 Acc: 0.9841 | Val Loss: 0.1720 Acc: 0.9582                                               \n",
      "Epoch 058 | Train Loss: 0.0552 Acc: 0.9828 | Val Loss: 0.1380 Acc: 0.9663                                               \n",
      "Epoch 059 | Train Loss: 0.0475 Acc: 0.9850 | Val Loss: 0.1724 Acc: 0.9606                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 64, 'cnn_dropout': 0.11720302936056153, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 64, 'learning_rate': 8.336156074143885e-05, 'lstm_dense': 32, 'lstm_hidden_size': 64, 'lstm_layers': 1, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 19.3177 Acc: 0.3678 | Val Loss: 4.1808 Acc: 0.4582                                              \n",
      "Epoch 002 | Train Loss: 6.6071 Acc: 0.4317 | Val Loss: 2.1381 Acc: 0.5090                                               \n",
      "Epoch 003 | Train Loss: 3.9708 Acc: 0.4697 | Val Loss: 2.1620 Acc: 0.4609                                               \n",
      "Epoch 004 | Train Loss: 2.7404 Acc: 0.5000 | Val Loss: 1.3148 Acc: 0.5600                                               \n",
      "Epoch 005 | Train Loss: 2.0955 Acc: 0.5065 | Val Loss: 1.1528 Acc: 0.5746                                               \n",
      "Epoch 006 | Train Loss: 1.6828 Acc: 0.5345 | Val Loss: 0.8918 Acc: 0.6084                                               \n",
      "Epoch 007 | Train Loss: 1.4130 Acc: 0.5591 | Val Loss: 0.7355 Acc: 0.6830                                               \n",
      "Epoch 008 | Train Loss: 1.2639 Acc: 0.5680 | Val Loss: 0.7191 Acc: 0.6522                                               \n",
      "Epoch 009 | Train Loss: 1.1602 Acc: 0.5820 | Val Loss: 0.7875 Acc: 0.6797                                               \n",
      "Epoch 010 | Train Loss: 1.0642 Acc: 0.5950 | Val Loss: 0.7076 Acc: 0.6227                                               \n",
      "Epoch 011 | Train Loss: 0.9948 Acc: 0.6104 | Val Loss: 0.6537 Acc: 0.7087                                               \n",
      "Epoch 012 | Train Loss: 0.9371 Acc: 0.6187 | Val Loss: 0.6775 Acc: 0.6770                                               \n",
      "Epoch 013 | Train Loss: 0.9071 Acc: 0.6349 | Val Loss: 0.7122 Acc: 0.6113                                               \n",
      "Epoch 014 | Train Loss: 0.8674 Acc: 0.6417 | Val Loss: 0.7387 Acc: 0.6606                                               \n",
      "Epoch 015 | Train Loss: 0.8572 Acc: 0.6446 | Val Loss: 0.6879 Acc: 0.6561                                               \n",
      "Epoch 016 | Train Loss: 0.8017 Acc: 0.6589 | Val Loss: 0.7130 Acc: 0.6782                                               \n",
      "Epoch 017 | Train Loss: 0.8102 Acc: 0.6573 | Val Loss: 0.6483 Acc: 0.7158                                               \n",
      "Epoch 018 | Train Loss: 0.7928 Acc: 0.6577 | Val Loss: 0.6276 Acc: 0.7206                                               \n",
      "Epoch 019 | Train Loss: 0.7706 Acc: 0.6657 | Val Loss: 0.5326 Acc: 0.7728                                               \n",
      "Epoch 020 | Train Loss: 0.7754 Acc: 0.6677 | Val Loss: 0.6628 Acc: 0.6334                                               \n",
      "Epoch 021 | Train Loss: 0.7541 Acc: 0.6738 | Val Loss: 0.7948 Acc: 0.5863                                               \n",
      "Epoch 022 | Train Loss: 0.7425 Acc: 0.6777 | Val Loss: 0.5621 Acc: 0.7597                                               \n",
      "Epoch 023 | Train Loss: 0.7499 Acc: 0.6754 | Val Loss: 0.5573 Acc: 0.7645                                               \n",
      "Epoch 024 | Train Loss: 0.7463 Acc: 0.6781 | Val Loss: 0.5501 Acc: 0.7713                                               \n",
      "Epoch 025 | Train Loss: 0.7239 Acc: 0.6804 | Val Loss: 0.5691 Acc: 0.7806                                               \n",
      "Epoch 026 | Train Loss: 0.7272 Acc: 0.6863 | Val Loss: 0.5682 Acc: 0.7275                                               \n",
      "Epoch 027 | Train Loss: 0.7048 Acc: 0.6962 | Val Loss: 0.5362 Acc: 0.7931                                               \n",
      "Epoch 028 | Train Loss: 0.6957 Acc: 0.7031 | Val Loss: 0.5678 Acc: 0.7457                                               \n",
      "Epoch 029 | Train Loss: 0.6802 Acc: 0.7112 | Val Loss: 0.4965 Acc: 0.7848                                               \n",
      "Epoch 030 | Train Loss: 0.6621 Acc: 0.7221 | Val Loss: 0.4949 Acc: 0.7875                                               \n",
      "Epoch 031 | Train Loss: 0.6271 Acc: 0.7419 | Val Loss: 0.6100 Acc: 0.7096                                               \n",
      "Epoch 032 | Train Loss: 0.6255 Acc: 0.7557 | Val Loss: 0.5399 Acc: 0.7746                                               \n",
      "Epoch 033 | Train Loss: 0.5913 Acc: 0.7640 | Val Loss: 0.6050 Acc: 0.6743                                               \n",
      "Epoch 034 | Train Loss: 0.5785 Acc: 0.7671 | Val Loss: 0.5217 Acc: 0.7290                                               \n",
      "Epoch 035 | Train Loss: 0.5549 Acc: 0.7827 | Val Loss: 0.4496 Acc: 0.8251                                               \n",
      "Epoch 036 | Train Loss: 0.5199 Acc: 0.7971 | Val Loss: 0.4269 Acc: 0.8301                                               \n",
      "Epoch 037 | Train Loss: 0.4928 Acc: 0.8116 | Val Loss: 0.4029 Acc: 0.8358                                               \n",
      "Epoch 038 | Train Loss: 0.4594 Acc: 0.8295 | Val Loss: 0.4299 Acc: 0.8221                                               \n",
      "Epoch 039 | Train Loss: 0.4278 Acc: 0.8392 | Val Loss: 0.3699 Acc: 0.8454                                               \n",
      "Epoch 040 | Train Loss: 0.3932 Acc: 0.8548 | Val Loss: 0.3133 Acc: 0.8767                                               \n",
      "Epoch 041 | Train Loss: 0.3781 Acc: 0.8639 | Val Loss: 0.3038 Acc: 0.8803                                               \n",
      "Epoch 042 | Train Loss: 0.3510 Acc: 0.8736 | Val Loss: 0.3200 Acc: 0.8752                                               \n",
      "Epoch 043 | Train Loss: 0.3225 Acc: 0.8890 | Val Loss: 0.2861 Acc: 0.8881                                               \n",
      "Epoch 044 | Train Loss: 0.3041 Acc: 0.8913 | Val Loss: 0.2019 Acc: 0.9275                                               \n",
      "Epoch 045 | Train Loss: 0.2808 Acc: 0.8995 | Val Loss: 0.1882 Acc: 0.9400                                               \n",
      "Epoch 046 | Train Loss: 0.2713 Acc: 0.9073 | Val Loss: 0.2087 Acc: 0.9301                                               \n",
      "Epoch 047 | Train Loss: 0.2321 Acc: 0.9195 | Val Loss: 0.2618 Acc: 0.9018                                               \n",
      "Epoch 048 | Train Loss: 0.2351 Acc: 0.9210 | Val Loss: 0.1988 Acc: 0.9269                                               \n",
      "Epoch 049 | Train Loss: 0.2197 Acc: 0.9242 | Val Loss: 0.1551 Acc: 0.9478                                               \n",
      "Epoch 050 | Train Loss: 0.2005 Acc: 0.9324 | Val Loss: 0.1963 Acc: 0.9260                                               \n",
      "Epoch 051 | Train Loss: 0.1951 Acc: 0.9328 | Val Loss: 0.2320 Acc: 0.9125                                               \n",
      "Epoch 052 | Train Loss: 0.1860 Acc: 0.9380 | Val Loss: 0.1984 Acc: 0.9290                                               \n",
      "Epoch 053 | Train Loss: 0.1733 Acc: 0.9425 | Val Loss: 0.1927 Acc: 0.9310                                               \n",
      "Epoch 054 | Train Loss: 0.1648 Acc: 0.9445 | Val Loss: 0.1384 Acc: 0.9537                                               \n",
      "Epoch 055 | Train Loss: 0.1495 Acc: 0.9494 | Val Loss: 0.1428 Acc: 0.9510                                               \n",
      "Epoch 056 | Train Loss: 0.1497 Acc: 0.9528 | Val Loss: 0.3064 Acc: 0.8976                                               \n",
      "Epoch 057 | Train Loss: 0.1439 Acc: 0.9540 | Val Loss: 0.1031 Acc: 0.9645                                               \n",
      "Epoch 058 | Train Loss: 0.1353 Acc: 0.9571 | Val Loss: 0.2599 Acc: 0.9078                                               \n",
      "Epoch 059 | Train Loss: 0.1364 Acc: 0.9561 | Val Loss: 0.1229 Acc: 0.9591                                               \n",
      "Epoch 060 | Train Loss: 0.1165 Acc: 0.9615 | Val Loss: 0.1263 Acc: 0.9579                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 128, 'cnn_dropout': 0.5525267757803333, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 32, 'learning_rate': 0.003390709282341803, 'lstm_dense': 256, 'lstm_hidden_size': 32, 'lstm_layers': 1, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 5.0043 Acc: 0.4426 | Val Loss: 0.9432 Acc: 0.5761                                               \n",
      "Epoch 002 | Train Loss: 0.9635 Acc: 0.5336 | Val Loss: 0.8117 Acc: 0.6131                                               \n",
      "Epoch 003 | Train Loss: 0.9337 Acc: 0.5497 | Val Loss: 0.8357 Acc: 0.5218                                               \n",
      "Epoch 004 | Train Loss: 0.9101 Acc: 0.5542 | Val Loss: 0.7975 Acc: 0.6316                                               \n",
      "Epoch 005 | Train Loss: 0.9088 Acc: 0.5631 | Val Loss: 0.8558 Acc: 0.5555                                               \n",
      "Epoch 006 | Train Loss: 0.8900 Acc: 0.5645 | Val Loss: 0.8580 Acc: 0.5612                                               \n",
      "Epoch 007 | Train Loss: 0.8658 Acc: 0.5786 | Val Loss: 0.9365 Acc: 0.4940                                               \n",
      "Epoch 008 | Train Loss: 0.8562 Acc: 0.5795 | Val Loss: 0.8016 Acc: 0.6272                                               \n",
      "Epoch 009 | Train Loss: 0.8570 Acc: 0.5701 | Val Loss: 0.7226 Acc: 0.6352                                               \n",
      "Epoch 010 | Train Loss: 0.8592 Acc: 0.5710 | Val Loss: 0.7939 Acc: 0.6531                                               \n",
      "Epoch 011 | Train Loss: 0.8552 Acc: 0.5619 | Val Loss: 0.8112 Acc: 0.6349                                               \n",
      "Epoch 012 | Train Loss: 0.8919 Acc: 0.5421 | Val Loss: 0.8128 Acc: 0.5985                                               \n",
      "Epoch 013 | Train Loss: 0.9078 Acc: 0.5541 | Val Loss: 0.7995 Acc: 0.5890                                               \n",
      "Epoch 014 | Train Loss: 0.8770 Acc: 0.5695 | Val Loss: 0.8202 Acc: 0.5278                                               \n",
      "Epoch 015 | Train Loss: 0.8703 Acc: 0.5895 | Val Loss: 0.7979 Acc: 0.6200                                               \n",
      "Epoch 016 | Train Loss: 0.8395 Acc: 0.6037 | Val Loss: 0.7403 Acc: 0.6570                                               \n",
      "Epoch 017 | Train Loss: 0.8439 Acc: 0.6062 | Val Loss: 0.7602 Acc: 0.6860                                               \n",
      "Epoch 018 | Train Loss: 0.8011 Acc: 0.6324 | Val Loss: 0.7700 Acc: 0.6218                                               \n",
      "Epoch 019 | Train Loss: 0.7906 Acc: 0.6412 | Val Loss: 0.7638 Acc: 0.6322                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 64, 'cnn_dropout': 0.22166236524573577, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 32, 'learning_rate': 3.630841095683583e-05, 'lstm_dense': 128, 'lstm_hidden_size': 128, 'lstm_layers': 5, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 3.8430 Acc: 0.4140 | Val Loss: 0.9855 Acc: 0.4734                                               \n",
      "Epoch 002 | Train Loss: 0.9935 Acc: 0.4728 | Val Loss: 0.9090 Acc: 0.5230                                               \n",
      "Epoch 003 | Train Loss: 0.9527 Acc: 0.4809 | Val Loss: 0.8875 Acc: 0.5140                                               \n",
      "Epoch 004 | Train Loss: 0.9275 Acc: 0.4955 | Val Loss: 0.8570 Acc: 0.5436                                               \n",
      "Epoch 005 | Train Loss: 0.9149 Acc: 0.5088 | Val Loss: 0.8428 Acc: 0.5657                                               \n",
      "Epoch 006 | Train Loss: 0.9100 Acc: 0.5086 | Val Loss: 0.8331 Acc: 0.5651                                               \n",
      "Epoch 007 | Train Loss: 0.9026 Acc: 0.5081 | Val Loss: 0.8348 Acc: 0.5173                                               \n",
      "Epoch 008 | Train Loss: 0.8956 Acc: 0.5150 | Val Loss: 0.8174 Acc: 0.5212                                               \n",
      "Epoch 009 | Train Loss: 0.8843 Acc: 0.5288 | Val Loss: 0.8132 Acc: 0.5236                                               \n",
      "Epoch 010 | Train Loss: 0.8746 Acc: 0.5363 | Val Loss: 0.8180 Acc: 0.5248                                               \n",
      "Epoch 011 | Train Loss: 0.8723 Acc: 0.5347 | Val Loss: 0.8064 Acc: 0.5955                                               \n",
      "Epoch 012 | Train Loss: 0.8658 Acc: 0.5371 | Val Loss: 0.7853 Acc: 0.6152                                               \n",
      "Epoch 013 | Train Loss: 0.8471 Acc: 0.5477 | Val Loss: 0.7818 Acc: 0.6072                                               \n",
      "Epoch 014 | Train Loss: 0.8464 Acc: 0.5505 | Val Loss: 0.7983 Acc: 0.6137                                               \n",
      "Epoch 015 | Train Loss: 0.8388 Acc: 0.5593 | Val Loss: 0.7664 Acc: 0.6069                                               \n",
      "Epoch 016 | Train Loss: 0.8316 Acc: 0.5622 | Val Loss: 0.7617 Acc: 0.6048                                               \n",
      "Epoch 017 | Train Loss: 0.8260 Acc: 0.5712 | Val Loss: 0.7710 Acc: 0.5967                                               \n",
      "Epoch 018 | Train Loss: 0.8194 Acc: 0.5723 | Val Loss: 0.7579 Acc: 0.6170                                               \n",
      "Epoch 019 | Train Loss: 0.8178 Acc: 0.5774 | Val Loss: 0.7366 Acc: 0.6358                                               \n",
      "Epoch 020 | Train Loss: 0.8095 Acc: 0.5753 | Val Loss: 0.7475 Acc: 0.6197                                               \n",
      "Epoch 021 | Train Loss: 0.8193 Acc: 0.5762 | Val Loss: 0.7439 Acc: 0.6024                                               \n",
      "Epoch 022 | Train Loss: 0.8087 Acc: 0.5792 | Val Loss: 0.7436 Acc: 0.6200                                               \n",
      "Epoch 023 | Train Loss: 0.7908 Acc: 0.5939 | Val Loss: 0.7243 Acc: 0.6343                                               \n",
      "Epoch 024 | Train Loss: 0.7889 Acc: 0.6018 | Val Loss: 0.7344 Acc: 0.6406                                               \n",
      "Epoch 025 | Train Loss: 0.7838 Acc: 0.6040 | Val Loss: 0.7099 Acc: 0.6033                                               \n",
      "Epoch 026 | Train Loss: 0.7803 Acc: 0.6028 | Val Loss: 0.7235 Acc: 0.5800                                               \n",
      "Epoch 027 | Train Loss: 0.7765 Acc: 0.6102 | Val Loss: 0.6935 Acc: 0.6481                                               \n",
      "Epoch 028 | Train Loss: 0.7687 Acc: 0.6085 | Val Loss: 0.6846 Acc: 0.6257                                               \n",
      "Epoch 029 | Train Loss: 0.7608 Acc: 0.6213 | Val Loss: 0.6976 Acc: 0.6054                                               \n",
      "Epoch 030 | Train Loss: 0.7538 Acc: 0.6170 | Val Loss: 0.7031 Acc: 0.6000                                               \n",
      "Epoch 031 | Train Loss: 0.7542 Acc: 0.6270 | Val Loss: 0.6906 Acc: 0.6451                                               \n",
      "Epoch 032 | Train Loss: 0.7547 Acc: 0.6233 | Val Loss: 0.6910 Acc: 0.6266                                               \n",
      "Epoch 033 | Train Loss: 0.7487 Acc: 0.6289 | Val Loss: 0.6921 Acc: 0.6182                                               \n",
      "Epoch 034 | Train Loss: 0.7432 Acc: 0.6315 | Val Loss: 0.6851 Acc: 0.6576                                               \n",
      "Epoch 035 | Train Loss: 0.7332 Acc: 0.6359 | Val Loss: 0.6609 Acc: 0.6591                                               \n",
      "Epoch 036 | Train Loss: 0.7432 Acc: 0.6316 | Val Loss: 0.6921 Acc: 0.6212                                               \n",
      "Epoch 037 | Train Loss: 0.7445 Acc: 0.6350 | Val Loss: 0.6636 Acc: 0.6800                                               \n",
      "Epoch 038 | Train Loss: 0.7207 Acc: 0.6474 | Val Loss: 0.6532 Acc: 0.7000                                               \n",
      "Epoch 039 | Train Loss: 0.7258 Acc: 0.6500 | Val Loss: 0.6514 Acc: 0.6931                                               \n",
      "Epoch 040 | Train Loss: 0.7254 Acc: 0.6520 | Val Loss: 0.6450 Acc: 0.7242                                               \n",
      "Epoch 041 | Train Loss: 0.7168 Acc: 0.6600 | Val Loss: 0.6432 Acc: 0.6582                                               \n",
      "Epoch 042 | Train Loss: 0.7235 Acc: 0.6564 | Val Loss: 0.6463 Acc: 0.6472                                               \n",
      "Epoch 043 | Train Loss: 0.7132 Acc: 0.6533 | Val Loss: 0.6449 Acc: 0.6597                                               \n",
      "Epoch 044 | Train Loss: 0.7074 Acc: 0.6609 | Val Loss: 0.6228 Acc: 0.7125                                               \n",
      "Epoch 045 | Train Loss: 0.7037 Acc: 0.6681 | Val Loss: 0.6273 Acc: 0.6881                                               \n",
      "Epoch 046 | Train Loss: 0.6932 Acc: 0.6736 | Val Loss: 0.6271 Acc: 0.7113                                               \n",
      "Epoch 047 | Train Loss: 0.7074 Acc: 0.6659 | Val Loss: 0.6170 Acc: 0.7033                                               \n",
      "Epoch 048 | Train Loss: 0.7008 Acc: 0.6665 | Val Loss: 0.6543 Acc: 0.6719                                               \n",
      "Epoch 049 | Train Loss: 0.6922 Acc: 0.6709 | Val Loss: 0.6006 Acc: 0.7128                                               \n",
      "Epoch 050 | Train Loss: 0.6808 Acc: 0.6818 | Val Loss: 0.5975 Acc: 0.7063                                               \n",
      "Epoch 051 | Train Loss: 0.6770 Acc: 0.6813 | Val Loss: 0.6031 Acc: 0.6913                                               \n",
      "Epoch 052 | Train Loss: 0.6816 Acc: 0.6784 | Val Loss: 0.5907 Acc: 0.7263                                               \n",
      "Epoch 053 | Train Loss: 0.6723 Acc: 0.6859 | Val Loss: 0.5973 Acc: 0.7642                                               \n",
      "Epoch 054 | Train Loss: 0.6714 Acc: 0.6850 | Val Loss: 0.6123 Acc: 0.6893                                               \n",
      "Epoch 055 | Train Loss: 0.6648 Acc: 0.6927 | Val Loss: 0.5619 Acc: 0.7296                                               \n",
      "Epoch 056 | Train Loss: 0.6688 Acc: 0.6930 | Val Loss: 0.5643 Acc: 0.7719                                               \n",
      "Epoch 057 | Train Loss: 0.6612 Acc: 0.6976 | Val Loss: 0.5565 Acc: 0.7519                                               \n",
      "Epoch 058 | Train Loss: 0.6577 Acc: 0.6917 | Val Loss: 0.5583 Acc: 0.7645                                               \n",
      "Epoch 059 | Train Loss: 0.6582 Acc: 0.6985 | Val Loss: 0.5650 Acc: 0.7564                                               \n",
      "Epoch 060 | Train Loss: 0.6378 Acc: 0.7039 | Val Loss: 0.5518 Acc: 0.7582                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 256, 'cnn_dropout': 0.38705418122536667, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 96, 'learning_rate': 0.0009152681722213663, 'lstm_dense': 128, 'lstm_hidden_size': 96, 'lstm_layers': 2, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 12.7169 Acc: 0.4259 | Val Loss: 2.7793 Acc: 0.5081                                              \n",
      "Epoch 002 | Train Loss: 3.2884 Acc: 0.4718 | Val Loss: 2.3547 Acc: 0.4648                                               \n",
      "Epoch 003 | Train Loss: 1.8351 Acc: 0.5410 | Val Loss: 0.9821 Acc: 0.6251                                               \n",
      "Epoch 004 | Train Loss: 1.2187 Acc: 0.5900 | Val Loss: 0.6876 Acc: 0.6854                                               \n",
      "Epoch 029 | Train Loss: 0.1072 Acc: 0.9660 | Val Loss: 0.0919 Acc: 0.9734                                               \n",
      "Epoch 030 | Train Loss: 0.0902 Acc: 0.9706 | Val Loss: 0.1057 Acc: 0.9678                                               \n",
      "Epoch 031 | Train Loss: 0.0793 Acc: 0.9734 | Val Loss: 0.0945 Acc: 0.9719                                               \n",
      "Epoch 032 | Train Loss: 0.0796 Acc: 0.9724 | Val Loss: 0.0946 Acc: 0.9716                                               \n",
      "Epoch 033 | Train Loss: 0.0790 Acc: 0.9721 | Val Loss: 0.1582 Acc: 0.9528                                               \n",
      "Epoch 034 | Train Loss: 0.1087 Acc: 0.9635 | Val Loss: 0.1086 Acc: 0.9654                                               \n",
      "Epoch 035 | Train Loss: 0.0911 Acc: 0.9713 | Val Loss: 0.0920 Acc: 0.9704                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 128, 'cnn_dropout': 0.44219048778816394, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 32, 'cnn_kernels_2': 96, 'learning_rate': 1.1082323815760054e-05, 'lstm_dense': 128, 'lstm_hidden_size': 64, 'lstm_layers': 1, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 13.4204 Acc: 0.3906 | Val Loss: 1.4500 Acc: 0.4734                                              \n",
      "Epoch 002 | Train Loss: 2.3216 Acc: 0.4365 | Val Loss: 0.9321 Acc: 0.5230                                               \n",
      "Epoch 003 | Train Loss: 1.6142 Acc: 0.4601 | Val Loss: 0.9931 Acc: 0.5069                                               \n",
      "Epoch 004 | Train Loss: 1.3608 Acc: 0.4612 | Val Loss: 0.9860 Acc: 0.4696                                               \n",
      "Epoch 005 | Train Loss: 1.2304 Acc: 0.4618 | Val Loss: 0.8451 Acc: 0.5349                                               \n",
      "Epoch 006 | Train Loss: 1.1863 Acc: 0.4695 | Val Loss: 0.9022 Acc: 0.4648                                               \n",
      "Epoch 007 | Train Loss: 1.1043 Acc: 0.4818 | Val Loss: 0.8465 Acc: 0.5922                                               \n",
      "Epoch 008 | Train Loss: 1.0472 Acc: 0.4915 | Val Loss: 0.8378 Acc: 0.5307                                               \n",
      "Epoch 009 | Train Loss: 1.0470 Acc: 0.4921 | Val Loss: 0.8215 Acc: 0.5594                                               \n",
      "Epoch 010 | Train Loss: 1.0041 Acc: 0.4858 | Val Loss: 0.8819 Acc: 0.5352                                               \n",
      "Epoch 011 | Train Loss: 1.0024 Acc: 0.4994 | Val Loss: 0.8551 Acc: 0.5430                                               \n",
      "Epoch 012 | Train Loss: 1.0050 Acc: 0.4982 | Val Loss: 0.8355 Acc: 0.5627                                               \n",
      "Epoch 013 | Train Loss: 0.9669 Acc: 0.4961 | Val Loss: 0.8191 Acc: 0.5833                                               \n",
      "Epoch 014 | Train Loss: 0.9912 Acc: 0.4968 | Val Loss: 0.8316 Acc: 0.5594                                               \n",
      "Epoch 015 | Train Loss: 0.9662 Acc: 0.5059 | Val Loss: 0.8233 Acc: 0.5624                                               \n",
      "Epoch 016 | Train Loss: 0.9520 Acc: 0.5067 | Val Loss: 0.8131 Acc: 0.5540                                               \n",
      "Epoch 017 | Train Loss: 0.9504 Acc: 0.5050 | Val Loss: 0.8039 Acc: 0.5215                                               \n",
      "Epoch 018 | Train Loss: 0.9395 Acc: 0.5107 | Val Loss: 0.8016 Acc: 0.5827                                               \n",
      "Epoch 019 | Train Loss: 0.9315 Acc: 0.5212 | Val Loss: 0.7895 Acc: 0.5594                                               \n",
      "Epoch 020 | Train Loss: 0.9285 Acc: 0.5096 | Val Loss: 0.7987 Acc: 0.6107                                               \n",
      "Epoch 021 | Train Loss: 0.9175 Acc: 0.5225 | Val Loss: 0.8093 Acc: 0.5445                                               \n",
      "Epoch 022 | Train Loss: 0.9245 Acc: 0.5170 | Val Loss: 0.7928 Acc: 0.5890                                               \n",
      "Epoch 023 | Train Loss: 0.9187 Acc: 0.5148 | Val Loss: 0.7918 Acc: 0.5934                                               \n",
      "Epoch 024 | Train Loss: 0.9009 Acc: 0.5264 | Val Loss: 0.8529 Acc: 0.5394                                               \n",
      "Epoch 025 | Train Loss: 0.9099 Acc: 0.5157 | Val Loss: 0.7804 Acc: 0.5639                                               \n",
      "Epoch 026 | Train Loss: 0.9165 Acc: 0.5135 | Val Loss: 0.7975 Acc: 0.5752                                               \n",
      "Epoch 027 | Train Loss: 0.9199 Acc: 0.5233 | Val Loss: 0.7918 Acc: 0.6012                                               \n",
      "Epoch 028 | Train Loss: 0.8943 Acc: 0.5244 | Val Loss: 0.8165 Acc: 0.5690                                               \n",
      "Epoch 029 | Train Loss: 0.8947 Acc: 0.5276 | Val Loss: 0.8006 Acc: 0.5588                                               \n",
      "Epoch 030 | Train Loss: 0.9020 Acc: 0.5219 | Val Loss: 0.7830 Acc: 0.5785                                               \n",
      "Epoch 031 | Train Loss: 0.8894 Acc: 0.5251 | Val Loss: 0.8245 Acc: 0.5696                                               \n",
      "Epoch 032 | Train Loss: 0.8907 Acc: 0.5290 | Val Loss: 0.7854 Acc: 0.5528                                               \n",
      "Epoch 033 | Train Loss: 0.8899 Acc: 0.5231 | Val Loss: 0.7967 Acc: 0.5952                                               \n",
      "Epoch 034 | Train Loss: 0.8773 Acc: 0.5307 | Val Loss: 0.7745 Acc: 0.6006                                               \n",
      "Epoch 035 | Train Loss: 0.8767 Acc: 0.5307 | Val Loss: 0.7867 Acc: 0.5430                                               \n",
      "Epoch 036 | Train Loss: 0.8758 Acc: 0.5344 | Val Loss: 0.7958 Acc: 0.5552                                               \n",
      "Epoch 037 | Train Loss: 0.8735 Acc: 0.5432 | Val Loss: 0.7763 Acc: 0.5755                                               \n",
      "Epoch 038 | Train Loss: 0.8759 Acc: 0.5374 | Val Loss: 0.7901 Acc: 0.5758                                               \n",
      "Epoch 039 | Train Loss: 0.8796 Acc: 0.5327 | Val Loss: 0.7677 Acc: 0.5916                                               \n",
      "Epoch 040 | Train Loss: 0.8810 Acc: 0.5314 | Val Loss: 0.7763 Acc: 0.5907                                               \n",
      "Epoch 041 | Train Loss: 0.8627 Acc: 0.5274 | Val Loss: 0.7683 Acc: 0.6230                                               \n",
      "Epoch 042 | Train Loss: 0.8661 Acc: 0.5315 | Val Loss: 0.7773 Acc: 0.5773                                               \n",
      "Epoch 043 | Train Loss: 0.8663 Acc: 0.5324 | Val Loss: 0.7706 Acc: 0.5436                                               \n",
      "Epoch 044 | Train Loss: 0.8729 Acc: 0.5275 | Val Loss: 0.7749 Acc: 0.5740                                               \n",
      "Epoch 045 | Train Loss: 0.8545 Acc: 0.5380 | Val Loss: 0.7776 Acc: 0.5609                                               \n",
      "Epoch 046 | Train Loss: 0.8573 Acc: 0.5417 | Val Loss: 0.7620 Acc: 0.6316                                               \n",
      "Epoch 047 | Train Loss: 0.8517 Acc: 0.5423 | Val Loss: 0.8046 Acc: 0.6251                                               \n",
      "Epoch 048 | Train Loss: 0.8509 Acc: 0.5498 | Val Loss: 0.7637 Acc: 0.5657                                               \n",
      "Epoch 049 | Train Loss: 0.8473 Acc: 0.5476 | Val Loss: 0.7601 Acc: 0.5967                                               \n",
      "Epoch 050 | Train Loss: 0.8562 Acc: 0.5468 | Val Loss: 0.7704 Acc: 0.5699                                               \n",
      "Epoch 051 | Train Loss: 0.8479 Acc: 0.5478 | Val Loss: 0.7548 Acc: 0.5794                                               \n",
      "Epoch 052 | Train Loss: 0.8477 Acc: 0.5521 | Val Loss: 0.7558 Acc: 0.6278                                               \n",
      "Epoch 053 | Train Loss: 0.8532 Acc: 0.5468 | Val Loss: 0.7616 Acc: 0.5791                                               \n",
      "Epoch 054 | Train Loss: 0.8392 Acc: 0.5534 | Val Loss: 0.7566 Acc: 0.5522                                               \n",
      "Epoch 055 | Train Loss: 0.8442 Acc: 0.5518 | Val Loss: 0.7503 Acc: 0.5549                                               \n",
      "Epoch 056 | Train Loss: 0.8555 Acc: 0.5482 | Val Loss: 0.7421 Acc: 0.6460                                               \n",
      "Epoch 057 | Train Loss: 0.8398 Acc: 0.5559 | Val Loss: 0.7455 Acc: 0.6379                                               \n",
      "Epoch 058 | Train Loss: 0.8395 Acc: 0.5562 | Val Loss: 0.7549 Acc: 0.6394                                               \n",
      "Epoch 059 | Train Loss: 0.8362 Acc: 0.5588 | Val Loss: 0.7499 Acc: 0.6355                                               \n",
      "Epoch 060 | Train Loss: 0.8405 Acc: 0.5585 | Val Loss: 0.7457 Acc: 0.5937                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 64, 'cnn_dropout': 0.6041353347495672, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 32, 'learning_rate': 0.0007264907259052647, 'lstm_dense': 32, 'lstm_hidden_size': 128, 'lstm_layers': 2, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 11.4810 Acc: 0.4356 | Val Loss: 2.5140 Acc: 0.4878                                              \n",
      "Epoch 002 | Train Loss: 2.6675 Acc: 0.5131 | Val Loss: 1.7813 Acc: 0.5143                                               \n",
      "Epoch 003 | Train Loss: 1.6775 Acc: 0.5377 | Val Loss: 1.0189 Acc: 0.6364                                               \n",
      "Epoch 004 | Train Loss: 1.4307 Acc: 0.5471 | Val Loss: 0.8036 Acc: 0.6961                                               \n",
      "Epoch 005 | Train Loss: 1.0874 Acc: 0.5759 | Val Loss: 0.7404 Acc: 0.6660                                               \n",
      "Epoch 006 | Train Loss: 0.9079 Acc: 0.6070 | Val Loss: 0.9151 Acc: 0.5937                                               \n",
      "Epoch 007 | Train Loss: 0.7789 Acc: 0.6759 | Val Loss: 0.6815 Acc: 0.6570                                               \n",
      "Epoch 008 | Train Loss: 0.6599 Acc: 0.7251 | Val Loss: 0.6048 Acc: 0.7496                                               \n",
      "Epoch 009 | Train Loss: 0.5494 Acc: 0.7760 | Val Loss: 0.4989 Acc: 0.8233                                               \n",
      "Epoch 010 | Train Loss: 0.4737 Acc: 0.8060 | Val Loss: 0.3510 Acc: 0.8591                                               \n",
      "Epoch 011 | Train Loss: 0.4046 Acc: 0.8407 | Val Loss: 0.3235 Acc: 0.8648                                               \n",
      "Epoch 012 | Train Loss: 0.3344 Acc: 0.8757 | Val Loss: 0.2955 Acc: 0.8937                                               \n",
      "Epoch 013 | Train Loss: 0.2847 Acc: 0.8946 | Val Loss: 0.2714 Acc: 0.8952                                               \n",
      "Epoch 014 | Train Loss: 0.2585 Acc: 0.9007 | Val Loss: 0.2082 Acc: 0.9296                                               \n",
      "Epoch 015 | Train Loss: 0.2207 Acc: 0.9212 | Val Loss: 0.2220 Acc: 0.9260                                               \n",
      "Epoch 016 | Train Loss: 0.2047 Acc: 0.9260 | Val Loss: 0.1740 Acc: 0.9343                                               \n",
      "Epoch 017 | Train Loss: 0.1885 Acc: 0.9338 | Val Loss: 0.1698 Acc: 0.9403                                               \n",
      "Epoch 018 | Train Loss: 0.1809 Acc: 0.9355 | Val Loss: 0.1811 Acc: 0.9343                                               \n",
      "Epoch 019 | Train Loss: 0.1598 Acc: 0.9440 | Val Loss: 0.1327 Acc: 0.9543                                               \n",
      "Epoch 020 | Train Loss: 0.1693 Acc: 0.9407 | Val Loss: 0.1667 Acc: 0.9454                                               \n",
      "Epoch 021 | Train Loss: 0.1544 Acc: 0.9457 | Val Loss: 0.1867 Acc: 0.9355                                               \n",
      "Epoch 022 | Train Loss: 0.1302 Acc: 0.9543 | Val Loss: 0.1518 Acc: 0.9487                                               \n",
      "Epoch 023 | Train Loss: 0.1316 Acc: 0.9550 | Val Loss: 0.1160 Acc: 0.9615                                               \n",
      "Epoch 024 | Train Loss: 0.1158 Acc: 0.9597 | Val Loss: 0.1742 Acc: 0.9460                                               \n",
      "Epoch 025 | Train Loss: 0.1184 Acc: 0.9586 | Val Loss: 0.1538 Acc: 0.9564                                               \n",
      "Epoch 026 | Train Loss: 0.1349 Acc: 0.9535 | Val Loss: 0.1236 Acc: 0.9609                                               \n",
      "Epoch 027 | Train Loss: 0.1283 Acc: 0.9563 | Val Loss: 0.1403 Acc: 0.9561                                               \n",
      "Epoch 028 | Train Loss: 0.1139 Acc: 0.9617 | Val Loss: 0.1269 Acc: 0.9543                                               \n",
      "Epoch 029 | Train Loss: 0.1049 Acc: 0.9660 | Val Loss: 0.1166 Acc: 0.9579                                               \n",
      "Epoch 030 | Train Loss: 0.1009 Acc: 0.9650 | Val Loss: 0.1160 Acc: 0.9657                                               \n",
      "Epoch 031 | Train Loss: 0.1124 Acc: 0.9625 | Val Loss: 0.1259 Acc: 0.9645                                               \n",
      "Epoch 032 | Train Loss: 0.0939 Acc: 0.9684 | Val Loss: 0.1125 Acc: 0.9684                                               \n",
      "Epoch 033 | Train Loss: 0.1159 Acc: 0.9611 | Val Loss: 0.1436 Acc: 0.9576                                               \n",
      "Epoch 034 | Train Loss: 0.1071 Acc: 0.9645 | Val Loss: 0.1274 Acc: 0.9576                                               \n",
      "Epoch 035 | Train Loss: 0.0984 Acc: 0.9676 | Val Loss: 0.1075 Acc: 0.9693                                               \n",
      "Epoch 036 | Train Loss: 0.0905 Acc: 0.9693 | Val Loss: 0.0930 Acc: 0.9755                                               \n",
      "Epoch 037 | Train Loss: 0.0819 Acc: 0.9716 | Val Loss: 0.0983 Acc: 0.9704                                               \n",
      "Epoch 038 | Train Loss: 0.0963 Acc: 0.9684 | Val Loss: 0.1083 Acc: 0.9669                                               \n",
      "Epoch 039 | Train Loss: 0.0836 Acc: 0.9706 | Val Loss: 0.0909 Acc: 0.9749                                               \n",
      "Epoch 040 | Train Loss: 0.0788 Acc: 0.9748 | Val Loss: 0.0770 Acc: 0.9761                                               \n",
      "Epoch 041 | Train Loss: 0.0863 Acc: 0.9704 | Val Loss: 0.0921 Acc: 0.9699                                               \n",
      "Epoch 042 | Train Loss: 0.0760 Acc: 0.9746 | Val Loss: 0.0749 Acc: 0.9734                                               \n",
      "Epoch 043 | Train Loss: 0.0959 Acc: 0.9687 | Val Loss: 0.1121 Acc: 0.9636                                               \n",
      "Epoch 044 | Train Loss: 0.0974 Acc: 0.9666 | Val Loss: 0.0777 Acc: 0.9770                                               \n",
      "Epoch 045 | Train Loss: 0.0746 Acc: 0.9751 | Val Loss: 0.0821 Acc: 0.9716                                               \n",
      "Epoch 046 | Train Loss: 0.0724 Acc: 0.9773 | Val Loss: 0.1002 Acc: 0.9722                                               \n",
      "Epoch 047 | Train Loss: 0.0778 Acc: 0.9731 | Val Loss: 0.0888 Acc: 0.9743                                               \n",
      "Epoch 048 | Train Loss: 0.0700 Acc: 0.9761 | Val Loss: 0.0775 Acc: 0.9761                                               \n",
      "Epoch 049 | Train Loss: 0.0752 Acc: 0.9749 | Val Loss: 0.1024 Acc: 0.9713                                               \n",
      "Epoch 050 | Train Loss: 0.0684 Acc: 0.9754 | Val Loss: 0.0774 Acc: 0.9737                                               \n",
      "Epoch 051 | Train Loss: 0.0737 Acc: 0.9751 | Val Loss: 0.1114 Acc: 0.9612                                               \n",
      "Epoch 052 | Train Loss: 0.0796 Acc: 0.9739 | Val Loss: 0.0883 Acc: 0.9746                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 36, 'cnn_dense': 64, 'cnn_dropout': 0.0034714878325699636, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 16, 'cnn_kernels_2': 96, 'learning_rate': 1.1629762227934446e-05, 'lstm_dense': 128, 'lstm_hidden_size': 96, 'lstm_layers': 1, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 112.9164 Acc: 0.3334 | Val Loss: 60.8938 Acc: 0.4287                                            \n",
      "Epoch 002 | Train Loss: 82.4701 Acc: 0.3306 | Val Loss: 40.3853 Acc: 0.4421                                             \n",
      "Epoch 003 | Train Loss: 57.0876 Acc: 0.3479 | Val Loss: 21.0769 Acc: 0.4331                                             \n",
      "Epoch 004 | Train Loss: 35.4864 Acc: 0.3654 | Val Loss: 6.7248 Acc: 0.4594                                              \n",
      "Epoch 005 | Train Loss: 24.0068 Acc: 0.3669 | Val Loss: 5.6163 Acc: 0.4504                                              \n",
      "Epoch 006 | Train Loss: 19.6921 Acc: 0.3793 | Val Loss: 4.6123 Acc: 0.4003                                              \n",
      "Epoch 007 | Train Loss: 16.5500 Acc: 0.3818 | Val Loss: 4.9186 Acc: 0.4630                                              \n",
      "Epoch 008 | Train Loss: 14.2594 Acc: 0.4059 | Val Loss: 4.1831 Acc: 0.4137                                              \n",
      "Epoch 009 | Train Loss: 12.9918 Acc: 0.4088 | Val Loss: 3.9172 Acc: 0.4499                                              \n",
      "Epoch 010 | Train Loss: 11.5980 Acc: 0.4187 | Val Loss: 3.7021 Acc: 0.4185                                              \n",
      "Epoch 011 | Train Loss: 11.0277 Acc: 0.4217 | Val Loss: 3.9420 Acc: 0.4603                                              \n",
      "Epoch 012 | Train Loss: 10.7221 Acc: 0.4218 | Val Loss: 3.2789 Acc: 0.4609                                              \n",
      "Epoch 013 | Train Loss: 9.6538 Acc: 0.4212 | Val Loss: 2.8004 Acc: 0.5355                                               \n",
      "Epoch 014 | Train Loss: 9.0403 Acc: 0.4353 | Val Loss: 3.4331 Acc: 0.5116                                               \n",
      "Epoch 015 | Train Loss: 8.3760 Acc: 0.4430 | Val Loss: 2.4606 Acc: 0.5561                                               \n",
      "Epoch 016 | Train Loss: 7.8873 Acc: 0.4482 | Val Loss: 3.0563 Acc: 0.5304                                               \n",
      "Epoch 017 | Train Loss: 7.3885 Acc: 0.4495 | Val Loss: 2.6524 Acc: 0.5701                                               \n",
      "Epoch 018 | Train Loss: 6.9633 Acc: 0.4599 | Val Loss: 2.6722 Acc: 0.5740                                               \n",
      "Epoch 019 | Train Loss: 6.6648 Acc: 0.4653 | Val Loss: 2.3799 Acc: 0.5976                                               \n",
      "Epoch 020 | Train Loss: 6.3033 Acc: 0.4772 | Val Loss: 2.3411 Acc: 0.5818                                               \n",
      "Epoch 021 | Train Loss: 5.7928 Acc: 0.4941 | Val Loss: 2.4658 Acc: 0.5943                                               \n",
      "Epoch 022 | Train Loss: 5.6497 Acc: 0.5001 | Val Loss: 2.1815 Acc: 0.6027                                               \n",
      "Epoch 023 | Train Loss: 5.1391 Acc: 0.5109 | Val Loss: 1.8486 Acc: 0.6334                                               \n",
      "Epoch 024 | Train Loss: 4.9563 Acc: 0.5202 | Val Loss: 1.8866 Acc: 0.6349                                               \n",
      "Epoch 025 | Train Loss: 4.7804 Acc: 0.5321 | Val Loss: 2.0042 Acc: 0.6206                                               \n",
      "Epoch 026 | Train Loss: 4.3631 Acc: 0.5406 | Val Loss: 1.6858 Acc: 0.6728                                               \n",
      "Epoch 027 | Train Loss: 4.3345 Acc: 0.5482 | Val Loss: 1.6033 Acc: 0.6752                                               \n",
      "Epoch 028 | Train Loss: 3.8987 Acc: 0.5683 | Val Loss: 1.8484 Acc: 0.6099                                               \n",
      "Epoch 029 | Train Loss: 3.8985 Acc: 0.5661 | Val Loss: 1.5509 Acc: 0.6663                                               \n",
      "Epoch 030 | Train Loss: 3.5999 Acc: 0.5775 | Val Loss: 1.4124 Acc: 0.6970                                               \n",
      "Epoch 031 | Train Loss: 3.5208 Acc: 0.5922 | Val Loss: 1.2910 Acc: 0.7119                                               \n",
      "Epoch 032 | Train Loss: 3.3518 Acc: 0.5886 | Val Loss: 1.1359 Acc: 0.7334                                               \n",
      "Epoch 033 | Train Loss: 3.0825 Acc: 0.6086 | Val Loss: 1.3120 Acc: 0.7036                                               \n",
      "Epoch 034 | Train Loss: 2.9697 Acc: 0.6235 | Val Loss: 1.0537 Acc: 0.7567                                               \n",
      "Epoch 035 | Train Loss: 2.8416 Acc: 0.6270 | Val Loss: 1.3675 Acc: 0.6982                                               \n",
      "Epoch 036 | Train Loss: 2.6309 Acc: 0.6330 | Val Loss: 0.9488 Acc: 0.7725                                               \n",
      "Epoch 037 | Train Loss: 2.5988 Acc: 0.6428 | Val Loss: 1.1659 Acc: 0.7281                                               \n",
      "Epoch 038 | Train Loss: 2.5127 Acc: 0.6456 | Val Loss: 1.0241 Acc: 0.7454                                               \n",
      "Epoch 039 | Train Loss: 2.4345 Acc: 0.6618 | Val Loss: 1.0841 Acc: 0.7424                                               \n",
      "Epoch 040 | Train Loss: 2.2257 Acc: 0.6635 | Val Loss: 1.0887 Acc: 0.7299                                               \n",
      "Epoch 041 | Train Loss: 2.1993 Acc: 0.6752 | Val Loss: 0.8619 Acc: 0.7940                                               \n",
      "Epoch 042 | Train Loss: 2.1233 Acc: 0.6765 | Val Loss: 0.7943 Acc: 0.7970                                               \n",
      "Epoch 043 | Train Loss: 2.0057 Acc: 0.6826 | Val Loss: 0.9364 Acc: 0.7770                                               \n",
      "Epoch 044 | Train Loss: 1.9285 Acc: 0.6989 | Val Loss: 0.7995 Acc: 0.8006                                               \n",
      "Epoch 045 | Train Loss: 1.8467 Acc: 0.6977 | Val Loss: 0.8724 Acc: 0.7779                                               \n",
      "Epoch 046 | Train Loss: 1.7439 Acc: 0.7061 | Val Loss: 0.6562 Acc: 0.8236                                               \n",
      "Epoch 047 | Train Loss: 1.6489 Acc: 0.7169 | Val Loss: 0.8516 Acc: 0.7887                                               \n",
      "Epoch 048 | Train Loss: 1.6263 Acc: 0.7243 | Val Loss: 0.6932 Acc: 0.8009                                               \n",
      "Epoch 049 | Train Loss: 1.5633 Acc: 0.7253 | Val Loss: 0.6417 Acc: 0.8337                                               \n",
      "Epoch 050 | Train Loss: 1.4714 Acc: 0.7338 | Val Loss: 0.5800 Acc: 0.8361                                               \n",
      "Epoch 051 | Train Loss: 1.3981 Acc: 0.7414 | Val Loss: 0.5441 Acc: 0.8472                                               \n",
      "Epoch 052 | Train Loss: 1.3742 Acc: 0.7416 | Val Loss: 0.5797 Acc: 0.8424                                               \n",
      "Epoch 053 | Train Loss: 1.3543 Acc: 0.7508 | Val Loss: 0.5996 Acc: 0.8319                                               \n",
      "Epoch 054 | Train Loss: 1.2687 Acc: 0.7568 | Val Loss: 0.5148 Acc: 0.8472                                               \n",
      "Epoch 055 | Train Loss: 1.2131 Acc: 0.7696 | Val Loss: 0.5358 Acc: 0.8463                                               \n",
      "Epoch 056 | Train Loss: 1.2802 Acc: 0.7583 | Val Loss: 0.5882 Acc: 0.8272                                               \n",
      "Epoch 057 | Train Loss: 1.1668 Acc: 0.7783 | Val Loss: 0.5461 Acc: 0.8504                                               \n",
      "Epoch 058 | Train Loss: 1.1011 Acc: 0.7798 | Val Loss: 0.5997 Acc: 0.8391                                               \n",
      "Epoch 059 | Train Loss: 1.1445 Acc: 0.7830 | Val Loss: 0.4935 Acc: 0.8567                                               \n",
      "Epoch 060 | Train Loss: 1.0683 Acc: 0.7900 | Val Loss: 0.4907 Acc: 0.8528                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 256, 'cnn_dropout': 0.48209130447799753, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 32, 'cnn_kernels_2': 64, 'learning_rate': 0.00029028330319826116, 'lstm_dense': 32, 'lstm_hidden_size': 128, 'lstm_layers': 1, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 11.2294 Acc: 0.4164 | Val Loss: 1.3244 Acc: 0.4490                                              \n",
      "Epoch 002 | Train Loss: 1.3166 Acc: 0.4463 | Val Loss: 1.2984 Acc: 0.4490                                               \n",
      "Epoch 003 | Train Loss: 1.2949 Acc: 0.4461 | Val Loss: 1.2793 Acc: 0.4490                                               \n",
      "Epoch 004 | Train Loss: 1.2792 Acc: 0.4459 | Val Loss: 1.2654 Acc: 0.4490                                               \n",
      "Epoch 005 | Train Loss: 1.2673 Acc: 0.4459 | Val Loss: 1.2548 Acc: 0.4490                                               \n",
      "Epoch 006 | Train Loss: 1.2581 Acc: 0.4460 | Val Loss: 1.2466 Acc: 0.4490                                               \n",
      "Epoch 007 | Train Loss: 1.2509 Acc: 0.4461 | Val Loss: 1.2403 Acc: 0.4490                                               \n",
      "Epoch 008 | Train Loss: 1.2468 Acc: 0.4455 | Val Loss: 1.2352 Acc: 0.4490                                               \n",
      "Epoch 009 | Train Loss: 1.2429 Acc: 0.4453 | Val Loss: 1.2310 Acc: 0.4490                                               \n",
      "Epoch 010 | Train Loss: 1.2392 Acc: 0.4453 | Val Loss: 1.2275 Acc: 0.4490                                               \n",
      "Epoch 011 | Train Loss: 1.2352 Acc: 0.4457 | Val Loss: 1.2243 Acc: 0.4490                                               \n",
      "Epoch 012 | Train Loss: 1.2320 Acc: 0.4459 | Val Loss: 1.2214 Acc: 0.4490                                               \n",
      "Epoch 013 | Train Loss: 1.2289 Acc: 0.4462 | Val Loss: 1.2190 Acc: 0.4490                                               \n",
      "Epoch 014 | Train Loss: 1.2268 Acc: 0.4461 | Val Loss: 1.2169 Acc: 0.4490                                               \n",
      "Epoch 015 | Train Loss: 1.2249 Acc: 0.4460 | Val Loss: 1.2146 Acc: 0.4490                                               \n",
      "Epoch 016 | Train Loss: 1.2223 Acc: 0.4464 | Val Loss: 1.2125 Acc: 0.4490                                               \n",
      "Epoch 017 | Train Loss: 1.2215 Acc: 0.4459 | Val Loss: 1.2103 Acc: 0.4490                                               \n",
      "Epoch 018 | Train Loss: 1.2184 Acc: 0.4463 | Val Loss: 1.2079 Acc: 0.4490                                               \n",
      "Epoch 019 | Train Loss: 1.2183 Acc: 0.4456 | Val Loss: 1.2059 Acc: 0.4490                                               \n",
      "Epoch 020 | Train Loss: 1.2154 Acc: 0.4459 | Val Loss: 1.2038 Acc: 0.4490                                               \n",
      "Epoch 021 | Train Loss: 1.2133 Acc: 0.4457 | Val Loss: 1.2012 Acc: 0.4490                                               \n",
      "Epoch 022 | Train Loss: 1.2104 Acc: 0.4458 | Val Loss: 1.1979 Acc: 0.4490                                               \n",
      "Epoch 023 | Train Loss: 1.2079 Acc: 0.4453 | Val Loss: 1.1938 Acc: 0.4490                                               \n",
      "Epoch 024 | Train Loss: 1.2012 Acc: 0.4461 | Val Loss: 1.1889 Acc: 0.4490                                               \n",
      "Epoch 025 | Train Loss: 1.1958 Acc: 0.4458 | Val Loss: 1.1794 Acc: 0.4493                                               \n",
      "Epoch 026 | Train Loss: 1.1882 Acc: 0.4465 | Val Loss: 1.1755 Acc: 0.4490                                               \n",
      "Epoch 027 | Train Loss: 1.1820 Acc: 0.4473 | Val Loss: 1.1665 Acc: 0.4600                                               \n",
      "Epoch 028 | Train Loss: 1.1724 Acc: 0.4560 | Val Loss: 1.1571 Acc: 0.4779                                               \n",
      "Epoch 029 | Train Loss: 1.1603 Acc: 0.4671 | Val Loss: 1.1419 Acc: 0.4949                                               \n",
      "Epoch 030 | Train Loss: 1.1510 Acc: 0.4873 | Val Loss: 1.1506 Acc: 0.5048                                               \n",
      "Epoch 031 | Train Loss: 1.1360 Acc: 0.5037 | Val Loss: 1.1351 Acc: 0.5054                                               \n",
      "Epoch 032 | Train Loss: 1.1236 Acc: 0.5126 | Val Loss: 1.1555 Acc: 0.4648                                               \n",
      "Epoch 033 | Train Loss: 1.1103 Acc: 0.5215 | Val Loss: 1.1389 Acc: 0.4713                                               \n",
      "Epoch 034 | Train Loss: 1.0931 Acc: 0.5257 | Val Loss: 1.0873 Acc: 0.5048                                               \n",
      "Epoch 035 | Train Loss: 1.0821 Acc: 0.5343 | Val Loss: 1.0727 Acc: 0.5072                                               \n",
      "Epoch 036 | Train Loss: 1.0690 Acc: 0.5388 | Val Loss: 1.0568 Acc: 0.5227                                               \n",
      "Epoch 037 | Train Loss: 1.0537 Acc: 0.5429 | Val Loss: 1.0341 Acc: 0.5340                                               \n",
      "Epoch 038 | Train Loss: 1.0444 Acc: 0.5418 | Val Loss: 1.0401 Acc: 0.5146                                               \n",
      "Epoch 039 | Train Loss: 1.0390 Acc: 0.5449 | Val Loss: 1.0352 Acc: 0.5221                                               \n",
      "Epoch 040 | Train Loss: 1.0252 Acc: 0.5493 | Val Loss: 0.9956 Acc: 0.5463                                               \n",
      "Epoch 041 | Train Loss: 1.0093 Acc: 0.5595 | Val Loss: 1.0388 Acc: 0.5251                                               \n",
      "Epoch 042 | Train Loss: 0.9964 Acc: 0.5646 | Val Loss: 1.0079 Acc: 0.5313                                               \n",
      "Epoch 043 | Train Loss: 0.9886 Acc: 0.5597 | Val Loss: 0.9965 Acc: 0.5364                                               \n",
      "Epoch 044 | Train Loss: 0.9799 Acc: 0.5663 | Val Loss: 1.0011 Acc: 0.5281                                               \n",
      "Epoch 045 | Train Loss: 0.9694 Acc: 0.5677 | Val Loss: 0.9568 Acc: 0.5513                                               \n",
      "Epoch 046 | Train Loss: 0.9581 Acc: 0.5679 | Val Loss: 0.9644 Acc: 0.5472                                               \n",
      "Epoch 047 | Train Loss: 0.9477 Acc: 0.5750 | Val Loss: 0.9402 Acc: 0.5618                                               \n",
      "Epoch 048 | Train Loss: 0.9447 Acc: 0.5715 | Val Loss: 0.9045 Acc: 0.5872                                               \n",
      "Epoch 049 | Train Loss: 0.9333 Acc: 0.5874 | Val Loss: 0.9021 Acc: 0.6054                                               \n",
      "Epoch 050 | Train Loss: 0.9122 Acc: 0.6053 | Val Loss: 0.9134 Acc: 0.5985                                               \n",
      "Epoch 051 | Train Loss: 0.9021 Acc: 0.6174 | Val Loss: 0.8820 Acc: 0.6254                                               \n",
      "Epoch 052 | Train Loss: 0.8865 Acc: 0.6253 | Val Loss: 0.8890 Acc: 0.6143                                               \n",
      "Epoch 053 | Train Loss: 0.8809 Acc: 0.6291 | Val Loss: 0.8685 Acc: 0.6394                                               \n",
      "Epoch 054 | Train Loss: 0.8742 Acc: 0.6371 | Val Loss: 0.8486 Acc: 0.6373                                               \n",
      "Epoch 055 | Train Loss: 0.8567 Acc: 0.6481 | Val Loss: 0.8840 Acc: 0.6415                                               \n",
      "Epoch 056 | Train Loss: 0.8399 Acc: 0.6562 | Val Loss: 0.8156 Acc: 0.6716                                               \n",
      "Epoch 057 | Train Loss: 0.8262 Acc: 0.6634 | Val Loss: 0.7907 Acc: 0.6863                                               \n",
      "Epoch 058 | Train Loss: 0.8003 Acc: 0.6759 | Val Loss: 0.7781 Acc: 0.6913                                               \n",
      "Epoch 059 | Train Loss: 0.8028 Acc: 0.6743 | Val Loss: 0.7784 Acc: 0.6899                                               \n",
      "Epoch 060 | Train Loss: 0.7729 Acc: 0.6889 | Val Loss: 0.7536 Acc: 0.6991                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 256, 'cnn_dropout': 0.26916461328004604, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 48, 'cnn_kernels_2': 16, 'learning_rate': 0.0028565227323993315, 'lstm_dense': 32, 'lstm_hidden_size': 96, 'lstm_layers': 2, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 7.3576 Acc: 0.4674 | Val Loss: 1.1839 Acc: 0.6107                                               \n",
      "Epoch 002 | Train Loss: 1.2920 Acc: 0.5521 | Val Loss: 0.8011 Acc: 0.6045                                               \n",
      "Epoch 003 | Train Loss: 0.8527 Acc: 0.6100 | Val Loss: 0.7951 Acc: 0.6096                                               \n",
      "Epoch 004 | Train Loss: 0.7783 Acc: 0.6333 | Val Loss: 0.7114 Acc: 0.6567                                               \n",
      "Epoch 005 | Train Loss: 0.7147 Acc: 0.6704 | Val Loss: 0.6928 Acc: 0.6143                                               \n",
      "Epoch 006 | Train Loss: 0.6558 Acc: 0.6980 | Val Loss: 0.6112 Acc: 0.7206                                               \n",
      "Epoch 007 | Train Loss: 0.6430 Acc: 0.7083 | Val Loss: 0.6218 Acc: 0.7155                                               \n",
      "Epoch 008 | Train Loss: 0.6081 Acc: 0.7253 | Val Loss: 0.5298 Acc: 0.7418                                               \n",
      "Epoch 009 | Train Loss: 0.6034 Acc: 0.7239 | Val Loss: 0.4897 Acc: 0.7803                                               \n",
      "Epoch 010 | Train Loss: 0.5820 Acc: 0.7401 | Val Loss: 0.5952 Acc: 0.7704                                               \n",
      "Epoch 011 | Train Loss: 0.5711 Acc: 0.7389 | Val Loss: 0.6928 Acc: 0.7245                                               \n",
      "Epoch 012 | Train Loss: 0.5794 Acc: 0.7407 | Val Loss: 0.6610 Acc: 0.7388                                               \n",
      "Epoch 013 | Train Loss: 0.5419 Acc: 0.7615 | Val Loss: 0.6011 Acc: 0.7484                                               \n",
      "Epoch 014 | Train Loss: 0.5260 Acc: 0.7648 | Val Loss: 0.7340 Acc: 0.7373                                               \n",
      "Epoch 015 | Train Loss: 0.5234 Acc: 0.7686 | Val Loss: 0.6885 Acc: 0.7155                                               \n",
      "Epoch 016 | Train Loss: 0.4972 Acc: 0.7771 | Val Loss: 0.7133 Acc: 0.7582                                               \n",
      "Epoch 017 | Train Loss: 0.5222 Acc: 0.7663 | Val Loss: 0.5124 Acc: 0.7875                                               \n",
      "Epoch 018 | Train Loss: 0.4937 Acc: 0.7812 | Val Loss: 0.7836 Acc: 0.6454                                               \n",
      "Epoch 019 | Train Loss: 0.5211 Acc: 0.7707 | Val Loss: 0.8453 Acc: 0.6919                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 36, 'cnn_dense': 32, 'cnn_dropout': 0.2859415229429808, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 32, 'cnn_kernels_2': 16, 'learning_rate': 0.00021550846023384116, 'lstm_dense': 64, 'lstm_hidden_size': 32, 'lstm_layers': 5, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 5.2179 Acc: 0.4372 | Val Loss: 1.3131 Acc: 0.4421                                               \n",
      "Epoch 002 | Train Loss: 1.2972 Acc: 0.4422 | Val Loss: 1.2846 Acc: 0.4421                                               \n",
      "Epoch 003 | Train Loss: 1.2752 Acc: 0.4422 | Val Loss: 1.2677 Acc: 0.4421                                               \n",
      "Epoch 004 | Train Loss: 1.2619 Acc: 0.4422 | Val Loss: 1.2575 Acc: 0.4421                                               \n",
      "Epoch 005 | Train Loss: 1.2539 Acc: 0.4422 | Val Loss: 1.2512 Acc: 0.4421                                               \n",
      "Epoch 006 | Train Loss: 1.2486 Acc: 0.4422 | Val Loss: 1.2471 Acc: 0.4421                                               \n",
      "Epoch 007 | Train Loss: 1.2456 Acc: 0.4422 | Val Loss: 1.2444 Acc: 0.4421                                               \n",
      "Epoch 008 | Train Loss: 1.2431 Acc: 0.4422 | Val Loss: 1.2424 Acc: 0.4421                                               \n",
      "Epoch 009 | Train Loss: 1.2414 Acc: 0.4422 | Val Loss: 1.2410 Acc: 0.4421                                               \n",
      "Epoch 010 | Train Loss: 1.2402 Acc: 0.4422 | Val Loss: 1.2400 Acc: 0.4421                                               \n",
      "Epoch 011 | Train Loss: 1.2394 Acc: 0.4422 | Val Loss: 1.2393 Acc: 0.4421                                               \n",
      "Epoch 012 | Train Loss: 1.2387 Acc: 0.4422 | Val Loss: 1.2387 Acc: 0.4421                                               \n",
      "Epoch 013 | Train Loss: 1.2384 Acc: 0.4422 | Val Loss: 1.2383 Acc: 0.4421                                               \n",
      "Epoch 014 | Train Loss: 1.2379 Acc: 0.4422 | Val Loss: 1.2381 Acc: 0.4421                                               \n",
      "Epoch 015 | Train Loss: 1.2380 Acc: 0.4422 | Val Loss: 1.2378 Acc: 0.4421                                               \n",
      "Epoch 016 | Train Loss: 1.2375 Acc: 0.4422 | Val Loss: 1.2376 Acc: 0.4421                                               \n",
      "Epoch 017 | Train Loss: 1.2376 Acc: 0.4422 | Val Loss: 1.2375 Acc: 0.4421                                               \n",
      "Epoch 018 | Train Loss: 1.2376 Acc: 0.4422 | Val Loss: 1.2374 Acc: 0.4421                                               \n",
      "Epoch 019 | Train Loss: 1.2376 Acc: 0.4422 | Val Loss: 1.2373 Acc: 0.4421                                               \n",
      "Epoch 020 | Train Loss: 1.2372 Acc: 0.4422 | Val Loss: 1.2373 Acc: 0.4421                                               \n",
      "Epoch 021 | Train Loss: 1.2371 Acc: 0.4422 | Val Loss: 1.2373 Acc: 0.4421                                               \n",
      "Epoch 022 | Train Loss: 1.2370 Acc: 0.4422 | Val Loss: 1.2372 Acc: 0.4421                                               \n",
      "Epoch 023 | Train Loss: 1.2375 Acc: 0.4422 | Val Loss: 1.2372 Acc: 0.4421                                               \n",
      "Epoch 024 | Train Loss: 1.2371 Acc: 0.4422 | Val Loss: 1.2372 Acc: 0.4421                                               \n",
      "Epoch 025 | Train Loss: 1.2370 Acc: 0.4422 | Val Loss: 1.2372 Acc: 0.4421                                               \n",
      "Epoch 026 | Train Loss: 1.2369 Acc: 0.4422 | Val Loss: 1.2372 Acc: 0.4421                                               \n",
      "Epoch 027 | Train Loss: 1.2374 Acc: 0.4422 | Val Loss: 1.2371 Acc: 0.4421                                               \n",
      "Epoch 028 | Train Loss: 1.2372 Acc: 0.4422 | Val Loss: 1.2371 Acc: 0.4421                                               \n",
      "Epoch 029 | Train Loss: 1.2372 Acc: 0.4422 | Val Loss: 1.2371 Acc: 0.4421                                               \n",
      "Epoch 030 | Train Loss: 1.2371 Acc: 0.4422 | Val Loss: 1.2371 Acc: 0.4421                                               \n",
      "Epoch 031 | Train Loss: 1.2373 Acc: 0.4422 | Val Loss: 1.2371 Acc: 0.4421                                               \n",
      "Epoch 032 | Train Loss: 1.2368 Acc: 0.4422 | Val Loss: 1.2371 Acc: 0.4421                                               \n",
      "Epoch 033 | Train Loss: 1.2373 Acc: 0.4422 | Val Loss: 1.2371 Acc: 0.4421                                               \n",
      "Epoch 034 | Train Loss: 1.2369 Acc: 0.4422 | Val Loss: 1.2371 Acc: 0.4421                                               \n",
      "Epoch 035 | Train Loss: 1.2371 Acc: 0.4422 | Val Loss: 1.2371 Acc: 0.4421                                               \n",
      "Epoch 036 | Train Loss: 1.2370 Acc: 0.4422 | Val Loss: 1.2371 Acc: 0.4421                                               \n",
      "Epoch 037 | Train Loss: 1.2370 Acc: 0.4422 | Val Loss: 1.2371 Acc: 0.4421                                               \n",
      "Epoch 038 | Train Loss: 1.2368 Acc: 0.4422 | Val Loss: 1.2371 Acc: 0.4421                                               \n",
      "Epoch 039 | Train Loss: 1.2370 Acc: 0.4422 | Val Loss: 1.2371 Acc: 0.4421                                               \n",
      "Epoch 040 | Train Loss: 1.2370 Acc: 0.4422 | Val Loss: 1.2371 Acc: 0.4421                                               \n",
      "Epoch 041 | Train Loss: 1.2372 Acc: 0.4422 | Val Loss: 1.2371 Acc: 0.4421                                               \n",
      "Epoch 042 | Train Loss: 1.2371 Acc: 0.4422 | Val Loss: 1.2371 Acc: 0.4421                                               \n",
      "Epoch 043 | Train Loss: 1.2368 Acc: 0.4422 | Val Loss: 1.2371 Acc: 0.4421                                               \n",
      "Epoch 044 | Train Loss: 1.2371 Acc: 0.4422 | Val Loss: 1.2371 Acc: 0.4421                                               \n",
      "Epoch 045 | Train Loss: 1.2373 Acc: 0.4422 | Val Loss: 1.2371 Acc: 0.4421                                               \n",
      "Epoch 046 | Train Loss: 1.2369 Acc: 0.4422 | Val Loss: 1.2371 Acc: 0.4421                                               \n",
      "Epoch 047 | Train Loss: 1.2372 Acc: 0.4422 | Val Loss: 1.2371 Acc: 0.4421                                               \n",
      "Epoch 048 | Train Loss: 1.2369 Acc: 0.4422 | Val Loss: 1.2371 Acc: 0.4421                                               \n",
      "Epoch 049 | Train Loss: 1.2374 Acc: 0.4422 | Val Loss: 1.2371 Acc: 0.4421                                               \n",
      "Epoch 050 | Train Loss: 1.2370 Acc: 0.4422 | Val Loss: 1.2371 Acc: 0.4421                                               \n",
      "Epoch 051 | Train Loss: 1.2369 Acc: 0.4422 | Val Loss: 1.2371 Acc: 0.4421                                               \n",
      "Epoch 052 | Train Loss: 1.2368 Acc: 0.4422 | Val Loss: 1.2371 Acc: 0.4421                                               \n",
      "Epoch 053 | Train Loss: 1.2370 Acc: 0.4422 | Val Loss: 1.2371 Acc: 0.4421                                               \n",
      "Epoch 054 | Train Loss: 1.2369 Acc: 0.4422 | Val Loss: 1.2371 Acc: 0.4421                                               \n",
      "Epoch 055 | Train Loss: 1.2371 Acc: 0.4422 | Val Loss: 1.2371 Acc: 0.4421                                               \n",
      "Epoch 056 | Train Loss: 1.2372 Acc: 0.4422 | Val Loss: 1.2371 Acc: 0.4421                                               \n",
      "Epoch 057 | Train Loss: 1.2369 Acc: 0.4422 | Val Loss: 1.2371 Acc: 0.4421                                               \n",
      "Epoch 058 | Train Loss: 1.2371 Acc: 0.4422 | Val Loss: 1.2371 Acc: 0.4421                                               \n",
      "Epoch 059 | Train Loss: 1.2368 Acc: 0.4422 | Val Loss: 1.2371 Acc: 0.4421                                               \n",
      "Epoch 060 | Train Loss: 1.2368 Acc: 0.4422 | Val Loss: 1.2370 Acc: 0.4421                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 256, 'cnn_dropout': 0.27399548252399847, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 96, 'learning_rate': 4.654929743465495e-05, 'lstm_dense': 256, 'lstm_hidden_size': 128, 'lstm_layers': 4, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 5.5079 Acc: 0.3967 | Val Loss: 1.1276 Acc: 0.3848                                               \n",
      "Epoch 002 | Train Loss: 1.1528 Acc: 0.4378 | Val Loss: 1.0352 Acc: 0.4743                                               \n",
      "Epoch 003 | Train Loss: 1.0823 Acc: 0.4425 | Val Loss: 1.0546 Acc: 0.4245                                               \n",
      "Epoch 004 | Train Loss: 1.0337 Acc: 0.4443 | Val Loss: 0.9674 Acc: 0.4421                                               \n",
      "Epoch 005 | Train Loss: 1.0039 Acc: 0.4555 | Val Loss: 0.9425 Acc: 0.4540                                               \n",
      "Epoch 006 | Train Loss: 0.9893 Acc: 0.4529 | Val Loss: 0.9139 Acc: 0.5093                                               \n",
      "Epoch 007 | Train Loss: 0.9629 Acc: 0.4507 | Val Loss: 0.8570 Acc: 0.4955                                               \n",
      "Epoch 008 | Train Loss: 0.9547 Acc: 0.4536 | Val Loss: 0.9059 Acc: 0.4648                                               \n",
      "Epoch 009 | Train Loss: 0.9356 Acc: 0.4685 | Val Loss: 0.8441 Acc: 0.5024                                               \n",
      "Epoch 010 | Train Loss: 0.9223 Acc: 0.4771 | Val Loss: 0.8292 Acc: 0.5188                                               \n",
      "Epoch 011 | Train Loss: 0.9076 Acc: 0.4831 | Val Loss: 0.8201 Acc: 0.4651                                               \n",
      "Epoch 012 | Train Loss: 0.8978 Acc: 0.4988 | Val Loss: 0.8188 Acc: 0.5349                                               \n",
      "Epoch 013 | Train Loss: 0.8913 Acc: 0.4985 | Val Loss: 0.8224 Acc: 0.5681                                               \n",
      "Epoch 014 | Train Loss: 0.8880 Acc: 0.4989 | Val Loss: 0.7975 Acc: 0.5307                                               \n",
      "Epoch 015 | Train Loss: 0.8793 Acc: 0.5090 | Val Loss: 0.8377 Acc: 0.5424                                               \n",
      "Epoch 016 | Train Loss: 0.8780 Acc: 0.5035 | Val Loss: 0.7943 Acc: 0.5313                                               \n",
      "Epoch 017 | Train Loss: 0.8679 Acc: 0.5144 | Val Loss: 0.7921 Acc: 0.5203                                               \n",
      "Epoch 018 | Train Loss: 0.8647 Acc: 0.5173 | Val Loss: 0.7950 Acc: 0.5519                                               \n",
      "Epoch 019 | Train Loss: 0.8589 Acc: 0.5235 | Val Loss: 0.8023 Acc: 0.5328                                               \n",
      "Epoch 020 | Train Loss: 0.8465 Acc: 0.5435 | Val Loss: 0.7769 Acc: 0.5794                                               \n",
      "Epoch 021 | Train Loss: 0.8422 Acc: 0.5571 | Val Loss: 0.7492 Acc: 0.6170                                               \n",
      "Epoch 022 | Train Loss: 0.8446 Acc: 0.5606 | Val Loss: 0.7844 Acc: 0.6116                                               \n",
      "Epoch 023 | Train Loss: 0.8255 Acc: 0.5664 | Val Loss: 0.7547 Acc: 0.6224                                               \n",
      "Epoch 024 | Train Loss: 0.8287 Acc: 0.5689 | Val Loss: 0.7499 Acc: 0.6149                                               \n",
      "Epoch 025 | Train Loss: 0.8212 Acc: 0.5747 | Val Loss: 0.7322 Acc: 0.6155                                               \n",
      "Epoch 026 | Train Loss: 0.8208 Acc: 0.5722 | Val Loss: 0.7246 Acc: 0.6188                                               \n",
      "Epoch 027 | Train Loss: 0.8121 Acc: 0.5806 | Val Loss: 0.7323 Acc: 0.6072                                               \n",
      "Epoch 028 | Train Loss: 0.8081 Acc: 0.5856 | Val Loss: 0.7144 Acc: 0.6146                                               \n",
      "Epoch 029 | Train Loss: 0.8073 Acc: 0.5871 | Val Loss: 0.7268 Acc: 0.6245                                               \n",
      "Epoch 030 | Train Loss: 0.8102 Acc: 0.5849 | Val Loss: 0.7370 Acc: 0.5988                                               \n",
      "Epoch 031 | Train Loss: 0.8017 Acc: 0.5886 | Val Loss: 0.7076 Acc: 0.6301                                               \n",
      "Epoch 032 | Train Loss: 0.7948 Acc: 0.5862 | Val Loss: 0.7092 Acc: 0.6516                                               \n",
      "Epoch 033 | Train Loss: 0.8092 Acc: 0.5880 | Val Loss: 0.7141 Acc: 0.6248                                               \n",
      "Epoch 034 | Train Loss: 0.7897 Acc: 0.5897 | Val Loss: 0.6997 Acc: 0.6257                                               \n",
      "Epoch 035 | Train Loss: 0.7838 Acc: 0.5955 | Val Loss: 0.6906 Acc: 0.6337                                               \n",
      "Epoch 036 | Train Loss: 0.7791 Acc: 0.6010 | Val Loss: 0.7060 Acc: 0.6200                                               \n",
      "Epoch 037 | Train Loss: 0.7714 Acc: 0.6011 | Val Loss: 0.6796 Acc: 0.6340                                               \n",
      "Epoch 038 | Train Loss: 0.7723 Acc: 0.6030 | Val Loss: 0.6860 Acc: 0.6660                                               \n",
      "Epoch 039 | Train Loss: 0.7653 Acc: 0.6087 | Val Loss: 0.6727 Acc: 0.6516                                               \n",
      "Epoch 040 | Train Loss: 0.7660 Acc: 0.6044 | Val Loss: 0.6765 Acc: 0.6770                                               \n",
      "Epoch 041 | Train Loss: 0.7674 Acc: 0.6083 | Val Loss: 0.6895 Acc: 0.6504                                               \n",
      "Epoch 042 | Train Loss: 0.7615 Acc: 0.6126 | Val Loss: 0.6733 Acc: 0.6901                                               \n",
      "Epoch 043 | Train Loss: 0.7581 Acc: 0.6122 | Val Loss: 0.7103 Acc: 0.6878                                               \n",
      "Epoch 044 | Train Loss: 0.7561 Acc: 0.6133 | Val Loss: 0.6886 Acc: 0.6242                                               \n",
      "Epoch 045 | Train Loss: 0.7525 Acc: 0.6115 | Val Loss: 0.6700 Acc: 0.6734                                               \n",
      "Epoch 046 | Train Loss: 0.7541 Acc: 0.6159 | Val Loss: 0.7132 Acc: 0.5991                                               \n",
      "Epoch 047 | Train Loss: 0.7872 Acc: 0.5792 | Val Loss: 0.6958 Acc: 0.5836                                               \n",
      "Epoch 048 | Train Loss: 0.7640 Acc: 0.5930 | Val Loss: 0.6878 Acc: 0.6146                                               \n",
      "Epoch 049 | Train Loss: 0.7579 Acc: 0.6006 | Val Loss: 0.6830 Acc: 0.6188                                               \n",
      "Epoch 050 | Train Loss: 0.7492 Acc: 0.6162 | Val Loss: 0.6640 Acc: 0.6343                                               \n",
      "Epoch 051 | Train Loss: 0.7474 Acc: 0.6242 | Val Loss: 0.6510 Acc: 0.6719                                               \n",
      "Epoch 052 | Train Loss: 0.7435 Acc: 0.6310 | Val Loss: 0.6684 Acc: 0.6340                                               \n",
      "Epoch 053 | Train Loss: 0.7368 Acc: 0.6342 | Val Loss: 0.6566 Acc: 0.6358                                               \n",
      "Epoch 054 | Train Loss: 0.7338 Acc: 0.6318 | Val Loss: 0.6549 Acc: 0.6388                                               \n",
      "Epoch 055 | Train Loss: 0.7277 Acc: 0.6375 | Val Loss: 0.6464 Acc: 0.6973                                               \n",
      "Epoch 056 | Train Loss: 0.7299 Acc: 0.6412 | Val Loss: 0.6830 Acc: 0.6648                                               \n",
      "Epoch 057 | Train Loss: 0.7275 Acc: 0.6497 | Val Loss: 0.6559 Acc: 0.7155                                               \n",
      "Epoch 058 | Train Loss: 0.7130 Acc: 0.6542 | Val Loss: 0.6437 Acc: 0.6815                                               \n",
      "Epoch 059 | Train Loss: 0.7201 Acc: 0.6506 | Val Loss: 0.6539 Acc: 0.7039                                               \n",
      "Epoch 060 | Train Loss: 0.7312 Acc: 0.6449 | Val Loss: 0.6323 Acc: 0.7272                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 32, 'cnn_dropout': 0.2838550759704502, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 48, 'cnn_kernels_2': 16, 'learning_rate': 0.001828022799674808, 'lstm_dense': 128, 'lstm_hidden_size': 64, 'lstm_layers': 1, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 26.7476 Acc: 0.4276 | Val Loss: 1.2786 Acc: 0.4421                                              \n",
      "Epoch 002 | Train Loss: 1.2576 Acc: 0.4421 | Val Loss: 1.2400 Acc: 0.4421                                               \n",
      "Epoch 003 | Train Loss: 1.2207 Acc: 0.4422 | Val Loss: 1.2028 Acc: 0.4421                                               \n",
      "Epoch 004 | Train Loss: 1.1860 Acc: 0.4423 | Val Loss: 1.1887 Acc: 0.4424                                               \n",
      "Epoch 005 | Train Loss: 1.1427 Acc: 0.4841 | Val Loss: 1.1519 Acc: 0.4875                                               \n",
      "Epoch 006 | Train Loss: 1.1048 Acc: 0.5279 | Val Loss: 1.0787 Acc: 0.5421                                               \n",
      "Epoch 007 | Train Loss: 1.0842 Acc: 0.5269 | Val Loss: 1.0503 Acc: 0.5490                                               \n",
      "Epoch 008 | Train Loss: 1.0643 Acc: 0.5385 | Val Loss: 1.0625 Acc: 0.5245                                               \n",
      "Epoch 009 | Train Loss: 0.9924 Acc: 0.5852 | Val Loss: 0.9584 Acc: 0.6081                                               \n",
      "Epoch 010 | Train Loss: 0.9467 Acc: 0.6144 | Val Loss: 0.8921 Acc: 0.6460                                               \n",
      "Epoch 011 | Train Loss: 0.8971 Acc: 0.6465 | Val Loss: 0.8925 Acc: 0.6337                                               \n",
      "Epoch 012 | Train Loss: 0.8383 Acc: 0.6742 | Val Loss: 0.8672 Acc: 0.6570                                               \n",
      "Epoch 013 | Train Loss: 0.7937 Acc: 0.6921 | Val Loss: 0.7614 Acc: 0.7158                                               \n",
      "Epoch 014 | Train Loss: 0.7364 Acc: 0.7204 | Val Loss: 0.7152 Acc: 0.7233                                               \n",
      "Epoch 015 | Train Loss: 0.7027 Acc: 0.7346 | Val Loss: 0.6578 Acc: 0.7543                                               \n",
      "Epoch 016 | Train Loss: 0.6852 Acc: 0.7377 | Val Loss: 0.6459 Acc: 0.7594                                               \n",
      "Epoch 017 | Train Loss: 0.6196 Acc: 0.7630 | Val Loss: 0.5817 Acc: 0.7815                                               \n",
      "Epoch 018 | Train Loss: 0.5891 Acc: 0.7729 | Val Loss: 0.5654 Acc: 0.7904                                               \n",
      "Epoch 019 | Train Loss: 0.5585 Acc: 0.7869 | Val Loss: 0.5207 Acc: 0.8063                                               \n",
      "Epoch 020 | Train Loss: 0.5300 Acc: 0.8018 | Val Loss: 0.4823 Acc: 0.8260                                               \n",
      "Epoch 021 | Train Loss: 0.4941 Acc: 0.8142 | Val Loss: 0.5358 Acc: 0.8000                                               \n",
      "Epoch 022 | Train Loss: 0.4587 Acc: 0.8330 | Val Loss: 0.4702 Acc: 0.8439                                               \n",
      "Epoch 023 | Train Loss: 0.4335 Acc: 0.8451 | Val Loss: 0.4551 Acc: 0.8439                                               \n",
      "Epoch 024 | Train Loss: 0.4155 Acc: 0.8589 | Val Loss: 0.4265 Acc: 0.8582                                               \n",
      "Epoch 025 | Train Loss: 0.4017 Acc: 0.8604 | Val Loss: 0.4434 Acc: 0.8558                                               \n",
      "Epoch 026 | Train Loss: 0.3847 Acc: 0.8672 | Val Loss: 0.3651 Acc: 0.8827                                               \n",
      "Epoch 027 | Train Loss: 0.3613 Acc: 0.8771 | Val Loss: 0.3384 Acc: 0.8893                                               \n",
      "Epoch 028 | Train Loss: 0.3438 Acc: 0.8835 | Val Loss: 0.3809 Acc: 0.8755                                               \n",
      "Epoch 029 | Train Loss: 0.3335 Acc: 0.8861 | Val Loss: 0.3675 Acc: 0.8824                                               \n",
      "Epoch 030 | Train Loss: 0.3183 Acc: 0.8892 | Val Loss: 0.3574 Acc: 0.8779                                               \n",
      "Epoch 031 | Train Loss: 0.2906 Acc: 0.9006 | Val Loss: 0.3370 Acc: 0.8857                                               \n",
      "Epoch 032 | Train Loss: 0.2805 Acc: 0.9041 | Val Loss: 0.2783 Acc: 0.9060                                               \n",
      "Epoch 033 | Train Loss: 0.2563 Acc: 0.9115 | Val Loss: 0.3774 Acc: 0.8755                                               \n",
      "Epoch 034 | Train Loss: 0.2961 Acc: 0.8965 | Val Loss: 0.2940 Acc: 0.8991                                               \n",
      "Epoch 035 | Train Loss: 0.2527 Acc: 0.9093 | Val Loss: 0.2949 Acc: 0.9051                                               \n",
      "Epoch 036 | Train Loss: 0.2391 Acc: 0.9194 | Val Loss: 0.3490 Acc: 0.8851                                               \n",
      "Epoch 037 | Train Loss: 0.2319 Acc: 0.9207 | Val Loss: 0.3134 Acc: 0.8970                                               \n",
      "Epoch 038 | Train Loss: 0.2200 Acc: 0.9239 | Val Loss: 0.3863 Acc: 0.8776                                               \n",
      "Epoch 039 | Train Loss: 0.2246 Acc: 0.9203 | Val Loss: 0.2913 Acc: 0.9078                                               \n",
      "Epoch 040 | Train Loss: 0.2040 Acc: 0.9308 | Val Loss: 0.2759 Acc: 0.9099                                               \n",
      "Epoch 041 | Train Loss: 0.2093 Acc: 0.9268 | Val Loss: 0.2555 Acc: 0.9170                                               \n",
      "Epoch 042 | Train Loss: 0.2015 Acc: 0.9296 | Val Loss: 0.3353 Acc: 0.8860                                               \n",
      "Epoch 043 | Train Loss: 0.1947 Acc: 0.9324 | Val Loss: 0.2798 Acc: 0.9075                                               \n",
      "Epoch 044 | Train Loss: 0.1774 Acc: 0.9398 | Val Loss: 0.2644 Acc: 0.9137                                               \n",
      "Epoch 045 | Train Loss: 0.1866 Acc: 0.9348 | Val Loss: 0.2016 Acc: 0.9373                                               \n",
      "Epoch 046 | Train Loss: 0.1819 Acc: 0.9369 | Val Loss: 0.2563 Acc: 0.9143                                               \n",
      "Epoch 047 | Train Loss: 0.1852 Acc: 0.9381 | Val Loss: 0.2334 Acc: 0.9236                                               \n",
      "Epoch 048 | Train Loss: 0.1574 Acc: 0.9453 | Val Loss: 0.2118 Acc: 0.9340                                               \n",
      "Epoch 049 | Train Loss: 0.1667 Acc: 0.9425 | Val Loss: 0.2502 Acc: 0.9206                                               \n",
      "Epoch 050 | Train Loss: 0.1615 Acc: 0.9425 | Val Loss: 0.1973 Acc: 0.9373                                               \n",
      "Epoch 051 | Train Loss: 0.1568 Acc: 0.9469 | Val Loss: 0.2574 Acc: 0.9149                                               \n",
      "Epoch 052 | Train Loss: 0.1575 Acc: 0.9459 | Val Loss: 0.2520 Acc: 0.9182                                               \n",
      "Epoch 053 | Train Loss: 0.1485 Acc: 0.9492 | Val Loss: 0.2325 Acc: 0.9236                                               \n",
      "Epoch 054 | Train Loss: 0.1408 Acc: 0.9525 | Val Loss: 0.2292 Acc: 0.9304                                               \n",
      "Epoch 055 | Train Loss: 0.1513 Acc: 0.9484 | Val Loss: 0.2165 Acc: 0.9310                                               \n",
      "Epoch 056 | Train Loss: 0.1442 Acc: 0.9502 | Val Loss: 0.1946 Acc: 0.9370                                               \n",
      "Epoch 057 | Train Loss: 0.1487 Acc: 0.9479 | Val Loss: 0.1736 Acc: 0.9469                                               \n",
      "Epoch 058 | Train Loss: 0.1426 Acc: 0.9508 | Val Loss: 0.1872 Acc: 0.9343                                               \n",
      "Epoch 059 | Train Loss: 0.1284 Acc: 0.9560 | Val Loss: 0.1812 Acc: 0.9451                                               \n",
      "Epoch 060 | Train Loss: 0.1269 Acc: 0.9569 | Val Loss: 0.2088 Acc: 0.9367                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 256, 'cnn_dropout': 0.09495318271859217, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 96, 'learning_rate': 0.006625845800954627, 'lstm_dense': 256, 'lstm_hidden_size': 64, 'lstm_layers': 5, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 614.7589 Acc: 0.4291 | Val Loss: 1.2451 Acc: 0.4421                                             \n",
      "Epoch 002 | Train Loss: 1.2454 Acc: 0.4422 | Val Loss: 1.2420 Acc: 0.4421                                               \n",
      "Epoch 003 | Train Loss: 1.2454 Acc: 0.4422 | Val Loss: 1.2420 Acc: 0.4421                                               \n",
      "Epoch 004 | Train Loss: 1.2445 Acc: 0.4422 | Val Loss: 1.2474 Acc: 0.4421                                               \n",
      "Epoch 005 | Train Loss: 1.2434 Acc: 0.4422 | Val Loss: 1.2545 Acc: 0.4421                                               \n",
      "Epoch 006 | Train Loss: 1.2459 Acc: 0.4422 | Val Loss: 1.2456 Acc: 0.4421                                               \n",
      "Epoch 007 | Train Loss: 1.2443 Acc: 0.4422 | Val Loss: 1.2466 Acc: 0.4421                                               \n",
      "Epoch 008 | Train Loss: 1.2452 Acc: 0.4422 | Val Loss: 1.2422 Acc: 0.4421                                               \n",
      "Epoch 009 | Train Loss: 1.2443 Acc: 0.4422 | Val Loss: 1.2437 Acc: 0.4421                                               \n",
      "Epoch 010 | Train Loss: 1.2444 Acc: 0.4422 | Val Loss: 1.2415 Acc: 0.4421                                               \n",
      "Epoch 011 | Train Loss: 1.2435 Acc: 0.4422 | Val Loss: 1.2425 Acc: 0.4421                                               \n",
      "Epoch 012 | Train Loss: 1.2430 Acc: 0.4422 | Val Loss: 1.2439 Acc: 0.4421                                               \n",
      "Epoch 013 | Train Loss: 1.2438 Acc: 0.4422 | Val Loss: 1.2408 Acc: 0.4421                                               \n",
      "Epoch 014 | Train Loss: 1.2425 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 015 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2393 Acc: 0.4421                                               \n",
      "Epoch 016 | Train Loss: 1.2418 Acc: 0.4422 | Val Loss: 1.2395 Acc: 0.4421                                               \n",
      "Epoch 017 | Train Loss: 1.2381 Acc: 0.4422 | Val Loss: 1.2391 Acc: 0.4421                                               \n",
      "Epoch 018 | Train Loss: 1.2102 Acc: 0.4455 | Val Loss: 1.1687 Acc: 0.4803                                               \n",
      "Epoch 019 | Train Loss: 1.1539 Acc: 0.4832 | Val Loss: 1.1132 Acc: 0.5209                                               \n",
      "Epoch 020 | Train Loss: 1.0938 Acc: 0.5137 | Val Loss: 1.1089 Acc: 0.5012                                               \n",
      "Epoch 021 | Train Loss: 1.0311 Acc: 0.5472 | Val Loss: 1.0571 Acc: 0.5722                                               \n",
      "Epoch 022 | Train Loss: 1.1609 Acc: 0.4670 | Val Loss: 1.2508 Acc: 0.4421                                               \n",
      "Epoch 023 | Train Loss: 1.2448 Acc: 0.4422 | Val Loss: 1.2428 Acc: 0.4421                                               \n",
      "Epoch 024 | Train Loss: 1.2425 Acc: 0.4422 | Val Loss: 1.2425 Acc: 0.4421                                               \n",
      "Epoch 025 | Train Loss: 1.2423 Acc: 0.4422 | Val Loss: 1.2437 Acc: 0.4421                                               \n",
      "Epoch 026 | Train Loss: 1.2424 Acc: 0.4422 | Val Loss: 1.2420 Acc: 0.4421                                               \n",
      "Epoch 027 | Train Loss: 1.2426 Acc: 0.4422 | Val Loss: 1.2420 Acc: 0.4421                                               \n",
      "Epoch 028 | Train Loss: 1.2427 Acc: 0.4422 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 029 | Train Loss: 1.2426 Acc: 0.4422 | Val Loss: 1.2420 Acc: 0.4421                                               \n",
      "Epoch 030 | Train Loss: 1.2426 Acc: 0.4422 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 031 | Train Loss: 1.2422 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 64, 'cnn_dropout': 0.44707287681075475, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 32, 'cnn_kernels_2': 64, 'learning_rate': 0.0001760187404529527, 'lstm_dense': 128, 'lstm_hidden_size': 32, 'lstm_layers': 2, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 10.2108 Acc: 0.4377 | Val Loss: 1.3015 Acc: 0.4421                                              \n",
      "Epoch 002 | Train Loss: 1.2920 Acc: 0.4422 | Val Loss: 1.2809 Acc: 0.4421                                               \n",
      "Epoch 003 | Train Loss: 1.2748 Acc: 0.4422 | Val Loss: 1.2662 Acc: 0.4421                                               \n",
      "Epoch 004 | Train Loss: 1.2626 Acc: 0.4422 | Val Loss: 1.2558 Acc: 0.4421                                               \n",
      "Epoch 005 | Train Loss: 1.2544 Acc: 0.4422 | Val Loss: 1.2488 Acc: 0.4421                                               \n",
      "Epoch 006 | Train Loss: 1.2478 Acc: 0.4422 | Val Loss: 1.2436 Acc: 0.4421                                               \n",
      "Epoch 007 | Train Loss: 1.2433 Acc: 0.4422 | Val Loss: 1.2396 Acc: 0.4421                                               \n",
      "Epoch 008 | Train Loss: 1.2405 Acc: 0.4422 | Val Loss: 1.2366 Acc: 0.4421                                               \n",
      "Epoch 009 | Train Loss: 1.2375 Acc: 0.4422 | Val Loss: 1.2340 Acc: 0.4421                                               \n",
      "Epoch 010 | Train Loss: 1.2354 Acc: 0.4422 | Val Loss: 1.2321 Acc: 0.4421                                               \n",
      "Epoch 011 | Train Loss: 1.2331 Acc: 0.4422 | Val Loss: 1.2304 Acc: 0.4421                                               \n",
      "Epoch 012 | Train Loss: 1.2315 Acc: 0.4422 | Val Loss: 1.2290 Acc: 0.4421                                               \n",
      "Epoch 013 | Train Loss: 1.2301 Acc: 0.4422 | Val Loss: 1.2277 Acc: 0.4421                                               \n",
      "Epoch 014 | Train Loss: 1.2299 Acc: 0.4422 | Val Loss: 1.2268 Acc: 0.4421                                               \n",
      "Epoch 015 | Train Loss: 1.2290 Acc: 0.4422 | Val Loss: 1.2258 Acc: 0.4421                                               \n",
      "Epoch 016 | Train Loss: 1.2283 Acc: 0.4422 | Val Loss: 1.2249 Acc: 0.4421                                               \n",
      "Epoch 017 | Train Loss: 1.2269 Acc: 0.4422 | Val Loss: 1.2240 Acc: 0.4421                                               \n",
      "Epoch 018 | Train Loss: 1.2260 Acc: 0.4422 | Val Loss: 1.2232 Acc: 0.4421                                               \n",
      "Epoch 019 | Train Loss: 1.2254 Acc: 0.4422 | Val Loss: 1.2223 Acc: 0.4421                                               \n",
      "Epoch 020 | Train Loss: 1.2247 Acc: 0.4422 | Val Loss: 1.2215 Acc: 0.4421                                               \n",
      "Epoch 021 | Train Loss: 1.2236 Acc: 0.4422 | Val Loss: 1.2205 Acc: 0.4421                                               \n",
      "Epoch 022 | Train Loss: 1.2229 Acc: 0.4422 | Val Loss: 1.2195 Acc: 0.4421                                               \n",
      "Epoch 023 | Train Loss: 1.2217 Acc: 0.4422 | Val Loss: 1.2185 Acc: 0.4421                                               \n",
      "Epoch 024 | Train Loss: 1.2210 Acc: 0.4422 | Val Loss: 1.2171 Acc: 0.4421                                               \n",
      "Epoch 025 | Train Loss: 1.2193 Acc: 0.4422 | Val Loss: 1.2158 Acc: 0.4421                                               \n",
      "Epoch 026 | Train Loss: 1.2177 Acc: 0.4422 | Val Loss: 1.2141 Acc: 0.4421                                               \n",
      "Epoch 027 | Train Loss: 1.2167 Acc: 0.4422 | Val Loss: 1.2122 Acc: 0.4421                                               \n",
      "Epoch 028 | Train Loss: 1.2140 Acc: 0.4422 | Val Loss: 1.2115 Acc: 0.4421                                               \n",
      "Epoch 029 | Train Loss: 1.2113 Acc: 0.4422 | Val Loss: 1.2083 Acc: 0.4421                                               \n",
      "Epoch 030 | Train Loss: 1.2091 Acc: 0.4422 | Val Loss: 1.2045 Acc: 0.4421                                               \n",
      "Epoch 031 | Train Loss: 1.2065 Acc: 0.4420 | Val Loss: 1.2008 Acc: 0.4421                                               \n",
      "Epoch 032 | Train Loss: 1.2021 Acc: 0.4423 | Val Loss: 1.1980 Acc: 0.4421                                               \n",
      "Epoch 033 | Train Loss: 1.1985 Acc: 0.4431 | Val Loss: 1.1918 Acc: 0.4442                                               \n",
      "Epoch 034 | Train Loss: 1.1922 Acc: 0.4470 | Val Loss: 1.1890 Acc: 0.4687                                               \n",
      "Epoch 035 | Train Loss: 1.1839 Acc: 0.4573 | Val Loss: 1.1731 Acc: 0.4812                                               \n",
      "Epoch 036 | Train Loss: 1.1755 Acc: 0.4719 | Val Loss: 1.1688 Acc: 0.5158                                               \n",
      "Epoch 037 | Train Loss: 1.1675 Acc: 0.4904 | Val Loss: 1.1567 Acc: 0.5239                                               \n",
      "Epoch 038 | Train Loss: 1.1589 Acc: 0.5016 | Val Loss: 1.1480 Acc: 0.5337                                               \n",
      "Epoch 039 | Train Loss: 1.1485 Acc: 0.5138 | Val Loss: 1.1403 Acc: 0.5412                                               \n",
      "Epoch 040 | Train Loss: 1.1386 Acc: 0.5203 | Val Loss: 1.1241 Acc: 0.5469                                               \n",
      "Epoch 041 | Train Loss: 1.1276 Acc: 0.5305 | Val Loss: 1.1116 Acc: 0.5493                                               \n",
      "Epoch 042 | Train Loss: 1.1180 Acc: 0.5347 | Val Loss: 1.0999 Acc: 0.5427                                               \n",
      "Epoch 043 | Train Loss: 1.1023 Acc: 0.5435 | Val Loss: 1.1111 Acc: 0.5224                                               \n",
      "Epoch 044 | Train Loss: 1.0889 Acc: 0.5477 | Val Loss: 1.0664 Acc: 0.5594                                               \n",
      "Epoch 045 | Train Loss: 1.0806 Acc: 0.5521 | Val Loss: 1.0872 Acc: 0.5260                                               \n",
      "Epoch 046 | Train Loss: 1.0655 Acc: 0.5553 | Val Loss: 1.0519 Acc: 0.5501                                               \n",
      "Epoch 047 | Train Loss: 1.0529 Acc: 0.5634 | Val Loss: 1.0382 Acc: 0.5525                                               \n",
      "Epoch 048 | Train Loss: 1.0454 Acc: 0.5631 | Val Loss: 1.0121 Acc: 0.5710                                               \n",
      "Epoch 049 | Train Loss: 1.0240 Acc: 0.5736 | Val Loss: 1.0073 Acc: 0.5731                                               \n",
      "Epoch 050 | Train Loss: 1.0158 Acc: 0.5744 | Val Loss: 1.1028 Acc: 0.5006                                               \n",
      "Epoch 051 | Train Loss: 1.0146 Acc: 0.5721 | Val Loss: 0.9832 Acc: 0.5839                                               \n",
      "Epoch 052 | Train Loss: 0.9956 Acc: 0.5813 | Val Loss: 0.9912 Acc: 0.5672                                               \n",
      "Epoch 053 | Train Loss: 0.9857 Acc: 0.5841 | Val Loss: 0.9892 Acc: 0.5627                                               \n",
      "Epoch 054 | Train Loss: 0.9872 Acc: 0.5807 | Val Loss: 0.9485 Acc: 0.6000                                               \n",
      "Epoch 055 | Train Loss: 0.9600 Acc: 0.5944 | Val Loss: 0.9613 Acc: 0.5845                                               \n",
      "Epoch 056 | Train Loss: 0.9547 Acc: 0.5936 | Val Loss: 0.9166 Acc: 0.6060                                               \n",
      "Epoch 057 | Train Loss: 0.9425 Acc: 0.6003 | Val Loss: 0.9294 Acc: 0.6137                                               \n",
      "Epoch 058 | Train Loss: 0.9278 Acc: 0.6077 | Val Loss: 0.9323 Acc: 0.5854                                               \n",
      "Epoch 059 | Train Loss: 0.9148 Acc: 0.6134 | Val Loss: 0.9089 Acc: 0.6018                                               \n",
      "Epoch 060 | Train Loss: 0.9489 Acc: 0.5976 | Val Loss: 0.9325 Acc: 0.5958                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 64, 'cnn_dropout': 0.5550825871887426, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 16, 'cnn_kernels_2': 16, 'learning_rate': 4.020468005154102e-05, 'lstm_dense': 64, 'lstm_hidden_size': 32, 'lstm_layers': 2, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 5.3542 Acc: 0.4193 | Val Loss: 1.0196 Acc: 0.4573                                               \n",
      "Epoch 002 | Train Loss: 1.0424 Acc: 0.4696 | Val Loss: 0.9179 Acc: 0.5155                                               \n",
      "Epoch 003 | Train Loss: 0.9765 Acc: 0.4888 | Val Loss: 0.8800 Acc: 0.5681                                               \n",
      "Epoch 004 | Train Loss: 0.9585 Acc: 0.5092 | Val Loss: 0.8628 Acc: 0.6161                                               \n",
      "Epoch 005 | Train Loss: 0.9226 Acc: 0.5288 | Val Loss: 0.8219 Acc: 0.6006                                               \n",
      "Epoch 006 | Train Loss: 0.9185 Acc: 0.5340 | Val Loss: 0.8338 Acc: 0.5940                                               \n",
      "Epoch 007 | Train Loss: 0.9048 Acc: 0.5419 | Val Loss: 0.8187 Acc: 0.6191                                               \n",
      "Epoch 008 | Train Loss: 0.8999 Acc: 0.5497 | Val Loss: 0.8166 Acc: 0.6045                                               \n",
      "Epoch 009 | Train Loss: 0.8863 Acc: 0.5594 | Val Loss: 0.7965 Acc: 0.6278                                               \n",
      "Epoch 010 | Train Loss: 0.8918 Acc: 0.5574 | Val Loss: 0.8126 Acc: 0.5928                                               \n",
      "Epoch 011 | Train Loss: 0.8833 Acc: 0.5577 | Val Loss: 0.7751 Acc: 0.6454                                               \n",
      "Epoch 012 | Train Loss: 0.8633 Acc: 0.5723 | Val Loss: 0.7816 Acc: 0.6433                                               \n",
      "Epoch 013 | Train Loss: 0.8624 Acc: 0.5746 | Val Loss: 0.7745 Acc: 0.6481                                               \n",
      "Epoch 014 | Train Loss: 0.8598 Acc: 0.5712 | Val Loss: 0.7559 Acc: 0.6791                                               \n",
      "Epoch 015 | Train Loss: 0.8537 Acc: 0.5739 | Val Loss: 0.7724 Acc: 0.6585                                               \n",
      "Epoch 016 | Train Loss: 0.8509 Acc: 0.5783 | Val Loss: 0.7573 Acc: 0.6615                                               \n",
      "Epoch 017 | Train Loss: 0.8548 Acc: 0.5812 | Val Loss: 0.7598 Acc: 0.6287                                               \n",
      "Epoch 018 | Train Loss: 0.8444 Acc: 0.5909 | Val Loss: 0.7465 Acc: 0.6516                                               \n",
      "Epoch 019 | Train Loss: 0.8373 Acc: 0.5881 | Val Loss: 0.7344 Acc: 0.6469                                               \n",
      "Epoch 020 | Train Loss: 0.8288 Acc: 0.5995 | Val Loss: 0.7244 Acc: 0.6397                                               \n",
      "Epoch 021 | Train Loss: 0.8335 Acc: 0.5930 | Val Loss: 0.7324 Acc: 0.6612                                               \n",
      "Epoch 022 | Train Loss: 0.8142 Acc: 0.6040 | Val Loss: 0.7133 Acc: 0.6982                                               \n",
      "Epoch 023 | Train Loss: 0.8177 Acc: 0.6035 | Val Loss: 0.7125 Acc: 0.7101                                               \n",
      "Epoch 024 | Train Loss: 0.8099 Acc: 0.6042 | Val Loss: 0.6962 Acc: 0.6806                                               \n",
      "Epoch 025 | Train Loss: 0.8171 Acc: 0.6060 | Val Loss: 0.6911 Acc: 0.6663                                               \n",
      "Epoch 026 | Train Loss: 0.8158 Acc: 0.6115 | Val Loss: 0.7141 Acc: 0.6979                                               \n",
      "Epoch 027 | Train Loss: 0.7963 Acc: 0.6179 | Val Loss: 0.6878 Acc: 0.6964                                               \n",
      "Epoch 028 | Train Loss: 0.8098 Acc: 0.6084 | Val Loss: 0.6886 Acc: 0.6872                                               \n",
      "Epoch 029 | Train Loss: 0.7945 Acc: 0.6235 | Val Loss: 0.6998 Acc: 0.6507                                               \n",
      "Epoch 030 | Train Loss: 0.7986 Acc: 0.6144 | Val Loss: 0.6713 Acc: 0.7230                                               \n",
      "Epoch 031 | Train Loss: 0.8147 Acc: 0.6103 | Val Loss: 0.6745 Acc: 0.7287                                               \n",
      "Epoch 032 | Train Loss: 0.7894 Acc: 0.6277 | Val Loss: 0.6693 Acc: 0.7224                                               \n",
      "Epoch 033 | Train Loss: 0.7983 Acc: 0.6226 | Val Loss: 0.6665 Acc: 0.7155                                               \n",
      "Epoch 034 | Train Loss: 0.7825 Acc: 0.6283 | Val Loss: 0.6570 Acc: 0.7251                                               \n",
      "Epoch 035 | Train Loss: 0.7648 Acc: 0.6409 | Val Loss: 0.6583 Acc: 0.7125                                               \n",
      "Epoch 036 | Train Loss: 0.7893 Acc: 0.6258 | Val Loss: 0.6502 Acc: 0.6931                                               \n",
      "Epoch 037 | Train Loss: 0.7788 Acc: 0.6309 | Val Loss: 0.6577 Acc: 0.7122                                               \n",
      "Epoch 038 | Train Loss: 0.7698 Acc: 0.6387 | Val Loss: 0.6362 Acc: 0.7833                                               \n",
      "Epoch 039 | Train Loss: 0.7723 Acc: 0.6368 | Val Loss: 0.6441 Acc: 0.7319                                               \n",
      "Epoch 040 | Train Loss: 0.7695 Acc: 0.6439 | Val Loss: 0.6616 Acc: 0.7284                                               \n",
      "Epoch 041 | Train Loss: 0.7730 Acc: 0.6370 | Val Loss: 0.6400 Acc: 0.7131                                               \n",
      "Epoch 042 | Train Loss: 0.7658 Acc: 0.6405 | Val Loss: 0.6545 Acc: 0.7475                                               \n",
      "Epoch 043 | Train Loss: 0.7597 Acc: 0.6408 | Val Loss: 0.6288 Acc: 0.7830                                               \n",
      "Epoch 044 | Train Loss: 0.7608 Acc: 0.6490 | Val Loss: 0.6089 Acc: 0.7445                                               \n",
      "Epoch 045 | Train Loss: 0.7568 Acc: 0.6417 | Val Loss: 0.6011 Acc: 0.7403                                               \n",
      "Epoch 046 | Train Loss: 0.7471 Acc: 0.6564 | Val Loss: 0.6103 Acc: 0.7675                                               \n",
      "Epoch 047 | Train Loss: 0.7583 Acc: 0.6477 | Val Loss: 0.6266 Acc: 0.7681                                               \n",
      "Epoch 048 | Train Loss: 0.7476 Acc: 0.6503 | Val Loss: 0.5880 Acc: 0.7606                                               \n",
      "Epoch 049 | Train Loss: 0.7460 Acc: 0.6521 | Val Loss: 0.6184 Acc: 0.7567                                               \n",
      "Epoch 050 | Train Loss: 0.7402 Acc: 0.6571 | Val Loss: 0.5924 Acc: 0.7409                                               \n",
      "Epoch 051 | Train Loss: 0.7472 Acc: 0.6553 | Val Loss: 0.6094 Acc: 0.7660                                               \n",
      "Epoch 052 | Train Loss: 0.7760 Acc: 0.6437 | Val Loss: 0.6277 Acc: 0.7272                                               \n",
      "Epoch 053 | Train Loss: 0.7549 Acc: 0.6496 | Val Loss: 0.6094 Acc: 0.7481                                               \n",
      "Epoch 054 | Train Loss: 0.7660 Acc: 0.6400 | Val Loss: 0.6034 Acc: 0.7475                                               \n",
      "Epoch 055 | Train Loss: 0.7574 Acc: 0.6496 | Val Loss: 0.6130 Acc: 0.7409                                               \n",
      "Epoch 056 | Train Loss: 0.7564 Acc: 0.6505 | Val Loss: 0.6042 Acc: 0.7896                                               \n",
      "Epoch 057 | Train Loss: 0.7513 Acc: 0.6495 | Val Loss: 0.6112 Acc: 0.7821                                               \n",
      "Epoch 058 | Train Loss: 0.7502 Acc: 0.6549 | Val Loss: 0.6182 Acc: 0.7472                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 128, 'cnn_dropout': 0.68051253617948, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 64, 'learning_rate': 0.0005595839395760027, 'lstm_dense': 32, 'lstm_hidden_size': 96, 'lstm_layers': 3, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 8.0269 Acc: 0.4025 | Val Loss: 1.5127 Acc: 0.4293                                               \n",
      "Epoch 002 | Train Loss: 2.5599 Acc: 0.4557 | Val Loss: 1.2729 Acc: 0.6006                                               \n",
      "Epoch 003 | Train Loss: 1.6023 Acc: 0.5115 | Val Loss: 3.4729 Acc: 0.4346                                               \n",
      "Epoch 004 | Train Loss: 1.2638 Acc: 0.5496 | Val Loss: 1.0755 Acc: 0.5884                                               \n",
      "Epoch 005 | Train Loss: 1.0119 Acc: 0.5830 | Val Loss: 0.8167 Acc: 0.6525                                               \n",
      "Epoch 006 | Train Loss: 0.8658 Acc: 0.6144 | Val Loss: 0.6562 Acc: 0.7164                                               \n",
      "Epoch 007 | Train Loss: 0.7916 Acc: 0.6431 | Val Loss: 0.6674 Acc: 0.6749                                               \n",
      "Epoch 008 | Train Loss: 0.7303 Acc: 0.6709 | Val Loss: 0.6675 Acc: 0.7167                                               \n",
      "Epoch 009 | Train Loss: 0.6931 Acc: 0.6997 | Val Loss: 0.5990 Acc: 0.7215                                               \n",
      "Epoch 010 | Train Loss: 0.6455 Acc: 0.7223 | Val Loss: 0.6160 Acc: 0.7290                                               \n",
      "Epoch 011 | Train Loss: 0.6173 Acc: 0.7366 | Val Loss: 0.5178 Acc: 0.8048                                               \n",
      "Epoch 012 | Train Loss: 0.5910 Acc: 0.7472 | Val Loss: 0.5652 Acc: 0.7319                                               \n",
      "Epoch 013 | Train Loss: 0.5661 Acc: 0.7621 | Val Loss: 0.5066 Acc: 0.7848                                               \n",
      "Epoch 014 | Train Loss: 0.5428 Acc: 0.7724 | Val Loss: 0.5734 Acc: 0.7493                                               \n",
      "Epoch 015 | Train Loss: 0.5144 Acc: 0.7907 | Val Loss: 0.4614 Acc: 0.8221                                               \n",
      "Epoch 016 | Train Loss: 0.4976 Acc: 0.8030 | Val Loss: 0.4476 Acc: 0.7904                                               \n",
      "Epoch 017 | Train Loss: 0.4592 Acc: 0.8195 | Val Loss: 0.4276 Acc: 0.8173                                               \n",
      "Epoch 018 | Train Loss: 0.4208 Acc: 0.8380 | Val Loss: 0.4672 Acc: 0.7940                                               \n",
      "Epoch 019 | Train Loss: 0.4124 Acc: 0.8478 | Val Loss: 0.6569 Acc: 0.7388                                               \n",
      "Epoch 020 | Train Loss: 0.3849 Acc: 0.8565 | Val Loss: 0.4826 Acc: 0.8299                                               \n",
      "Epoch 021 | Train Loss: 0.3377 Acc: 0.8757 | Val Loss: 0.5345 Acc: 0.7800                                               \n",
      "Epoch 022 | Train Loss: 0.3402 Acc: 0.8750 | Val Loss: 0.5540 Acc: 0.7991                                               \n",
      "Epoch 023 | Train Loss: 0.3051 Acc: 0.8913 | Val Loss: 0.3529 Acc: 0.8767                                               \n",
      "Epoch 024 | Train Loss: 0.3047 Acc: 0.8922 | Val Loss: 0.3314 Acc: 0.8707                                               \n",
      "Epoch 025 | Train Loss: 0.2959 Acc: 0.8967 | Val Loss: 0.3687 Acc: 0.8615                                               \n",
      "Epoch 026 | Train Loss: 0.2639 Acc: 0.9075 | Val Loss: 0.3690 Acc: 0.8725                                               \n",
      "Epoch 027 | Train Loss: 0.2617 Acc: 0.9061 | Val Loss: 0.3991 Acc: 0.8373                                               \n",
      "Epoch 028 | Train Loss: 0.2384 Acc: 0.9175 | Val Loss: 0.3890 Acc: 0.8976                                               \n",
      "Epoch 029 | Train Loss: 0.2324 Acc: 0.9211 | Val Loss: 0.2694 Acc: 0.9030                                               \n",
      "Epoch 030 | Train Loss: 0.2102 Acc: 0.9267 | Val Loss: 0.4726 Acc: 0.8827                                               \n",
      "Epoch 031 | Train Loss: 0.2313 Acc: 0.9283 | Val Loss: 0.2819 Acc: 0.9045                                               \n",
      "Epoch 032 | Train Loss: 0.1955 Acc: 0.9347 | Val Loss: 0.4792 Acc: 0.8293                                               \n",
      "Epoch 033 | Train Loss: 0.1783 Acc: 0.9394 | Val Loss: 0.9030 Acc: 0.7797                                               \n",
      "Epoch 034 | Train Loss: 0.1764 Acc: 0.9422 | Val Loss: 0.3725 Acc: 0.8699                                               \n",
      "Epoch 035 | Train Loss: 0.1672 Acc: 0.9440 | Val Loss: 0.1942 Acc: 0.9239                                               \n",
      "Epoch 036 | Train Loss: 0.1476 Acc: 0.9524 | Val Loss: 0.4561 Acc: 0.8409                                               \n",
      "Epoch 037 | Train Loss: 0.1497 Acc: 0.9517 | Val Loss: 0.2642 Acc: 0.9099                                               \n",
      "Epoch 038 | Train Loss: 0.1560 Acc: 0.9479 | Val Loss: 0.3201 Acc: 0.8833                                               \n",
      "Epoch 039 | Train Loss: 0.1280 Acc: 0.9597 | Val Loss: 0.2811 Acc: 0.9188                                               \n",
      "Epoch 040 | Train Loss: 0.1351 Acc: 0.9570 | Val Loss: 0.2892 Acc: 0.8904                                               \n",
      "Epoch 041 | Train Loss: 0.1279 Acc: 0.9596 | Val Loss: 0.2432 Acc: 0.9310                                               \n",
      "Epoch 042 | Train Loss: 0.1119 Acc: 0.9652 | Val Loss: 0.3028 Acc: 0.9167                                               \n",
      "Epoch 043 | Train Loss: 0.1104 Acc: 0.9645 | Val Loss: 0.5907 Acc: 0.8696                                               \n",
      "Epoch 044 | Train Loss: 0.1175 Acc: 0.9617 | Val Loss: 0.2218 Acc: 0.9463                                               \n",
      "Epoch 045 | Train Loss: 0.1181 Acc: 0.9644 | Val Loss: 0.1564 Acc: 0.9403                                               \n",
      "Epoch 046 | Train Loss: 0.0970 Acc: 0.9695 | Val Loss: 0.2339 Acc: 0.9319                                               \n",
      "Epoch 047 | Train Loss: 0.0835 Acc: 0.9744 | Val Loss: 0.3490 Acc: 0.9125                                               \n",
      "Epoch 048 | Train Loss: 0.0881 Acc: 0.9722 | Val Loss: 0.2335 Acc: 0.9307                                               \n",
      "Epoch 049 | Train Loss: 0.0859 Acc: 0.9728 | Val Loss: 0.1416 Acc: 0.9609                                               \n",
      "Epoch 050 | Train Loss: 0.0905 Acc: 0.9722 | Val Loss: 0.1943 Acc: 0.9376                                               \n",
      "Epoch 051 | Train Loss: 0.0760 Acc: 0.9756 | Val Loss: 0.2374 Acc: 0.9304                                               \n",
      "Epoch 052 | Train Loss: 0.0832 Acc: 0.9755 | Val Loss: 0.2119 Acc: 0.9316                                               \n",
      "Epoch 053 | Train Loss: 0.0784 Acc: 0.9765 | Val Loss: 0.1868 Acc: 0.9391                                               \n",
      "Epoch 054 | Train Loss: 0.0712 Acc: 0.9776 | Val Loss: 0.1867 Acc: 0.9421                                               \n",
      "Epoch 055 | Train Loss: 0.0750 Acc: 0.9765 | Val Loss: 0.1358 Acc: 0.9618                                               \n",
      "Epoch 056 | Train Loss: 0.0599 Acc: 0.9799 | Val Loss: 0.4540 Acc: 0.8893                                               \n",
      "Epoch 057 | Train Loss: 0.0898 Acc: 0.9741 | Val Loss: 0.1538 Acc: 0.9546                                               \n",
      "Epoch 058 | Train Loss: 0.0702 Acc: 0.9772 | Val Loss: 0.2891 Acc: 0.9099                                               \n",
      "Epoch 059 | Train Loss: 0.0722 Acc: 0.9789 | Val Loss: 0.0830 Acc: 0.9687                                               \n",
      "Epoch 060 | Train Loss: 0.0578 Acc: 0.9810 | Val Loss: 0.1379 Acc: 0.9540                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 256, 'cnn_dropout': 0.6378294688413177, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 32, 'learning_rate': 0.0009839564951612146, 'lstm_dense': 32, 'lstm_hidden_size': 128, 'lstm_layers': 6, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 13.0431 Acc: 0.4373 | Val Loss: 1.2548 Acc: 0.5779                                              \n",
      "Epoch 002 | Train Loss: 2.5399 Acc: 0.5091 | Val Loss: 2.3333 Acc: 0.5367                                               \n",
      "Epoch 003 | Train Loss: 1.4073 Acc: 0.5575 | Val Loss: 1.2856 Acc: 0.5313                                               \n",
      "Epoch 004 | Train Loss: 0.9704 Acc: 0.6093 | Val Loss: 0.8536 Acc: 0.6227                                               \n",
      "Epoch 005 | Train Loss: 0.7798 Acc: 0.6556 | Val Loss: 0.5905 Acc: 0.7358                                               \n",
      "Epoch 006 | Train Loss: 0.6981 Acc: 0.6844 | Val Loss: 0.5559 Acc: 0.7466                                               \n",
      "Epoch 007 | Train Loss: 0.6467 Acc: 0.7054 | Val Loss: 0.4852 Acc: 0.7558                                               \n",
      "Epoch 008 | Train Loss: 0.5941 Acc: 0.7351 | Val Loss: 0.4773 Acc: 0.7827                                               \n",
      "Epoch 009 | Train Loss: 0.5499 Acc: 0.7538 | Val Loss: 0.4805 Acc: 0.7731                                               \n",
      "Epoch 010 | Train Loss: 0.5363 Acc: 0.7651 | Val Loss: 0.4729 Acc: 0.7821                                               \n",
      "Epoch 011 | Train Loss: 0.4909 Acc: 0.7895 | Val Loss: 0.5544 Acc: 0.7319                                               \n",
      "Epoch 012 | Train Loss: 0.4606 Acc: 0.7995 | Val Loss: 0.4909 Acc: 0.7654                                               \n",
      "Epoch 013 | Train Loss: 0.4844 Acc: 0.7991 | Val Loss: 0.4158 Acc: 0.8206                                               \n",
      "Epoch 014 | Train Loss: 0.4254 Acc: 0.8218 | Val Loss: 0.4161 Acc: 0.8316                                               \n",
      "Epoch 015 | Train Loss: 0.4248 Acc: 0.8186 | Val Loss: 0.4182 Acc: 0.8036                                               \n",
      "Epoch 016 | Train Loss: 0.4220 Acc: 0.8231 | Val Loss: 0.4728 Acc: 0.7988                                               \n",
      "Epoch 017 | Train Loss: 0.3893 Acc: 0.8354 | Val Loss: 0.4295 Acc: 0.7767                                               \n",
      "Epoch 018 | Train Loss: 0.3929 Acc: 0.8342 | Val Loss: 0.4780 Acc: 0.7907                                               \n",
      "Epoch 019 | Train Loss: 0.3797 Acc: 0.8436 | Val Loss: 0.4035 Acc: 0.8081                                               \n",
      "Epoch 020 | Train Loss: 0.3765 Acc: 0.8441 | Val Loss: 0.4511 Acc: 0.7904                                               \n",
      "Epoch 021 | Train Loss: 0.3847 Acc: 0.8416 | Val Loss: 0.5465 Acc: 0.7666                                               \n",
      "Epoch 022 | Train Loss: 0.3482 Acc: 0.8549 | Val Loss: 0.5603 Acc: 0.7430                                               \n",
      "Epoch 023 | Train Loss: 0.3574 Acc: 0.8515 | Val Loss: 0.3900 Acc: 0.7854                                               \n",
      "Epoch 024 | Train Loss: 0.3481 Acc: 0.8590 | Val Loss: 0.4573 Acc: 0.8176                                               \n",
      "Epoch 025 | Train Loss: 0.3276 Acc: 0.8652 | Val Loss: 0.5197 Acc: 0.7830                                               \n",
      "Epoch 026 | Train Loss: 0.3334 Acc: 0.8643 | Val Loss: 0.6217 Acc: 0.6934                                               \n",
      "Epoch 027 | Train Loss: 0.3325 Acc: 0.8682 | Val Loss: 0.5593 Acc: 0.7558                                               \n",
      "Epoch 028 | Train Loss: 0.3355 Acc: 0.8639 | Val Loss: 0.4981 Acc: 0.8749                                               \n",
      "Epoch 029 | Train Loss: 0.3206 Acc: 0.8707 | Val Loss: 0.3484 Acc: 0.8227                                               \n",
      "Epoch 030 | Train Loss: 0.3161 Acc: 0.8710 | Val Loss: 0.4195 Acc: 0.8301                                               \n",
      "Epoch 031 | Train Loss: 0.3248 Acc: 0.8696 | Val Loss: 1.1455 Acc: 0.7182                                               \n",
      "Epoch 032 | Train Loss: 0.3169 Acc: 0.8751 | Val Loss: 0.4546 Acc: 0.8436                                               \n",
      "Epoch 033 | Train Loss: 0.2985 Acc: 0.8752 | Val Loss: 0.3961 Acc: 0.8349                                               \n",
      "Epoch 034 | Train Loss: 0.3043 Acc: 0.8786 | Val Loss: 0.3987 Acc: 0.8367                                               \n",
      "Epoch 035 | Train Loss: 0.2980 Acc: 0.8786 | Val Loss: 0.4333 Acc: 0.8140                                               \n",
      "Epoch 036 | Train Loss: 0.3154 Acc: 0.8760 | Val Loss: 0.6178 Acc: 0.7534                                               \n",
      "Epoch 037 | Train Loss: 0.2967 Acc: 0.8834 | Val Loss: 0.4399 Acc: 0.8382                                               \n",
      "Epoch 038 | Train Loss: 0.2831 Acc: 0.8876 | Val Loss: 0.4191 Acc: 0.8000                                               \n",
      "Epoch 039 | Train Loss: 0.2904 Acc: 0.8845 | Val Loss: 0.3181 Acc: 0.8770                                               \n",
      "Epoch 040 | Train Loss: 0.3034 Acc: 0.8816 | Val Loss: 0.5054 Acc: 0.8215                                               \n",
      "Epoch 041 | Train Loss: 0.2970 Acc: 0.8817 | Val Loss: 0.3165 Acc: 0.8842                                               \n",
      "Epoch 042 | Train Loss: 0.2812 Acc: 0.8901 | Val Loss: 0.3775 Acc: 0.8576                                               \n",
      "Epoch 043 | Train Loss: 0.2863 Acc: 0.8851 | Val Loss: 0.3901 Acc: 0.8310                                               \n",
      "Epoch 044 | Train Loss: 0.2669 Acc: 0.8939 | Val Loss: 0.5055 Acc: 0.7591                                               \n",
      "Epoch 045 | Train Loss: 0.2811 Acc: 0.8907 | Val Loss: 0.4825 Acc: 0.8397                                               \n",
      "Epoch 046 | Train Loss: 0.2807 Acc: 0.8913 | Val Loss: 0.4881 Acc: 0.7576                                               \n",
      "Epoch 047 | Train Loss: 0.2694 Acc: 0.8960 | Val Loss: 0.4291 Acc: 0.8107                                               \n",
      "Epoch 048 | Train Loss: 0.2691 Acc: 0.8937 | Val Loss: 0.3846 Acc: 0.8218                                               \n",
      "Epoch 049 | Train Loss: 0.2745 Acc: 0.8945 | Val Loss: 0.6272 Acc: 0.7585                                               \n",
      "Epoch 050 | Train Loss: 0.2711 Acc: 0.8939 | Val Loss: 0.4600 Acc: 0.7973                                               \n",
      "Epoch 051 | Train Loss: 0.2694 Acc: 0.8943 | Val Loss: 0.4694 Acc: 0.8164                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 64, 'cnn_dropout': 0.3468594979348151, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 64, 'learning_rate': 9.185206408743339e-05, 'lstm_dense': 32, 'lstm_hidden_size': 96, 'lstm_layers': 6, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 21.0175 Acc: 0.3699 | Val Loss: 5.1299 Acc: 0.4101                                              \n",
      "Epoch 002 | Train Loss: 9.5600 Acc: 0.4066 | Val Loss: 2.2309 Acc: 0.4597                                               \n",
      "Epoch 003 | Train Loss: 6.7296 Acc: 0.4286 | Val Loss: 1.9277 Acc: 0.5212                                               \n",
      "Epoch 004 | Train Loss: 4.9836 Acc: 0.4603 | Val Loss: 1.2596 Acc: 0.5770                                               \n",
      "Epoch 005 | Train Loss: 3.9208 Acc: 0.4869 | Val Loss: 1.1010 Acc: 0.5916                                               \n",
      "Epoch 006 | Train Loss: 3.1680 Acc: 0.5143 | Val Loss: 1.4330 Acc: 0.6394                                               \n",
      "Epoch 007 | Train Loss: 2.7263 Acc: 0.5294 | Val Loss: 0.9434 Acc: 0.6540                                               \n",
      "Epoch 008 | Train Loss: 2.1936 Acc: 0.5624 | Val Loss: 0.8236 Acc: 0.6713                                               \n",
      "Epoch 009 | Train Loss: 1.9051 Acc: 0.5799 | Val Loss: 1.2321 Acc: 0.6487                                               \n",
      "Epoch 010 | Train Loss: 1.6073 Acc: 0.6105 | Val Loss: 0.8291 Acc: 0.6794                                               \n",
      "Epoch 011 | Train Loss: 1.4472 Acc: 0.6250 | Val Loss: 0.7831 Acc: 0.7012                                               \n",
      "Epoch 012 | Train Loss: 1.3390 Acc: 0.6474 | Val Loss: 1.6569 Acc: 0.6451                                               \n",
      "Epoch 013 | Train Loss: 1.1801 Acc: 0.6615 | Val Loss: 1.7754 Acc: 0.5579                                               \n",
      "Epoch 014 | Train Loss: 1.1072 Acc: 0.6803 | Val Loss: 0.6913 Acc: 0.7585                                               \n",
      "Epoch 015 | Train Loss: 1.0105 Acc: 0.6931 | Val Loss: 0.8406 Acc: 0.6645                                               \n",
      "Epoch 016 | Train Loss: 0.9040 Acc: 0.7165 | Val Loss: 0.6817 Acc: 0.7630                                               \n",
      "Epoch 017 | Train Loss: 0.8299 Acc: 0.7353 | Val Loss: 0.4708 Acc: 0.8143                                               \n",
      "Epoch 018 | Train Loss: 0.7747 Acc: 0.7474 | Val Loss: 0.4480 Acc: 0.8284                                               \n",
      "Epoch 019 | Train Loss: 0.6747 Acc: 0.7740 | Val Loss: 0.5792 Acc: 0.7943                                               \n",
      "Epoch 020 | Train Loss: 0.6138 Acc: 0.7945 | Val Loss: 0.4205 Acc: 0.8615                                               \n",
      "Epoch 021 | Train Loss: 0.5712 Acc: 0.8083 | Val Loss: 0.7844 Acc: 0.7481                                               \n",
      "Epoch 022 | Train Loss: 0.5236 Acc: 0.8195 | Val Loss: 0.4232 Acc: 0.8466                                               \n",
      "Epoch 023 | Train Loss: 0.4791 Acc: 0.8352 | Val Loss: 0.4188 Acc: 0.8579                                               \n",
      "Epoch 024 | Train Loss: 0.4389 Acc: 0.8492 | Val Loss: 0.6125 Acc: 0.7654                                               \n",
      "Epoch 025 | Train Loss: 0.4120 Acc: 0.8619 | Val Loss: 0.3807 Acc: 0.8770                                               \n",
      "Epoch 026 | Train Loss: 0.3681 Acc: 0.8711 | Val Loss: 0.3584 Acc: 0.8633                                               \n",
      "Epoch 027 | Train Loss: 0.3319 Acc: 0.8844 | Val Loss: 0.3139 Acc: 0.8857                                               \n",
      "Epoch 028 | Train Loss: 0.3026 Acc: 0.8974 | Val Loss: 0.3246 Acc: 0.8925                                               \n",
      "Epoch 029 | Train Loss: 0.2899 Acc: 0.9001 | Val Loss: 0.2189 Acc: 0.9260                                               \n",
      "Epoch 030 | Train Loss: 0.2525 Acc: 0.9121 | Val Loss: 0.1980 Acc: 0.9331                                               \n",
      "Epoch 031 | Train Loss: 0.2407 Acc: 0.9172 | Val Loss: 0.2917 Acc: 0.8979                                               \n",
      "Epoch 032 | Train Loss: 0.2009 Acc: 0.9313 | Val Loss: 0.4499 Acc: 0.8690                                               \n",
      "Epoch 033 | Train Loss: 0.2065 Acc: 0.9304 | Val Loss: 0.1724 Acc: 0.9442                                               \n",
      "Epoch 034 | Train Loss: 0.1780 Acc: 0.9384 | Val Loss: 0.2287 Acc: 0.9242                                               \n",
      "Epoch 035 | Train Loss: 0.1719 Acc: 0.9430 | Val Loss: 0.3140 Acc: 0.8997                                               \n",
      "Epoch 036 | Train Loss: 0.1642 Acc: 0.9459 | Val Loss: 0.1490 Acc: 0.9478                                               \n",
      "Epoch 037 | Train Loss: 0.1440 Acc: 0.9521 | Val Loss: 0.1333 Acc: 0.9618                                               \n",
      "Epoch 038 | Train Loss: 0.1421 Acc: 0.9532 | Val Loss: 0.1178 Acc: 0.9642                                               \n",
      "Epoch 039 | Train Loss: 0.1291 Acc: 0.9569 | Val Loss: 0.2194 Acc: 0.9409                                               \n",
      "Epoch 040 | Train Loss: 0.1154 Acc: 0.9612 | Val Loss: 0.2310 Acc: 0.9328                                               \n",
      "Epoch 041 | Train Loss: 0.1122 Acc: 0.9613 | Val Loss: 0.1845 Acc: 0.9385                                               \n",
      "Epoch 042 | Train Loss: 0.0998 Acc: 0.9656 | Val Loss: 0.1065 Acc: 0.9678                                               \n",
      "Epoch 043 | Train Loss: 0.1194 Acc: 0.9630 | Val Loss: 0.1268 Acc: 0.9552                                               \n",
      "Epoch 044 | Train Loss: 0.0894 Acc: 0.9720 | Val Loss: 0.3271 Acc: 0.8904                                               \n",
      "Epoch 045 | Train Loss: 0.0855 Acc: 0.9716 | Val Loss: 0.1238 Acc: 0.9600                                               \n",
      "Epoch 046 | Train Loss: 0.0804 Acc: 0.9723 | Val Loss: 0.1619 Acc: 0.9519                                               \n",
      "Epoch 047 | Train Loss: 0.0852 Acc: 0.9724 | Val Loss: 0.1220 Acc: 0.9612                                               \n",
      "Epoch 048 | Train Loss: 0.0793 Acc: 0.9754 | Val Loss: 0.0917 Acc: 0.9704                                               \n",
      "Epoch 049 | Train Loss: 0.0693 Acc: 0.9769 | Val Loss: 0.0811 Acc: 0.9725                                               \n",
      "Epoch 050 | Train Loss: 0.0659 Acc: 0.9778 | Val Loss: 0.1162 Acc: 0.9627                                               \n",
      "Epoch 051 | Train Loss: 0.0736 Acc: 0.9767 | Val Loss: 0.1424 Acc: 0.9549                                               \n",
      "Epoch 052 | Train Loss: 0.0606 Acc: 0.9791 | Val Loss: 0.1020 Acc: 0.9684                                               \n",
      "Epoch 053 | Train Loss: 0.0511 Acc: 0.9824 | Val Loss: 0.0715 Acc: 0.9806                                               \n",
      "Epoch 054 | Train Loss: 0.0637 Acc: 0.9801 | Val Loss: 0.1349 Acc: 0.9618                                               \n",
      "Epoch 055 | Train Loss: 0.0566 Acc: 0.9816 | Val Loss: 0.1767 Acc: 0.9564                                               \n",
      "Epoch 056 | Train Loss: 0.0526 Acc: 0.9836 | Val Loss: 0.0873 Acc: 0.9758                                               \n",
      "Epoch 057 | Train Loss: 0.0483 Acc: 0.9848 | Val Loss: 0.0879 Acc: 0.9758                                               \n",
      "Epoch 058 | Train Loss: 0.0580 Acc: 0.9836 | Val Loss: 0.1834 Acc: 0.9481                                               \n",
      "Epoch 059 | Train Loss: 0.0487 Acc: 0.9839 | Val Loss: 0.0884 Acc: 0.9749                                               \n",
      "Epoch 060 | Train Loss: 0.0428 Acc: 0.9866 | Val Loss: 0.0896 Acc: 0.9737                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 256, 'cnn_dropout': 0.348672274128797, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 64, 'learning_rate': 7.917156308963146e-05, 'lstm_dense': 64, 'lstm_hidden_size': 96, 'lstm_layers': 6, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 23.3927 Acc: 0.3699 | Val Loss: 7.2042 Acc: 0.3096                                              \n",
      "Epoch 002 | Train Loss: 9.7724 Acc: 0.3964 | Val Loss: 4.0990 Acc: 0.3660                                               \n",
      "Epoch 003 | Train Loss: 6.8272 Acc: 0.4249 | Val Loss: 2.2738 Acc: 0.4872                                               \n",
      "Epoch 004 | Train Loss: 5.1195 Acc: 0.4523 | Val Loss: 2.6006 Acc: 0.4666                                               \n",
      "Epoch 005 | Train Loss: 3.9595 Acc: 0.4845 | Val Loss: 1.7459 Acc: 0.5663                                               \n",
      "Epoch 006 | Train Loss: 3.3378 Acc: 0.5109 | Val Loss: 1.5339 Acc: 0.6104                                               \n",
      "Epoch 007 | Train Loss: 2.7514 Acc: 0.5396 | Val Loss: 0.9565 Acc: 0.6761                                               \n",
      "Epoch 008 | Train Loss: 2.3660 Acc: 0.5668 | Val Loss: 1.0477 Acc: 0.6755                                               \n",
      "Epoch 009 | Train Loss: 2.0782 Acc: 0.5871 | Val Loss: 1.2431 Acc: 0.6319                                               \n",
      "Epoch 010 | Train Loss: 1.7870 Acc: 0.6013 | Val Loss: 1.3403 Acc: 0.6549                                               \n",
      "Epoch 011 | Train Loss: 1.6398 Acc: 0.6253 | Val Loss: 0.9241 Acc: 0.7313                                               \n",
      "Epoch 012 | Train Loss: 1.4533 Acc: 0.6458 | Val Loss: 0.8326 Acc: 0.7752                                               \n",
      "Epoch 013 | Train Loss: 1.2721 Acc: 0.6786 | Val Loss: 0.7803 Acc: 0.7510                                               \n",
      "Epoch 014 | Train Loss: 1.1166 Acc: 0.7092 | Val Loss: 0.8830 Acc: 0.7576                                               \n",
      "Epoch 015 | Train Loss: 1.0067 Acc: 0.7313 | Val Loss: 0.6747 Acc: 0.8113                                               \n",
      "Epoch 016 | Train Loss: 0.9182 Acc: 0.7462 | Val Loss: 0.5987 Acc: 0.8316                                               \n",
      "Epoch 017 | Train Loss: 0.8151 Acc: 0.7650 | Val Loss: 0.9394 Acc: 0.7534                                               \n",
      "Epoch 018 | Train Loss: 0.7740 Acc: 0.7777 | Val Loss: 0.8219 Acc: 0.7997                                               \n",
      "Epoch 019 | Train Loss: 0.6976 Acc: 0.8015 | Val Loss: 0.6353 Acc: 0.8200                                               \n",
      "Epoch 020 | Train Loss: 0.6272 Acc: 0.8167 | Val Loss: 0.5319 Acc: 0.8478                                               \n",
      "Epoch 021 | Train Loss: 0.6018 Acc: 0.8199 | Val Loss: 0.4440 Acc: 0.8615                                               \n",
      "Epoch 022 | Train Loss: 0.5685 Acc: 0.8296 | Val Loss: 0.3681 Acc: 0.8770                                               \n",
      "Epoch 023 | Train Loss: 0.5323 Acc: 0.8417 | Val Loss: 0.3855 Acc: 0.8937                                               \n",
      "Epoch 024 | Train Loss: 0.4641 Acc: 0.8548 | Val Loss: 0.3534 Acc: 0.9009                                               \n",
      "Epoch 025 | Train Loss: 0.4329 Acc: 0.8617 | Val Loss: 0.6200 Acc: 0.8140                                               \n",
      "Epoch 026 | Train Loss: 0.4052 Acc: 0.8745 | Val Loss: 0.3405 Acc: 0.8788                                               \n",
      "Epoch 027 | Train Loss: 0.3481 Acc: 0.8909 | Val Loss: 0.3139 Acc: 0.8737                                               \n",
      "Epoch 028 | Train Loss: 0.3212 Acc: 0.8968 | Val Loss: 0.2285 Acc: 0.9287                                               \n",
      "Epoch 029 | Train Loss: 0.3076 Acc: 0.9031 | Val Loss: 0.2482 Acc: 0.9197                                               \n",
      "Epoch 030 | Train Loss: 0.2918 Acc: 0.9083 | Val Loss: 0.4615 Acc: 0.8430                                               \n",
      "Epoch 031 | Train Loss: 0.2892 Acc: 0.9116 | Val Loss: 0.2364 Acc: 0.9164                                               \n",
      "Epoch 032 | Train Loss: 0.2468 Acc: 0.9245 | Val Loss: 0.4364 Acc: 0.8531                                               \n",
      "Epoch 033 | Train Loss: 0.2415 Acc: 0.9252 | Val Loss: 0.2340 Acc: 0.9122                                               \n",
      "Epoch 034 | Train Loss: 0.2311 Acc: 0.9334 | Val Loss: 0.2714 Acc: 0.9027                                               \n",
      "Epoch 035 | Train Loss: 0.2081 Acc: 0.9361 | Val Loss: 0.2445 Acc: 0.9188                                               \n",
      "Epoch 036 | Train Loss: 0.1918 Acc: 0.9417 | Val Loss: 0.2687 Acc: 0.9048                                               \n",
      "Epoch 037 | Train Loss: 0.1772 Acc: 0.9442 | Val Loss: 0.3835 Acc: 0.9072                                               \n",
      "Epoch 038 | Train Loss: 0.1823 Acc: 0.9450 | Val Loss: 0.4933 Acc: 0.8519                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 64, 'cnn_dropout': 0.08788600806025366, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 64, 'learning_rate': 9.440637778893905e-05, 'lstm_dense': 32, 'lstm_hidden_size': 96, 'lstm_layers': 6, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 15.9616 Acc: 0.3588 | Val Loss: 2.8128 Acc: 0.5699                                              \n",
      "Epoch 002 | Train Loss: 7.0840 Acc: 0.4197 | Val Loss: 2.5452 Acc: 0.4842                                               \n",
      "Epoch 003 | Train Loss: 4.5961 Acc: 0.4614 | Val Loss: 1.4654 Acc: 0.5454                                               \n",
      "Epoch 004 | Train Loss: 3.3935 Acc: 0.4903 | Val Loss: 1.0991 Acc: 0.6248                                               \n",
      "Epoch 005 | Train Loss: 2.6393 Acc: 0.5204 | Val Loss: 1.2502 Acc: 0.6418                                               \n",
      "Epoch 006 | Train Loss: 2.2206 Acc: 0.5541 | Val Loss: 1.5350 Acc: 0.5985                                               \n",
      "Epoch 007 | Train Loss: 1.7490 Acc: 0.5902 | Val Loss: 1.5990 Acc: 0.5669                                               \n",
      "Epoch 008 | Train Loss: 1.4889 Acc: 0.6197 | Val Loss: 0.7853 Acc: 0.7218                                               \n",
      "Epoch 009 | Train Loss: 1.2927 Acc: 0.6545 | Val Loss: 0.7576 Acc: 0.7221                                               \n",
      "Epoch 010 | Train Loss: 1.0816 Acc: 0.6835 | Val Loss: 0.7662 Acc: 0.7451                                               \n",
      "Epoch 011 | Train Loss: 0.9454 Acc: 0.7101 | Val Loss: 0.8211 Acc: 0.7466                                               \n",
      "Epoch 012 | Train Loss: 0.8037 Acc: 0.7403 | Val Loss: 0.6000 Acc: 0.8036                                               \n",
      "Epoch 013 | Train Loss: 0.7480 Acc: 0.7606 | Val Loss: 0.5059 Acc: 0.8278                                               \n",
      "Epoch 014 | Train Loss: 0.6461 Acc: 0.7874 | Val Loss: 0.5107 Acc: 0.8131                                               \n",
      "Epoch 015 | Train Loss: 0.6139 Acc: 0.7980 | Val Loss: 0.5413 Acc: 0.8287                                               \n",
      "Epoch 016 | Train Loss: 0.5554 Acc: 0.8170 | Val Loss: 0.4980 Acc: 0.8233                                               \n",
      "Epoch 017 | Train Loss: 0.4989 Acc: 0.8349 | Val Loss: 0.5073 Acc: 0.8176                                               \n",
      "Epoch 018 | Train Loss: 0.4618 Acc: 0.8474 | Val Loss: 0.5415 Acc: 0.8116                                               \n",
      "Epoch 019 | Train Loss: 0.4344 Acc: 0.8601 | Val Loss: 0.5119 Acc: 0.8349                                               \n",
      "Epoch 020 | Train Loss: 0.3982 Acc: 0.8732 | Val Loss: 0.3285 Acc: 0.8845                                               \n",
      "Epoch 021 | Train Loss: 0.3549 Acc: 0.8891 | Val Loss: 0.3069 Acc: 0.8875                                               \n",
      "Epoch 022 | Train Loss: 0.3422 Acc: 0.8936 | Val Loss: 0.3545 Acc: 0.8734                                               \n",
      "Epoch 023 | Train Loss: 0.3071 Acc: 0.9064 | Val Loss: 0.3426 Acc: 0.8896                                               \n",
      "Epoch 024 | Train Loss: 0.2798 Acc: 0.9134 | Val Loss: 0.3356 Acc: 0.8884                                               \n",
      "Epoch 025 | Train Loss: 0.2528 Acc: 0.9220 | Val Loss: 0.4465 Acc: 0.8328                                               \n",
      "Epoch 026 | Train Loss: 0.2456 Acc: 0.9231 | Val Loss: 0.2343 Acc: 0.9224                                               \n",
      "Epoch 027 | Train Loss: 0.2023 Acc: 0.9363 | Val Loss: 0.2380 Acc: 0.9152                                               \n",
      "Epoch 028 | Train Loss: 0.1976 Acc: 0.9365 | Val Loss: 0.2600 Acc: 0.9063                                               \n",
      "Epoch 029 | Train Loss: 0.1860 Acc: 0.9418 | Val Loss: 0.1964 Acc: 0.9281                                               \n",
      "Epoch 030 | Train Loss: 0.1529 Acc: 0.9498 | Val Loss: 0.2117 Acc: 0.9284                                               \n",
      "Epoch 031 | Train Loss: 0.1391 Acc: 0.9540 | Val Loss: 0.1551 Acc: 0.9472                                               \n",
      "Epoch 032 | Train Loss: 0.1437 Acc: 0.9553 | Val Loss: 0.1399 Acc: 0.9504                                               \n",
      "Epoch 033 | Train Loss: 0.1133 Acc: 0.9643 | Val Loss: 0.2220 Acc: 0.9263                                               \n",
      "Epoch 034 | Train Loss: 0.1239 Acc: 0.9611 | Val Loss: 0.1398 Acc: 0.9573                                               \n",
      "Epoch 035 | Train Loss: 0.1023 Acc: 0.9681 | Val Loss: 0.1275 Acc: 0.9570                                               \n",
      "Epoch 036 | Train Loss: 0.1029 Acc: 0.9687 | Val Loss: 0.1526 Acc: 0.9516                                               \n",
      "Epoch 037 | Train Loss: 0.0924 Acc: 0.9728 | Val Loss: 0.1581 Acc: 0.9496                                               \n",
      "Epoch 038 | Train Loss: 0.0906 Acc: 0.9743 | Val Loss: 0.1362 Acc: 0.9561                                               \n",
      "Epoch 039 | Train Loss: 0.0842 Acc: 0.9743 | Val Loss: 0.1196 Acc: 0.9588                                               \n",
      "Epoch 040 | Train Loss: 0.0799 Acc: 0.9764 | Val Loss: 0.1217 Acc: 0.9645                                               \n",
      "Epoch 041 | Train Loss: 0.0629 Acc: 0.9814 | Val Loss: 0.2381 Acc: 0.9260                                               \n",
      "Epoch 042 | Train Loss: 0.0757 Acc: 0.9770 | Val Loss: 0.1205 Acc: 0.9630                                               \n",
      "Epoch 043 | Train Loss: 0.0647 Acc: 0.9811 | Val Loss: 0.3356 Acc: 0.9116                                               \n",
      "Epoch 044 | Train Loss: 0.0632 Acc: 0.9828 | Val Loss: 0.2218 Acc: 0.9490                                               \n",
      "Epoch 045 | Train Loss: 0.0636 Acc: 0.9816 | Val Loss: 0.1262 Acc: 0.9624                                               \n",
      "Epoch 046 | Train Loss: 0.0548 Acc: 0.9847 | Val Loss: 0.1160 Acc: 0.9666                                               \n",
      "Epoch 047 | Train Loss: 0.0546 Acc: 0.9832 | Val Loss: 0.1164 Acc: 0.9675                                               \n",
      "Epoch 048 | Train Loss: 0.0510 Acc: 0.9846 | Val Loss: 0.2385 Acc: 0.9343                                               \n",
      "Epoch 049 | Train Loss: 0.0471 Acc: 0.9870 | Val Loss: 0.1072 Acc: 0.9699                                               \n",
      "Epoch 050 | Train Loss: 0.0478 Acc: 0.9860 | Val Loss: 0.1075 Acc: 0.9722                                               \n",
      "Epoch 051 | Train Loss: 0.0426 Acc: 0.9886 | Val Loss: 0.1217 Acc: 0.9642                                               \n",
      "Epoch 052 | Train Loss: 0.0359 Acc: 0.9898 | Val Loss: 0.1135 Acc: 0.9710                                               \n",
      "Epoch 053 | Train Loss: 0.0424 Acc: 0.9889 | Val Loss: 0.1848 Acc: 0.9582                                               \n",
      "Epoch 054 | Train Loss: 0.0462 Acc: 0.9878 | Val Loss: 0.1071 Acc: 0.9722                                               \n",
      "Epoch 055 | Train Loss: 0.0525 Acc: 0.9866 | Val Loss: 0.0987 Acc: 0.9749                                               \n",
      "Epoch 056 | Train Loss: 0.0329 Acc: 0.9909 | Val Loss: 0.1004 Acc: 0.9740                                               \n",
      "Epoch 057 | Train Loss: 0.0363 Acc: 0.9894 | Val Loss: 0.1144 Acc: 0.9713                                               \n",
      "Epoch 058 | Train Loss: 0.0350 Acc: 0.9904 | Val Loss: 0.0953 Acc: 0.9746                                               \n",
      "Epoch 059 | Train Loss: 0.0317 Acc: 0.9913 | Val Loss: 0.1201 Acc: 0.9672                                               \n",
      "Epoch 060 | Train Loss: 0.0348 Acc: 0.9901 | Val Loss: 0.1166 Acc: 0.9687                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 128, 'cnn_dropout': 0.16881154259439313, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 64, 'learning_rate': 1.977779399184755e-05, 'lstm_dense': 128, 'lstm_hidden_size': 96, 'lstm_layers': 3, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 59.7237 Acc: 0.3321 | Val Loss: 22.3521 Acc: 0.3125                                             \n",
      "Epoch 002 | Train Loss: 41.1941 Acc: 0.3276 | Val Loss: 12.3300 Acc: 0.3134                                             \n",
      "Epoch 003 | Train Loss: 30.6406 Acc: 0.3354 | Val Loss: 7.2814 Acc: 0.3346                                              \n",
      "Epoch 004 | Train Loss: 24.4870 Acc: 0.3415 | Val Loss: 5.4243 Acc: 0.3310                                              \n",
      "Epoch 005 | Train Loss: 19.5015 Acc: 0.3431 | Val Loss: 4.7726 Acc: 0.3657                                              \n",
      "Epoch 006 | Train Loss: 16.8992 Acc: 0.3539 | Val Loss: 3.4859 Acc: 0.4304                                              \n",
      "Epoch 007 | Train Loss: 14.5680 Acc: 0.3719 | Val Loss: 3.3035 Acc: 0.4343                                              \n",
      "Epoch 008 | Train Loss: 13.1403 Acc: 0.3786 | Val Loss: 2.7415 Acc: 0.4690                                              \n",
      "Epoch 009 | Train Loss: 12.1870 Acc: 0.3853 | Val Loss: 2.6428 Acc: 0.4845                                              \n",
      "Epoch 010 | Train Loss: 10.9680 Acc: 0.3907 | Val Loss: 2.5962 Acc: 0.4507                                              \n",
      "Epoch 011 | Train Loss: 9.9582 Acc: 0.3957 | Val Loss: 2.0960 Acc: 0.5191                                               \n",
      "Epoch 012 | Train Loss: 9.0879 Acc: 0.4064 | Val Loss: 1.9443 Acc: 0.5406                                               \n",
      "Epoch 013 | Train Loss: 8.3665 Acc: 0.4140 | Val Loss: 1.9147 Acc: 0.5072                                               \n",
      "Epoch 014 | Train Loss: 7.8648 Acc: 0.4208 | Val Loss: 2.1691 Acc: 0.5421                                               \n",
      "Epoch 015 | Train Loss: 7.3643 Acc: 0.4291 | Val Loss: 1.6704 Acc: 0.5734                                               \n",
      "Epoch 016 | Train Loss: 6.8721 Acc: 0.4311 | Val Loss: 1.7389 Acc: 0.5603                                               \n",
      "Epoch 017 | Train Loss: 6.1269 Acc: 0.4473 | Val Loss: 1.9274 Acc: 0.5675                                               \n",
      "Epoch 018 | Train Loss: 5.8912 Acc: 0.4496 | Val Loss: 1.7234 Acc: 0.5645                                               \n",
      "Epoch 019 | Train Loss: 5.7826 Acc: 0.4553 | Val Loss: 1.6897 Acc: 0.5761                                               \n",
      "Epoch 020 | Train Loss: 5.3951 Acc: 0.4602 | Val Loss: 1.7066 Acc: 0.5821                                               \n",
      "Epoch 021 | Train Loss: 5.1178 Acc: 0.4615 | Val Loss: 1.5391 Acc: 0.6087                                               \n",
      "Epoch 022 | Train Loss: 4.6942 Acc: 0.4720 | Val Loss: 1.9248 Acc: 0.5128                                               \n",
      "Epoch 023 | Train Loss: 4.5375 Acc: 0.4791 | Val Loss: 1.9536 Acc: 0.5558                                               \n",
      "Epoch 024 | Train Loss: 4.3661 Acc: 0.4792 | Val Loss: 1.5794 Acc: 0.6137                                               \n",
      "Epoch 025 | Train Loss: 4.0931 Acc: 0.4938 | Val Loss: 1.6633 Acc: 0.6173                                               \n",
      "Epoch 026 | Train Loss: 3.8613 Acc: 0.5018 | Val Loss: 1.6166 Acc: 0.6290                                               \n",
      "Epoch 027 | Train Loss: 3.5906 Acc: 0.5185 | Val Loss: 1.6039 Acc: 0.6304                                               \n",
      "Epoch 028 | Train Loss: 3.4399 Acc: 0.5229 | Val Loss: 1.7949 Acc: 0.6110                                               \n",
      "Epoch 029 | Train Loss: 3.3556 Acc: 0.5201 | Val Loss: 1.8711 Acc: 0.6110                                               \n",
      "Epoch 030 | Train Loss: 3.2112 Acc: 0.5268 | Val Loss: 1.5964 Acc: 0.6552                                               \n",
      "Epoch 031 | Train Loss: 2.9695 Acc: 0.5455 | Val Loss: 1.3197 Acc: 0.6985                                               \n",
      "Epoch 032 | Train Loss: 2.9571 Acc: 0.5415 | Val Loss: 1.5191 Acc: 0.6567                                               \n",
      "Epoch 033 | Train Loss: 2.7579 Acc: 0.5528 | Val Loss: 1.4637 Acc: 0.6519                                               \n",
      "Epoch 034 | Train Loss: 2.6161 Acc: 0.5614 | Val Loss: 1.2207 Acc: 0.6890                                               \n",
      "Epoch 035 | Train Loss: 2.6333 Acc: 0.5625 | Val Loss: 1.3737 Acc: 0.6869                                               \n",
      "Epoch 036 | Train Loss: 2.5496 Acc: 0.5628 | Val Loss: 1.2061 Acc: 0.6439                                               \n",
      "Epoch 037 | Train Loss: 2.4587 Acc: 0.5703 | Val Loss: 1.4405 Acc: 0.6528                                               \n",
      "Epoch 038 | Train Loss: 2.3596 Acc: 0.5717 | Val Loss: 1.4571 Acc: 0.6621                                               \n",
      "Epoch 039 | Train Loss: 2.2731 Acc: 0.5752 | Val Loss: 1.1193 Acc: 0.6696                                               \n",
      "Epoch 040 | Train Loss: 2.2532 Acc: 0.5871 | Val Loss: 1.3062 Acc: 0.6773                                               \n",
      "Epoch 041 | Train Loss: 2.0653 Acc: 0.5861 | Val Loss: 1.5840 Acc: 0.6066                                               \n",
      "Epoch 021 | Train Loss: 0.5525 Acc: 0.8207 | Val Loss: 0.4337 Acc: 0.8543                                               \n",
      "Epoch 022 | Train Loss: 0.5129 Acc: 0.8381 | Val Loss: 0.3973 Acc: 0.8707                                               \n",
      "Epoch 023 | Train Loss: 0.4674 Acc: 0.8454 | Val Loss: 0.3776 Acc: 0.8624                                               \n",
      "Epoch 024 | Train Loss: 0.4287 Acc: 0.8590 | Val Loss: 0.3852 Acc: 0.8827                                               \n",
      "Epoch 025 | Train Loss: 0.3966 Acc: 0.8683 | Val Loss: 0.3631 Acc: 0.8737                                               \n",
      "Epoch 026 | Train Loss: 0.3778 Acc: 0.8772 | Val Loss: 0.3471 Acc: 0.8818                                               \n",
      "Epoch 027 | Train Loss: 0.3384 Acc: 0.8906 | Val Loss: 0.3175 Acc: 0.8893                                               \n",
      "Epoch 028 | Train Loss: 0.2991 Acc: 0.8995 | Val Loss: 0.3715 Acc: 0.8788                                               \n",
      "Epoch 029 | Train Loss: 0.2726 Acc: 0.9145 | Val Loss: 0.1908 Acc: 0.9379                                               \n",
      "Epoch 030 | Train Loss: 0.2379 Acc: 0.9201 | Val Loss: 0.1649 Acc: 0.9418                                               \n",
      "Epoch 031 | Train Loss: 0.2195 Acc: 0.9299 | Val Loss: 0.2037 Acc: 0.9361                                               \n",
      "Epoch 032 | Train Loss: 0.1963 Acc: 0.9359 | Val Loss: 0.1791 Acc: 0.9394                                               \n",
      "Epoch 033 | Train Loss: 0.1778 Acc: 0.9408 | Val Loss: 0.1545 Acc: 0.9504                                               \n",
      "Epoch 034 | Train Loss: 0.1707 Acc: 0.9467 | Val Loss: 0.3740 Acc: 0.8767                                               \n",
      "Epoch 035 | Train Loss: 0.1639 Acc: 0.9460 | Val Loss: 0.1998 Acc: 0.9307                                               \n",
      "Epoch 036 | Train Loss: 0.1482 Acc: 0.9504 | Val Loss: 0.1766 Acc: 0.9394                                               \n",
      "Epoch 037 | Train Loss: 0.1326 Acc: 0.9566 | Val Loss: 0.2458 Acc: 0.9101                                               \n",
      "Epoch 038 | Train Loss: 0.1276 Acc: 0.9595 | Val Loss: 0.1619 Acc: 0.9397                                               \n",
      "Epoch 039 | Train Loss: 0.1120 Acc: 0.9646 | Val Loss: 0.1403 Acc: 0.9573                                               \n",
      "Epoch 040 | Train Loss: 0.1088 Acc: 0.9651 | Val Loss: 0.1350 Acc: 0.9499                                               \n",
      "Epoch 041 | Train Loss: 0.0899 Acc: 0.9722 | Val Loss: 0.2116 Acc: 0.9287                                               \n",
      "Epoch 042 | Train Loss: 0.0827 Acc: 0.9735 | Val Loss: 0.1814 Acc: 0.9307                                               \n",
      "Epoch 043 | Train Loss: 0.0842 Acc: 0.9740 | Val Loss: 0.1217 Acc: 0.9612                                               \n",
      "Epoch 044 | Train Loss: 0.0702 Acc: 0.9788 | Val Loss: 0.1047 Acc: 0.9624                                               \n",
      "Epoch 045 | Train Loss: 0.0650 Acc: 0.9798 | Val Loss: 0.1102 Acc: 0.9645                                               \n",
      "Epoch 046 | Train Loss: 0.0790 Acc: 0.9766 | Val Loss: 0.1612 Acc: 0.9475                                               \n",
      "Epoch 047 | Train Loss: 0.0559 Acc: 0.9824 | Val Loss: 0.1239 Acc: 0.9603                                               \n",
      "Epoch 048 | Train Loss: 0.0777 Acc: 0.9787 | Val Loss: 0.0897 Acc: 0.9755                                               \n",
      "Epoch 049 | Train Loss: 0.0487 Acc: 0.9847 | Val Loss: 0.0856 Acc: 0.9678                                               \n",
      "Epoch 050 | Train Loss: 0.0462 Acc: 0.9865 | Val Loss: 0.1188 Acc: 0.9624                                               \n",
      "Epoch 051 | Train Loss: 0.0423 Acc: 0.9873 | Val Loss: 0.0728 Acc: 0.9743                                               \n",
      "Epoch 052 | Train Loss: 0.0475 Acc: 0.9843 | Val Loss: 0.1071 Acc: 0.9597                                               \n",
      "Epoch 053 | Train Loss: 0.0436 Acc: 0.9861 | Val Loss: 0.0852 Acc: 0.9722                                               \n",
      "Epoch 054 | Train Loss: 0.0421 Acc: 0.9874 | Val Loss: 0.1078 Acc: 0.9701                                               \n",
      "Epoch 055 | Train Loss: 0.0389 Acc: 0.9876 | Val Loss: 0.1197 Acc: 0.9630                                               \n",
      "Epoch 056 | Train Loss: 0.0377 Acc: 0.9890 | Val Loss: 0.1006 Acc: 0.9678                                               \n",
      "Epoch 057 | Train Loss: 0.0321 Acc: 0.9903 | Val Loss: 0.1266 Acc: 0.9615                                               \n",
      "Epoch 058 | Train Loss: 0.0544 Acc: 0.9858 | Val Loss: 0.0761 Acc: 0.9782                                               \n",
      "Epoch 059 | Train Loss: 0.0301 Acc: 0.9905 | Val Loss: 0.0953 Acc: 0.9693                                               \n",
      "Epoch 060 | Train Loss: 0.0277 Acc: 0.9910 | Val Loss: 0.1428 Acc: 0.9573                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 256, 'cnn_dropout': 0.045584219537671944, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 64, 'learning_rate': 0.00042988239625970195, 'lstm_dense': 32, 'lstm_hidden_size': 96, 'lstm_layers': 6, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 17.9532 Acc: 0.4190 | Val Loss: 4.4910 Acc: 0.4991                                              \n",
      "Epoch 002 | Train Loss: 6.5734 Acc: 0.4668 | Val Loss: 4.3110 Acc: 0.4940                                               \n",
      "Epoch 003 | Train Loss: 4.2060 Acc: 0.5019 | Val Loss: 5.6975 Acc: 0.5415                                               \n",
      "Epoch 004 | Train Loss: 3.0355 Acc: 0.5151 | Val Loss: 1.5776 Acc: 0.5281                                               \n",
      "Epoch 005 | Train Loss: 1.9899 Acc: 0.5514 | Val Loss: 2.6855 Acc: 0.5678                                               \n",
      "Epoch 006 | Train Loss: 1.4746 Acc: 0.5744 | Val Loss: 3.0732 Acc: 0.5081                                               \n",
      "Epoch 007 | Train Loss: 1.1527 Acc: 0.5899 | Val Loss: 0.8692 Acc: 0.6842                                               \n",
      "Epoch 008 | Train Loss: 0.8829 Acc: 0.6289 | Val Loss: 0.7145 Acc: 0.6934                                               \n",
      "Epoch 009 | Train Loss: 0.7877 Acc: 0.6612 | Val Loss: 0.8015 Acc: 0.6734                                               \n",
      "Epoch 010 | Train Loss: 0.7172 Acc: 0.6904 | Val Loss: 0.6588 Acc: 0.6907                                               \n",
      "Epoch 011 | Train Loss: 0.6734 Acc: 0.7038 | Val Loss: 0.5252 Acc: 0.8009                                               \n",
      "Epoch 012 | Train Loss: 0.6343 Acc: 0.7168 | Val Loss: 0.5280 Acc: 0.7475                                               \n",
      "Epoch 013 | Train Loss: 0.5937 Acc: 0.7394 | Val Loss: 0.5187 Acc: 0.7845                                               \n",
      "Epoch 014 | Train Loss: 0.5740 Acc: 0.7545 | Val Loss: 0.4135 Acc: 0.8379                                               \n",
      "Epoch 015 | Train Loss: 0.5397 Acc: 0.7683 | Val Loss: 0.4045 Acc: 0.8319                                               \n",
      "Epoch 016 | Train Loss: 0.5056 Acc: 0.7816 | Val Loss: 0.6470 Acc: 0.6812                                               \n",
      "Epoch 017 | Train Loss: 0.4968 Acc: 0.7902 | Val Loss: 0.3679 Acc: 0.8448                                               \n",
      "Epoch 018 | Train Loss: 0.4818 Acc: 0.8010 | Val Loss: 0.4097 Acc: 0.8134                                               \n",
      "Epoch 019 | Train Loss: 0.4587 Acc: 0.8124 | Val Loss: 0.6618 Acc: 0.7451                                               \n",
      "Epoch 020 | Train Loss: 0.4617 Acc: 0.8123 | Val Loss: 0.4783 Acc: 0.7469                                               \n",
      "Epoch 021 | Train Loss: 0.4416 Acc: 0.8213 | Val Loss: 0.3811 Acc: 0.8388                                               \n",
      "Epoch 022 | Train Loss: 0.4209 Acc: 0.8327 | Val Loss: 0.3837 Acc: 0.8391                                               \n",
      "Epoch 023 | Train Loss: 0.4108 Acc: 0.8346 | Val Loss: 0.4441 Acc: 0.8033                                               \n",
      "Epoch 024 | Train Loss: 0.4027 Acc: 0.8376 | Val Loss: 0.4586 Acc: 0.7773                                               \n",
      "Epoch 025 | Train Loss: 0.3972 Acc: 0.8426 | Val Loss: 0.4109 Acc: 0.8358                                               \n",
      "Epoch 026 | Train Loss: 0.3860 Acc: 0.8482 | Val Loss: 0.3829 Acc: 0.8373                                               \n",
      "Epoch 027 | Train Loss: 0.3788 Acc: 0.8542 | Val Loss: 0.3497 Acc: 0.8540                                               \n",
      "Epoch 028 | Train Loss: 0.3727 Acc: 0.8572 | Val Loss: 0.3918 Acc: 0.8594                                               \n",
      "Epoch 029 | Train Loss: 0.3591 Acc: 0.8575 | Val Loss: 0.3552 Acc: 0.8675                                               \n",
      "Epoch 030 | Train Loss: 0.3725 Acc: 0.8580 | Val Loss: 0.4114 Acc: 0.8167                                               \n",
      "Epoch 031 | Train Loss: 0.3598 Acc: 0.8601 | Val Loss: 0.2845 Acc: 0.8860                                               \n",
      "Epoch 032 | Train Loss: 0.3515 Acc: 0.8664 | Val Loss: 0.2633 Acc: 0.9042                                               \n",
      "Epoch 033 | Train Loss: 0.3430 Acc: 0.8673 | Val Loss: 0.4095 Acc: 0.8060                                               \n",
      "Epoch 034 | Train Loss: 0.3348 Acc: 0.8721 | Val Loss: 0.3339 Acc: 0.8567                                               \n",
      "Epoch 035 | Train Loss: 0.3246 Acc: 0.8733 | Val Loss: 0.3793 Acc: 0.8588                                               \n",
      "Epoch 036 | Train Loss: 0.3350 Acc: 0.8701 | Val Loss: 0.4890 Acc: 0.7603                                               \n",
      "Epoch 037 | Train Loss: 0.3095 Acc: 0.8804 | Val Loss: 0.3311 Acc: 0.8466                                               \n",
      "Epoch 038 | Train Loss: 0.3141 Acc: 0.8771 | Val Loss: 0.3531 Acc: 0.8618                                               \n",
      "Epoch 039 | Train Loss: 0.3326 Acc: 0.8705 | Val Loss: 0.2837 Acc: 0.8943                                               \n",
      "Epoch 040 | Train Loss: 0.3104 Acc: 0.8776 | Val Loss: 0.2662 Acc: 0.8824                                               \n",
      "Epoch 041 | Train Loss: 0.3096 Acc: 0.8777 | Val Loss: 0.2622 Acc: 0.9021                                               \n",
      "Epoch 042 | Train Loss: 0.3065 Acc: 0.8827 | Val Loss: 0.3579 Acc: 0.8322                                               \n",
      "Epoch 043 | Train Loss: 0.2985 Acc: 0.8856 | Val Loss: 0.3035 Acc: 0.8773                                               \n",
      "Epoch 044 | Train Loss: 0.2910 Acc: 0.8881 | Val Loss: 0.3184 Acc: 0.8684                                               \n",
      "Epoch 045 | Train Loss: 0.2896 Acc: 0.8873 | Val Loss: 0.4169 Acc: 0.8045                                               \n",
      "Epoch 046 | Train Loss: 0.2903 Acc: 0.8865 | Val Loss: 0.3741 Acc: 0.8642                                               \n",
      "Epoch 047 | Train Loss: 0.2974 Acc: 0.8842 | Val Loss: 0.4378 Acc: 0.8549                                               \n",
      "Epoch 048 | Train Loss: 0.2867 Acc: 0.8869 | Val Loss: 0.3742 Acc: 0.8394                                               \n",
      "Epoch 049 | Train Loss: 0.2706 Acc: 0.8911 | Val Loss: 0.2882 Acc: 0.8878                                               \n",
      "Epoch 050 | Train Loss: 0.2840 Acc: 0.8900 | Val Loss: 0.3616 Acc: 0.8481                                               \n",
      "Epoch 051 | Train Loss: 0.2789 Acc: 0.8912 | Val Loss: 0.2644 Acc: 0.8904                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 64, 'cnn_dropout': 0.18459768081350192, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 64, 'learning_rate': 1.6075750494174745e-05, 'lstm_dense': 128, 'lstm_hidden_size': 96, 'lstm_layers': 4, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 125.5552 Acc: 0.1718 | Val Loss: 74.2567 Acc: 0.1973                                            \n",
      "Epoch 002 | Train Loss: 67.4346 Acc: 0.1681 | Val Loss: 27.8152 Acc: 0.2042                                             \n",
      "Epoch 003 | Train Loss: 36.2326 Acc: 0.2423 | Val Loss: 15.1194 Acc: 0.4322                                             \n",
      "Epoch 004 | Train Loss: 25.9527 Acc: 0.3083 | Val Loss: 14.0706 Acc: 0.4328                                             \n",
      "Epoch 005 | Train Loss: 20.6685 Acc: 0.3260 | Val Loss: 10.8441 Acc: 0.4272                                             \n",
      "Epoch 006 | Train Loss: 16.4154 Acc: 0.3485 | Val Loss: 7.7404 Acc: 0.4340                                              \n",
      "Epoch 007 | Train Loss: 13.4582 Acc: 0.3561 | Val Loss: 6.6501 Acc: 0.4349                                              \n",
      "Epoch 008 | Train Loss: 10.8381 Acc: 0.3844 | Val Loss: 5.0565 Acc: 0.4454                                              \n",
      "Epoch 009 | Train Loss: 9.3979 Acc: 0.3848 | Val Loss: 4.2260 Acc: 0.4540                                               \n",
      "Epoch 010 | Train Loss: 8.1352 Acc: 0.3998 | Val Loss: 3.0527 Acc: 0.4919                                               \n",
      "Epoch 011 | Train Loss: 7.1740 Acc: 0.4078 | Val Loss: 2.1479 Acc: 0.5445                                               \n",
      "Epoch 012 | Train Loss: 6.2174 Acc: 0.4122 | Val Loss: 2.0836 Acc: 0.5087                                               \n",
      "Epoch 013 | Train Loss: 5.5120 Acc: 0.4317 | Val Loss: 1.6250 Acc: 0.5358                                               \n",
      "Epoch 014 | Train Loss: 4.9621 Acc: 0.4332 | Val Loss: 1.5096 Acc: 0.5397                                               \n",
      "Epoch 015 | Train Loss: 4.6446 Acc: 0.4376 | Val Loss: 1.4409 Acc: 0.5454                                               \n",
      "Epoch 016 | Train Loss: 4.1764 Acc: 0.4426 | Val Loss: 1.3197 Acc: 0.5567                                               \n",
      "Epoch 017 | Train Loss: 3.9266 Acc: 0.4477 | Val Loss: 1.1330 Acc: 0.5958                                               \n",
      "Epoch 018 | Train Loss: 3.5842 Acc: 0.4525 | Val Loss: 1.0436 Acc: 0.6325                                               \n",
      "Epoch 019 | Train Loss: 3.3610 Acc: 0.4544 | Val Loss: 0.9925 Acc: 0.5910                                               \n",
      "Epoch 020 | Train Loss: 3.0412 Acc: 0.4741 | Val Loss: 0.9513 Acc: 0.6137                                               \n",
      "Epoch 021 | Train Loss: 2.9148 Acc: 0.4800 | Val Loss: 0.9020 Acc: 0.6197                                               \n",
      "Epoch 022 | Train Loss: 2.7257 Acc: 0.4898 | Val Loss: 0.9235 Acc: 0.6358                                               \n",
      "Epoch 023 | Train Loss: 2.5062 Acc: 0.4924 | Val Loss: 0.9143 Acc: 0.6245                                               \n",
      "Epoch 024 | Train Loss: 2.3926 Acc: 0.5003 | Val Loss: 0.8740 Acc: 0.6516                                               \n",
      "Epoch 025 | Train Loss: 2.3245 Acc: 0.5017 | Val Loss: 0.8582 Acc: 0.6621                                               \n",
      "Epoch 026 | Train Loss: 2.1802 Acc: 0.5065 | Val Loss: 0.8756 Acc: 0.6066                                               \n",
      "Epoch 027 | Train Loss: 2.0551 Acc: 0.5116 | Val Loss: 0.8216 Acc: 0.6872                                               \n",
      "Epoch 028 | Train Loss: 1.9422 Acc: 0.5227 | Val Loss: 0.8010 Acc: 0.6809                                               \n",
      "Epoch 029 | Train Loss: 1.9037 Acc: 0.5221 | Val Loss: 0.7895 Acc: 0.6967                                               \n",
      "Epoch 030 | Train Loss: 1.7985 Acc: 0.5361 | Val Loss: 0.8151 Acc: 0.6827                                               \n",
      "Epoch 031 | Train Loss: 1.7095 Acc: 0.5292 | Val Loss: 0.8564 Acc: 0.6493                                               \n",
      "Epoch 032 | Train Loss: 1.6221 Acc: 0.5433 | Val Loss: 0.7920 Acc: 0.7024                                               \n",
      "Epoch 033 | Train Loss: 1.5796 Acc: 0.5498 | Val Loss: 0.8033 Acc: 0.6922                                               \n",
      "Epoch 034 | Train Loss: 1.4992 Acc: 0.5559 | Val Loss: 0.8016 Acc: 0.6690                                               \n",
      "Epoch 035 | Train Loss: 1.4823 Acc: 0.5598 | Val Loss: 0.8261 Acc: 0.6439                                               \n",
      "Epoch 036 | Train Loss: 1.3999 Acc: 0.5663 | Val Loss: 0.7893 Acc: 0.6618                                               \n",
      "Epoch 037 | Train Loss: 1.3578 Acc: 0.5719 | Val Loss: 0.7241 Acc: 0.6779                                               \n",
      "Epoch 038 | Train Loss: 1.3541 Acc: 0.5759 | Val Loss: 0.8019 Acc: 0.6570                                               \n",
      "Epoch 039 | Train Loss: 1.3279 Acc: 0.5756 | Val Loss: 0.7815 Acc: 0.6740                                               \n",
      "Epoch 040 | Train Loss: 1.2710 Acc: 0.5888 | Val Loss: 0.8011 Acc: 0.6573                                               \n",
      "Epoch 041 | Train Loss: 1.2516 Acc: 0.5886 | Val Loss: 0.8187 Acc: 0.6964                                               \n",
      "Epoch 042 | Train Loss: 1.1907 Acc: 0.5956 | Val Loss: 0.7095 Acc: 0.6734                                               \n",
      "Epoch 043 | Train Loss: 1.1485 Acc: 0.6036 | Val Loss: 0.6996 Acc: 0.6800                                               \n",
      "Epoch 044 | Train Loss: 1.1400 Acc: 0.6041 | Val Loss: 0.7551 Acc: 0.6716                                               \n",
      "Epoch 045 | Train Loss: 1.1420 Acc: 0.6080 | Val Loss: 0.6677 Acc: 0.6827                                               \n",
      "Epoch 046 | Train Loss: 1.0989 Acc: 0.6167 | Val Loss: 0.7302 Acc: 0.6618                                               \n",
      "Epoch 047 | Train Loss: 1.0837 Acc: 0.6164 | Val Loss: 0.7145 Acc: 0.6842                                               \n",
      "Epoch 048 | Train Loss: 1.0655 Acc: 0.6225 | Val Loss: 0.8354 Acc: 0.6701                                               \n",
      "Epoch 049 | Train Loss: 1.0340 Acc: 0.6272 | Val Loss: 0.8262 Acc: 0.6725                                               \n",
      "Epoch 050 | Train Loss: 1.0326 Acc: 0.6267 | Val Loss: 0.7224 Acc: 0.6567                                               \n",
      "Epoch 051 | Train Loss: 1.0024 Acc: 0.6350 | Val Loss: 0.7927 Acc: 0.6791                                               \n",
      "Epoch 052 | Train Loss: 1.0029 Acc: 0.6347 | Val Loss: 0.8484 Acc: 0.6690                                               \n",
      "Epoch 053 | Train Loss: 0.9808 Acc: 0.6414 | Val Loss: 0.7337 Acc: 0.7036                                               \n",
      "Epoch 054 | Train Loss: 0.9825 Acc: 0.6380 | Val Loss: 0.7722 Acc: 0.7003                                               \n",
      "Epoch 055 | Train Loss: 0.9457 Acc: 0.6494 | Val Loss: 0.7078 Acc: 0.7015                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 36, 'cnn_dense': 64, 'cnn_dropout': 0.16861194829299583, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 64, 'cnn_kernels_2': 64, 'learning_rate': 6.279523109709984e-05, 'lstm_dense': 256, 'lstm_hidden_size': 96, 'lstm_layers': 3, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 34.9385 Acc: 0.3301 | Val Loss: 11.2192 Acc: 0.4391                                             \n",
      "Epoch 002 | Train Loss: 14.1709 Acc: 0.3713 | Val Loss: 3.9241 Acc: 0.4063                                              \n",
      "Epoch 003 | Train Loss: 8.5563 Acc: 0.3928 | Val Loss: 2.3493 Acc: 0.5078                                               \n",
      "Epoch 004 | Train Loss: 6.0455 Acc: 0.4172 | Val Loss: 4.7600 Acc: 0.4066                                               \n",
      "Epoch 005 | Train Loss: 4.5810 Acc: 0.4465 | Val Loss: 1.4980 Acc: 0.4934                                               \n",
      "Epoch 006 | Train Loss: 3.7508 Acc: 0.4670 | Val Loss: 1.5718 Acc: 0.5555                                               \n",
      "Epoch 007 | Train Loss: 2.9963 Acc: 0.4943 | Val Loss: 2.1412 Acc: 0.5212                                               \n",
      "Epoch 008 | Train Loss: 2.5099 Acc: 0.5065 | Val Loss: 1.1764 Acc: 0.6287                                               \n",
      "Epoch 009 | Train Loss: 2.1887 Acc: 0.5302 | Val Loss: 1.2586 Acc: 0.6466                                               \n",
      "Epoch 010 | Train Loss: 1.8113 Acc: 0.5633 | Val Loss: 1.0344 Acc: 0.6818                                               \n",
      "Epoch 011 | Train Loss: 1.6168 Acc: 0.5880 | Val Loss: 0.9732 Acc: 0.7131                                               \n",
      "Epoch 012 | Train Loss: 1.3680 Acc: 0.6276 | Val Loss: 0.7500 Acc: 0.7633                                               \n",
      "Epoch 013 | Train Loss: 1.1884 Acc: 0.6650 | Val Loss: 0.7582 Acc: 0.7609                                               \n",
      "Epoch 014 | Train Loss: 1.0582 Acc: 0.6964 | Val Loss: 0.7853 Acc: 0.7642                                               \n",
      "Epoch 015 | Train Loss: 0.9103 Acc: 0.7288 | Val Loss: 0.9402 Acc: 0.6860                                               \n",
      "Epoch 016 | Train Loss: 0.8098 Acc: 0.7548 | Val Loss: 0.5369 Acc: 0.8388                                               \n",
      "Epoch 017 | Train Loss: 0.7414 Acc: 0.7705 | Val Loss: 0.7599 Acc: 0.7516                                               \n",
      "Epoch 018 | Train Loss: 0.6501 Acc: 0.7972 | Val Loss: 0.5751 Acc: 0.8078                                               \n",
      "Epoch 019 | Train Loss: 0.5719 Acc: 0.8213 | Val Loss: 0.4160 Acc: 0.8513                                               \n",
      "Epoch 020 | Train Loss: 0.5124 Acc: 0.8387 | Val Loss: 0.5541 Acc: 0.8137                                               \n",
      "Epoch 021 | Train Loss: 0.4629 Acc: 0.8547 | Val Loss: 0.7989 Acc: 0.7528                                               \n",
      "Epoch 022 | Train Loss: 0.4058 Acc: 0.8680 | Val Loss: 0.2709 Acc: 0.9107                                               \n",
      "Epoch 023 | Train Loss: 0.3653 Acc: 0.8773 | Val Loss: 0.3468 Acc: 0.8782                                               \n",
      "Epoch 024 | Train Loss: 0.3223 Acc: 0.8951 | Val Loss: 0.2450 Acc: 0.9182                                               \n",
      "Epoch 025 | Train Loss: 0.2842 Acc: 0.9051 | Val Loss: 0.2501 Acc: 0.9170                                               \n",
      "Epoch 026 | Train Loss: 0.2714 Acc: 0.9104 | Val Loss: 0.3671 Acc: 0.8731                                               \n",
      "Epoch 027 | Train Loss: 0.2424 Acc: 0.9187 | Val Loss: 0.2102 Acc: 0.9328                                               \n",
      "Epoch 028 | Train Loss: 0.2273 Acc: 0.9249 | Val Loss: 0.2771 Acc: 0.9078                                               \n",
      "Epoch 029 | Train Loss: 0.2016 Acc: 0.9326 | Val Loss: 0.2829 Acc: 0.9104                                               \n",
      "Epoch 030 | Train Loss: 0.1842 Acc: 0.9397 | Val Loss: 0.1835 Acc: 0.9349                                               \n",
      "Epoch 031 | Train Loss: 0.1648 Acc: 0.9440 | Val Loss: 0.2032 Acc: 0.9343                                               \n",
      "Epoch 032 | Train Loss: 0.1554 Acc: 0.9489 | Val Loss: 0.3325 Acc: 0.8925                                               \n",
      "Epoch 033 | Train Loss: 0.1527 Acc: 0.9488 | Val Loss: 0.1377 Acc: 0.9585                                               \n",
      "Epoch 034 | Train Loss: 0.1287 Acc: 0.9576 | Val Loss: 0.2968 Acc: 0.9039                                               \n",
      "Epoch 035 | Train Loss: 0.1243 Acc: 0.9587 | Val Loss: 0.1573 Acc: 0.9501                                               \n",
      "Epoch 036 | Train Loss: 0.1211 Acc: 0.9601 | Val Loss: 0.1387 Acc: 0.9546                                               \n",
      "Epoch 037 | Train Loss: 0.1142 Acc: 0.9633 | Val Loss: 0.1172 Acc: 0.9621                                               \n",
      "Epoch 038 | Train Loss: 0.1028 Acc: 0.9651 | Val Loss: 0.1248 Acc: 0.9576                                               \n",
      "Epoch 039 | Train Loss: 0.0975 Acc: 0.9691 | Val Loss: 0.1089 Acc: 0.9648                                               \n",
      "Epoch 040 | Train Loss: 0.0927 Acc: 0.9694 | Val Loss: 0.1343 Acc: 0.9552                                               \n",
      "Epoch 041 | Train Loss: 0.0870 Acc: 0.9704 | Val Loss: 0.0903 Acc: 0.9710                                               \n",
      "Epoch 042 | Train Loss: 0.0780 Acc: 0.9745 | Val Loss: 0.1226 Acc: 0.9618                                               \n",
      "Epoch 043 | Train Loss: 0.0778 Acc: 0.9751 | Val Loss: 0.1948 Acc: 0.9376                                               \n",
      "Epoch 044 | Train Loss: 0.0637 Acc: 0.9792 | Val Loss: 0.0940 Acc: 0.9722                                               \n",
      "Epoch 045 | Train Loss: 0.0678 Acc: 0.9781 | Val Loss: 0.0910 Acc: 0.9713                                               \n",
      "Epoch 046 | Train Loss: 0.0675 Acc: 0.9772 | Val Loss: 0.1249 Acc: 0.9573                                               \n",
      "Epoch 047 | Train Loss: 0.0623 Acc: 0.9798 | Val Loss: 0.1064 Acc: 0.9660                                               \n",
      "Epoch 048 | Train Loss: 0.0631 Acc: 0.9797 | Val Loss: 0.1105 Acc: 0.9645                                               \n",
      "Epoch 049 | Train Loss: 0.0573 Acc: 0.9808 | Val Loss: 0.1348 Acc: 0.9588                                               \n",
      "Epoch 050 | Train Loss: 0.0518 Acc: 0.9842 | Val Loss: 0.1303 Acc: 0.9576                                               \n",
      "Epoch 051 | Train Loss: 0.0542 Acc: 0.9826 | Val Loss: 0.1599 Acc: 0.9507                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "100%|███████████████████████████████████████████████| 30/30 [26:30<00:00, 53.03s/trial, best loss: 0.040669322038875584]\n",
      "Best hyperparameters: {'batch_size': np.int64(4), 'cnn_dense': np.int64(3), 'cnn_dropout': np.float64(0.10645755494528952), 'cnn_kernel_size_1': np.int64(1), 'cnn_kernel_size_2': np.int64(1), 'cnn_kernels_1': np.int64(2), 'cnn_kernels_2': np.int64(2), 'learning_rate': np.float64(0.0002269292511272618), 'lstm_dense': np.int64(2), 'lstm_hidden_size': np.int64(2), 'lstm_layers': np.int64(0), 'optimizer': np.int64(1)}\n",
      "TPE search finished in 1590.96 seconds\n",
      "Best (raw indices): {'batch_size': np.int64(4), 'cnn_dense': np.int64(3), 'cnn_dropout': np.float64(0.10645755494528952), 'cnn_kernel_size_1': np.int64(1), 'cnn_kernel_size_2': np.int64(1), 'cnn_kernels_1': np.int64(2), 'cnn_kernels_2': np.int64(2), 'learning_rate': np.float64(0.0002269292511272618), 'lstm_dense': np.int64(2), 'lstm_hidden_size': np.int64(2), 'lstm_layers': np.int64(0), 'optimizer': np.int64(1)}\n",
      "Best (interpreted): {'batch_size': 80, 'cnn_dense': 256, 'cnn_dropout': np.float64(0.10645755494528952), 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 64, 'learning_rate': np.float64(0.0002269292511272618), 'lstm_dense': 128, 'lstm_hidden_size': 96, 'lstm_layers': 1, 'optimizer': 'rmsprop'}\n",
      "Starting TPE search...\n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 128, 'cnn_dropout': 0.6502750338730191, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 16, 'cnn_kernels_2': 16, 'learning_rate': 0.0008574555272259, 'lstm_dense': 256, 'lstm_hidden_size': 128, 'lstm_layers': 6, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 6.9414 Acc: 0.4702 | Val Loss: 1.9881 Acc: 0.4427                                               \n",
      "Epoch 002 | Train Loss: 1.6693 Acc: 0.5333 | Val Loss: 1.1364 Acc: 0.5833                                               \n",
      "Epoch 003 | Train Loss: 1.0827 Acc: 0.5894 | Val Loss: 0.8366 Acc: 0.6143                                               \n",
      "Epoch 004 | Train Loss: 0.8586 Acc: 0.6309 | Val Loss: 0.5998 Acc: 0.7493                                               \n",
      "Epoch 005 | Train Loss: 0.7527 Acc: 0.6690 | Val Loss: 0.5862 Acc: 0.7233                                               \n",
      "Epoch 006 | Train Loss: 0.6772 Acc: 0.6942 | Val Loss: 0.6485 Acc: 0.6496                                               \n",
      "Epoch 007 | Train Loss: 0.6231 Acc: 0.7130 | Val Loss: 0.5906 Acc: 0.7600                                               \n",
      "Epoch 008 | Train Loss: 0.5871 Acc: 0.7336 | Val Loss: 0.5278 Acc: 0.7487                                               \n",
      "Epoch 009 | Train Loss: 0.5541 Acc: 0.7524 | Val Loss: 0.5949 Acc: 0.7191                                               \n",
      "Epoch 010 | Train Loss: 0.5276 Acc: 0.7639 | Val Loss: 0.4982 Acc: 0.7827                                               \n",
      "Epoch 011 | Train Loss: 0.5030 Acc: 0.7766 | Val Loss: 0.6828 Acc: 0.7588                                               \n",
      "Epoch 012 | Train Loss: 0.5062 Acc: 0.7739 | Val Loss: 0.6097 Acc: 0.7469                                               \n",
      "Epoch 013 | Train Loss: 0.4785 Acc: 0.7924 | Val Loss: 0.6466 Acc: 0.7260                                               \n",
      "Epoch 014 | Train Loss: 0.4609 Acc: 0.7977 | Val Loss: 0.7352 Acc: 0.6519                                               \n",
      "Epoch 015 | Train Loss: 0.4454 Acc: 0.8080 | Val Loss: 0.5581 Acc: 0.7400                                               \n",
      "Epoch 016 | Train Loss: 0.4408 Acc: 0.8116 | Val Loss: 0.4207 Acc: 0.8066                                               \n",
      "Epoch 017 | Train Loss: 0.4370 Acc: 0.8075 | Val Loss: 0.4871 Acc: 0.7567                                               \n",
      "Epoch 018 | Train Loss: 0.4185 Acc: 0.8169 | Val Loss: 0.4659 Acc: 0.7287                                               \n",
      "Epoch 019 | Train Loss: 0.4165 Acc: 0.8213 | Val Loss: 0.4943 Acc: 0.7743                                               \n",
      "Epoch 020 | Train Loss: 0.4123 Acc: 0.8230 | Val Loss: 0.5723 Acc: 0.7388                                               \n",
      "Epoch 021 | Train Loss: 0.4097 Acc: 0.8251 | Val Loss: 0.5746 Acc: 0.7170                                               \n",
      "Epoch 022 | Train Loss: 0.4009 Acc: 0.8261 | Val Loss: 0.8123 Acc: 0.6767                                               \n",
      "Epoch 023 | Train Loss: 0.3952 Acc: 0.8341 | Val Loss: 0.5531 Acc: 0.7457                                               \n",
      "Epoch 024 | Train Loss: 0.4249 Acc: 0.8294 | Val Loss: 0.6872 Acc: 0.7725                                               \n",
      "Epoch 025 | Train Loss: 0.3800 Acc: 0.8369 | Val Loss: 0.6173 Acc: 0.7099                                               \n",
      "Epoch 026 | Train Loss: 0.3865 Acc: 0.8353 | Val Loss: 0.5280 Acc: 0.7206                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 128, 'cnn_dropout': 0.3147856218864353, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 32, 'cnn_kernels_2': 16, 'learning_rate': 0.0003422869839074165, 'lstm_dense': 128, 'lstm_hidden_size': 64, 'lstm_layers': 3, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 19.3079 Acc: 0.4046 | Val Loss: 4.8523 Acc: 0.4887                                              \n",
      "Epoch 002 | Train Loss: 7.4401 Acc: 0.4469 | Val Loss: 4.6764 Acc: 0.4573                                               \n",
      "Epoch 003 | Train Loss: 4.6124 Acc: 0.4711 | Val Loss: 5.7596 Acc: 0.4791                                               \n",
      "Epoch 004 | Train Loss: 3.3277 Acc: 0.4872 | Val Loss: 1.9824 Acc: 0.4096                                               \n",
      "Epoch 005 | Train Loss: 2.5060 Acc: 0.4986 | Val Loss: 0.9968 Acc: 0.6361                                               \n",
      "Epoch 006 | Train Loss: 1.8902 Acc: 0.5271 | Val Loss: 1.0198 Acc: 0.6290                                               \n",
      "Epoch 007 | Train Loss: 1.4638 Acc: 0.5590 | Val Loss: 1.2644 Acc: 0.6054                                               \n",
      "Epoch 008 | Train Loss: 1.2269 Acc: 0.5776 | Val Loss: 0.9445 Acc: 0.6469                                               \n",
      "Epoch 009 | Train Loss: 1.0111 Acc: 0.6073 | Val Loss: 0.9157 Acc: 0.6122                                               \n",
      "Epoch 010 | Train Loss: 0.8889 Acc: 0.6322 | Val Loss: 0.6536 Acc: 0.7364                                               \n",
      "Epoch 011 | Train Loss: 0.7854 Acc: 0.6593 | Val Loss: 0.6200 Acc: 0.7119                                               \n",
      "Epoch 012 | Train Loss: 0.7460 Acc: 0.6735 | Val Loss: 0.5455 Acc: 0.7257                                               \n",
      "Epoch 013 | Train Loss: 0.6979 Acc: 0.6942 | Val Loss: 0.5297 Acc: 0.7433                                               \n",
      "Epoch 014 | Train Loss: 0.6741 Acc: 0.7040 | Val Loss: 0.5462 Acc: 0.7639                                               \n",
      "Epoch 015 | Train Loss: 0.6412 Acc: 0.7184 | Val Loss: 0.5444 Acc: 0.7576                                               \n",
      "Epoch 016 | Train Loss: 0.6339 Acc: 0.7247 | Val Loss: 0.5473 Acc: 0.7567                                               \n",
      "Epoch 017 | Train Loss: 0.6046 Acc: 0.7414 | Val Loss: 0.7182 Acc: 0.6899                                               \n",
      "Epoch 018 | Train Loss: 0.6089 Acc: 0.7324 | Val Loss: 0.4820 Acc: 0.7848                                               \n",
      "Epoch 019 | Train Loss: 0.5821 Acc: 0.7480 | Val Loss: 0.6289 Acc: 0.7069                                               \n",
      "Epoch 020 | Train Loss: 0.5689 Acc: 0.7544 | Val Loss: 0.4718 Acc: 0.7976                                               \n",
      "Epoch 021 | Train Loss: 0.5575 Acc: 0.7560 | Val Loss: 0.4403 Acc: 0.8101                                               \n",
      "Epoch 022 | Train Loss: 0.5400 Acc: 0.7740 | Val Loss: 0.4236 Acc: 0.8233                                               \n",
      "Epoch 023 | Train Loss: 0.5348 Acc: 0.7680 | Val Loss: 0.5463 Acc: 0.7310                                               \n",
      "Epoch 024 | Train Loss: 0.5002 Acc: 0.7786 | Val Loss: 0.4368 Acc: 0.8027                                               \n",
      "Epoch 025 | Train Loss: 0.5200 Acc: 0.7783 | Val Loss: 0.6437 Acc: 0.6884                                               \n",
      "Epoch 026 | Train Loss: 0.4981 Acc: 0.7842 | Val Loss: 0.8935 Acc: 0.6439                                               \n",
      "Epoch 027 | Train Loss: 0.4808 Acc: 0.7851 | Val Loss: 0.4748 Acc: 0.7809                                               \n",
      "Epoch 028 | Train Loss: 0.4809 Acc: 0.7922 | Val Loss: 0.4142 Acc: 0.8188                                               \n",
      "Epoch 029 | Train Loss: 0.4765 Acc: 0.7904 | Val Loss: 0.5904 Acc: 0.7000                                               \n",
      "Epoch 030 | Train Loss: 0.4651 Acc: 0.8064 | Val Loss: 0.7812 Acc: 0.6531                                               \n",
      "Epoch 031 | Train Loss: 0.4617 Acc: 0.8012 | Val Loss: 0.3537 Acc: 0.8531                                               \n",
      "Epoch 032 | Train Loss: 0.4458 Acc: 0.8089 | Val Loss: 0.4801 Acc: 0.8015                                               \n",
      "Epoch 033 | Train Loss: 0.4482 Acc: 0.8103 | Val Loss: 0.3873 Acc: 0.8349                                               \n",
      "Epoch 034 | Train Loss: 0.4523 Acc: 0.8116 | Val Loss: 0.4175 Acc: 0.8191                                               \n",
      "Epoch 035 | Train Loss: 0.4329 Acc: 0.8230 | Val Loss: 0.4333 Acc: 0.7863                                               \n",
      "Epoch 036 | Train Loss: 0.4224 Acc: 0.8303 | Val Loss: 0.4074 Acc: 0.8209                                               \n",
      "Epoch 037 | Train Loss: 0.3915 Acc: 0.8451 | Val Loss: 0.5270 Acc: 0.8033                                               \n",
      "Epoch 038 | Train Loss: 0.3899 Acc: 0.8489 | Val Loss: 0.3944 Acc: 0.8549                                               \n",
      "Epoch 039 | Train Loss: 0.3692 Acc: 0.8568 | Val Loss: 0.3809 Acc: 0.8660                                               \n",
      "Epoch 040 | Train Loss: 0.3660 Acc: 0.8648 | Val Loss: 0.3346 Acc: 0.8672                                               \n",
      "Epoch 041 | Train Loss: 0.3498 Acc: 0.8692 | Val Loss: 0.3232 Acc: 0.8681                                               \n",
      "Epoch 042 | Train Loss: 0.3325 Acc: 0.8768 | Val Loss: 0.4417 Acc: 0.8316                                               \n",
      "Epoch 043 | Train Loss: 0.3201 Acc: 0.8804 | Val Loss: 0.3228 Acc: 0.8481                                               \n",
      "Epoch 044 | Train Loss: 0.3160 Acc: 0.8848 | Val Loss: 0.3271 Acc: 0.8690                                               \n",
      "Epoch 045 | Train Loss: 0.3091 Acc: 0.8871 | Val Loss: 0.2807 Acc: 0.9185                                               \n",
      "Epoch 046 | Train Loss: 0.2856 Acc: 0.8948 | Val Loss: 0.2593 Acc: 0.8988                                               \n",
      "Epoch 047 | Train Loss: 0.2827 Acc: 0.8980 | Val Loss: 0.3947 Acc: 0.8442                                               \n",
      "Epoch 048 | Train Loss: 0.2771 Acc: 0.9001 | Val Loss: 0.2521 Acc: 0.9110                                               \n",
      "Epoch 049 | Train Loss: 0.2781 Acc: 0.8994 | Val Loss: 0.2589 Acc: 0.9188                                               \n",
      "Epoch 050 | Train Loss: 0.2660 Acc: 0.9039 | Val Loss: 0.2762 Acc: 0.8540                                               \n",
      "Epoch 051 | Train Loss: 0.2612 Acc: 0.9042 | Val Loss: 0.3190 Acc: 0.8904                                               \n",
      "Epoch 052 | Train Loss: 0.2439 Acc: 0.9114 | Val Loss: 0.3961 Acc: 0.8525                                               \n",
      "Epoch 053 | Train Loss: 0.2417 Acc: 0.9145 | Val Loss: 0.2314 Acc: 0.9140                                               \n",
      "Epoch 054 | Train Loss: 0.2332 Acc: 0.9177 | Val Loss: 0.2818 Acc: 0.8654                                               \n",
      "Epoch 055 | Train Loss: 0.2215 Acc: 0.9230 | Val Loss: 0.2155 Acc: 0.9170                                               \n",
      "Epoch 056 | Train Loss: 0.2165 Acc: 0.9231 | Val Loss: 0.2079 Acc: 0.9146                                               \n",
      "Epoch 057 | Train Loss: 0.1980 Acc: 0.9275 | Val Loss: 0.2255 Acc: 0.9188                                               \n",
      "Epoch 058 | Train Loss: 0.1859 Acc: 0.9339 | Val Loss: 0.2089 Acc: 0.9239                                               \n",
      "Epoch 059 | Train Loss: 0.2004 Acc: 0.9316 | Val Loss: 0.1841 Acc: 0.9385                                               \n",
      "Epoch 060 | Train Loss: 0.1643 Acc: 0.9416 | Val Loss: 0.1697 Acc: 0.9242                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 32, 'cnn_dropout': 0.11071875034023924, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 16, 'cnn_kernels_2': 32, 'learning_rate': 0.002218851852194978, 'lstm_dense': 32, 'lstm_hidden_size': 64, 'lstm_layers': 1, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 22.3523 Acc: 0.4382 | Val Loss: 1.2074 Acc: 0.4645                                              \n",
      "Epoch 002 | Train Loss: 1.1581 Acc: 0.5041 | Val Loss: 1.1283 Acc: 0.5185                                               \n",
      "Epoch 003 | Train Loss: 1.1166 Acc: 0.5157 | Val Loss: 1.0987 Acc: 0.5179                                               \n",
      "Epoch 004 | Train Loss: 1.0798 Acc: 0.5325 | Val Loss: 1.0692 Acc: 0.5284                                               \n",
      "Epoch 005 | Train Loss: 1.0521 Acc: 0.5435 | Val Loss: 1.0394 Acc: 0.5430                                               \n",
      "Epoch 006 | Train Loss: 1.0231 Acc: 0.5537 | Val Loss: 1.0241 Acc: 0.5448                                               \n",
      "Epoch 007 | Train Loss: 0.9924 Acc: 0.5712 | Val Loss: 0.9721 Acc: 0.5797                                               \n",
      "Epoch 008 | Train Loss: 0.9621 Acc: 0.5819 | Val Loss: 0.9530 Acc: 0.5758                                               \n",
      "Epoch 009 | Train Loss: 0.9294 Acc: 0.5903 | Val Loss: 0.9215 Acc: 0.5896                                               \n",
      "Epoch 010 | Train Loss: 0.8944 Acc: 0.6077 | Val Loss: 0.8696 Acc: 0.6325                                               \n",
      "Epoch 011 | Train Loss: 0.8484 Acc: 0.6530 | Val Loss: 0.8827 Acc: 0.6558                                               \n",
      "Epoch 012 | Train Loss: 0.8124 Acc: 0.6836 | Val Loss: 0.8292 Acc: 0.6630                                               \n",
      "Epoch 013 | Train Loss: 0.7269 Acc: 0.7248 | Val Loss: 0.7194 Acc: 0.7331                                               \n",
      "Epoch 014 | Train Loss: 0.6635 Acc: 0.7560 | Val Loss: 0.6367 Acc: 0.7630                                               \n",
      "Epoch 015 | Train Loss: 0.6228 Acc: 0.7716 | Val Loss: 0.5584 Acc: 0.7982                                               \n",
      "Epoch 016 | Train Loss: 0.5500 Acc: 0.7992 | Val Loss: 0.6811 Acc: 0.7397                                               \n",
      "Epoch 017 | Train Loss: 0.5287 Acc: 0.8060 | Val Loss: 0.5480 Acc: 0.8057                                               \n",
      "Epoch 018 | Train Loss: 0.4797 Acc: 0.8233 | Val Loss: 0.5241 Acc: 0.8134                                               \n",
      "Epoch 019 | Train Loss: 0.4457 Acc: 0.8389 | Val Loss: 0.5137 Acc: 0.8158                                               \n",
      "Epoch 020 | Train Loss: 0.4187 Acc: 0.8516 | Val Loss: 0.4327 Acc: 0.8451                                               \n",
      "Epoch 021 | Train Loss: 0.3973 Acc: 0.8595 | Val Loss: 0.4414 Acc: 0.8409                                               \n",
      "Epoch 022 | Train Loss: 0.3534 Acc: 0.8753 | Val Loss: 0.4008 Acc: 0.8597                                               \n",
      "Epoch 023 | Train Loss: 0.3464 Acc: 0.8785 | Val Loss: 0.3796 Acc: 0.8693                                               \n",
      "Epoch 024 | Train Loss: 0.3293 Acc: 0.8820 | Val Loss: 0.3494 Acc: 0.8800                                               \n",
      "Epoch 025 | Train Loss: 0.3045 Acc: 0.8953 | Val Loss: 0.3226 Acc: 0.8955                                               \n",
      "Epoch 026 | Train Loss: 0.2644 Acc: 0.9129 | Val Loss: 0.3119 Acc: 0.8946                                               \n",
      "Epoch 027 | Train Loss: 0.2597 Acc: 0.9147 | Val Loss: 0.3115 Acc: 0.8994                                               \n",
      "Epoch 028 | Train Loss: 0.2410 Acc: 0.9201 | Val Loss: 0.2773 Acc: 0.9149                                               \n",
      "Epoch 029 | Train Loss: 0.2334 Acc: 0.9237 | Val Loss: 0.2854 Acc: 0.9054                                               \n",
      "Epoch 030 | Train Loss: 0.2242 Acc: 0.9272 | Val Loss: 0.3387 Acc: 0.8887                                               \n",
      "Epoch 031 | Train Loss: 0.2116 Acc: 0.9307 | Val Loss: 0.2628 Acc: 0.9161                                               \n",
      "Epoch 032 | Train Loss: 0.1797 Acc: 0.9427 | Val Loss: 0.2168 Acc: 0.9367                                               \n",
      "Epoch 033 | Train Loss: 0.1923 Acc: 0.9384 | Val Loss: 0.2376 Acc: 0.9239                                               \n",
      "Epoch 034 | Train Loss: 0.1832 Acc: 0.9388 | Val Loss: 0.2437 Acc: 0.9215                                               \n",
      "Epoch 035 | Train Loss: 0.1802 Acc: 0.9405 | Val Loss: 0.2583 Acc: 0.9194                                               \n",
      "Epoch 036 | Train Loss: 0.1689 Acc: 0.9460 | Val Loss: 0.2177 Acc: 0.9278                                               \n",
      "Epoch 037 | Train Loss: 0.1434 Acc: 0.9519 | Val Loss: 0.2516 Acc: 0.9200                                               \n",
      "Epoch 038 | Train Loss: 0.1421 Acc: 0.9532 | Val Loss: 0.2232 Acc: 0.9316                                               \n",
      "Epoch 039 | Train Loss: 0.1199 Acc: 0.9607 | Val Loss: 0.2206 Acc: 0.9301                                               \n",
      "Epoch 040 | Train Loss: 0.1298 Acc: 0.9579 | Val Loss: 0.2273 Acc: 0.9272                                               \n",
      "Epoch 041 | Train Loss: 0.1154 Acc: 0.9645 | Val Loss: 0.2034 Acc: 0.9373                                               \n",
      "Epoch 042 | Train Loss: 0.1352 Acc: 0.9574 | Val Loss: 0.2073 Acc: 0.9364                                               \n",
      "Epoch 043 | Train Loss: 0.1173 Acc: 0.9628 | Val Loss: 0.1889 Acc: 0.9445                                               \n",
      "Epoch 044 | Train Loss: 0.1156 Acc: 0.9619 | Val Loss: 0.2069 Acc: 0.9418                                               \n",
      "Epoch 045 | Train Loss: 0.1529 Acc: 0.9473 | Val Loss: 0.3160 Acc: 0.9039                                               \n",
      "Epoch 046 | Train Loss: 0.1649 Acc: 0.9445 | Val Loss: 0.2717 Acc: 0.9149                                               \n",
      "Epoch 047 | Train Loss: 0.1419 Acc: 0.9531 | Val Loss: 0.2195 Acc: 0.9397                                               \n",
      "Epoch 048 | Train Loss: 0.1371 Acc: 0.9566 | Val Loss: 0.2103 Acc: 0.9325                                               \n",
      "Epoch 049 | Train Loss: 0.1108 Acc: 0.9651 | Val Loss: 0.2178 Acc: 0.9370                                               \n",
      "Epoch 050 | Train Loss: 0.0893 Acc: 0.9720 | Val Loss: 0.1974 Acc: 0.9400                                               \n",
      "Epoch 051 | Train Loss: 0.0870 Acc: 0.9708 | Val Loss: 0.2434 Acc: 0.9334                                               \n",
      "Epoch 052 | Train Loss: 0.0893 Acc: 0.9702 | Val Loss: 0.2201 Acc: 0.9388                                               \n",
      "Epoch 053 | Train Loss: 0.0805 Acc: 0.9738 | Val Loss: 0.1856 Acc: 0.9433                                               \n",
      "Epoch 054 | Train Loss: 0.1116 Acc: 0.9615 | Val Loss: 0.1766 Acc: 0.9457                                               \n",
      "Epoch 055 | Train Loss: 0.0994 Acc: 0.9655 | Val Loss: 0.2118 Acc: 0.9391                                               \n",
      "Epoch 056 | Train Loss: 0.1206 Acc: 0.9592 | Val Loss: 0.2011 Acc: 0.9340                                               \n",
      "Epoch 057 | Train Loss: 0.0936 Acc: 0.9677 | Val Loss: 0.1979 Acc: 0.9385                                               \n",
      "Epoch 058 | Train Loss: 0.0763 Acc: 0.9732 | Val Loss: 0.1599 Acc: 0.9531                                               \n",
      "Epoch 059 | Train Loss: 0.0689 Acc: 0.9754 | Val Loss: 0.1605 Acc: 0.9475                                               \n",
      "Epoch 060 | Train Loss: 0.0577 Acc: 0.9811 | Val Loss: 0.1640 Acc: 0.9537                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 128, 'cnn_dropout': 0.10596601451321264, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 96, 'learning_rate': 0.000353012565087217, 'lstm_dense': 64, 'lstm_hidden_size': 96, 'lstm_layers': 5, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 12.8503 Acc: 0.4366 | Val Loss: 1.2428 Acc: 0.4421                                              \n",
      "Epoch 002 | Train Loss: 1.2410 Acc: 0.4422 | Val Loss: 1.2385 Acc: 0.4421                                               \n",
      "Epoch 003 | Train Loss: 1.2394 Acc: 0.4422 | Val Loss: 1.2380 Acc: 0.4421                                               \n",
      "Epoch 004 | Train Loss: 1.2388 Acc: 0.4422 | Val Loss: 1.2379 Acc: 0.4421                                               \n",
      "Epoch 005 | Train Loss: 1.2388 Acc: 0.4422 | Val Loss: 1.2378 Acc: 0.4421                                               \n",
      "Epoch 006 | Train Loss: 1.2390 Acc: 0.4422 | Val Loss: 1.2378 Acc: 0.4421                                               \n",
      "Epoch 007 | Train Loss: 1.2387 Acc: 0.4422 | Val Loss: 1.2378 Acc: 0.4421                                               \n",
      "Epoch 008 | Train Loss: 1.2381 Acc: 0.4422 | Val Loss: 1.2378 Acc: 0.4421                                               \n",
      "Epoch 009 | Train Loss: 1.2389 Acc: 0.4422 | Val Loss: 1.2378 Acc: 0.4421                                               \n",
      "Epoch 010 | Train Loss: 1.2385 Acc: 0.4422 | Val Loss: 1.2377 Acc: 0.4421                                               \n",
      "Epoch 011 | Train Loss: 1.2385 Acc: 0.4422 | Val Loss: 1.2377 Acc: 0.4421                                               \n",
      "Epoch 012 | Train Loss: 1.2389 Acc: 0.4422 | Val Loss: 1.2377 Acc: 0.4421                                               \n",
      "Epoch 013 | Train Loss: 1.2386 Acc: 0.4422 | Val Loss: 1.2377 Acc: 0.4421                                               \n",
      "Epoch 014 | Train Loss: 1.2380 Acc: 0.4422 | Val Loss: 1.2377 Acc: 0.4421                                               \n",
      "Epoch 015 | Train Loss: 1.2388 Acc: 0.4422 | Val Loss: 1.2377 Acc: 0.4421                                               \n",
      "Epoch 016 | Train Loss: 1.2387 Acc: 0.4422 | Val Loss: 1.2377 Acc: 0.4421                                               \n",
      "Epoch 017 | Train Loss: 1.2387 Acc: 0.4422 | Val Loss: 1.2377 Acc: 0.4421                                               \n",
      "Epoch 018 | Train Loss: 1.2385 Acc: 0.4422 | Val Loss: 1.2377 Acc: 0.4421                                               \n",
      "Epoch 019 | Train Loss: 1.2387 Acc: 0.4422 | Val Loss: 1.2376 Acc: 0.4421                                               \n",
      "Epoch 020 | Train Loss: 1.2384 Acc: 0.4422 | Val Loss: 1.2377 Acc: 0.4421                                               \n",
      "Epoch 021 | Train Loss: 1.2389 Acc: 0.4422 | Val Loss: 1.2376 Acc: 0.4421                                               \n",
      "Epoch 022 | Train Loss: 1.2386 Acc: 0.4422 | Val Loss: 1.2376 Acc: 0.4421                                               \n",
      "Epoch 023 | Train Loss: 1.2386 Acc: 0.4422 | Val Loss: 1.2376 Acc: 0.4421                                               \n",
      "Epoch 024 | Train Loss: 1.2387 Acc: 0.4422 | Val Loss: 1.2376 Acc: 0.4421                                               \n",
      "Epoch 025 | Train Loss: 1.2385 Acc: 0.4422 | Val Loss: 1.2376 Acc: 0.4421                                               \n",
      "Epoch 026 | Train Loss: 1.2385 Acc: 0.4422 | Val Loss: 1.2376 Acc: 0.4421                                               \n",
      "Epoch 027 | Train Loss: 1.2380 Acc: 0.4422 | Val Loss: 1.2375 Acc: 0.4421                                               \n",
      "Epoch 028 | Train Loss: 1.2389 Acc: 0.4422 | Val Loss: 1.2375 Acc: 0.4421                                               \n",
      "Epoch 029 | Train Loss: 1.2385 Acc: 0.4422 | Val Loss: 1.2375 Acc: 0.4421                                               \n",
      "Epoch 030 | Train Loss: 1.2386 Acc: 0.4422 | Val Loss: 1.2375 Acc: 0.4421                                               \n",
      "Epoch 031 | Train Loss: 1.2383 Acc: 0.4422 | Val Loss: 1.2375 Acc: 0.4421                                               \n",
      "Epoch 032 | Train Loss: 1.2383 Acc: 0.4422 | Val Loss: 1.2374 Acc: 0.4421                                               \n",
      "Epoch 033 | Train Loss: 1.2382 Acc: 0.4422 | Val Loss: 1.2374 Acc: 0.4421                                               \n",
      "Epoch 034 | Train Loss: 1.2380 Acc: 0.4422 | Val Loss: 1.2374 Acc: 0.4421                                               \n",
      "Epoch 035 | Train Loss: 1.2379 Acc: 0.4422 | Val Loss: 1.2374 Acc: 0.4421                                               \n",
      "Epoch 036 | Train Loss: 1.2380 Acc: 0.4422 | Val Loss: 1.2374 Acc: 0.4421                                               \n",
      "Epoch 037 | Train Loss: 1.2385 Acc: 0.4422 | Val Loss: 1.2374 Acc: 0.4421                                               \n",
      "Epoch 038 | Train Loss: 1.2377 Acc: 0.4422 | Val Loss: 1.2373 Acc: 0.4421                                               \n",
      "Epoch 039 | Train Loss: 1.2376 Acc: 0.4422 | Val Loss: 1.2373 Acc: 0.4421                                               \n",
      "Epoch 040 | Train Loss: 1.2381 Acc: 0.4422 | Val Loss: 1.2373 Acc: 0.4421                                               \n",
      "Epoch 041 | Train Loss: 1.2382 Acc: 0.4422 | Val Loss: 1.2372 Acc: 0.4421                                               \n",
      "Epoch 042 | Train Loss: 1.2385 Acc: 0.4422 | Val Loss: 1.2372 Acc: 0.4421                                               \n",
      "Epoch 043 | Train Loss: 1.2381 Acc: 0.4422 | Val Loss: 1.2372 Acc: 0.4421                                               \n",
      "Epoch 044 | Train Loss: 1.2383 Acc: 0.4422 | Val Loss: 1.2372 Acc: 0.4421                                               \n",
      "Epoch 045 | Train Loss: 1.2382 Acc: 0.4422 | Val Loss: 1.2372 Acc: 0.4421                                               \n",
      "Epoch 046 | Train Loss: 1.2381 Acc: 0.4422 | Val Loss: 1.2371 Acc: 0.4421                                               \n",
      "Epoch 047 | Train Loss: 1.2379 Acc: 0.4422 | Val Loss: 1.2371 Acc: 0.4421                                               \n",
      "Epoch 048 | Train Loss: 1.2381 Acc: 0.4422 | Val Loss: 1.2371 Acc: 0.4421                                               \n",
      "Epoch 049 | Train Loss: 1.2377 Acc: 0.4422 | Val Loss: 1.2371 Acc: 0.4421                                               \n",
      "Epoch 050 | Train Loss: 1.2382 Acc: 0.4422 | Val Loss: 1.2370 Acc: 0.4421                                               \n",
      "Epoch 051 | Train Loss: 1.2379 Acc: 0.4422 | Val Loss: 1.2369 Acc: 0.4421                                               \n",
      "Epoch 052 | Train Loss: 1.2379 Acc: 0.4422 | Val Loss: 1.2369 Acc: 0.4421                                               \n",
      "Epoch 053 | Train Loss: 1.2377 Acc: 0.4422 | Val Loss: 1.2369 Acc: 0.4421                                               \n",
      "Epoch 054 | Train Loss: 1.2373 Acc: 0.4422 | Val Loss: 1.2369 Acc: 0.4421                                               \n",
      "Epoch 055 | Train Loss: 1.2370 Acc: 0.4422 | Val Loss: 1.2368 Acc: 0.4421                                               \n",
      "Epoch 056 | Train Loss: 1.2381 Acc: 0.4422 | Val Loss: 1.2367 Acc: 0.4421                                               \n",
      "Epoch 057 | Train Loss: 1.2375 Acc: 0.4422 | Val Loss: 1.2367 Acc: 0.4421                                               \n",
      "Epoch 058 | Train Loss: 1.2374 Acc: 0.4422 | Val Loss: 1.2366 Acc: 0.4421                                               \n",
      "Epoch 059 | Train Loss: 1.2378 Acc: 0.4422 | Val Loss: 1.2366 Acc: 0.4421                                               \n",
      "Epoch 060 | Train Loss: 1.2371 Acc: 0.4422 | Val Loss: 1.2365 Acc: 0.4421                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 128, 'cnn_dropout': 0.14299798412638987, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 16, 'cnn_kernels_2': 96, 'learning_rate': 4.19529752495727e-05, 'lstm_dense': 128, 'lstm_hidden_size': 32, 'lstm_layers': 3, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 179.0532 Acc: 0.2864 | Val Loss: 54.0213 Acc: 0.2952                                            \n",
      "Epoch 002 | Train Loss: 68.5537 Acc: 0.3027 | Val Loss: 27.7300 Acc: 0.3504                                             \n",
      "Epoch 003 | Train Loss: 44.1063 Acc: 0.3271 | Val Loss: 20.7107 Acc: 0.3400                                             \n",
      "Epoch 004 | Train Loss: 30.7806 Acc: 0.3477 | Val Loss: 15.3159 Acc: 0.2979                                             \n",
      "Epoch 005 | Train Loss: 23.8119 Acc: 0.3654 | Val Loss: 10.7307 Acc: 0.3128                                             \n",
      "Epoch 006 | Train Loss: 19.0981 Acc: 0.3835 | Val Loss: 6.9185 Acc: 0.3893                                              \n",
      "Epoch 007 | Train Loss: 16.3825 Acc: 0.3920 | Val Loss: 5.2138 Acc: 0.4588                                              \n",
      "Epoch 008 | Train Loss: 13.9705 Acc: 0.3944 | Val Loss: 3.7605 Acc: 0.4931                                              \n",
      "Epoch 009 | Train Loss: 12.2780 Acc: 0.4046 | Val Loss: 3.7282 Acc: 0.4699                                              \n",
      "Epoch 010 | Train Loss: 11.1789 Acc: 0.4016 | Val Loss: 3.0397 Acc: 0.4296                                              \n",
      "Epoch 011 | Train Loss: 9.8220 Acc: 0.4117 | Val Loss: 2.9352 Acc: 0.4591                                               \n",
      "Epoch 012 | Train Loss: 9.1244 Acc: 0.4114 | Val Loss: 2.4386 Acc: 0.4725                                               \n",
      "Epoch 013 | Train Loss: 8.2035 Acc: 0.4196 | Val Loss: 2.4205 Acc: 0.4985                                               \n",
      "Epoch 014 | Train Loss: 7.5228 Acc: 0.4211 | Val Loss: 2.2683 Acc: 0.5051                                               \n",
      "Epoch 015 | Train Loss: 7.1000 Acc: 0.4212 | Val Loss: 2.0523 Acc: 0.5325                                               \n",
      "Epoch 016 | Train Loss: 6.4823 Acc: 0.4276 | Val Loss: 1.7442 Acc: 0.5081                                               \n",
      "Epoch 017 | Train Loss: 6.2366 Acc: 0.4373 | Val Loss: 2.3818 Acc: 0.5307                                               \n",
      "Epoch 018 | Train Loss: 5.5729 Acc: 0.4376 | Val Loss: 1.9440 Acc: 0.5012                                               \n",
      "Epoch 019 | Train Loss: 5.2704 Acc: 0.4441 | Val Loss: 1.6569 Acc: 0.5579                                               \n",
      "Epoch 020 | Train Loss: 4.9054 Acc: 0.4547 | Val Loss: 1.5782 Acc: 0.5090                                               \n",
      "Epoch 021 | Train Loss: 4.7127 Acc: 0.4536 | Val Loss: 1.6427 Acc: 0.5325                                               \n",
      "Epoch 022 | Train Loss: 4.4032 Acc: 0.4582 | Val Loss: 1.6512 Acc: 0.5487                                               \n",
      "Epoch 023 | Train Loss: 4.0764 Acc: 0.4712 | Val Loss: 1.3615 Acc: 0.5260                                               \n",
      "Epoch 024 | Train Loss: 3.9188 Acc: 0.4736 | Val Loss: 1.3806 Acc: 0.5427                                               \n",
      "Epoch 025 | Train Loss: 3.6321 Acc: 0.4848 | Val Loss: 1.8016 Acc: 0.5713                                               \n",
      "Epoch 026 | Train Loss: 3.4468 Acc: 0.4868 | Val Loss: 1.6953 Acc: 0.5690                                               \n",
      "Epoch 027 | Train Loss: 3.3401 Acc: 0.4858 | Val Loss: 1.6587 Acc: 0.5522                                               \n",
      "Epoch 028 | Train Loss: 3.1451 Acc: 0.5029 | Val Loss: 1.1844 Acc: 0.5845                                               \n",
      "Epoch 029 | Train Loss: 2.9502 Acc: 0.5060 | Val Loss: 1.1907 Acc: 0.5687                                               \n",
      "Epoch 030 | Train Loss: 2.8809 Acc: 0.5111 | Val Loss: 1.3252 Acc: 0.5690                                               \n",
      "Epoch 031 | Train Loss: 2.7021 Acc: 0.5084 | Val Loss: 1.1921 Acc: 0.6313                                               \n",
      "Epoch 032 | Train Loss: 2.7111 Acc: 0.5156 | Val Loss: 1.1300 Acc: 0.6137                                               \n",
      "Epoch 033 | Train Loss: 2.5195 Acc: 0.5204 | Val Loss: 1.5961 Acc: 0.6066                                               \n",
      "Epoch 034 | Train Loss: 2.4011 Acc: 0.5321 | Val Loss: 1.0867 Acc: 0.6155                                               \n",
      "Epoch 035 | Train Loss: 2.2408 Acc: 0.5371 | Val Loss: 1.1082 Acc: 0.5878                                               \n",
      "Epoch 036 | Train Loss: 2.1836 Acc: 0.5433 | Val Loss: 0.9673 Acc: 0.6525                                               \n",
      "Epoch 037 | Train Loss: 2.1181 Acc: 0.5456 | Val Loss: 1.0002 Acc: 0.6600                                               \n",
      "Epoch 038 | Train Loss: 2.0532 Acc: 0.5573 | Val Loss: 0.9387 Acc: 0.6752                                               \n",
      "Epoch 039 | Train Loss: 1.9351 Acc: 0.5613 | Val Loss: 0.9325 Acc: 0.6725                                               \n",
      "Epoch 040 | Train Loss: 1.8613 Acc: 0.5689 | Val Loss: 1.1486 Acc: 0.6728                                               \n",
      "Epoch 041 | Train Loss: 1.8303 Acc: 0.5741 | Val Loss: 0.8460 Acc: 0.6785                                               \n",
      "Epoch 042 | Train Loss: 1.7313 Acc: 0.5812 | Val Loss: 1.1854 Acc: 0.6475                                               \n",
      "Epoch 043 | Train Loss: 1.7184 Acc: 0.5790 | Val Loss: 0.8822 Acc: 0.6740                                               \n",
      "Epoch 044 | Train Loss: 1.6057 Acc: 0.5990 | Val Loss: 1.0827 Acc: 0.6552                                               \n",
      "Epoch 045 | Train Loss: 1.6221 Acc: 0.5915 | Val Loss: 1.0921 Acc: 0.6301                                               \n",
      "Epoch 046 | Train Loss: 1.5424 Acc: 0.6099 | Val Loss: 0.8397 Acc: 0.6890                                               \n",
      "Epoch 047 | Train Loss: 1.4663 Acc: 0.6130 | Val Loss: 0.9090 Acc: 0.7107                                               \n",
      "Epoch 048 | Train Loss: 1.5372 Acc: 0.6024 | Val Loss: 0.9099 Acc: 0.6681                                               \n",
      "Epoch 049 | Train Loss: 1.4095 Acc: 0.6206 | Val Loss: 0.8414 Acc: 0.6818                                               \n",
      "Epoch 050 | Train Loss: 1.4041 Acc: 0.6226 | Val Loss: 0.9307 Acc: 0.7054                                               \n",
      "Epoch 051 | Train Loss: 1.3932 Acc: 0.6197 | Val Loss: 0.8154 Acc: 0.7179                                               \n",
      "Epoch 052 | Train Loss: 1.2770 Acc: 0.6392 | Val Loss: 1.0809 Acc: 0.6934                                               \n",
      "Epoch 053 | Train Loss: 1.2598 Acc: 0.6436 | Val Loss: 0.8483 Acc: 0.7227                                               \n",
      "Epoch 054 | Train Loss: 1.2373 Acc: 0.6522 | Val Loss: 0.8741 Acc: 0.6913                                               \n",
      "Epoch 055 | Train Loss: 1.2111 Acc: 0.6498 | Val Loss: 0.9107 Acc: 0.7015                                               \n",
      "Epoch 056 | Train Loss: 1.1737 Acc: 0.6525 | Val Loss: 0.7820 Acc: 0.7212                                               \n",
      "Epoch 057 | Train Loss: 1.1869 Acc: 0.6603 | Val Loss: 0.6442 Acc: 0.7600                                               \n",
      "Epoch 058 | Train Loss: 1.1658 Acc: 0.6627 | Val Loss: 0.7012 Acc: 0.7316                                               \n",
      "Epoch 059 | Train Loss: 1.1134 Acc: 0.6701 | Val Loss: 0.9904 Acc: 0.6588                                               \n",
      "Epoch 060 | Train Loss: 1.0646 Acc: 0.6780 | Val Loss: 0.8351 Acc: 0.7185                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 32, 'cnn_dropout': 0.19530884131399406, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 16, 'learning_rate': 0.0004930239675451111, 'lstm_dense': 64, 'lstm_hidden_size': 32, 'lstm_layers': 3, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 20.6911 Acc: 0.3879 | Val Loss: 6.0222 Acc: 0.4722                                              \n",
      "Epoch 002 | Train Loss: 6.6721 Acc: 0.4552 | Val Loss: 2.0290 Acc: 0.5630                                               \n",
      "Epoch 003 | Train Loss: 4.0227 Acc: 0.4906 | Val Loss: 1.3426 Acc: 0.5442                                               \n",
      "Epoch 004 | Train Loss: 2.8953 Acc: 0.5075 | Val Loss: 2.6663 Acc: 0.4579                                               \n",
      "Epoch 005 | Train Loss: 2.0299 Acc: 0.5351 | Val Loss: 2.6741 Acc: 0.5209                                               \n",
      "Epoch 006 | Train Loss: 1.6546 Acc: 0.5530 | Val Loss: 1.7308 Acc: 0.5597                                               \n",
      "Epoch 007 | Train Loss: 1.2672 Acc: 0.5866 | Val Loss: 1.3028 Acc: 0.6603                                               \n",
      "Epoch 008 | Train Loss: 1.0965 Acc: 0.5995 | Val Loss: 0.8083 Acc: 0.6725                                               \n",
      "Epoch 009 | Train Loss: 0.9076 Acc: 0.6311 | Val Loss: 0.8595 Acc: 0.6272                                               \n",
      "Epoch 010 | Train Loss: 0.7896 Acc: 0.6607 | Val Loss: 0.5694 Acc: 0.7251                                               \n",
      "Epoch 011 | Train Loss: 0.7440 Acc: 0.6728 | Val Loss: 0.5866 Acc: 0.7591                                               \n",
      "Epoch 012 | Train Loss: 0.6917 Acc: 0.6906 | Val Loss: 0.5261 Acc: 0.8134                                               \n",
      "Epoch 013 | Train Loss: 0.6504 Acc: 0.7077 | Val Loss: 0.5274 Acc: 0.7463                                               \n",
      "Epoch 014 | Train Loss: 0.6336 Acc: 0.7175 | Val Loss: 0.4806 Acc: 0.8036                                               \n",
      "Epoch 015 | Train Loss: 0.5945 Acc: 0.7319 | Val Loss: 0.4999 Acc: 0.7863                                               \n",
      "Epoch 016 | Train Loss: 0.5727 Acc: 0.7422 | Val Loss: 0.4377 Acc: 0.7961                                               \n",
      "Epoch 017 | Train Loss: 0.5495 Acc: 0.7540 | Val Loss: 0.5198 Acc: 0.7860                                               \n",
      "Epoch 018 | Train Loss: 0.5339 Acc: 0.7616 | Val Loss: 0.4872 Acc: 0.7743                                               \n",
      "Epoch 019 | Train Loss: 0.5214 Acc: 0.7710 | Val Loss: 0.4607 Acc: 0.7878                                               \n",
      "Epoch 020 | Train Loss: 0.5056 Acc: 0.7775 | Val Loss: 0.4885 Acc: 0.8110                                               \n",
      "Epoch 021 | Train Loss: 0.5092 Acc: 0.7738 | Val Loss: 0.4939 Acc: 0.7803                                               \n",
      "Epoch 022 | Train Loss: 0.4961 Acc: 0.7780 | Val Loss: 0.4219 Acc: 0.8224                                               \n",
      "Epoch 023 | Train Loss: 0.4860 Acc: 0.7821 | Val Loss: 0.3923 Acc: 0.8296                                               \n",
      "Epoch 024 | Train Loss: 0.4856 Acc: 0.7828 | Val Loss: 0.3895 Acc: 0.8576                                               \n",
      "Epoch 025 | Train Loss: 0.4731 Acc: 0.7894 | Val Loss: 0.6504 Acc: 0.7212                                               \n",
      "Epoch 026 | Train Loss: 0.4640 Acc: 0.7940 | Val Loss: 0.3572 Acc: 0.8391                                               \n",
      "Epoch 027 | Train Loss: 0.4532 Acc: 0.7953 | Val Loss: 0.4203 Acc: 0.8003                                               \n",
      "Epoch 028 | Train Loss: 0.4487 Acc: 0.8010 | Val Loss: 0.4893 Acc: 0.8057                                               \n",
      "Epoch 029 | Train Loss: 0.4382 Acc: 0.8094 | Val Loss: 0.4388 Acc: 0.8227                                               \n",
      "Epoch 030 | Train Loss: 0.4334 Acc: 0.8082 | Val Loss: 0.4027 Acc: 0.8224                                               \n",
      "Epoch 031 | Train Loss: 0.4338 Acc: 0.8079 | Val Loss: 0.3299 Acc: 0.8469                                               \n",
      "Epoch 032 | Train Loss: 0.4174 Acc: 0.8154 | Val Loss: 0.3885 Acc: 0.8466                                               \n",
      "Epoch 033 | Train Loss: 0.4128 Acc: 0.8180 | Val Loss: 0.4116 Acc: 0.8266                                               \n",
      "Epoch 034 | Train Loss: 0.4055 Acc: 0.8171 | Val Loss: 0.4175 Acc: 0.8182                                               \n",
      "Epoch 035 | Train Loss: 0.4078 Acc: 0.8243 | Val Loss: 0.3079 Acc: 0.9143                                               \n",
      "Epoch 036 | Train Loss: 0.4001 Acc: 0.8251 | Val Loss: 0.3198 Acc: 0.8773                                               \n",
      "Epoch 037 | Train Loss: 0.3983 Acc: 0.8270 | Val Loss: 0.3608 Acc: 0.8555                                               \n",
      "Epoch 038 | Train Loss: 0.3877 Acc: 0.8313 | Val Loss: 0.3602 Acc: 0.8463                                               \n",
      "Epoch 039 | Train Loss: 0.3815 Acc: 0.8290 | Val Loss: 0.3345 Acc: 0.8884                                               \n",
      "Epoch 040 | Train Loss: 0.3753 Acc: 0.8349 | Val Loss: 0.3413 Acc: 0.8322                                               \n",
      "Epoch 041 | Train Loss: 0.3819 Acc: 0.8298 | Val Loss: 0.4417 Acc: 0.8245                                               \n",
      "Epoch 042 | Train Loss: 0.3759 Acc: 0.8331 | Val Loss: 0.3301 Acc: 0.8552                                               \n",
      "Epoch 043 | Train Loss: 0.3724 Acc: 0.8407 | Val Loss: 0.3926 Acc: 0.8499                                               \n",
      "Epoch 044 | Train Loss: 0.3652 Acc: 0.8397 | Val Loss: 0.3107 Acc: 0.8701                                               \n",
      "Epoch 045 | Train Loss: 0.3667 Acc: 0.8397 | Val Loss: 0.3140 Acc: 0.9015                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 64, 'cnn_dropout': 0.1071101803367284, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 16, 'cnn_kernels_2': 32, 'learning_rate': 1.7690943532202843e-05, 'lstm_dense': 128, 'lstm_hidden_size': 128, 'lstm_layers': 5, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 62.2660 Acc: 0.2779 | Val Loss: 19.9973 Acc: 0.2385                                             \n",
      "Epoch 002 | Train Loss: 33.1573 Acc: 0.3164 | Val Loss: 8.7461 Acc: 0.3669                                              \n",
      "Epoch 003 | Train Loss: 23.6601 Acc: 0.3386 | Val Loss: 6.3375 Acc: 0.3958                                              \n",
      "Epoch 004 | Train Loss: 18.1395 Acc: 0.3387 | Val Loss: 4.3541 Acc: 0.3952                                              \n",
      "Epoch 005 | Train Loss: 14.3208 Acc: 0.3485 | Val Loss: 2.9660 Acc: 0.4355                                              \n",
      "Epoch 006 | Train Loss: 11.8906 Acc: 0.3605 | Val Loss: 2.9774 Acc: 0.4663                                              \n",
      "Epoch 007 | Train Loss: 10.2545 Acc: 0.3727 | Val Loss: 2.6832 Acc: 0.4797                                              \n",
      "Epoch 008 | Train Loss: 8.9862 Acc: 0.3853 | Val Loss: 2.3675 Acc: 0.5352                                               \n",
      "Epoch 009 | Train Loss: 8.2829 Acc: 0.3924 | Val Loss: 2.0049 Acc: 0.5570                                               \n",
      "Epoch 010 | Train Loss: 7.3992 Acc: 0.4015 | Val Loss: 1.8241 Acc: 0.5675                                               \n",
      "Epoch 011 | Train Loss: 6.7063 Acc: 0.4159 | Val Loss: 1.6168 Acc: 0.5713                                               \n",
      "Epoch 012 | Train Loss: 6.1486 Acc: 0.4241 | Val Loss: 1.8870 Acc: 0.5585                                               \n",
      "Epoch 013 | Train Loss: 5.8022 Acc: 0.4277 | Val Loss: 1.6218 Acc: 0.5549                                               \n",
      "Epoch 014 | Train Loss: 5.2345 Acc: 0.4436 | Val Loss: 1.3992 Acc: 0.5949                                               \n",
      "Epoch 015 | Train Loss: 4.8635 Acc: 0.4510 | Val Loss: 1.3372 Acc: 0.5979                                               \n",
      "Epoch 016 | Train Loss: 4.5704 Acc: 0.4641 | Val Loss: 1.3040 Acc: 0.6113                                               \n",
      "Epoch 017 | Train Loss: 4.3354 Acc: 0.4636 | Val Loss: 1.2917 Acc: 0.6627                                               \n",
      "Epoch 018 | Train Loss: 3.9686 Acc: 0.4762 | Val Loss: 1.1717 Acc: 0.6513                                               \n",
      "Epoch 019 | Train Loss: 3.8022 Acc: 0.4869 | Val Loss: 1.2518 Acc: 0.6558                                               \n",
      "Epoch 020 | Train Loss: 3.6665 Acc: 0.4843 | Val Loss: 1.0409 Acc: 0.6555                                               \n",
      "Epoch 021 | Train Loss: 3.4177 Acc: 0.4992 | Val Loss: 1.1533 Acc: 0.6540                                               \n",
      "Epoch 022 | Train Loss: 3.2688 Acc: 0.5066 | Val Loss: 1.1758 Acc: 0.6358                                               \n",
      "Epoch 023 | Train Loss: 3.0220 Acc: 0.5235 | Val Loss: 1.0276 Acc: 0.6684                                               \n",
      "Epoch 024 | Train Loss: 2.9507 Acc: 0.5212 | Val Loss: 1.2463 Acc: 0.6675                                               \n",
      "Epoch 025 | Train Loss: 2.7805 Acc: 0.5265 | Val Loss: 1.0622 Acc: 0.6567                                               \n",
      "Epoch 026 | Train Loss: 2.7089 Acc: 0.5400 | Val Loss: 1.0096 Acc: 0.6600                                               \n",
      "Epoch 027 | Train Loss: 2.5661 Acc: 0.5391 | Val Loss: 0.9425 Acc: 0.6884                                               \n",
      "Epoch 028 | Train Loss: 2.4035 Acc: 0.5525 | Val Loss: 1.0009 Acc: 0.6666                                               \n",
      "Epoch 029 | Train Loss: 2.3402 Acc: 0.5615 | Val Loss: 0.9231 Acc: 0.6988                                               \n",
      "Epoch 030 | Train Loss: 2.2619 Acc: 0.5619 | Val Loss: 0.9303 Acc: 0.6863                                               \n",
      "Epoch 031 | Train Loss: 2.1585 Acc: 0.5709 | Val Loss: 0.8817 Acc: 0.7045                                               \n",
      "Epoch 032 | Train Loss: 2.0757 Acc: 0.5803 | Val Loss: 0.8198 Acc: 0.7125                                               \n",
      "Epoch 033 | Train Loss: 1.9967 Acc: 0.5965 | Val Loss: 0.8608 Acc: 0.7093                                               \n",
      "Epoch 034 | Train Loss: 1.9811 Acc: 0.5909 | Val Loss: 0.8567 Acc: 0.7215                                               \n",
      "Epoch 035 | Train Loss: 1.8829 Acc: 0.6015 | Val Loss: 0.9450 Acc: 0.6997                                               \n",
      "Epoch 036 | Train Loss: 1.8035 Acc: 0.6074 | Val Loss: 0.9106 Acc: 0.7167                                               \n",
      "Epoch 037 | Train Loss: 1.7355 Acc: 0.6157 | Val Loss: 0.7379 Acc: 0.7540                                               \n",
      "Epoch 038 | Train Loss: 1.6266 Acc: 0.6247 | Val Loss: 0.7442 Acc: 0.7564                                               \n",
      "Epoch 039 | Train Loss: 1.6784 Acc: 0.6288 | Val Loss: 0.6985 Acc: 0.7642                                               \n",
      "Epoch 040 | Train Loss: 1.6061 Acc: 0.6347 | Val Loss: 0.7110 Acc: 0.7645                                               \n",
      "Epoch 041 | Train Loss: 1.5018 Acc: 0.6468 | Val Loss: 0.7535 Acc: 0.7558                                               \n",
      "Epoch 042 | Train Loss: 1.5021 Acc: 0.6495 | Val Loss: 0.7051 Acc: 0.7672                                               \n",
      "Epoch 043 | Train Loss: 1.4211 Acc: 0.6575 | Val Loss: 0.6591 Acc: 0.7773                                               \n",
      "Epoch 044 | Train Loss: 1.4044 Acc: 0.6653 | Val Loss: 0.8456 Acc: 0.7167                                               \n",
      "Epoch 045 | Train Loss: 1.3567 Acc: 0.6675 | Val Loss: 0.8121 Acc: 0.7027                                               \n",
      "Epoch 046 | Train Loss: 1.2663 Acc: 0.6810 | Val Loss: 0.6190 Acc: 0.7833                                               \n",
      "Epoch 047 | Train Loss: 1.2621 Acc: 0.6832 | Val Loss: 0.6587 Acc: 0.7773                                               \n",
      "Epoch 048 | Train Loss: 1.2168 Acc: 0.6951 | Val Loss: 0.7246 Acc: 0.7627                                               \n",
      "Epoch 049 | Train Loss: 1.1969 Acc: 0.6956 | Val Loss: 0.6009 Acc: 0.7979                                               \n",
      "Epoch 050 | Train Loss: 1.1541 Acc: 0.7059 | Val Loss: 0.8395 Acc: 0.7624                                               \n",
      "Epoch 051 | Train Loss: 1.1462 Acc: 0.7060 | Val Loss: 0.5440 Acc: 0.8158                                               \n",
      "Epoch 052 | Train Loss: 1.1020 Acc: 0.7121 | Val Loss: 0.5502 Acc: 0.8185                                               \n",
      "Epoch 053 | Train Loss: 1.0756 Acc: 0.7184 | Val Loss: 0.5314 Acc: 0.8221                                               \n",
      "Epoch 054 | Train Loss: 1.0315 Acc: 0.7298 | Val Loss: 0.6251 Acc: 0.7872                                               \n",
      "Epoch 055 | Train Loss: 1.0213 Acc: 0.7275 | Val Loss: 0.5851 Acc: 0.8096                                               \n",
      "Epoch 056 | Train Loss: 1.0064 Acc: 0.7333 | Val Loss: 0.6150 Acc: 0.7779                                               \n",
      "Epoch 057 | Train Loss: 0.9653 Acc: 0.7464 | Val Loss: 0.5792 Acc: 0.8155                                               \n",
      "Epoch 058 | Train Loss: 0.9482 Acc: 0.7452 | Val Loss: 0.4787 Acc: 0.8287                                               \n",
      "Epoch 059 | Train Loss: 0.9264 Acc: 0.7460 | Val Loss: 0.5184 Acc: 0.8266                                               \n",
      "Epoch 060 | Train Loss: 0.8813 Acc: 0.7544 | Val Loss: 0.5448 Acc: 0.8206                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 32, 'cnn_dropout': 0.3418388018393108, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 32, 'cnn_kernels_2': 64, 'learning_rate': 1.6355751302683747e-05, 'lstm_dense': 128, 'lstm_hidden_size': 96, 'lstm_layers': 4, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 11.3886 Acc: 0.3709 | Val Loss: 1.0758 Acc: 0.3752                                              \n",
      "Epoch 002 | Train Loss: 1.5703 Acc: 0.4089 | Val Loss: 0.9788 Acc: 0.4463                                               \n",
      "Epoch 003 | Train Loss: 1.2970 Acc: 0.4339 | Val Loss: 0.9908 Acc: 0.4857                                               \n",
      "Epoch 004 | Train Loss: 1.1828 Acc: 0.4368 | Val Loss: 0.9561 Acc: 0.5430                                               \n",
      "Epoch 005 | Train Loss: 1.1481 Acc: 0.4447 | Val Loss: 0.9365 Acc: 0.5164                                               \n",
      "Epoch 006 | Train Loss: 1.1029 Acc: 0.4605 | Val Loss: 0.9271 Acc: 0.5113                                               \n",
      "Epoch 007 | Train Loss: 1.0777 Acc: 0.4625 | Val Loss: 0.9213 Acc: 0.4707                                               \n",
      "Epoch 008 | Train Loss: 1.0577 Acc: 0.4614 | Val Loss: 0.9228 Acc: 0.4707                                               \n",
      "Epoch 009 | Train Loss: 1.0232 Acc: 0.4645 | Val Loss: 0.8992 Acc: 0.4890                                               \n",
      "Epoch 010 | Train Loss: 1.0256 Acc: 0.4644 | Val Loss: 0.8955 Acc: 0.5039                                               \n",
      "Epoch 011 | Train Loss: 1.0050 Acc: 0.4652 | Val Loss: 0.8863 Acc: 0.5284                                               \n",
      "Epoch 012 | Train Loss: 1.0006 Acc: 0.4723 | Val Loss: 0.8899 Acc: 0.4576                                               \n",
      "Epoch 013 | Train Loss: 0.9868 Acc: 0.4818 | Val Loss: 0.8951 Acc: 0.4546                                               \n",
      "Epoch 014 | Train Loss: 0.9879 Acc: 0.4846 | Val Loss: 0.8802 Acc: 0.5400                                               \n",
      "Epoch 015 | Train Loss: 0.9783 Acc: 0.4831 | Val Loss: 0.9078 Acc: 0.4654                                               \n",
      "Epoch 016 | Train Loss: 0.9682 Acc: 0.4824 | Val Loss: 0.8742 Acc: 0.5746                                               \n",
      "Epoch 017 | Train Loss: 0.9655 Acc: 0.4856 | Val Loss: 0.8699 Acc: 0.5872                                               \n",
      "Epoch 018 | Train Loss: 0.9618 Acc: 0.4873 | Val Loss: 0.8773 Acc: 0.5054                                               \n",
      "Epoch 019 | Train Loss: 0.9624 Acc: 0.4913 | Val Loss: 0.8805 Acc: 0.5310                                               \n",
      "Epoch 020 | Train Loss: 0.9554 Acc: 0.4872 | Val Loss: 0.9003 Acc: 0.4827                                               \n",
      "Epoch 021 | Train Loss: 0.9592 Acc: 0.4877 | Val Loss: 0.8824 Acc: 0.5540                                               \n",
      "Epoch 022 | Train Loss: 0.9580 Acc: 0.4908 | Val Loss: 0.8677 Acc: 0.5881                                               \n",
      "Epoch 023 | Train Loss: 0.9466 Acc: 0.4956 | Val Loss: 0.8666 Acc: 0.5734                                               \n",
      "Epoch 024 | Train Loss: 0.9554 Acc: 0.4893 | Val Loss: 0.8619 Acc: 0.5406                                               \n",
      "Epoch 025 | Train Loss: 0.9452 Acc: 0.4950 | Val Loss: 0.8731 Acc: 0.5206                                               \n",
      "Epoch 026 | Train Loss: 0.9520 Acc: 0.4929 | Val Loss: 0.8563 Acc: 0.5755                                               \n",
      "Epoch 027 | Train Loss: 0.9446 Acc: 0.5000 | Val Loss: 0.8754 Acc: 0.5501                                               \n",
      "Epoch 028 | Train Loss: 0.9445 Acc: 0.4988 | Val Loss: 0.8639 Acc: 0.4833                                               \n",
      "Epoch 029 | Train Loss: 0.9489 Acc: 0.4938 | Val Loss: 0.8677 Acc: 0.5513                                               \n",
      "Epoch 030 | Train Loss: 0.9407 Acc: 0.5010 | Val Loss: 0.8620 Acc: 0.4851                                               \n",
      "Epoch 031 | Train Loss: 0.9495 Acc: 0.4952 | Val Loss: 0.8703 Acc: 0.5499                                               \n",
      "Epoch 032 | Train Loss: 0.9453 Acc: 0.4940 | Val Loss: 0.8601 Acc: 0.4839                                               \n",
      "Epoch 033 | Train Loss: 0.9491 Acc: 0.4938 | Val Loss: 0.8621 Acc: 0.6104                                               \n",
      "Epoch 034 | Train Loss: 0.9359 Acc: 0.5018 | Val Loss: 0.8499 Acc: 0.6191                                               \n",
      "Epoch 035 | Train Loss: 0.9377 Acc: 0.5005 | Val Loss: 0.8465 Acc: 0.5558                                               \n",
      "Epoch 036 | Train Loss: 0.9305 Acc: 0.5039 | Val Loss: 0.8559 Acc: 0.4967                                               \n",
      "Epoch 037 | Train Loss: 0.9345 Acc: 0.5042 | Val Loss: 0.8742 Acc: 0.4845                                               \n",
      "Epoch 038 | Train Loss: 0.9281 Acc: 0.5100 | Val Loss: 0.8565 Acc: 0.4955                                               \n",
      "Epoch 039 | Train Loss: 0.9239 Acc: 0.5185 | Val Loss: 0.8552 Acc: 0.5343                                               \n",
      "Epoch 040 | Train Loss: 0.9260 Acc: 0.5167 | Val Loss: 0.8662 Acc: 0.5869                                               \n",
      "Epoch 041 | Train Loss: 0.9296 Acc: 0.5206 | Val Loss: 0.8329 Acc: 0.6194                                               \n",
      "Epoch 042 | Train Loss: 0.9271 Acc: 0.5232 | Val Loss: 0.8445 Acc: 0.5316                                               \n",
      "Epoch 043 | Train Loss: 0.9234 Acc: 0.5140 | Val Loss: 0.8375 Acc: 0.5501                                               \n",
      "Epoch 044 | Train Loss: 0.9201 Acc: 0.5240 | Val Loss: 0.8496 Acc: 0.5304                                               \n",
      "Epoch 045 | Train Loss: 0.9222 Acc: 0.5177 | Val Loss: 0.8535 Acc: 0.4937                                               \n",
      "Epoch 046 | Train Loss: 0.9169 Acc: 0.5314 | Val Loss: 0.8425 Acc: 0.5696                                               \n",
      "Epoch 047 | Train Loss: 0.9123 Acc: 0.5300 | Val Loss: 0.8542 Acc: 0.5101                                               \n",
      "Epoch 048 | Train Loss: 0.9162 Acc: 0.5340 | Val Loss: 0.8241 Acc: 0.6212                                               \n",
      "Epoch 049 | Train Loss: 0.9113 Acc: 0.5359 | Val Loss: 0.8312 Acc: 0.5740                                               \n",
      "Epoch 050 | Train Loss: 0.9140 Acc: 0.5256 | Val Loss: 0.8374 Acc: 0.5845                                               \n",
      "Epoch 051 | Train Loss: 0.9113 Acc: 0.5338 | Val Loss: 0.8352 Acc: 0.5579                                               \n",
      "Epoch 052 | Train Loss: 0.9220 Acc: 0.5235 | Val Loss: 0.8393 Acc: 0.5669                                               \n",
      "Epoch 053 | Train Loss: 0.9097 Acc: 0.5384 | Val Loss: 0.8225 Acc: 0.6018                                               \n",
      "Epoch 054 | Train Loss: 0.9067 Acc: 0.5277 | Val Loss: 0.8229 Acc: 0.5872                                               \n",
      "Epoch 055 | Train Loss: 0.9115 Acc: 0.5238 | Val Loss: 0.8278 Acc: 0.5904                                               \n",
      "Epoch 056 | Train Loss: 0.9120 Acc: 0.5236 | Val Loss: 0.8687 Acc: 0.5024                                               \n",
      "Epoch 057 | Train Loss: 0.9106 Acc: 0.5265 | Val Loss: 0.8408 Acc: 0.6063                                               \n",
      "Epoch 058 | Train Loss: 0.9113 Acc: 0.5299 | Val Loss: 0.8145 Acc: 0.6373                                               \n",
      "Epoch 059 | Train Loss: 0.9121 Acc: 0.5357 | Val Loss: 0.8394 Acc: 0.5922                                               \n",
      "Epoch 060 | Train Loss: 0.9108 Acc: 0.5282 | Val Loss: 0.8224 Acc: 0.6090                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 32, 'cnn_dropout': 0.6924155015664307, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 32, 'cnn_kernels_2': 16, 'learning_rate': 6.0215984396094246e-05, 'lstm_dense': 128, 'lstm_hidden_size': 32, 'lstm_layers': 4, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 63.9454 Acc: 0.3556 | Val Loss: 10.8852 Acc: 0.4113                                             \n",
      "Epoch 002 | Train Loss: 22.5680 Acc: 0.3754 | Val Loss: 4.7529 Acc: 0.4433                                              \n",
      "Epoch 003 | Train Loss: 14.4631 Acc: 0.3876 | Val Loss: 3.2837 Acc: 0.3812                                              \n",
      "Epoch 004 | Train Loss: 9.6685 Acc: 0.4104 | Val Loss: 3.8295 Acc: 0.4558                                               \n",
      "Epoch 005 | Train Loss: 7.0372 Acc: 0.4241 | Val Loss: 1.7172 Acc: 0.4901                                               \n",
      "Epoch 006 | Train Loss: 5.6390 Acc: 0.4294 | Val Loss: 1.4601 Acc: 0.5030                                               \n",
      "Epoch 007 | Train Loss: 4.5779 Acc: 0.4421 | Val Loss: 1.5698 Acc: 0.4782                                               \n",
      "Epoch 008 | Train Loss: 3.8530 Acc: 0.4498 | Val Loss: 1.2136 Acc: 0.5284                                               \n",
      "Epoch 009 | Train Loss: 3.3136 Acc: 0.4499 | Val Loss: 1.2636 Acc: 0.5931                                               \n",
      "Epoch 010 | Train Loss: 2.8387 Acc: 0.4815 | Val Loss: 1.1513 Acc: 0.5636                                               \n",
      "Epoch 011 | Train Loss: 2.5383 Acc: 0.4963 | Val Loss: 1.0727 Acc: 0.6170                                               \n",
      "Epoch 012 | Train Loss: 2.3378 Acc: 0.5046 | Val Loss: 0.9260 Acc: 0.6272                                               \n",
      "Epoch 013 | Train Loss: 2.1357 Acc: 0.5152 | Val Loss: 0.8216 Acc: 0.5863                                               \n",
      "Epoch 014 | Train Loss: 1.9046 Acc: 0.5216 | Val Loss: 0.9026 Acc: 0.5925                                               \n",
      "Epoch 015 | Train Loss: 1.7561 Acc: 0.5397 | Val Loss: 0.9637 Acc: 0.6340                                               \n",
      "Epoch 016 | Train Loss: 1.6465 Acc: 0.5421 | Val Loss: 0.9790 Acc: 0.6352                                               \n",
      "Epoch 017 | Train Loss: 1.5616 Acc: 0.5489 | Val Loss: 0.9115 Acc: 0.6179                                               \n",
      "Epoch 018 | Train Loss: 1.4488 Acc: 0.5645 | Val Loss: 0.7920 Acc: 0.6361                                               \n",
      "Epoch 019 | Train Loss: 1.3895 Acc: 0.5793 | Val Loss: 0.9020 Acc: 0.6573                                               \n",
      "Epoch 020 | Train Loss: 1.2599 Acc: 0.5877 | Val Loss: 0.8939 Acc: 0.6421                                               \n",
      "Epoch 021 | Train Loss: 1.2083 Acc: 0.5979 | Val Loss: 0.8488 Acc: 0.7084                                               \n",
      "Epoch 022 | Train Loss: 1.1657 Acc: 0.6050 | Val Loss: 1.0177 Acc: 0.5904                                               \n",
      "Epoch 023 | Train Loss: 1.1155 Acc: 0.6118 | Val Loss: 0.7843 Acc: 0.7039                                               \n",
      "Epoch 024 | Train Loss: 1.0776 Acc: 0.6202 | Val Loss: 0.7041 Acc: 0.7469                                               \n",
      "Epoch 025 | Train Loss: 1.0128 Acc: 0.6361 | Val Loss: 0.7495 Acc: 0.6857                                               \n",
      "Epoch 026 | Train Loss: 0.9911 Acc: 0.6306 | Val Loss: 0.7017 Acc: 0.7269                                               \n",
      "Epoch 027 | Train Loss: 0.9455 Acc: 0.6552 | Val Loss: 0.7567 Acc: 0.6221                                               \n",
      "Epoch 028 | Train Loss: 0.9344 Acc: 0.6550 | Val Loss: 0.7458 Acc: 0.6770                                               \n",
      "Epoch 029 | Train Loss: 0.9173 Acc: 0.6623 | Val Loss: 0.7274 Acc: 0.7128                                               \n",
      "Epoch 030 | Train Loss: 0.9038 Acc: 0.6657 | Val Loss: 0.6918 Acc: 0.7149                                               \n",
      "Epoch 031 | Train Loss: 0.8543 Acc: 0.6797 | Val Loss: 0.6491 Acc: 0.7403                                               \n",
      "Epoch 032 | Train Loss: 0.8488 Acc: 0.6904 | Val Loss: 0.6505 Acc: 0.7143                                               \n",
      "Epoch 033 | Train Loss: 0.8274 Acc: 0.6885 | Val Loss: 0.7073 Acc: 0.7128                                               \n",
      "Epoch 034 | Train Loss: 0.8011 Acc: 0.7001 | Val Loss: 0.6298 Acc: 0.6588                                               \n",
      "Epoch 035 | Train Loss: 0.7860 Acc: 0.7030 | Val Loss: 0.6954 Acc: 0.7409                                               \n",
      "Epoch 036 | Train Loss: 0.7511 Acc: 0.7177 | Val Loss: 0.7166 Acc: 0.7254                                               \n",
      "Epoch 037 | Train Loss: 0.7471 Acc: 0.7143 | Val Loss: 0.6443 Acc: 0.7728                                               \n",
      "Epoch 038 | Train Loss: 0.7185 Acc: 0.7348 | Val Loss: 0.6958 Acc: 0.7519                                               \n",
      "Epoch 039 | Train Loss: 0.7189 Acc: 0.7312 | Val Loss: 0.5874 Acc: 0.7899                                               \n",
      "Epoch 040 | Train Loss: 0.7057 Acc: 0.7384 | Val Loss: 0.7053 Acc: 0.7463                                               \n",
      "Epoch 041 | Train Loss: 0.6750 Acc: 0.7487 | Val Loss: 0.6736 Acc: 0.7651                                               \n",
      "Epoch 042 | Train Loss: 0.6616 Acc: 0.7551 | Val Loss: 0.6108 Acc: 0.7681                                               \n",
      "Epoch 043 | Train Loss: 0.6523 Acc: 0.7639 | Val Loss: 0.5970 Acc: 0.7591                                               \n",
      "Epoch 044 | Train Loss: 0.6492 Acc: 0.7617 | Val Loss: 0.5984 Acc: 0.7797                                               \n",
      "Epoch 045 | Train Loss: 0.6343 Acc: 0.7721 | Val Loss: 0.5231 Acc: 0.8191                                               \n",
      "Epoch 046 | Train Loss: 0.6148 Acc: 0.7739 | Val Loss: 0.5562 Acc: 0.8006                                               \n",
      "Epoch 047 | Train Loss: 0.5987 Acc: 0.7844 | Val Loss: 0.5435 Acc: 0.8167                                               \n",
      "Epoch 048 | Train Loss: 0.6008 Acc: 0.7857 | Val Loss: 0.6796 Acc: 0.7728                                               \n",
      "Epoch 049 | Train Loss: 0.5693 Acc: 0.7978 | Val Loss: 0.5448 Acc: 0.7958                                               \n",
      "Epoch 050 | Train Loss: 0.5528 Acc: 0.7995 | Val Loss: 0.5275 Acc: 0.7928                                               \n",
      "Epoch 051 | Train Loss: 0.5401 Acc: 0.8079 | Val Loss: 0.4956 Acc: 0.8143                                               \n",
      "Epoch 052 | Train Loss: 0.5324 Acc: 0.8074 | Val Loss: 0.5309 Acc: 0.7970                                               \n",
      "Epoch 053 | Train Loss: 0.5437 Acc: 0.8092 | Val Loss: 0.4621 Acc: 0.8376                                               \n",
      "Epoch 054 | Train Loss: 0.5170 Acc: 0.8154 | Val Loss: 0.4528 Acc: 0.8439                                               \n",
      "Epoch 055 | Train Loss: 0.5011 Acc: 0.8229 | Val Loss: 0.4758 Acc: 0.8352                                               \n",
      "Epoch 056 | Train Loss: 0.4770 Acc: 0.8289 | Val Loss: 0.4358 Acc: 0.8388                                               \n",
      "Epoch 057 | Train Loss: 0.4829 Acc: 0.8287 | Val Loss: 0.6329 Acc: 0.7791                                               \n",
      "Epoch 058 | Train Loss: 0.4588 Acc: 0.8368 | Val Loss: 0.4459 Acc: 0.8475                                               \n",
      "Epoch 059 | Train Loss: 0.4764 Acc: 0.8301 | Val Loss: 0.5259 Acc: 0.8149                                               \n",
      "Epoch 060 | Train Loss: 0.4609 Acc: 0.8380 | Val Loss: 0.4754 Acc: 0.8331                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 32, 'cnn_dropout': 0.2848540871121849, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 48, 'cnn_kernels_2': 32, 'learning_rate': 0.00042008255069187487, 'lstm_dense': 128, 'lstm_hidden_size': 32, 'lstm_layers': 1, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 26.1832 Acc: 0.3830 | Val Loss: 2.3473 Acc: 0.4890                                              \n",
      "Epoch 002 | Train Loss: 7.3836 Acc: 0.4270 | Val Loss: 2.5200 Acc: 0.4558                                               \n",
      "Epoch 003 | Train Loss: 4.2179 Acc: 0.4370 | Val Loss: 1.2406 Acc: 0.4743                                               \n",
      "Epoch 004 | Train Loss: 2.7886 Acc: 0.4712 | Val Loss: 0.8505 Acc: 0.6579                                               \n",
      "Epoch 005 | Train Loss: 2.0722 Acc: 0.4975 | Val Loss: 0.8768 Acc: 0.6090                                               \n",
      "Epoch 006 | Train Loss: 1.6600 Acc: 0.5064 | Val Loss: 0.7643 Acc: 0.7101                                               \n",
      "Epoch 007 | Train Loss: 1.4897 Acc: 0.5206 | Val Loss: 1.5396 Acc: 0.5352                                               \n",
      "Epoch 008 | Train Loss: 1.3281 Acc: 0.5397 | Val Loss: 1.0486 Acc: 0.5842                                               \n",
      "Epoch 009 | Train Loss: 1.1349 Acc: 0.5642 | Val Loss: 0.7108 Acc: 0.7000                                               \n",
      "Epoch 010 | Train Loss: 1.0083 Acc: 0.5859 | Val Loss: 0.8265 Acc: 0.6510                                               \n",
      "Epoch 011 | Train Loss: 0.9700 Acc: 0.5972 | Val Loss: 0.8597 Acc: 0.6104                                               \n",
      "Epoch 012 | Train Loss: 0.9222 Acc: 0.6025 | Val Loss: 0.6922 Acc: 0.7063                                               \n",
      "Epoch 013 | Train Loss: 0.8732 Acc: 0.6146 | Val Loss: 0.6779 Acc: 0.7033                                               \n",
      "Epoch 014 | Train Loss: 0.8139 Acc: 0.6270 | Val Loss: 0.7037 Acc: 0.6445                                               \n",
      "Epoch 015 | Train Loss: 0.7655 Acc: 0.6653 | Val Loss: 0.6404 Acc: 0.7349                                               \n",
      "Epoch 016 | Train Loss: 0.7016 Acc: 0.7030 | Val Loss: 0.6504 Acc: 0.7021                                               \n",
      "Epoch 017 | Train Loss: 0.6145 Acc: 0.7487 | Val Loss: 0.4767 Acc: 0.8078                                               \n",
      "Epoch 018 | Train Loss: 0.5457 Acc: 0.7850 | Val Loss: 0.4244 Acc: 0.8430                                               \n",
      "Epoch 019 | Train Loss: 0.4554 Acc: 0.8236 | Val Loss: 0.3376 Acc: 0.8737                                               \n",
      "Epoch 020 | Train Loss: 0.4036 Acc: 0.8466 | Val Loss: 0.2742 Acc: 0.9039                                               \n",
      "Epoch 021 | Train Loss: 0.3420 Acc: 0.8710 | Val Loss: 0.3034 Acc: 0.8797                                               \n",
      "Epoch 022 | Train Loss: 0.2996 Acc: 0.8913 | Val Loss: 0.2522 Acc: 0.9078                                               \n",
      "Epoch 023 | Train Loss: 0.2577 Acc: 0.9066 | Val Loss: 0.2009 Acc: 0.9269                                               \n",
      "Epoch 024 | Train Loss: 0.2386 Acc: 0.9145 | Val Loss: 0.1889 Acc: 0.9307                                               \n",
      "Epoch 025 | Train Loss: 0.2195 Acc: 0.9228 | Val Loss: 0.1976 Acc: 0.9281                                               \n",
      "Epoch 026 | Train Loss: 0.1982 Acc: 0.9294 | Val Loss: 0.1420 Acc: 0.9448                                               \n",
      "Epoch 027 | Train Loss: 0.1779 Acc: 0.9378 | Val Loss: 0.1677 Acc: 0.9358                                               \n",
      "Epoch 028 | Train Loss: 0.1695 Acc: 0.9419 | Val Loss: 0.1421 Acc: 0.9537                                               \n",
      "Epoch 029 | Train Loss: 0.1614 Acc: 0.9415 | Val Loss: 0.1328 Acc: 0.9525                                               \n",
      "Epoch 030 | Train Loss: 0.1415 Acc: 0.9510 | Val Loss: 0.1265 Acc: 0.9558                                               \n",
      "Epoch 031 | Train Loss: 0.1386 Acc: 0.9513 | Val Loss: 0.1463 Acc: 0.9507                                               \n",
      "Epoch 032 | Train Loss: 0.1143 Acc: 0.9611 | Val Loss: 0.1184 Acc: 0.9591                                               \n",
      "Epoch 033 | Train Loss: 0.1285 Acc: 0.9563 | Val Loss: 0.1507 Acc: 0.9430                                               \n",
      "Epoch 034 | Train Loss: 0.1051 Acc: 0.9638 | Val Loss: 0.1035 Acc: 0.9624                                               \n",
      "Epoch 035 | Train Loss: 0.1091 Acc: 0.9615 | Val Loss: 0.1356 Acc: 0.9501                                               \n",
      "Epoch 036 | Train Loss: 0.1038 Acc: 0.9639 | Val Loss: 0.1160 Acc: 0.9642                                               \n",
      "Epoch 037 | Train Loss: 0.1061 Acc: 0.9632 | Val Loss: 0.1067 Acc: 0.9612                                               \n",
      "Epoch 038 | Train Loss: 0.0947 Acc: 0.9686 | Val Loss: 0.0794 Acc: 0.9722                                               \n",
      "Epoch 039 | Train Loss: 0.0784 Acc: 0.9719 | Val Loss: 0.1171 Acc: 0.9591                                               \n",
      "Epoch 040 | Train Loss: 0.0816 Acc: 0.9738 | Val Loss: 0.1034 Acc: 0.9633                                               \n",
      "Epoch 041 | Train Loss: 0.0860 Acc: 0.9707 | Val Loss: 0.1021 Acc: 0.9669                                               \n",
      "Epoch 042 | Train Loss: 0.0664 Acc: 0.9781 | Val Loss: 0.0811 Acc: 0.9687                                               \n",
      "Epoch 043 | Train Loss: 0.0729 Acc: 0.9749 | Val Loss: 0.0643 Acc: 0.9803                                               \n",
      "Epoch 044 | Train Loss: 0.0661 Acc: 0.9773 | Val Loss: 0.1064 Acc: 0.9648                                               \n",
      "Epoch 045 | Train Loss: 0.0794 Acc: 0.9713 | Val Loss: 0.0981 Acc: 0.9675                                               \n",
      "Epoch 046 | Train Loss: 0.0674 Acc: 0.9769 | Val Loss: 0.0826 Acc: 0.9707                                               \n",
      "Epoch 047 | Train Loss: 0.0574 Acc: 0.9813 | Val Loss: 0.0624 Acc: 0.9809                                               \n",
      "Epoch 048 | Train Loss: 0.0617 Acc: 0.9789 | Val Loss: 0.0739 Acc: 0.9770                                               \n",
      "Epoch 049 | Train Loss: 0.0514 Acc: 0.9830 | Val Loss: 0.0664 Acc: 0.9776                                               \n",
      "Epoch 050 | Train Loss: 0.0584 Acc: 0.9796 | Val Loss: 0.0665 Acc: 0.9746                                               \n",
      "Epoch 051 | Train Loss: 0.0539 Acc: 0.9813 | Val Loss: 0.0523 Acc: 0.9803                                               \n",
      "Epoch 052 | Train Loss: 0.0490 Acc: 0.9837 | Val Loss: 0.0653 Acc: 0.9782                                               \n",
      "Epoch 053 | Train Loss: 0.0543 Acc: 0.9816 | Val Loss: 0.1070 Acc: 0.9639                                               \n",
      "Epoch 054 | Train Loss: 0.0499 Acc: 0.9823 | Val Loss: 0.0647 Acc: 0.9776                                               \n",
      "Epoch 055 | Train Loss: 0.0525 Acc: 0.9826 | Val Loss: 0.0786 Acc: 0.9725                                               \n",
      "Epoch 056 | Train Loss: 0.0537 Acc: 0.9829 | Val Loss: 0.0669 Acc: 0.9773                                               \n",
      "Epoch 057 | Train Loss: 0.0521 Acc: 0.9810 | Val Loss: 0.0701 Acc: 0.9770                                               \n",
      "Epoch 058 | Train Loss: 0.0487 Acc: 0.9835 | Val Loss: 0.0619 Acc: 0.9773                                               \n",
      "Epoch 059 | Train Loss: 0.0450 Acc: 0.9845 | Val Loss: 0.0592 Acc: 0.9788                                               \n",
      "Epoch 060 | Train Loss: 0.0379 Acc: 0.9884 | Val Loss: 0.0670 Acc: 0.9800                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 256, 'cnn_dropout': 0.6564170900421019, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 16, 'cnn_kernels_2': 32, 'learning_rate': 0.006534714698627456, 'lstm_dense': 256, 'lstm_hidden_size': 96, 'lstm_layers': 3, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 5.9775 Acc: 0.4638 | Val Loss: 0.8604 Acc: 0.5561                                               \n",
      "Epoch 002 | Train Loss: 0.8774 Acc: 0.5698 | Val Loss: 0.7324 Acc: 0.6406                                               \n",
      "Epoch 003 | Train Loss: 0.7792 Acc: 0.6286 | Val Loss: 0.6741 Acc: 0.6212                                               \n",
      "Epoch 004 | Train Loss: 0.7343 Acc: 0.6552 | Val Loss: 0.6329 Acc: 0.6916                                               \n",
      "Epoch 005 | Train Loss: 0.7032 Acc: 0.6764 | Val Loss: 0.6118 Acc: 0.7290                                               \n",
      "Epoch 006 | Train Loss: 0.6781 Acc: 0.6856 | Val Loss: 0.5727 Acc: 0.7651                                               \n",
      "Epoch 007 | Train Loss: 0.7187 Acc: 0.6678 | Val Loss: 0.6688 Acc: 0.6922                                               \n",
      "Epoch 008 | Train Loss: 0.6633 Acc: 0.6944 | Val Loss: 0.6859 Acc: 0.6827                                               \n",
      "Epoch 009 | Train Loss: 0.6464 Acc: 0.7012 | Val Loss: 0.6152 Acc: 0.6713                                               \n",
      "Epoch 010 | Train Loss: 0.6593 Acc: 0.7016 | Val Loss: 0.6410 Acc: 0.6836                                               \n",
      "Epoch 011 | Train Loss: 0.6366 Acc: 0.7142 | Val Loss: 0.6697 Acc: 0.6952                                               \n",
      "Epoch 012 | Train Loss: 0.6551 Acc: 0.7018 | Val Loss: 0.6633 Acc: 0.7549                                               \n",
      "Epoch 013 | Train Loss: 0.6769 Acc: 0.6967 | Val Loss: 0.6804 Acc: 0.6421                                               \n",
      "Epoch 014 | Train Loss: 0.6771 Acc: 0.6891 | Val Loss: 0.7394 Acc: 0.6373                                               \n",
      "Epoch 015 | Train Loss: 0.6644 Acc: 0.6928 | Val Loss: 0.7254 Acc: 0.6561                                               \n",
      "Epoch 016 | Train Loss: 0.6804 Acc: 0.6907 | Val Loss: 0.6649 Acc: 0.6949                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 256, 'cnn_dropout': 0.46974012217580485, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 48, 'cnn_kernels_2': 32, 'learning_rate': 0.0018483787804999174, 'lstm_dense': 256, 'lstm_hidden_size': 96, 'lstm_layers': 3, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 9.0636 Acc: 0.4353 | Val Loss: 1.6520 Acc: 0.5925                                               \n",
      "Epoch 002 | Train Loss: 1.7361 Acc: 0.5288 | Val Loss: 1.0264 Acc: 0.5322                                               \n",
      "Epoch 003 | Train Loss: 1.1008 Acc: 0.5721 | Val Loss: 0.9742 Acc: 0.6224                                               \n",
      "Epoch 004 | Train Loss: 0.8991 Acc: 0.6109 | Val Loss: 0.7606 Acc: 0.6224                                               \n",
      "Epoch 005 | Train Loss: 0.8084 Acc: 0.6350 | Val Loss: 0.6571 Acc: 0.6973                                               \n",
      "Epoch 006 | Train Loss: 0.7329 Acc: 0.6613 | Val Loss: 0.6916 Acc: 0.6946                                               \n",
      "Epoch 007 | Train Loss: 0.7011 Acc: 0.6783 | Val Loss: 0.5993 Acc: 0.7260                                               \n",
      "Epoch 008 | Train Loss: 0.6590 Acc: 0.6949 | Val Loss: 0.6203 Acc: 0.7334                                               \n",
      "Epoch 009 | Train Loss: 0.6387 Acc: 0.7039 | Val Loss: 0.5804 Acc: 0.7221                                               \n",
      "Epoch 010 | Train Loss: 0.6058 Acc: 0.7253 | Val Loss: 0.5215 Acc: 0.7713                                               \n",
      "Epoch 011 | Train Loss: 0.5661 Acc: 0.7578 | Val Loss: 0.5681 Acc: 0.7430                                               \n",
      "Epoch 012 | Train Loss: 0.5311 Acc: 0.7725 | Val Loss: 0.5142 Acc: 0.7946                                               \n",
      "Epoch 013 | Train Loss: 0.4980 Acc: 0.7926 | Val Loss: 0.4495 Acc: 0.8176                                               \n",
      "Epoch 014 | Train Loss: 0.4943 Acc: 0.7933 | Val Loss: 0.4980 Acc: 0.7549                                               \n",
      "Epoch 015 | Train Loss: 0.4732 Acc: 0.8058 | Val Loss: 0.4164 Acc: 0.8152                                               \n",
      "Epoch 016 | Train Loss: 0.4783 Acc: 0.8056 | Val Loss: 0.4483 Acc: 0.8039                                               \n",
      "Epoch 017 | Train Loss: 0.4325 Acc: 0.8191 | Val Loss: 0.4573 Acc: 0.7949                                               \n",
      "Epoch 018 | Train Loss: 0.4189 Acc: 0.8314 | Val Loss: 0.4203 Acc: 0.8146                                               \n",
      "Epoch 019 | Train Loss: 0.4048 Acc: 0.8366 | Val Loss: 0.3946 Acc: 0.8403                                               \n",
      "Epoch 020 | Train Loss: 0.3823 Acc: 0.8443 | Val Loss: 0.3193 Acc: 0.8600                                               \n",
      "Epoch 021 | Train Loss: 0.3585 Acc: 0.8593 | Val Loss: 0.4737 Acc: 0.7833                                               \n",
      "Epoch 022 | Train Loss: 0.3629 Acc: 0.8554 | Val Loss: 0.3068 Acc: 0.8767                                               \n",
      "Epoch 023 | Train Loss: 0.3368 Acc: 0.8655 | Val Loss: 0.3241 Acc: 0.8478                                               \n",
      "Epoch 024 | Train Loss: 0.3713 Acc: 0.8510 | Val Loss: 0.3149 Acc: 0.8594                                               \n",
      "Epoch 025 | Train Loss: 0.3342 Acc: 0.8685 | Val Loss: 0.3631 Acc: 0.8600                                               \n",
      "Epoch 026 | Train Loss: 0.3611 Acc: 0.8548 | Val Loss: 0.4760 Acc: 0.8125                                               \n",
      "Epoch 027 | Train Loss: 0.3267 Acc: 0.8669 | Val Loss: 0.3006 Acc: 0.8773                                               \n",
      "Epoch 028 | Train Loss: 0.3289 Acc: 0.8698 | Val Loss: 0.3011 Acc: 0.8767                                               \n",
      "Epoch 029 | Train Loss: 0.2893 Acc: 0.8890 | Val Loss: 0.2587 Acc: 0.9030                                               \n",
      "Epoch 030 | Train Loss: 0.2773 Acc: 0.8927 | Val Loss: 0.2968 Acc: 0.8630                                               \n",
      "Epoch 031 | Train Loss: 0.2957 Acc: 0.8889 | Val Loss: 0.3218 Acc: 0.8815                                               \n",
      "Epoch 032 | Train Loss: 0.2690 Acc: 0.8960 | Val Loss: 0.3150 Acc: 0.8675                                               \n",
      "Epoch 033 | Train Loss: 0.3147 Acc: 0.8822 | Val Loss: 0.3591 Acc: 0.8639                                               \n",
      "Epoch 034 | Train Loss: 0.3095 Acc: 0.8833 | Val Loss: 0.2897 Acc: 0.8866                                               \n",
      "Epoch 035 | Train Loss: 0.2769 Acc: 0.8939 | Val Loss: 0.3049 Acc: 0.8585                                               \n",
      "Epoch 036 | Train Loss: 0.2721 Acc: 0.8947 | Val Loss: 0.2222 Acc: 0.9081                                               \n",
      "Epoch 037 | Train Loss: 0.2570 Acc: 0.9051 | Val Loss: 0.2817 Acc: 0.9036                                               \n",
      "Epoch 038 | Train Loss: 0.2466 Acc: 0.9049 | Val Loss: 0.2179 Acc: 0.9131                                               \n",
      "Epoch 039 | Train Loss: 0.2790 Acc: 0.8948 | Val Loss: 0.2676 Acc: 0.8910                                               \n",
      "Epoch 040 | Train Loss: 0.2899 Acc: 0.8883 | Val Loss: 0.2254 Acc: 0.8982                                               \n",
      "Epoch 041 | Train Loss: 0.2717 Acc: 0.8934 | Val Loss: 0.3516 Acc: 0.8573                                               \n",
      "Epoch 042 | Train Loss: 0.2415 Acc: 0.9048 | Val Loss: 0.2257 Acc: 0.9209                                               \n",
      "Epoch 043 | Train Loss: 0.2525 Acc: 0.9030 | Val Loss: 0.2980 Acc: 0.8881                                               \n",
      "Epoch 044 | Train Loss: 0.2248 Acc: 0.9160 | Val Loss: 0.2557 Acc: 0.8943                                               \n",
      "Epoch 045 | Train Loss: 0.2492 Acc: 0.9069 | Val Loss: 0.2647 Acc: 0.9051                                               \n",
      "Epoch 046 | Train Loss: 0.2571 Acc: 0.9032 | Val Loss: 0.1994 Acc: 0.9266                                               \n",
      "Epoch 047 | Train Loss: 0.2486 Acc: 0.9041 | Val Loss: 0.2035 Acc: 0.9227                                               \n",
      "Epoch 048 | Train Loss: 0.2552 Acc: 0.9034 | Val Loss: 0.3418 Acc: 0.8737                                               \n",
      "Epoch 049 | Train Loss: 0.2463 Acc: 0.9071 | Val Loss: 0.2653 Acc: 0.9024                                               \n",
      "Epoch 050 | Train Loss: 0.2681 Acc: 0.9021 | Val Loss: 0.3046 Acc: 0.8800                                               \n",
      "Epoch 051 | Train Loss: 0.2206 Acc: 0.9172 | Val Loss: 0.2888 Acc: 0.8958                                               \n",
      "Epoch 052 | Train Loss: 0.2303 Acc: 0.9145 | Val Loss: 0.2390 Acc: 0.9096                                               \n",
      "Epoch 053 | Train Loss: 0.2138 Acc: 0.9231 | Val Loss: 0.2365 Acc: 0.9122                                               \n",
      "Epoch 054 | Train Loss: 0.2328 Acc: 0.9151 | Val Loss: 0.1848 Acc: 0.9331                                               \n",
      "Epoch 055 | Train Loss: 0.2396 Acc: 0.9121 | Val Loss: 0.3383 Acc: 0.8833                                               \n",
      "Epoch 056 | Train Loss: 0.2415 Acc: 0.9125 | Val Loss: 0.1765 Acc: 0.9313                                               \n",
      "Epoch 057 | Train Loss: 0.2002 Acc: 0.9251 | Val Loss: 0.2134 Acc: 0.9206                                               \n",
      "Epoch 058 | Train Loss: 0.1958 Acc: 0.9263 | Val Loss: 0.1940 Acc: 0.9266                                               \n",
      "Epoch 059 | Train Loss: 0.2154 Acc: 0.9190 | Val Loss: 0.2116 Acc: 0.9260                                               \n",
      "Epoch 060 | Train Loss: 0.2088 Acc: 0.9234 | Val Loss: 0.2530 Acc: 0.9137                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 256, 'cnn_dropout': 0.5664051506248281, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 48, 'cnn_kernels_2': 16, 'learning_rate': 0.0007195559890248806, 'lstm_dense': 64, 'lstm_hidden_size': 128, 'lstm_layers': 2, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 12.0939 Acc: 0.4288 | Val Loss: 1.6333 Acc: 0.5349                                              \n",
      "Epoch 002 | Train Loss: 3.1451 Acc: 0.4966 | Val Loss: 1.0626 Acc: 0.5997                                               \n",
      "Epoch 003 | Train Loss: 1.9556 Acc: 0.5252 | Val Loss: 1.4103 Acc: 0.5654                                               \n",
      "Epoch 004 | Train Loss: 1.2819 Acc: 0.5657 | Val Loss: 0.9913 Acc: 0.6672                                               \n",
      "Epoch 005 | Train Loss: 0.9608 Acc: 0.6093 | Val Loss: 0.6662 Acc: 0.7084                                               \n",
      "Epoch 006 | Train Loss: 0.8194 Acc: 0.6377 | Val Loss: 0.6114 Acc: 0.7346                                               \n",
      "Epoch 007 | Train Loss: 0.7453 Acc: 0.6646 | Val Loss: 0.5991 Acc: 0.7090                                               \n",
      "Epoch 008 | Train Loss: 0.6618 Acc: 0.6962 | Val Loss: 0.5097 Acc: 0.7493                                               \n",
      "Epoch 009 | Train Loss: 0.6272 Acc: 0.7138 | Val Loss: 0.4887 Acc: 0.7800                                               \n",
      "Epoch 010 | Train Loss: 0.5980 Acc: 0.7343 | Val Loss: 0.5363 Acc: 0.7412                                               \n",
      "Epoch 011 | Train Loss: 0.5573 Acc: 0.7591 | Val Loss: 0.5671 Acc: 0.7558                                               \n",
      "Epoch 012 | Train Loss: 0.4985 Acc: 0.7916 | Val Loss: 0.3965 Acc: 0.7973                                               \n",
      "Epoch 013 | Train Loss: 0.4483 Acc: 0.8110 | Val Loss: 0.4223 Acc: 0.8352                                               \n",
      "Epoch 014 | Train Loss: 0.4115 Acc: 0.8288 | Val Loss: 0.3850 Acc: 0.8579                                               \n",
      "Epoch 015 | Train Loss: 0.3698 Acc: 0.8519 | Val Loss: 0.3015 Acc: 0.8743                                               \n",
      "Epoch 016 | Train Loss: 0.3659 Acc: 0.8519 | Val Loss: 0.2968 Acc: 0.8812                                               \n",
      "Epoch 017 | Train Loss: 0.3253 Acc: 0.8716 | Val Loss: 0.3252 Acc: 0.8669                                               \n",
      "Epoch 018 | Train Loss: 0.3050 Acc: 0.8802 | Val Loss: 0.2749 Acc: 0.8785                                               \n",
      "Epoch 019 | Train Loss: 0.2841 Acc: 0.8913 | Val Loss: 0.1890 Acc: 0.9388                                               \n",
      "Epoch 020 | Train Loss: 0.2713 Acc: 0.8977 | Val Loss: 0.3023 Acc: 0.8919                                               \n",
      "Epoch 021 | Train Loss: 0.2561 Acc: 0.9054 | Val Loss: 0.2331 Acc: 0.9063                                               \n",
      "Epoch 022 | Train Loss: 0.2620 Acc: 0.9035 | Val Loss: 0.2747 Acc: 0.8854                                               \n",
      "Epoch 023 | Train Loss: 0.2220 Acc: 0.9183 | Val Loss: 0.1802 Acc: 0.9293                                               \n",
      "Epoch 024 | Train Loss: 0.2014 Acc: 0.9278 | Val Loss: 0.1836 Acc: 0.9316                                               \n",
      "Epoch 025 | Train Loss: 0.2056 Acc: 0.9292 | Val Loss: 0.2140 Acc: 0.9275                                               \n",
      "Epoch 026 | Train Loss: 0.1855 Acc: 0.9372 | Val Loss: 0.1872 Acc: 0.9319                                               \n",
      "Epoch 027 | Train Loss: 0.1833 Acc: 0.9357 | Val Loss: 0.1418 Acc: 0.9493                                               \n",
      "Epoch 028 | Train Loss: 0.1636 Acc: 0.9423 | Val Loss: 0.1453 Acc: 0.9543                                               \n",
      "Epoch 029 | Train Loss: 0.1715 Acc: 0.9413 | Val Loss: 0.1221 Acc: 0.9484                                               \n",
      "Epoch 030 | Train Loss: 0.1537 Acc: 0.9487 | Val Loss: 0.1322 Acc: 0.9591                                               \n",
      "Epoch 031 | Train Loss: 0.1459 Acc: 0.9495 | Val Loss: 0.1500 Acc: 0.9481                                               \n",
      "Epoch 032 | Train Loss: 0.1360 Acc: 0.9529 | Val Loss: 0.2000 Acc: 0.9287                                               \n",
      "Epoch 033 | Train Loss: 0.1284 Acc: 0.9561 | Val Loss: 0.0910 Acc: 0.9696                                               \n",
      "Epoch 034 | Train Loss: 0.1492 Acc: 0.9526 | Val Loss: 0.1379 Acc: 0.9519                                               \n",
      "Epoch 035 | Train Loss: 0.1364 Acc: 0.9557 | Val Loss: 0.0974 Acc: 0.9710                                               \n",
      "Epoch 036 | Train Loss: 0.1220 Acc: 0.9568 | Val Loss: 0.0929 Acc: 0.9699                                               \n",
      "Epoch 037 | Train Loss: 0.1224 Acc: 0.9640 | Val Loss: 0.0894 Acc: 0.9767                                               \n",
      "Epoch 038 | Train Loss: 0.1140 Acc: 0.9653 | Val Loss: 0.1679 Acc: 0.9388                                               \n",
      "Epoch 039 | Train Loss: 0.1127 Acc: 0.9604 | Val Loss: 0.0770 Acc: 0.9728                                               \n",
      "Epoch 040 | Train Loss: 0.1048 Acc: 0.9660 | Val Loss: 0.0952 Acc: 0.9696                                               \n",
      "Epoch 041 | Train Loss: 0.1079 Acc: 0.9632 | Val Loss: 0.0872 Acc: 0.9663                                               \n",
      "Epoch 042 | Train Loss: 0.1017 Acc: 0.9668 | Val Loss: 0.1279 Acc: 0.9618                                               \n",
      "Epoch 043 | Train Loss: 0.1014 Acc: 0.9673 | Val Loss: 0.1579 Acc: 0.9540                                               \n",
      "Epoch 044 | Train Loss: 0.0900 Acc: 0.9708 | Val Loss: 0.1052 Acc: 0.9615                                               \n",
      "Epoch 045 | Train Loss: 0.0882 Acc: 0.9728 | Val Loss: 0.2382 Acc: 0.9215                                               \n",
      "Epoch 046 | Train Loss: 0.0908 Acc: 0.9707 | Val Loss: 0.1969 Acc: 0.9397                                               \n",
      "Epoch 047 | Train Loss: 0.0845 Acc: 0.9727 | Val Loss: 0.1219 Acc: 0.9510                                               \n",
      "Epoch 048 | Train Loss: 0.0889 Acc: 0.9732 | Val Loss: 0.0816 Acc: 0.9749                                               \n",
      "Epoch 049 | Train Loss: 0.0840 Acc: 0.9727 | Val Loss: 0.0989 Acc: 0.9669                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 32, 'cnn_dropout': 0.6108468897817194, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 16, 'cnn_kernels_2': 32, 'learning_rate': 0.0015094398028362465, 'lstm_dense': 256, 'lstm_hidden_size': 64, 'lstm_layers': 3, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 9.6507 Acc: 0.4327 | Val Loss: 1.6122 Acc: 0.5451                                               \n",
      "Epoch 002 | Train Loss: 1.6150 Acc: 0.5028 | Val Loss: 1.1517 Acc: 0.6024                                               \n",
      "Epoch 003 | Train Loss: 1.0720 Acc: 0.5656 | Val Loss: 0.7336 Acc: 0.6842                                               \n",
      "Epoch 004 | Train Loss: 0.9384 Acc: 0.5947 | Val Loss: 0.9698 Acc: 0.5713                                               \n",
      "Epoch 005 | Train Loss: 0.8162 Acc: 0.6200 | Val Loss: 0.6612 Acc: 0.7057                                               \n",
      "Epoch 006 | Train Loss: 0.7389 Acc: 0.6597 | Val Loss: 0.6687 Acc: 0.6958                                               \n",
      "Epoch 007 | Train Loss: 0.7142 Acc: 0.6755 | Val Loss: 0.6526 Acc: 0.6636                                               \n",
      "Epoch 008 | Train Loss: 0.6710 Acc: 0.6977 | Val Loss: 0.6260 Acc: 0.7119                                               \n",
      "Epoch 009 | Train Loss: 0.6541 Acc: 0.7006 | Val Loss: 0.6655 Acc: 0.6672                                               \n",
      "Epoch 010 | Train Loss: 0.6204 Acc: 0.7120 | Val Loss: 0.5859 Acc: 0.7278                                               \n",
      "Epoch 011 | Train Loss: 0.6292 Acc: 0.7163 | Val Loss: 0.5649 Acc: 0.7481                                               \n",
      "Epoch 012 | Train Loss: 0.6003 Acc: 0.7257 | Val Loss: 0.6833 Acc: 0.6696                                               \n",
      "Epoch 013 | Train Loss: 0.5907 Acc: 0.7308 | Val Loss: 0.6310 Acc: 0.7087                                               \n",
      "Epoch 014 | Train Loss: 0.5823 Acc: 0.7378 | Val Loss: 0.5270 Acc: 0.7833                                               \n",
      "Epoch 015 | Train Loss: 0.5636 Acc: 0.7386 | Val Loss: 0.6355 Acc: 0.6899                                               \n",
      "Epoch 016 | Train Loss: 0.5726 Acc: 0.7362 | Val Loss: 0.6259 Acc: 0.6985                                               \n",
      "Epoch 017 | Train Loss: 0.5468 Acc: 0.7410 | Val Loss: 0.6925 Acc: 0.7024                                               \n",
      "Epoch 018 | Train Loss: 0.5510 Acc: 0.7476 | Val Loss: 0.5957 Acc: 0.7170                                               \n",
      "Epoch 019 | Train Loss: 0.5399 Acc: 0.7512 | Val Loss: 0.6541 Acc: 0.7078                                               \n",
      "Epoch 020 | Train Loss: 0.5648 Acc: 0.7417 | Val Loss: 0.6425 Acc: 0.7009                                               \n",
      "Epoch 021 | Train Loss: 0.5297 Acc: 0.7540 | Val Loss: 0.5042 Acc: 0.7958                                               \n",
      "Epoch 022 | Train Loss: 0.5048 Acc: 0.7713 | Val Loss: 0.6694 Acc: 0.7125                                               \n",
      "Epoch 023 | Train Loss: 0.5301 Acc: 0.7627 | Val Loss: 0.6106 Acc: 0.7403                                               \n",
      "Epoch 024 | Train Loss: 0.5373 Acc: 0.7555 | Val Loss: 0.8711 Acc: 0.5958                                               \n",
      "Epoch 025 | Train Loss: 0.5361 Acc: 0.7564 | Val Loss: 0.6278 Acc: 0.7090                                               \n",
      "Epoch 026 | Train Loss: 0.5162 Acc: 0.7689 | Val Loss: 0.6210 Acc: 0.7397                                               \n",
      "Epoch 027 | Train Loss: 0.5109 Acc: 0.7716 | Val Loss: 0.6971 Acc: 0.6922                                               \n",
      "Epoch 028 | Train Loss: 0.5203 Acc: 0.7662 | Val Loss: 0.6257 Acc: 0.7218                                               \n",
      "Epoch 029 | Train Loss: 0.5142 Acc: 0.7672 | Val Loss: 0.6592 Acc: 0.6716                                               \n",
      "Epoch 030 | Train Loss: 0.5059 Acc: 0.7718 | Val Loss: 0.5415 Acc: 0.7991                                               \n",
      "Epoch 031 | Train Loss: 0.5213 Acc: 0.7635 | Val Loss: 0.6913 Acc: 0.6836                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 32, 'cnn_dropout': 0.6396633967436184, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 48, 'cnn_kernels_2': 64, 'learning_rate': 3.636750288689353e-05, 'lstm_dense': 256, 'lstm_hidden_size': 32, 'lstm_layers': 6, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 112.4740 Acc: 0.2538 | Val Loss: 41.0764 Acc: 0.4364                                            \n",
      "Epoch 002 | Train Loss: 53.4702 Acc: 0.3388 | Val Loss: 27.9151 Acc: 0.4681                                             \n",
      "Epoch 003 | Train Loss: 40.5126 Acc: 0.3603 | Val Loss: 22.0795 Acc: 0.4666                                             \n",
      "Epoch 004 | Train Loss: 32.1037 Acc: 0.3752 | Val Loss: 19.9715 Acc: 0.4600                                             \n",
      "Epoch 005 | Train Loss: 26.4905 Acc: 0.3879 | Val Loss: 16.4655 Acc: 0.4624                                             \n",
      "Epoch 006 | Train Loss: 23.3305 Acc: 0.3859 | Val Loss: 13.0875 Acc: 0.4669                                             \n",
      "Epoch 007 | Train Loss: 19.1311 Acc: 0.3967 | Val Loss: 10.6624 Acc: 0.4815                                             \n",
      "Epoch 008 | Train Loss: 17.1462 Acc: 0.4009 | Val Loss: 9.8249 Acc: 0.4728                                              \n",
      "Epoch 009 | Train Loss: 15.0042 Acc: 0.4056 | Val Loss: 6.9020 Acc: 0.4919                                              \n",
      "Epoch 010 | Train Loss: 13.7856 Acc: 0.4064 | Val Loss: 5.0763 Acc: 0.5072                                              \n",
      "Epoch 011 | Train Loss: 12.3040 Acc: 0.4128 | Val Loss: 4.6029 Acc: 0.4752                                              \n",
      "Epoch 012 | Train Loss: 10.7403 Acc: 0.4241 | Val Loss: 4.2822 Acc: 0.4749                                              \n",
      "Epoch 013 | Train Loss: 10.1890 Acc: 0.4241 | Val Loss: 3.4633 Acc: 0.5093                                              \n",
      "Epoch 014 | Train Loss: 9.1165 Acc: 0.4248 | Val Loss: 3.7347 Acc: 0.4934                                               \n",
      "Epoch 015 | Train Loss: 8.4377 Acc: 0.4294 | Val Loss: 2.8859 Acc: 0.5185                                               \n",
      "Epoch 016 | Train Loss: 7.9146 Acc: 0.4328 | Val Loss: 2.6205 Acc: 0.5075                                               \n",
      "Epoch 017 | Train Loss: 7.4205 Acc: 0.4323 | Val Loss: 2.3421 Acc: 0.5197                                               \n",
      "Epoch 018 | Train Loss: 6.9479 Acc: 0.4387 | Val Loss: 2.1060 Acc: 0.5412                                               \n",
      "Epoch 019 | Train Loss: 6.4795 Acc: 0.4388 | Val Loss: 2.0591 Acc: 0.5487                                               \n",
      "Epoch 020 | Train Loss: 6.1178 Acc: 0.4445 | Val Loss: 1.9553 Acc: 0.5803                                               \n",
      "Epoch 021 | Train Loss: 5.7961 Acc: 0.4586 | Val Loss: 1.6526 Acc: 0.5887                                               \n",
      "Epoch 022 | Train Loss: 5.4665 Acc: 0.4684 | Val Loss: 1.7742 Acc: 0.5821                                               \n",
      "Epoch 023 | Train Loss: 4.9978 Acc: 0.4654 | Val Loss: 1.4021 Acc: 0.6051                                               \n",
      "Epoch 024 | Train Loss: 4.6793 Acc: 0.4767 | Val Loss: 1.8851 Acc: 0.5666                                               \n",
      "Epoch 025 | Train Loss: 4.4109 Acc: 0.4829 | Val Loss: 1.4358 Acc: 0.6543                                               \n",
      "Epoch 026 | Train Loss: 4.1199 Acc: 0.4903 | Val Loss: 1.4950 Acc: 0.6096                                               \n",
      "Epoch 027 | Train Loss: 3.9642 Acc: 0.4929 | Val Loss: 1.3002 Acc: 0.6433                                               \n",
      "Epoch 028 | Train Loss: 3.8012 Acc: 0.5086 | Val Loss: 1.5191 Acc: 0.6191                                               \n",
      "Epoch 029 | Train Loss: 3.5467 Acc: 0.5100 | Val Loss: 1.1093 Acc: 0.6699                                               \n",
      "Epoch 030 | Train Loss: 3.4412 Acc: 0.5197 | Val Loss: 1.1402 Acc: 0.6824                                               \n",
      "Epoch 031 | Train Loss: 3.2860 Acc: 0.5235 | Val Loss: 1.1697 Acc: 0.6588                                               \n",
      "Epoch 032 | Train Loss: 2.9240 Acc: 0.5308 | Val Loss: 1.2032 Acc: 0.6334                                               \n",
      "Epoch 033 | Train Loss: 2.8478 Acc: 0.5356 | Val Loss: 1.0174 Acc: 0.6707                                               \n",
      "Epoch 034 | Train Loss: 2.7500 Acc: 0.5429 | Val Loss: 0.9254 Acc: 0.7003                                               \n",
      "Epoch 035 | Train Loss: 2.6255 Acc: 0.5534 | Val Loss: 0.8867 Acc: 0.7110                                               \n",
      "Epoch 036 | Train Loss: 2.6044 Acc: 0.5544 | Val Loss: 0.9503 Acc: 0.6770                                               \n",
      "Epoch 037 | Train Loss: 2.3691 Acc: 0.5700 | Val Loss: 0.8898 Acc: 0.7036                                               \n",
      "Epoch 038 | Train Loss: 2.3025 Acc: 0.5732 | Val Loss: 0.8189 Acc: 0.7099                                               \n",
      "Epoch 039 | Train Loss: 2.1997 Acc: 0.5806 | Val Loss: 0.8644 Acc: 0.6964                                               \n",
      "Epoch 040 | Train Loss: 2.1078 Acc: 0.5912 | Val Loss: 0.8511 Acc: 0.6869                                               \n",
      "Epoch 041 | Train Loss: 1.9875 Acc: 0.5968 | Val Loss: 0.7510 Acc: 0.7212                                               \n",
      "Epoch 042 | Train Loss: 1.9509 Acc: 0.6025 | Val Loss: 0.7315 Acc: 0.7403                                               \n",
      "Epoch 043 | Train Loss: 1.8913 Acc: 0.6068 | Val Loss: 0.7085 Acc: 0.7224                                               \n",
      "Epoch 044 | Train Loss: 1.7506 Acc: 0.6175 | Val Loss: 0.7302 Acc: 0.7218                                               \n",
      "Epoch 045 | Train Loss: 1.7504 Acc: 0.6227 | Val Loss: 0.7621 Acc: 0.7194                                               \n",
      "                                                                                                                        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 047 | Train Loss: 0.7509 Acc: 0.6330 | Val Loss: 0.6963 Acc: 0.6713                                               \n",
      "Epoch 048 | Train Loss: 0.7648 Acc: 0.6337 | Val Loss: 0.6753 Acc: 0.7179                                               \n",
      "Epoch 049 | Train Loss: 0.7556 Acc: 0.6367 | Val Loss: 0.6998 Acc: 0.6684                                               \n",
      "Epoch 050 | Train Loss: 0.7415 Acc: 0.6413 | Val Loss: 0.6714 Acc: 0.7460                                               \n",
      "Epoch 051 | Train Loss: 0.7630 Acc: 0.6384 | Val Loss: 0.6866 Acc: 0.7281                                               \n",
      "Epoch 052 | Train Loss: 0.7497 Acc: 0.6425 | Val Loss: 0.6800 Acc: 0.7081                                               \n",
      "Epoch 053 | Train Loss: 0.7456 Acc: 0.6449 | Val Loss: 0.6802 Acc: 0.6901                                               \n",
      "Epoch 054 | Train Loss: 0.7460 Acc: 0.6410 | Val Loss: 0.6760 Acc: 0.6910                                               \n",
      "Epoch 055 | Train Loss: 0.7371 Acc: 0.6492 | Val Loss: 0.6757 Acc: 0.6925                                               \n",
      "Epoch 056 | Train Loss: 0.7362 Acc: 0.6491 | Val Loss: 0.6744 Acc: 0.6851                                               \n",
      "Epoch 057 | Train Loss: 0.7393 Acc: 0.6439 | Val Loss: 0.6662 Acc: 0.7063                                               \n",
      "Epoch 058 | Train Loss: 0.7393 Acc: 0.6500 | Val Loss: 0.6635 Acc: 0.7015                                               \n",
      "Epoch 059 | Train Loss: 0.7339 Acc: 0.6529 | Val Loss: 0.6690 Acc: 0.6597                                               \n",
      "Epoch 060 | Train Loss: 0.7290 Acc: 0.6514 | Val Loss: 0.6604 Acc: 0.7230                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 256, 'cnn_dropout': 0.3347908545676027, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 32, 'cnn_kernels_2': 16, 'learning_rate': 0.00032210375565273214, 'lstm_dense': 64, 'lstm_hidden_size': 96, 'lstm_layers': 5, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 2.0397 Acc: 0.3960 | Val Loss: 1.2689 Acc: 0.4421                                               \n",
      "Epoch 002 | Train Loss: 1.2488 Acc: 0.4432 | Val Loss: 1.2031 Acc: 0.4749                                               \n",
      "Epoch 003 | Train Loss: 1.2718 Acc: 0.4426 | Val Loss: 1.2917 Acc: 0.4421                                               \n",
      "Epoch 004 | Train Loss: 1.2700 Acc: 0.4422 | Val Loss: 1.2747 Acc: 0.4421                                               \n",
      "Epoch 005 | Train Loss: 1.2601 Acc: 0.4422 | Val Loss: 1.2649 Acc: 0.4421                                               \n",
      "Epoch 006 | Train Loss: 1.2559 Acc: 0.4422 | Val Loss: 1.2587 Acc: 0.4421                                               \n",
      "Epoch 007 | Train Loss: 1.2515 Acc: 0.4422 | Val Loss: 1.2547 Acc: 0.4421                                               \n",
      "Epoch 008 | Train Loss: 1.2504 Acc: 0.4422 | Val Loss: 1.2518 Acc: 0.4421                                               \n",
      "Epoch 009 | Train Loss: 1.2482 Acc: 0.4422 | Val Loss: 1.2497 Acc: 0.4421                                               \n",
      "Epoch 010 | Train Loss: 1.2460 Acc: 0.4422 | Val Loss: 1.2481 Acc: 0.4421                                               \n",
      "Epoch 011 | Train Loss: 1.2468 Acc: 0.4422 | Val Loss: 1.2469 Acc: 0.4421                                               \n",
      "Epoch 012 | Train Loss: 1.2452 Acc: 0.4422 | Val Loss: 1.2460 Acc: 0.4421                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 32, 'cnn_dropout': 0.14524046638718155, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 64, 'cnn_kernels_2': 16, 'learning_rate': 0.0002918044284033363, 'lstm_dense': 64, 'lstm_hidden_size': 96, 'lstm_layers': 3, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 9.3154 Acc: 0.4278 | Val Loss: 1.4120 Acc: 0.6331                                               \n",
      "Epoch 002 | Train Loss: 2.8399 Acc: 0.5009 | Val Loss: 0.9917 Acc: 0.6027                                               \n",
      "Epoch 003 | Train Loss: 1.5617 Acc: 0.5951 | Val Loss: 1.3427 Acc: 0.6767                                               \n",
      "Epoch 004 | Train Loss: 1.0452 Acc: 0.6778 | Val Loss: 0.5366 Acc: 0.7839                                               \n",
      "Epoch 005 | Train Loss: 0.7617 Acc: 0.7340 | Val Loss: 0.5017 Acc: 0.8057                                               \n",
      "Epoch 006 | Train Loss: 0.6095 Acc: 0.7692 | Val Loss: 0.3941 Acc: 0.8513                                               \n",
      "Epoch 007 | Train Loss: 0.4921 Acc: 0.8151 | Val Loss: 0.5494 Acc: 0.7570                                               \n",
      "Epoch 008 | Train Loss: 0.4165 Acc: 0.8458 | Val Loss: 0.2612 Acc: 0.8988                                               \n",
      "Epoch 009 | Train Loss: 0.3502 Acc: 0.8710 | Val Loss: 0.2617 Acc: 0.8922                                               \n",
      "Epoch 010 | Train Loss: 0.3023 Acc: 0.8912 | Val Loss: 0.2030 Acc: 0.9176                                               \n",
      "Epoch 011 | Train Loss: 0.2521 Acc: 0.9097 | Val Loss: 0.2182 Acc: 0.9155                                               \n",
      "Epoch 012 | Train Loss: 0.2280 Acc: 0.9175 | Val Loss: 0.1509 Acc: 0.9415                                               \n",
      "Epoch 013 | Train Loss: 0.2074 Acc: 0.9284 | Val Loss: 0.1425 Acc: 0.9484                                               \n",
      "Epoch 014 | Train Loss: 0.1752 Acc: 0.9376 | Val Loss: 0.1718 Acc: 0.9400                                               \n",
      "Epoch 015 | Train Loss: 0.1543 Acc: 0.9453 | Val Loss: 0.1317 Acc: 0.9576                                               \n",
      "Epoch 016 | Train Loss: 0.1489 Acc: 0.9483 | Val Loss: 0.1020 Acc: 0.9657                                               \n",
      "Epoch 017 | Train Loss: 0.1383 Acc: 0.9537 | Val Loss: 0.1114 Acc: 0.9669                                               \n",
      "Epoch 018 | Train Loss: 0.1196 Acc: 0.9607 | Val Loss: 0.0922 Acc: 0.9743                                               \n",
      "Epoch 019 | Train Loss: 0.1161 Acc: 0.9608 | Val Loss: 0.1180 Acc: 0.9573                                               \n",
      "Epoch 020 | Train Loss: 0.1012 Acc: 0.9654 | Val Loss: 0.1421 Acc: 0.9552                                               \n",
      "Epoch 021 | Train Loss: 0.1007 Acc: 0.9669 | Val Loss: 0.1341 Acc: 0.9579                                               \n",
      "Epoch 022 | Train Loss: 0.0886 Acc: 0.9709 | Val Loss: 0.1835 Acc: 0.9370                                               \n",
      "Epoch 023 | Train Loss: 0.0854 Acc: 0.9713 | Val Loss: 0.0851 Acc: 0.9725                                               \n",
      "Epoch 024 | Train Loss: 0.0855 Acc: 0.9721 | Val Loss: 0.0936 Acc: 0.9743                                               \n",
      "Epoch 025 | Train Loss: 0.0764 Acc: 0.9758 | Val Loss: 0.0762 Acc: 0.9767                                               \n",
      "Epoch 026 | Train Loss: 0.0720 Acc: 0.9778 | Val Loss: 0.0637 Acc: 0.9824                                               \n",
      "Epoch 027 | Train Loss: 0.0705 Acc: 0.9766 | Val Loss: 0.0607 Acc: 0.9839                                               \n",
      "Epoch 028 | Train Loss: 0.0658 Acc: 0.9775 | Val Loss: 0.0858 Acc: 0.9740                                               \n",
      "Epoch 029 | Train Loss: 0.0580 Acc: 0.9819 | Val Loss: 0.0671 Acc: 0.9803                                               \n",
      "Epoch 030 | Train Loss: 0.0588 Acc: 0.9813 | Val Loss: 0.0707 Acc: 0.9776                                               \n",
      "Epoch 031 | Train Loss: 0.0628 Acc: 0.9792 | Val Loss: 0.0748 Acc: 0.9743                                               \n",
      "Epoch 032 | Train Loss: 0.0539 Acc: 0.9826 | Val Loss: 0.0884 Acc: 0.9731                                               \n",
      "Epoch 033 | Train Loss: 0.0556 Acc: 0.9826 | Val Loss: 0.1636 Acc: 0.9528                                               \n",
      "Epoch 034 | Train Loss: 0.0568 Acc: 0.9819 | Val Loss: 0.0520 Acc: 0.9842                                               \n",
      "Epoch 035 | Train Loss: 0.0516 Acc: 0.9830 | Val Loss: 0.0679 Acc: 0.9812                                               \n",
      "Epoch 036 | Train Loss: 0.0485 Acc: 0.9840 | Val Loss: 0.0817 Acc: 0.9761                                               \n",
      "Epoch 037 | Train Loss: 0.0487 Acc: 0.9851 | Val Loss: 0.0644 Acc: 0.9821                                               \n",
      "Epoch 038 | Train Loss: 0.0492 Acc: 0.9848 | Val Loss: 0.1461 Acc: 0.9504                                               \n",
      "Epoch 039 | Train Loss: 0.0488 Acc: 0.9868 | Val Loss: 0.0676 Acc: 0.9812                                               \n",
      "Epoch 040 | Train Loss: 0.0340 Acc: 0.9896 | Val Loss: 0.0889 Acc: 0.9758                                               \n",
      "Epoch 041 | Train Loss: 0.0480 Acc: 0.9851 | Val Loss: 0.0502 Acc: 0.9875                                               \n",
      "Epoch 042 | Train Loss: 0.0428 Acc: 0.9866 | Val Loss: 0.0727 Acc: 0.9815                                               \n",
      "Epoch 043 | Train Loss: 0.0414 Acc: 0.9867 | Val Loss: 0.0783 Acc: 0.9797                                               \n",
      "Epoch 044 | Train Loss: 0.0378 Acc: 0.9876 | Val Loss: 0.0879 Acc: 0.9767                                               \n",
      "Epoch 045 | Train Loss: 0.0345 Acc: 0.9894 | Val Loss: 0.0544 Acc: 0.9824                                               \n",
      "Epoch 046 | Train Loss: 0.0456 Acc: 0.9859 | Val Loss: 0.0680 Acc: 0.9794                                               \n",
      "Epoch 047 | Train Loss: 0.0351 Acc: 0.9903 | Val Loss: 0.0839 Acc: 0.9761                                               \n",
      "Epoch 048 | Train Loss: 0.0338 Acc: 0.9884 | Val Loss: 0.0539 Acc: 0.9872                                               \n",
      "Epoch 049 | Train Loss: 0.0379 Acc: 0.9890 | Val Loss: 0.0640 Acc: 0.9812                                               \n",
      "Epoch 050 | Train Loss: 0.0328 Acc: 0.9902 | Val Loss: 0.0548 Acc: 0.9845                                               \n",
      "Epoch 051 | Train Loss: 0.0351 Acc: 0.9888 | Val Loss: 0.0324 Acc: 0.9904                                               \n",
      "Epoch 052 | Train Loss: 0.0349 Acc: 0.9899 | Val Loss: 0.0749 Acc: 0.9773                                               \n",
      "Epoch 053 | Train Loss: 0.0297 Acc: 0.9908 | Val Loss: 0.0769 Acc: 0.9779                                               \n",
      "Epoch 054 | Train Loss: 0.0292 Acc: 0.9902 | Val Loss: 0.0735 Acc: 0.9812                                               \n",
      "Epoch 055 | Train Loss: 0.0328 Acc: 0.9883 | Val Loss: 0.0480 Acc: 0.9872                                               \n",
      "Epoch 056 | Train Loss: 0.0303 Acc: 0.9904 | Val Loss: 0.0605 Acc: 0.9848                                               \n",
      "Epoch 057 | Train Loss: 0.0295 Acc: 0.9908 | Val Loss: 0.0437 Acc: 0.9875                                               \n",
      "Epoch 058 | Train Loss: 0.0280 Acc: 0.9910 | Val Loss: 0.0355 Acc: 0.9887                                               \n",
      "Epoch 059 | Train Loss: 0.0281 Acc: 0.9914 | Val Loss: 0.0656 Acc: 0.9809                                               \n",
      "Epoch 060 | Train Loss: 0.0329 Acc: 0.9881 | Val Loss: 0.0884 Acc: 0.9728                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 64, 'cnn_dropout': 0.5077850596754725, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 16, 'learning_rate': 3.0699473886945505e-05, 'lstm_dense': 64, 'lstm_hidden_size': 64, 'lstm_layers': 3, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 96.5069 Acc: 0.2546 | Val Loss: 22.9671 Acc: 0.3890                                             \n",
      "Epoch 002 | Train Loss: 34.2243 Acc: 0.3185 | Val Loss: 17.1682 Acc: 0.4421                                             \n",
      "Epoch 003 | Train Loss: 24.6881 Acc: 0.3341 | Val Loss: 10.0476 Acc: 0.4412                                             \n",
      "Epoch 004 | Train Loss: 18.8370 Acc: 0.3575 | Val Loss: 7.5455 Acc: 0.4543                                              \n",
      "Epoch 005 | Train Loss: 13.9973 Acc: 0.3734 | Val Loss: 5.3841 Acc: 0.4406                                              \n",
      "Epoch 006 | Train Loss: 11.4812 Acc: 0.3687 | Val Loss: 4.6224 Acc: 0.4463                                              \n",
      "Epoch 007 | Train Loss: 9.6088 Acc: 0.3773 | Val Loss: 3.4743 Acc: 0.4170                                               \n",
      "Epoch 008 | Train Loss: 8.2572 Acc: 0.3818 | Val Loss: 3.5040 Acc: 0.4051                                               \n",
      "Epoch 009 | Train Loss: 6.9166 Acc: 0.3959 | Val Loss: 2.3512 Acc: 0.4134                                               \n",
      "Epoch 010 | Train Loss: 6.3091 Acc: 0.3941 | Val Loss: 2.0586 Acc: 0.4104                                               \n",
      "Epoch 011 | Train Loss: 5.5317 Acc: 0.4030 | Val Loss: 1.7800 Acc: 0.3845                                               \n",
      "Epoch 012 | Train Loss: 4.8162 Acc: 0.4126 | Val Loss: 2.1750 Acc: 0.4328                                               \n",
      "Epoch 013 | Train Loss: 4.3303 Acc: 0.4174 | Val Loss: 1.9206 Acc: 0.4943                                               \n",
      "Epoch 014 | Train Loss: 4.2330 Acc: 0.4265 | Val Loss: 1.5558 Acc: 0.4313                                               \n",
      "Epoch 015 | Train Loss: 3.7141 Acc: 0.4414 | Val Loss: 1.3992 Acc: 0.4725                                               \n",
      "Epoch 016 | Train Loss: 3.3592 Acc: 0.4521 | Val Loss: 1.2050 Acc: 0.5107                                               \n",
      "Epoch 017 | Train Loss: 3.1120 Acc: 0.4620 | Val Loss: 1.1903 Acc: 0.5224                                               \n",
      "Epoch 018 | Train Loss: 2.8468 Acc: 0.4742 | Val Loss: 1.1443 Acc: 0.5349                                               \n",
      "Epoch 019 | Train Loss: 2.7153 Acc: 0.4772 | Val Loss: 1.0874 Acc: 0.5779                                               \n",
      "Epoch 020 | Train Loss: 2.5619 Acc: 0.4831 | Val Loss: 1.1428 Acc: 0.5851                                               \n",
      "Epoch 021 | Train Loss: 2.3770 Acc: 0.4950 | Val Loss: 1.1184 Acc: 0.5155                                               \n",
      "Epoch 022 | Train Loss: 2.2323 Acc: 0.5060 | Val Loss: 1.4368 Acc: 0.5448                                               \n",
      "Epoch 023 | Train Loss: 2.0965 Acc: 0.5177 | Val Loss: 1.2956 Acc: 0.5525                                               \n",
      "Epoch 024 | Train Loss: 1.9650 Acc: 0.5179 | Val Loss: 1.1061 Acc: 0.5821                                               \n",
      "Epoch 025 | Train Loss: 1.8983 Acc: 0.5311 | Val Loss: 1.0337 Acc: 0.5749                                               \n",
      "Epoch 026 | Train Loss: 1.8462 Acc: 0.5298 | Val Loss: 0.9096 Acc: 0.5800                                               \n",
      "Epoch 027 | Train Loss: 1.7053 Acc: 0.5502 | Val Loss: 0.9275 Acc: 0.5487                                               \n",
      "Epoch 028 | Train Loss: 1.6452 Acc: 0.5564 | Val Loss: 1.2401 Acc: 0.5806                                               \n",
      "Epoch 029 | Train Loss: 1.5596 Acc: 0.5515 | Val Loss: 1.0432 Acc: 0.5881                                               \n",
      "Epoch 030 | Train Loss: 1.5102 Acc: 0.5566 | Val Loss: 0.9212 Acc: 0.6424                                               \n",
      "Epoch 031 | Train Loss: 1.4433 Acc: 0.5656 | Val Loss: 1.0755 Acc: 0.5469                                               \n",
      "Epoch 032 | Train Loss: 1.3637 Acc: 0.5712 | Val Loss: 1.1472 Acc: 0.6066                                               \n",
      "Epoch 033 | Train Loss: 1.3228 Acc: 0.5787 | Val Loss: 0.9610 Acc: 0.6627                                               \n",
      "Epoch 034 | Train Loss: 1.2558 Acc: 0.5793 | Val Loss: 0.9197 Acc: 0.6316                                               \n",
      "Epoch 035 | Train Loss: 1.2160 Acc: 0.5903 | Val Loss: 0.9265 Acc: 0.6442                                               \n",
      "Epoch 036 | Train Loss: 1.2275 Acc: 0.5901 | Val Loss: 0.9179 Acc: 0.5925                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 64, 'cnn_dropout': 0.260614774268618, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 16, 'learning_rate': 0.0021020034325612154, 'lstm_dense': 64, 'lstm_hidden_size': 32, 'lstm_layers': 6, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 9.6787 Acc: 0.4852 | Val Loss: 1.2768 Acc: 0.6158                                               \n",
      "Epoch 002 | Train Loss: 0.9984 Acc: 0.5841 | Val Loss: 0.9092 Acc: 0.5639                                               \n",
      "Epoch 003 | Train Loss: 0.8911 Acc: 0.6000 | Val Loss: 0.7781 Acc: 0.6266                                               \n",
      "Epoch 004 | Train Loss: 0.8146 Acc: 0.6194 | Val Loss: 1.0372 Acc: 0.5325                                               \n",
      "Epoch 005 | Train Loss: 0.7721 Acc: 0.6278 | Val Loss: 1.1776 Acc: 0.5916                                               \n",
      "Epoch 006 | Train Loss: 0.7638 Acc: 0.6407 | Val Loss: 0.6148 Acc: 0.7191                                               \n",
      "Epoch 007 | Train Loss: 0.7191 Acc: 0.6651 | Val Loss: 0.7182 Acc: 0.6537                                               \n",
      "Epoch 008 | Train Loss: 0.6985 Acc: 0.6771 | Val Loss: 0.9255 Acc: 0.6003                                               \n",
      "Epoch 009 | Train Loss: 0.6748 Acc: 0.6943 | Val Loss: 0.9453 Acc: 0.5690                                               \n",
      "Epoch 010 | Train Loss: 0.7014 Acc: 0.6789 | Val Loss: 0.7778 Acc: 0.6170                                               \n",
      "Epoch 011 | Train Loss: 0.6877 Acc: 0.6886 | Val Loss: 0.7400 Acc: 0.6337                                               \n",
      "Epoch 012 | Train Loss: 0.6531 Acc: 0.7079 | Val Loss: 0.6627 Acc: 0.7107                                               \n",
      "Epoch 013 | Train Loss: 0.6454 Acc: 0.7220 | Val Loss: 0.6009 Acc: 0.7299                                               \n",
      "Epoch 014 | Train Loss: 0.6463 Acc: 0.7118 | Val Loss: 0.8321 Acc: 0.6346                                               \n",
      "Epoch 015 | Train Loss: 0.6683 Acc: 0.7001 | Val Loss: 0.8481 Acc: 0.6218                                               \n",
      "Epoch 016 | Train Loss: 0.6600 Acc: 0.7077 | Val Loss: 0.7772 Acc: 0.6221                                               \n",
      "Epoch 017 | Train Loss: 0.6542 Acc: 0.7023 | Val Loss: 0.6607 Acc: 0.6681                                               \n",
      "Epoch 018 | Train Loss: 0.6871 Acc: 0.6856 | Val Loss: 0.5947 Acc: 0.7012                                               \n",
      "Epoch 019 | Train Loss: 0.7337 Acc: 0.6543 | Val Loss: 0.9612 Acc: 0.5776                                               \n",
      "Epoch 020 | Train Loss: 0.7177 Acc: 0.6639 | Val Loss: 0.7175 Acc: 0.6621                                               \n",
      "Epoch 021 | Train Loss: 0.7085 Acc: 0.6659 | Val Loss: 0.7645 Acc: 0.5588                                               \n",
      "Epoch 022 | Train Loss: 0.7042 Acc: 0.6697 | Val Loss: 0.6696 Acc: 0.6818                                               \n",
      "Epoch 023 | Train Loss: 0.6974 Acc: 0.6745 | Val Loss: 0.8085 Acc: 0.6782                                               \n",
      "Epoch 024 | Train Loss: 0.6981 Acc: 0.6709 | Val Loss: 0.8960 Acc: 0.6182                                               \n",
      "Epoch 025 | Train Loss: 0.6963 Acc: 0.6774 | Val Loss: 0.7461 Acc: 0.6134                                               \n",
      "Epoch 026 | Train Loss: 0.7010 Acc: 0.6742 | Val Loss: 0.8311 Acc: 0.6466                                               \n",
      "Epoch 027 | Train Loss: 0.7114 Acc: 0.6682 | Val Loss: 2.3422 Acc: 0.4606                                               \n",
      "Epoch 028 | Train Loss: 0.7104 Acc: 0.6724 | Val Loss: 0.8167 Acc: 0.6197                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 32, 'cnn_dropout': 0.2604111768698937, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 32, 'learning_rate': 3.138072928935169e-05, 'lstm_dense': 64, 'lstm_hidden_size': 96, 'lstm_layers': 3, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 43.4933 Acc: 0.3429 | Val Loss: 11.1005 Acc: 0.4230                                             \n",
      "Epoch 002 | Train Loss: 23.9380 Acc: 0.3545 | Val Loss: 10.1400 Acc: 0.3949                                             \n",
      "Epoch 003 | Train Loss: 18.2874 Acc: 0.3649 | Val Loss: 7.5414 Acc: 0.3979                                              \n",
      "Epoch 004 | Train Loss: 14.7854 Acc: 0.3718 | Val Loss: 6.6768 Acc: 0.3460                                              \n",
      "Epoch 005 | Train Loss: 12.2655 Acc: 0.3838 | Val Loss: 4.7310 Acc: 0.3490                                              \n",
      "Epoch 006 | Train Loss: 10.8069 Acc: 0.3906 | Val Loss: 3.9300 Acc: 0.3728                                              \n",
      "Epoch 007 | Train Loss: 9.3769 Acc: 0.4011 | Val Loss: 2.9213 Acc: 0.3678                                               \n",
      "Epoch 008 | Train Loss: 8.1382 Acc: 0.4144 | Val Loss: 2.2365 Acc: 0.4585                                               \n",
      "Epoch 009 | Train Loss: 7.2822 Acc: 0.4238 | Val Loss: 2.1241 Acc: 0.4307                                               \n",
      "Epoch 010 | Train Loss: 6.4017 Acc: 0.4292 | Val Loss: 1.7645 Acc: 0.4716                                               \n",
      "Epoch 011 | Train Loss: 5.7938 Acc: 0.4501 | Val Loss: 1.3618 Acc: 0.5301                                               \n",
      "Epoch 012 | Train Loss: 5.3093 Acc: 0.4518 | Val Loss: 1.2489 Acc: 0.5770                                               \n",
      "Epoch 013 | Train Loss: 4.6799 Acc: 0.4686 | Val Loss: 1.1794 Acc: 0.5439                                               \n",
      "Epoch 014 | Train Loss: 4.2323 Acc: 0.4709 | Val Loss: 1.2379 Acc: 0.6170                                               \n",
      "Epoch 015 | Train Loss: 3.9255 Acc: 0.4791 | Val Loss: 1.2614 Acc: 0.5833                                               \n",
      "Epoch 016 | Train Loss: 3.5586 Acc: 0.4915 | Val Loss: 1.4417 Acc: 0.5728                                               \n",
      "Epoch 017 | Train Loss: 3.2383 Acc: 0.5026 | Val Loss: 1.1965 Acc: 0.5940                                               \n",
      "Epoch 018 | Train Loss: 3.0048 Acc: 0.5135 | Val Loss: 1.2267 Acc: 0.6119                                               \n",
      "Epoch 019 | Train Loss: 2.7470 Acc: 0.5344 | Val Loss: 0.9598 Acc: 0.6287                                               \n",
      "Epoch 020 | Train Loss: 2.5285 Acc: 0.5406 | Val Loss: 1.1994 Acc: 0.6746                                               \n",
      "Epoch 021 | Train Loss: 2.4018 Acc: 0.5505 | Val Loss: 1.1134 Acc: 0.6878                                               \n",
      "Epoch 022 | Train Loss: 2.2345 Acc: 0.5544 | Val Loss: 1.2954 Acc: 0.6313                                               \n",
      "Epoch 023 | Train Loss: 2.1112 Acc: 0.5653 | Val Loss: 1.0889 Acc: 0.6373                                               \n",
      "Epoch 024 | Train Loss: 2.0262 Acc: 0.5687 | Val Loss: 1.4162 Acc: 0.5851                                               \n",
      "Epoch 025 | Train Loss: 1.8934 Acc: 0.5793 | Val Loss: 1.4095 Acc: 0.6528                                               \n",
      "Epoch 026 | Train Loss: 1.8022 Acc: 0.5865 | Val Loss: 1.0993 Acc: 0.6881                                               \n",
      "Epoch 027 | Train Loss: 1.7594 Acc: 0.5940 | Val Loss: 1.2157 Acc: 0.6693                                               \n",
      "Epoch 028 | Train Loss: 1.6570 Acc: 0.5959 | Val Loss: 1.2396 Acc: 0.6719                                               \n",
      "Epoch 029 | Train Loss: 1.5890 Acc: 0.6027 | Val Loss: 1.3263 Acc: 0.6439                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 36, 'cnn_dense': 32, 'cnn_dropout': 0.5623869836268478, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 32, 'learning_rate': 8.765551941314069e-05, 'lstm_dense': 32, 'lstm_hidden_size': 96, 'lstm_layers': 6, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 10.0325 Acc: 0.4023 | Val Loss: 3.4648 Acc: 0.3821                                              \n",
      "Epoch 002 | Train Loss: 4.2173 Acc: 0.4424 | Val Loss: 1.8204 Acc: 0.4478                                               \n",
      "Epoch 003 | Train Loss: 2.8525 Acc: 0.4795 | Val Loss: 1.1860 Acc: 0.5457                                               \n",
      "Epoch 004 | Train Loss: 2.0702 Acc: 0.5147 | Val Loss: 0.9962 Acc: 0.6334                                               \n",
      "Epoch 005 | Train Loss: 1.6538 Acc: 0.5406 | Val Loss: 1.0394 Acc: 0.6304                                               \n",
      "Epoch 006 | Train Loss: 1.3349 Acc: 0.5765 | Val Loss: 0.7843 Acc: 0.6358                                               \n",
      "Epoch 007 | Train Loss: 1.0955 Acc: 0.6119 | Val Loss: 1.0468 Acc: 0.5803                                               \n",
      "Epoch 008 | Train Loss: 0.9781 Acc: 0.6400 | Val Loss: 2.1753 Acc: 0.5749                                               \n",
      "Epoch 009 | Train Loss: 0.9019 Acc: 0.6576 | Val Loss: 0.6135 Acc: 0.7215                                               \n",
      "Epoch 010 | Train Loss: 0.8122 Acc: 0.6837 | Val Loss: 0.9681 Acc: 0.6463                                               \n",
      "Epoch 011 | Train Loss: 0.7307 Acc: 0.7093 | Val Loss: 0.5006 Acc: 0.7734                                               \n",
      "Epoch 012 | Train Loss: 0.6920 Acc: 0.7337 | Val Loss: 0.6526 Acc: 0.7224                                               \n",
      "Epoch 013 | Train Loss: 0.6172 Acc: 0.7557 | Val Loss: 0.5827 Acc: 0.7657                                               \n",
      "Epoch 014 | Train Loss: 0.5894 Acc: 0.7739 | Val Loss: 0.6389 Acc: 0.7284                                               \n",
      "Epoch 015 | Train Loss: 0.5462 Acc: 0.7866 | Val Loss: 1.0118 Acc: 0.6591                                               \n",
      "Epoch 016 | Train Loss: 0.5069 Acc: 0.8056 | Val Loss: 0.5506 Acc: 0.7809                                               \n",
      "Epoch 017 | Train Loss: 0.4686 Acc: 0.8230 | Val Loss: 0.3457 Acc: 0.8663                                               \n",
      "Epoch 018 | Train Loss: 0.4437 Acc: 0.8332 | Val Loss: 0.8514 Acc: 0.7093                                               \n",
      "Epoch 019 | Train Loss: 0.3998 Acc: 0.8513 | Val Loss: 1.2074 Acc: 0.6899                                               \n",
      "Epoch 020 | Train Loss: 0.3850 Acc: 0.8569 | Val Loss: 0.6052 Acc: 0.7928                                               \n",
      "Epoch 021 | Train Loss: 0.3531 Acc: 0.8733 | Val Loss: 0.2626 Acc: 0.9060                                               \n",
      "Epoch 022 | Train Loss: 0.3303 Acc: 0.8854 | Val Loss: 0.5966 Acc: 0.7764                                               \n",
      "Epoch 023 | Train Loss: 0.3014 Acc: 0.8910 | Val Loss: 0.2445 Acc: 0.9096                                               \n",
      "Epoch 024 | Train Loss: 0.2784 Acc: 0.9040 | Val Loss: 0.1742 Acc: 0.9454                                               \n",
      "Epoch 025 | Train Loss: 0.2772 Acc: 0.9071 | Val Loss: 0.3174 Acc: 0.8836                                               \n",
      "Epoch 026 | Train Loss: 0.2558 Acc: 0.9157 | Val Loss: 0.3518 Acc: 0.8693                                               \n",
      "Epoch 027 | Train Loss: 0.2380 Acc: 0.9178 | Val Loss: 0.3186 Acc: 0.8812                                               \n",
      "Epoch 028 | Train Loss: 0.2221 Acc: 0.9254 | Val Loss: 0.1921 Acc: 0.9322                                               \n",
      "Epoch 029 | Train Loss: 0.2091 Acc: 0.9296 | Val Loss: 0.2460 Acc: 0.9131                                               \n",
      "Epoch 030 | Train Loss: 0.1952 Acc: 0.9325 | Val Loss: 0.1769 Acc: 0.9370                                               \n",
      "Epoch 031 | Train Loss: 0.1796 Acc: 0.9399 | Val Loss: 0.1341 Acc: 0.9540                                               \n",
      "Epoch 032 | Train Loss: 0.1789 Acc: 0.9382 | Val Loss: 0.2427 Acc: 0.9116                                               \n",
      "Epoch 033 | Train Loss: 0.1715 Acc: 0.9413 | Val Loss: 0.1996 Acc: 0.9346                                               \n",
      "Epoch 034 | Train Loss: 0.1591 Acc: 0.9452 | Val Loss: 0.7867 Acc: 0.7731                                               \n",
      "Epoch 035 | Train Loss: 0.1525 Acc: 0.9492 | Val Loss: 0.1375 Acc: 0.9543                                               \n",
      "Epoch 036 | Train Loss: 0.1474 Acc: 0.9516 | Val Loss: 0.1774 Acc: 0.9397                                               \n",
      "Epoch 037 | Train Loss: 0.1440 Acc: 0.9519 | Val Loss: 0.1078 Acc: 0.9654                                               \n",
      "Epoch 038 | Train Loss: 0.1314 Acc: 0.9563 | Val Loss: 0.2313 Acc: 0.9134                                               \n",
      "Epoch 039 | Train Loss: 0.1262 Acc: 0.9578 | Val Loss: 0.2205 Acc: 0.9322                                               \n",
      "Epoch 040 | Train Loss: 0.1220 Acc: 0.9588 | Val Loss: 0.1311 Acc: 0.9579                                               \n",
      "Epoch 041 | Train Loss: 0.1194 Acc: 0.9613 | Val Loss: 0.1061 Acc: 0.9633                                               \n",
      "Epoch 042 | Train Loss: 0.1179 Acc: 0.9596 | Val Loss: 0.1003 Acc: 0.9675                                               \n",
      "Epoch 043 | Train Loss: 0.1066 Acc: 0.9648 | Val Loss: 0.1007 Acc: 0.9678                                               \n",
      "Epoch 044 | Train Loss: 0.1043 Acc: 0.9661 | Val Loss: 0.0831 Acc: 0.9746                                               \n",
      "Epoch 045 | Train Loss: 0.1018 Acc: 0.9661 | Val Loss: 0.0884 Acc: 0.9710                                               \n",
      "Epoch 046 | Train Loss: 0.0995 Acc: 0.9655 | Val Loss: 0.1841 Acc: 0.9367                                               \n",
      "Epoch 047 | Train Loss: 0.0948 Acc: 0.9691 | Val Loss: 0.3168 Acc: 0.9030                                               \n",
      "Epoch 048 | Train Loss: 0.0928 Acc: 0.9696 | Val Loss: 0.1588 Acc: 0.9499                                               \n",
      "Epoch 049 | Train Loss: 0.0895 Acc: 0.9727 | Val Loss: 0.0708 Acc: 0.9761                                               \n",
      "Epoch 050 | Train Loss: 0.0856 Acc: 0.9721 | Val Loss: 0.4561 Acc: 0.8869                                               \n",
      "Epoch 051 | Train Loss: 0.0803 Acc: 0.9750 | Val Loss: 0.1090 Acc: 0.9654                                               \n",
      "Epoch 052 | Train Loss: 0.0796 Acc: 0.9734 | Val Loss: 0.1436 Acc: 0.9543                                               \n",
      "Epoch 053 | Train Loss: 0.0814 Acc: 0.9756 | Val Loss: 0.0878 Acc: 0.9743                                               \n",
      "Epoch 054 | Train Loss: 0.0805 Acc: 0.9729 | Val Loss: 0.0626 Acc: 0.9830                                               \n",
      "Epoch 055 | Train Loss: 0.0716 Acc: 0.9772 | Val Loss: 0.0601 Acc: 0.9827                                               \n",
      "Epoch 056 | Train Loss: 0.0766 Acc: 0.9738 | Val Loss: 0.0817 Acc: 0.9758                                               \n",
      "Epoch 057 | Train Loss: 0.0713 Acc: 0.9766 | Val Loss: 0.0850 Acc: 0.9716                                               \n",
      "Epoch 058 | Train Loss: 0.0705 Acc: 0.9785 | Val Loss: 0.0627 Acc: 0.9836                                               \n",
      "Epoch 059 | Train Loss: 0.0667 Acc: 0.9785 | Val Loss: 0.1297 Acc: 0.9657                                               \n",
      "Epoch 060 | Train Loss: 0.0665 Acc: 0.9784 | Val Loss: 0.0794 Acc: 0.9758                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 256, 'cnn_dropout': 0.2557264421031315, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 16, 'learning_rate': 0.0010046516971727933, 'lstm_dense': 64, 'lstm_hidden_size': 96, 'lstm_layers': 3, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 17.6275 Acc: 0.4147 | Val Loss: 2.9967 Acc: 0.4827                                              \n",
      "Epoch 002 | Train Loss: 3.6455 Acc: 0.4586 | Val Loss: 1.9102 Acc: 0.5699                                               \n",
      "Epoch 003 | Train Loss: 1.4152 Acc: 0.5371 | Val Loss: 0.7493 Acc: 0.7170                                               \n",
      "Epoch 004 | Train Loss: 0.8486 Acc: 0.6320 | Val Loss: 0.6859 Acc: 0.6469                                               \n",
      "Epoch 005 | Train Loss: 0.6928 Acc: 0.7050 | Val Loss: 0.7828 Acc: 0.6325                                               \n",
      "Epoch 006 | Train Loss: 0.5996 Acc: 0.7480 | Val Loss: 0.5283 Acc: 0.7728                                               \n",
      "Epoch 007 | Train Loss: 0.5496 Acc: 0.7769 | Val Loss: 0.4211 Acc: 0.8200                                               \n",
      "Epoch 008 | Train Loss: 0.4555 Acc: 0.8221 | Val Loss: 0.5885 Acc: 0.7549                                               \n",
      "Epoch 009 | Train Loss: 0.4070 Acc: 0.8449 | Val Loss: 0.3058 Acc: 0.8818                                               \n",
      "Epoch 010 | Train Loss: 0.3334 Acc: 0.8746 | Val Loss: 0.4563 Acc: 0.8439                                               \n",
      "Epoch 011 | Train Loss: 0.3094 Acc: 0.8873 | Val Loss: 0.2450 Acc: 0.9081                                               \n",
      "Epoch 012 | Train Loss: 0.2609 Acc: 0.9030 | Val Loss: 0.2852 Acc: 0.8890                                               \n",
      "Epoch 013 | Train Loss: 0.2494 Acc: 0.9111 | Val Loss: 0.3094 Acc: 0.8890                                               \n",
      "Epoch 014 | Train Loss: 0.2240 Acc: 0.9199 | Val Loss: 0.5052 Acc: 0.8322                                               \n",
      "Epoch 015 | Train Loss: 0.2045 Acc: 0.9255 | Val Loss: 0.6060 Acc: 0.8200                                               \n",
      "Epoch 016 | Train Loss: 0.2007 Acc: 0.9281 | Val Loss: 0.3225 Acc: 0.8740                                               \n",
      "Epoch 017 | Train Loss: 0.1807 Acc: 0.9375 | Val Loss: 0.1166 Acc: 0.9675                                               \n",
      "Epoch 018 | Train Loss: 0.1529 Acc: 0.9459 | Val Loss: 0.1024 Acc: 0.9687                                               \n",
      "Epoch 019 | Train Loss: 0.1390 Acc: 0.9525 | Val Loss: 0.2080 Acc: 0.9269                                               \n",
      "Epoch 020 | Train Loss: 0.1457 Acc: 0.9505 | Val Loss: 0.7995 Acc: 0.8009                                               \n",
      "Epoch 021 | Train Loss: 0.1368 Acc: 0.9551 | Val Loss: 0.1546 Acc: 0.9531                                               \n",
      "Epoch 022 | Train Loss: 0.1213 Acc: 0.9587 | Val Loss: 0.1491 Acc: 0.9534                                               \n",
      "Epoch 023 | Train Loss: 0.1171 Acc: 0.9597 | Val Loss: 0.0950 Acc: 0.9693                                               \n",
      "Epoch 024 | Train Loss: 0.1267 Acc: 0.9573 | Val Loss: 0.3760 Acc: 0.8937                                               \n",
      "Epoch 025 | Train Loss: 0.1108 Acc: 0.9621 | Val Loss: 0.0894 Acc: 0.9716                                               \n",
      "Epoch 026 | Train Loss: 0.1064 Acc: 0.9645 | Val Loss: 0.1702 Acc: 0.9445                                               \n",
      "Epoch 027 | Train Loss: 0.1089 Acc: 0.9653 | Val Loss: 0.1540 Acc: 0.9585                                               \n",
      "Epoch 028 | Train Loss: 0.0999 Acc: 0.9655 | Val Loss: 0.0637 Acc: 0.9809                                               \n",
      "Epoch 029 | Train Loss: 0.0899 Acc: 0.9688 | Val Loss: 0.1089 Acc: 0.9684                                               \n",
      "Epoch 030 | Train Loss: 0.1014 Acc: 0.9665 | Val Loss: 0.1637 Acc: 0.9481                                               \n",
      "Epoch 031 | Train Loss: 0.0837 Acc: 0.9720 | Val Loss: 0.1829 Acc: 0.9448                                               \n",
      "Epoch 032 | Train Loss: 0.1017 Acc: 0.9665 | Val Loss: 0.1724 Acc: 0.9528                                               \n",
      "Epoch 033 | Train Loss: 0.1018 Acc: 0.9710 | Val Loss: 0.0869 Acc: 0.9707                                               \n",
      "Epoch 034 | Train Loss: 0.0784 Acc: 0.9747 | Val Loss: 0.0672 Acc: 0.9782                                               \n",
      "Epoch 035 | Train Loss: 0.0864 Acc: 0.9716 | Val Loss: 0.0443 Acc: 0.9869                                               \n",
      "Epoch 036 | Train Loss: 0.0999 Acc: 0.9684 | Val Loss: 0.0605 Acc: 0.9824                                               \n",
      "Epoch 037 | Train Loss: 0.0760 Acc: 0.9738 | Val Loss: 0.0808 Acc: 0.9791                                               \n",
      "Epoch 038 | Train Loss: 0.0797 Acc: 0.9727 | Val Loss: 0.1183 Acc: 0.9627                                               \n",
      "Epoch 039 | Train Loss: 0.0742 Acc: 0.9753 | Val Loss: 0.0703 Acc: 0.9800                                               \n",
      "Epoch 040 | Train Loss: 0.0708 Acc: 0.9761 | Val Loss: 0.1098 Acc: 0.9699                                               \n",
      "Epoch 041 | Train Loss: 0.0728 Acc: 0.9765 | Val Loss: 0.1608 Acc: 0.9469                                               \n",
      "Epoch 042 | Train Loss: 0.0739 Acc: 0.9741 | Val Loss: 0.0643 Acc: 0.9788                                               \n",
      "Epoch 043 | Train Loss: 0.0739 Acc: 0.9769 | Val Loss: 0.0828 Acc: 0.9660                                               \n",
      "Epoch 044 | Train Loss: 0.0619 Acc: 0.9796 | Val Loss: 0.0830 Acc: 0.9785                                               \n",
      "Epoch 045 | Train Loss: 0.0598 Acc: 0.9798 | Val Loss: 0.0526 Acc: 0.9812                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 64, 'cnn_dropout': 0.1503149079793205, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 64, 'cnn_kernels_2': 16, 'learning_rate': 0.001226187546905321, 'lstm_dense': 64, 'lstm_hidden_size': 32, 'lstm_layers': 3, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 15.7453 Acc: 0.4206 | Val Loss: 2.2136 Acc: 0.5696                                              \n",
      "Epoch 002 | Train Loss: 2.7969 Acc: 0.4918 | Val Loss: 1.2090 Acc: 0.5913                                               \n",
      "Epoch 003 | Train Loss: 1.8632 Acc: 0.5228 | Val Loss: 1.1271 Acc: 0.5955                                               \n",
      "Epoch 004 | Train Loss: 1.2625 Acc: 0.5544 | Val Loss: 0.7911 Acc: 0.6534                                               \n",
      "Epoch 005 | Train Loss: 0.9718 Acc: 0.5867 | Val Loss: 0.6954 Acc: 0.6749                                               \n",
      "Epoch 006 | Train Loss: 0.8561 Acc: 0.6293 | Val Loss: 0.6615 Acc: 0.7119                                               \n",
      "Epoch 007 | Train Loss: 0.7038 Acc: 0.6998 | Val Loss: 0.6464 Acc: 0.6773                                               \n",
      "Epoch 008 | Train Loss: 0.6034 Acc: 0.7446 | Val Loss: 0.5109 Acc: 0.7955                                               \n",
      "Epoch 009 | Train Loss: 0.5341 Acc: 0.7816 | Val Loss: 0.4671 Acc: 0.8158                                               \n",
      "Epoch 010 | Train Loss: 0.4909 Acc: 0.8021 | Val Loss: 0.3895 Acc: 0.8379                                               \n",
      "Epoch 011 | Train Loss: 0.4553 Acc: 0.8216 | Val Loss: 0.4559 Acc: 0.8287                                               \n",
      "Epoch 012 | Train Loss: 0.4196 Acc: 0.8354 | Val Loss: 0.2758 Acc: 0.8913                                               \n",
      "Epoch 013 | Train Loss: 0.3675 Acc: 0.8599 | Val Loss: 0.3377 Acc: 0.8657                                               \n",
      "Epoch 014 | Train Loss: 0.3417 Acc: 0.8737 | Val Loss: 0.3484 Acc: 0.8594                                               \n",
      "Epoch 015 | Train Loss: 0.3178 Acc: 0.8804 | Val Loss: 0.3139 Acc: 0.8791                                               \n",
      "Epoch 016 | Train Loss: 0.2951 Acc: 0.8905 | Val Loss: 0.3452 Acc: 0.8734                                               \n",
      "Epoch 017 | Train Loss: 0.2875 Acc: 0.8920 | Val Loss: 0.2274 Acc: 0.9164                                               \n",
      "Epoch 018 | Train Loss: 0.2772 Acc: 0.8981 | Val Loss: 0.3536 Acc: 0.8755                                               \n",
      "Epoch 019 | Train Loss: 0.2502 Acc: 0.9116 | Val Loss: 0.2151 Acc: 0.9230                                               \n",
      "Epoch 020 | Train Loss: 0.2409 Acc: 0.9127 | Val Loss: 0.2760 Acc: 0.9027                                               \n",
      "Epoch 021 | Train Loss: 0.2336 Acc: 0.9160 | Val Loss: 0.1947 Acc: 0.9248                                               \n",
      "Epoch 022 | Train Loss: 0.2214 Acc: 0.9189 | Val Loss: 0.1884 Acc: 0.9301                                               \n",
      "Epoch 023 | Train Loss: 0.1989 Acc: 0.9295 | Val Loss: 0.1630 Acc: 0.9397                                               \n",
      "Epoch 024 | Train Loss: 0.2038 Acc: 0.9254 | Val Loss: 0.2670 Acc: 0.8982                                               \n",
      "Epoch 025 | Train Loss: 0.2283 Acc: 0.9171 | Val Loss: 0.2156 Acc: 0.9149                                               \n",
      "Epoch 026 | Train Loss: 0.2052 Acc: 0.9259 | Val Loss: 0.1552 Acc: 0.9358                                               \n",
      "Epoch 027 | Train Loss: 0.1920 Acc: 0.9315 | Val Loss: 0.2527 Acc: 0.9170                                               \n",
      "Epoch 028 | Train Loss: 0.2024 Acc: 0.9294 | Val Loss: 0.1762 Acc: 0.9382                                               \n",
      "Epoch 029 | Train Loss: 0.1951 Acc: 0.9313 | Val Loss: 0.1801 Acc: 0.9400                                               \n",
      "Epoch 030 | Train Loss: 0.1851 Acc: 0.9344 | Val Loss: 0.2257 Acc: 0.9125                                               \n",
      "Epoch 031 | Train Loss: 0.1796 Acc: 0.9363 | Val Loss: 0.1947 Acc: 0.9325                                               \n",
      "Epoch 032 | Train Loss: 0.1727 Acc: 0.9402 | Val Loss: 0.2142 Acc: 0.9194                                               \n",
      "Epoch 033 | Train Loss: 0.1708 Acc: 0.9403 | Val Loss: 0.1797 Acc: 0.9385                                               \n",
      "Epoch 034 | Train Loss: 0.1793 Acc: 0.9351 | Val Loss: 0.1504 Acc: 0.9525                                               \n",
      "Epoch 035 | Train Loss: 0.1770 Acc: 0.9362 | Val Loss: 0.1486 Acc: 0.9496                                               \n",
      "Epoch 036 | Train Loss: 0.1573 Acc: 0.9440 | Val Loss: 0.1457 Acc: 0.9501                                               \n",
      "Epoch 037 | Train Loss: 0.1612 Acc: 0.9415 | Val Loss: 0.1821 Acc: 0.9370                                               \n",
      "Epoch 038 | Train Loss: 0.1813 Acc: 0.9369 | Val Loss: 0.1707 Acc: 0.9487                                               \n",
      "Epoch 039 | Train Loss: 0.1580 Acc: 0.9440 | Val Loss: 0.2412 Acc: 0.9161                                               \n",
      "Epoch 040 | Train Loss: 0.1616 Acc: 0.9421 | Val Loss: 0.2042 Acc: 0.9358                                               \n",
      "Epoch 041 | Train Loss: 0.1690 Acc: 0.9404 | Val Loss: 0.1905 Acc: 0.9454                                               \n",
      "Epoch 042 | Train Loss: 0.1438 Acc: 0.9490 | Val Loss: 0.2354 Acc: 0.9251                                               \n",
      "Epoch 043 | Train Loss: 0.1584 Acc: 0.9448 | Val Loss: 0.2197 Acc: 0.9304                                               \n",
      "Epoch 044 | Train Loss: 0.1507 Acc: 0.9470 | Val Loss: 0.1810 Acc: 0.9478                                               \n",
      "Epoch 045 | Train Loss: 0.1928 Acc: 0.9311 | Val Loss: 0.1644 Acc: 0.9418                                               \n",
      "Epoch 046 | Train Loss: 0.1522 Acc: 0.9462 | Val Loss: 0.1535 Acc: 0.9534                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 32, 'cnn_dropout': 0.21637238364427916, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 64, 'cnn_kernels_2': 16, 'learning_rate': 0.002312885317764835, 'lstm_dense': 64, 'lstm_hidden_size': 64, 'lstm_layers': 3, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 6.6690 Acc: 0.4689 | Val Loss: 1.4451 Acc: 0.4860                                               \n",
      "Epoch 002 | Train Loss: 0.9638 Acc: 0.5777 | Val Loss: 0.8528 Acc: 0.6149                                               \n",
      "Epoch 003 | Train Loss: 0.7751 Acc: 0.6483 | Val Loss: 0.6484 Acc: 0.7084                                               \n",
      "Epoch 004 | Train Loss: 0.7040 Acc: 0.6847 | Val Loss: 0.6065 Acc: 0.7346                                               \n",
      "Epoch 005 | Train Loss: 0.6733 Acc: 0.6963 | Val Loss: 0.7485 Acc: 0.6218                                               \n",
      "Epoch 006 | Train Loss: 0.6472 Acc: 0.7154 | Val Loss: 1.0366 Acc: 0.6164                                               \n",
      "Epoch 007 | Train Loss: 0.6342 Acc: 0.7180 | Val Loss: 0.6088 Acc: 0.7412                                               \n",
      "Epoch 008 | Train Loss: 0.6083 Acc: 0.7257 | Val Loss: 0.6292 Acc: 0.7164                                               \n",
      "Epoch 009 | Train Loss: 0.6083 Acc: 0.7277 | Val Loss: 0.6339 Acc: 0.7549                                               \n",
      "Epoch 010 | Train Loss: 0.6021 Acc: 0.7283 | Val Loss: 0.7186 Acc: 0.7451                                               \n",
      "Epoch 011 | Train Loss: 0.6013 Acc: 0.7297 | Val Loss: 0.5946 Acc: 0.7137                                               \n",
      "Epoch 012 | Train Loss: 0.6057 Acc: 0.7283 | Val Loss: 0.5258 Acc: 0.7600                                               \n",
      "Epoch 013 | Train Loss: 0.5905 Acc: 0.7329 | Val Loss: 0.5982 Acc: 0.7555                                               \n",
      "Epoch 014 | Train Loss: 0.5684 Acc: 0.7467 | Val Loss: 0.6880 Acc: 0.6940                                               \n",
      "Epoch 015 | Train Loss: 0.6014 Acc: 0.7319 | Val Loss: 0.7099 Acc: 0.7454                                               \n",
      "Epoch 016 | Train Loss: 0.5783 Acc: 0.7378 | Val Loss: 0.6716 Acc: 0.6788                                               \n",
      "Epoch 017 | Train Loss: 0.5890 Acc: 0.7428 | Val Loss: 0.6991 Acc: 0.7633                                               \n",
      "Epoch 018 | Train Loss: 0.5630 Acc: 0.7418 | Val Loss: 0.6283 Acc: 0.7176                                               \n",
      "Epoch 019 | Train Loss: 0.5819 Acc: 0.7392 | Val Loss: 0.6852 Acc: 0.7051                                               \n",
      "Epoch 020 | Train Loss: 0.5988 Acc: 0.7325 | Val Loss: 0.6129 Acc: 0.7119                                               \n",
      "Epoch 021 | Train Loss: 0.5947 Acc: 0.7330 | Val Loss: 0.4929 Acc: 0.7824                                               \n",
      "Epoch 022 | Train Loss: 0.6042 Acc: 0.7351 | Val Loss: 0.6160 Acc: 0.7651                                               \n",
      "Epoch 023 | Train Loss: 0.5685 Acc: 0.7502 | Val Loss: 0.5395 Acc: 0.7576                                               \n",
      "Epoch 024 | Train Loss: 0.5880 Acc: 0.7391 | Val Loss: 0.7263 Acc: 0.6928                                               \n",
      "Epoch 025 | Train Loss: 0.5734 Acc: 0.7437 | Val Loss: 0.6912 Acc: 0.7099                                               \n",
      "Epoch 026 | Train Loss: 0.5926 Acc: 0.7504 | Val Loss: 0.6065 Acc: 0.7215                                               \n",
      "Epoch 027 | Train Loss: 0.6106 Acc: 0.7269 | Val Loss: 0.5890 Acc: 0.7794                                               \n",
      "Epoch 028 | Train Loss: 0.5906 Acc: 0.7327 | Val Loss: 0.4970 Acc: 0.7812                                               \n",
      "Epoch 029 | Train Loss: 0.6030 Acc: 0.7294 | Val Loss: 0.5013 Acc: 0.7746                                               \n",
      "Epoch 015 | Train Loss: 0.3482 Acc: 0.8643 | Val Loss: 0.3271 Acc: 0.9116                                               \n",
      "Epoch 016 | Train Loss: 0.3508 Acc: 0.8645 | Val Loss: 0.3538 Acc: 0.8633                                               \n",
      "Epoch 017 | Train Loss: 0.3215 Acc: 0.8786 | Val Loss: 0.4010 Acc: 0.8645                                               \n",
      "Epoch 018 | Train Loss: 0.3064 Acc: 0.8879 | Val Loss: 0.3510 Acc: 0.8164                                               \n",
      "Epoch 019 | Train Loss: 0.3029 Acc: 0.8881 | Val Loss: 0.4509 Acc: 0.8615                                               \n",
      "Epoch 020 | Train Loss: 0.2965 Acc: 0.8929 | Val Loss: 0.3669 Acc: 0.8549                                               \n",
      "Epoch 021 | Train Loss: 0.2727 Acc: 0.9051 | Val Loss: 0.2620 Acc: 0.9167                                               \n",
      "Epoch 022 | Train Loss: 0.2722 Acc: 0.9051 | Val Loss: 0.5401 Acc: 0.8257                                               \n",
      "Epoch 023 | Train Loss: 0.2593 Acc: 0.9098 | Val Loss: 0.3209 Acc: 0.9006                                               \n",
      "Epoch 024 | Train Loss: 0.2518 Acc: 0.9171 | Val Loss: 0.3518 Acc: 0.8899                                               \n",
      "Epoch 025 | Train Loss: 0.2402 Acc: 0.9177 | Val Loss: 0.3806 Acc: 0.8373                                               \n",
      "Epoch 026 | Train Loss: 0.2516 Acc: 0.9178 | Val Loss: 0.2958 Acc: 0.8997                                               \n",
      "Epoch 027 | Train Loss: 0.2209 Acc: 0.9251 | Val Loss: 0.2570 Acc: 0.9203                                               \n",
      "Epoch 028 | Train Loss: 0.2298 Acc: 0.9210 | Val Loss: 0.4769 Acc: 0.8910                                               \n",
      "Epoch 029 | Train Loss: 0.2139 Acc: 0.9281 | Val Loss: 0.3042 Acc: 0.9122                                               \n",
      "Epoch 030 | Train Loss: 0.3886 Acc: 0.8455 | Val Loss: 0.5216 Acc: 0.7758                                               \n",
      "Epoch 031 | Train Loss: 0.3729 Acc: 0.8401 | Val Loss: 0.6217 Acc: 0.7152                                               \n",
      "Epoch 032 | Train Loss: 0.3732 Acc: 0.8428 | Val Loss: 0.4353 Acc: 0.7696                                               \n",
      "Epoch 033 | Train Loss: 0.3604 Acc: 0.8521 | Val Loss: 0.4385 Acc: 0.8161                                               \n",
      "Epoch 034 | Train Loss: 0.3463 Acc: 0.8618 | Val Loss: 0.4134 Acc: 0.8236                                               \n",
      "Epoch 035 | Train Loss: 0.2964 Acc: 0.8931 | Val Loss: 0.3366 Acc: 0.8564                                               \n",
      "Epoch 036 | Train Loss: 0.2359 Acc: 0.9182 | Val Loss: 0.4131 Acc: 0.8600                                               \n",
      "Epoch 037 | Train Loss: 0.2303 Acc: 0.9229 | Val Loss: 0.2846 Acc: 0.9164                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 256, 'cnn_dropout': 0.125426260510647, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 48, 'cnn_kernels_2': 16, 'learning_rate': 0.008527801332628336, 'lstm_dense': 64, 'lstm_hidden_size': 32, 'lstm_layers': 3, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 6.9853 Acc: 0.4971 | Val Loss: 0.7862 Acc: 0.6687                                               \n",
      "Epoch 002 | Train Loss: 0.8238 Acc: 0.6251 | Val Loss: 0.7400 Acc: 0.6334                                               \n",
      "Epoch 003 | Train Loss: 0.7762 Acc: 0.6438 | Val Loss: 0.6833 Acc: 0.7104                                               \n",
      "Epoch 004 | Train Loss: 0.7492 Acc: 0.6525 | Val Loss: 0.7733 Acc: 0.6239                                               \n",
      "Epoch 005 | Train Loss: 0.7630 Acc: 0.6367 | Val Loss: 0.7587 Acc: 0.6758                                               \n",
      "Epoch 006 | Train Loss: 0.7612 Acc: 0.6374 | Val Loss: 0.7322 Acc: 0.5994                                               \n",
      "Epoch 007 | Train Loss: 0.7408 Acc: 0.6492 | Val Loss: 0.7107 Acc: 0.6755                                               \n",
      "Epoch 008 | Train Loss: 0.7174 Acc: 0.6659 | Val Loss: 0.7325 Acc: 0.6997                                               \n",
      "Epoch 009 | Train Loss: 0.7407 Acc: 0.6548 | Val Loss: 0.7039 Acc: 0.6421                                               \n",
      "Epoch 010 | Train Loss: 0.7646 Acc: 0.6376 | Val Loss: 0.7706 Acc: 0.6746                                               \n",
      "Epoch 011 | Train Loss: 0.8401 Acc: 0.6065 | Val Loss: 0.8241 Acc: 0.6346                                               \n",
      "Epoch 012 | Train Loss: 0.8614 Acc: 0.5907 | Val Loss: 0.7017 Acc: 0.7284                                               \n",
      "Epoch 013 | Train Loss: 0.8274 Acc: 0.6156 | Val Loss: 0.7424 Acc: 0.6651                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 32, 'cnn_dropout': 0.30232290420582564, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 16, 'learning_rate': 0.00012837946529286215, 'lstm_dense': 64, 'lstm_hidden_size': 96, 'lstm_layers': 3, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 34.9293 Acc: 0.3715 | Val Loss: 3.8102 Acc: 0.5084                                              \n",
      "Epoch 002 | Train Loss: 13.0612 Acc: 0.4183 | Val Loss: 4.5390 Acc: 0.4949                                              \n",
      "Epoch 003 | Train Loss: 8.6575 Acc: 0.4429 | Val Loss: 2.2032 Acc: 0.4937                                               \n",
      "Epoch 004 | Train Loss: 6.6961 Acc: 0.4552 | Val Loss: 3.8585 Acc: 0.5269                                               \n",
      "Epoch 005 | Train Loss: 5.3779 Acc: 0.4803 | Val Loss: 3.0100 Acc: 0.6110                                               \n",
      "Epoch 006 | Train Loss: 4.4460 Acc: 0.5058 | Val Loss: 1.4963 Acc: 0.6418                                               \n",
      "Epoch 007 | Train Loss: 3.6602 Acc: 0.5285 | Val Loss: 1.6508 Acc: 0.6445                                               \n",
      "Epoch 008 | Train Loss: 3.1672 Acc: 0.5496 | Val Loss: 1.9824 Acc: 0.6045                                               \n",
      "Epoch 009 | Train Loss: 2.8043 Acc: 0.5692 | Val Loss: 1.4342 Acc: 0.6487                                               \n",
      "Epoch 010 | Train Loss: 2.4714 Acc: 0.5894 | Val Loss: 1.1410 Acc: 0.6991                                               \n",
      "Epoch 011 | Train Loss: 2.2056 Acc: 0.5934 | Val Loss: 1.5224 Acc: 0.6376                                               \n",
      "Epoch 012 | Train Loss: 1.8680 Acc: 0.6345 | Val Loss: 0.7670 Acc: 0.7627                                               \n",
      "Epoch 013 | Train Loss: 1.6272 Acc: 0.6553 | Val Loss: 0.9004 Acc: 0.7388                                               \n",
      "Epoch 014 | Train Loss: 1.4240 Acc: 0.6753 | Val Loss: 0.8676 Acc: 0.7439                                               \n",
      "Epoch 015 | Train Loss: 1.2678 Acc: 0.6939 | Val Loss: 0.9389 Acc: 0.7537                                               \n",
      "Epoch 016 | Train Loss: 1.1396 Acc: 0.7159 | Val Loss: 1.4213 Acc: 0.7024                                               \n",
      "Epoch 017 | Train Loss: 1.0280 Acc: 0.7327 | Val Loss: 0.6059 Acc: 0.7973                                               \n",
      "Epoch 018 | Train Loss: 0.9518 Acc: 0.7451 | Val Loss: 0.9106 Acc: 0.7531                                               \n",
      "Epoch 019 | Train Loss: 0.8706 Acc: 0.7615 | Val Loss: 1.5160 Acc: 0.6531                                               \n",
      "Epoch 020 | Train Loss: 0.7547 Acc: 0.7874 | Val Loss: 0.8358 Acc: 0.7719                                               \n",
      "Epoch 021 | Train Loss: 0.6739 Acc: 0.7974 | Val Loss: 1.4630 Acc: 0.6716                                               \n",
      "Epoch 022 | Train Loss: 0.6287 Acc: 0.8083 | Val Loss: 0.7169 Acc: 0.7663                                               \n",
      "Epoch 023 | Train Loss: 0.5751 Acc: 0.8277 | Val Loss: 0.3433 Acc: 0.8821                                               \n",
      "Epoch 024 | Train Loss: 0.5274 Acc: 0.8404 | Val Loss: 0.5146 Acc: 0.8257                                               \n",
      "Epoch 025 | Train Loss: 0.4510 Acc: 0.8582 | Val Loss: 0.3354 Acc: 0.8910                                               \n",
      "Epoch 026 | Train Loss: 0.4244 Acc: 0.8633 | Val Loss: 0.3092 Acc: 0.9027                                               \n",
      "Epoch 027 | Train Loss: 0.3833 Acc: 0.8807 | Val Loss: 0.2822 Acc: 0.9057                                               \n",
      "Epoch 028 | Train Loss: 0.3532 Acc: 0.8878 | Val Loss: 0.4277 Acc: 0.8549                                               \n",
      "Epoch 029 | Train Loss: 0.3352 Acc: 0.8925 | Val Loss: 0.2887 Acc: 0.9015                                               \n",
      "Epoch 030 | Train Loss: 0.2896 Acc: 0.9030 | Val Loss: 0.2258 Acc: 0.9245                                               \n",
      "Epoch 031 | Train Loss: 0.2790 Acc: 0.9111 | Val Loss: 0.2979 Acc: 0.9000                                               \n",
      "Epoch 032 | Train Loss: 0.2640 Acc: 0.9120 | Val Loss: 0.2570 Acc: 0.9200                                               \n",
      "Epoch 033 | Train Loss: 0.2424 Acc: 0.9231 | Val Loss: 0.1674 Acc: 0.9448                                               \n",
      "Epoch 034 | Train Loss: 0.2094 Acc: 0.9300 | Val Loss: 0.1864 Acc: 0.9394                                               \n",
      "Epoch 035 | Train Loss: 0.2003 Acc: 0.9325 | Val Loss: 0.2047 Acc: 0.9349                                               \n",
      "Epoch 036 | Train Loss: 0.1843 Acc: 0.9387 | Val Loss: 0.1684 Acc: 0.9469                                               \n",
      "Epoch 037 | Train Loss: 0.1855 Acc: 0.9372 | Val Loss: 0.3041 Acc: 0.9099                                               \n",
      "Epoch 038 | Train Loss: 0.1706 Acc: 0.9449 | Val Loss: 0.2146 Acc: 0.9370                                               \n",
      "Epoch 039 | Train Loss: 0.1596 Acc: 0.9478 | Val Loss: 0.2279 Acc: 0.9182                                               \n",
      "Epoch 040 | Train Loss: 0.1547 Acc: 0.9478 | Val Loss: 0.1186 Acc: 0.9594                                               \n",
      "Epoch 041 | Train Loss: 0.1462 Acc: 0.9506 | Val Loss: 0.1392 Acc: 0.9567                                               \n",
      "Epoch 042 | Train Loss: 0.1278 Acc: 0.9537 | Val Loss: 0.1084 Acc: 0.9687                                               \n",
      "Epoch 043 | Train Loss: 0.1308 Acc: 0.9557 | Val Loss: 0.1924 Acc: 0.9403                                               \n",
      "Epoch 044 | Train Loss: 0.1309 Acc: 0.9590 | Val Loss: 0.1678 Acc: 0.9364                                               \n",
      "Epoch 045 | Train Loss: 0.1182 Acc: 0.9595 | Val Loss: 0.1367 Acc: 0.9525                                               \n",
      "Epoch 046 | Train Loss: 0.1066 Acc: 0.9648 | Val Loss: 0.0957 Acc: 0.9684                                               \n",
      "Epoch 047 | Train Loss: 0.1038 Acc: 0.9644 | Val Loss: 0.1251 Acc: 0.9624                                               \n",
      "Epoch 048 | Train Loss: 0.1048 Acc: 0.9652 | Val Loss: 0.1106 Acc: 0.9696                                               \n",
      "Epoch 049 | Train Loss: 0.0932 Acc: 0.9701 | Val Loss: 0.1604 Acc: 0.9427                                               \n",
      "Epoch 050 | Train Loss: 0.0857 Acc: 0.9731 | Val Loss: 0.0970 Acc: 0.9654                                               \n",
      "Epoch 051 | Train Loss: 0.0883 Acc: 0.9712 | Val Loss: 0.1262 Acc: 0.9606                                               \n",
      "Epoch 052 | Train Loss: 0.0843 Acc: 0.9730 | Val Loss: 0.1387 Acc: 0.9573                                               \n",
      "Epoch 053 | Train Loss: 0.0824 Acc: 0.9748 | Val Loss: 0.1101 Acc: 0.9660                                               \n",
      "Epoch 054 | Train Loss: 0.0760 Acc: 0.9747 | Val Loss: 0.1069 Acc: 0.9693                                               \n",
      "Epoch 055 | Train Loss: 0.0708 Acc: 0.9767 | Val Loss: 0.0875 Acc: 0.9755                                               \n",
      "Epoch 056 | Train Loss: 0.0780 Acc: 0.9746 | Val Loss: 0.0854 Acc: 0.9728                                               \n",
      "Epoch 057 | Train Loss: 0.0634 Acc: 0.9789 | Val Loss: 0.0989 Acc: 0.9701                                               \n",
      "Epoch 058 | Train Loss: 0.0764 Acc: 0.9767 | Val Loss: 0.1398 Acc: 0.9579                                               \n",
      "Epoch 059 | Train Loss: 0.0574 Acc: 0.9810 | Val Loss: 0.0908 Acc: 0.9701                                               \n",
      "Epoch 060 | Train Loss: 0.0643 Acc: 0.9782 | Val Loss: 0.1034 Acc: 0.9672                                               \n",
      "100%|████████████████████████████████████████████████| 30/30 [24:34<00:00, 49.16s/trial, best loss: 0.03237664831468641]\n",
      "Best hyperparameters: {'batch_size': np.int64(0), 'cnn_dense': np.int64(0), 'cnn_dropout': np.float64(0.14524046638718155), 'cnn_kernel_size_1': np.int64(0), 'cnn_kernel_size_2': np.int64(0), 'cnn_kernels_1': np.int64(3), 'cnn_kernels_2': np.int64(0), 'learning_rate': np.float64(0.0002918044284033363), 'lstm_dense': np.int64(1), 'lstm_hidden_size': np.int64(2), 'lstm_layers': np.int64(2), 'optimizer': np.int64(1)}\n",
      "TPE search finished in 1474.66 seconds\n",
      "Best (raw indices): {'batch_size': np.int64(0), 'cnn_dense': np.int64(0), 'cnn_dropout': np.float64(0.14524046638718155), 'cnn_kernel_size_1': np.int64(0), 'cnn_kernel_size_2': np.int64(0), 'cnn_kernels_1': np.int64(3), 'cnn_kernels_2': np.int64(0), 'learning_rate': np.float64(0.0002918044284033363), 'lstm_dense': np.int64(1), 'lstm_hidden_size': np.int64(2), 'lstm_layers': np.int64(2), 'optimizer': np.int64(1)}\n",
      "Best (interpreted): {'batch_size': 32, 'cnn_dense': 32, 'cnn_dropout': np.float64(0.14524046638718155), 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 64, 'cnn_kernels_2': 16, 'learning_rate': np.float64(0.0002918044284033363), 'lstm_dense': 64, 'lstm_hidden_size': 96, 'lstm_layers': 3, 'optimizer': 'rmsprop'}\n",
      "Starting TPE search...\n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 32, 'cnn_dropout': 0.6355295402398653, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 64, 'learning_rate': 1.5887110901381966e-05, 'lstm_dense': 128, 'lstm_hidden_size': 96, 'lstm_layers': 5, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 65.9954 Acc: 0.3400 | Val Loss: 40.3426 Acc: 0.4373                                             \n",
      "Epoch 002 | Train Loss: 44.8489 Acc: 0.3270 | Val Loss: 26.6260 Acc: 0.4421                                             \n",
      "Epoch 003 | Train Loss: 33.7164 Acc: 0.3186 | Val Loss: 19.6884 Acc: 0.4421                                             \n",
      "Epoch 004 | Train Loss: 28.3797 Acc: 0.3191 | Val Loss: 14.2934 Acc: 0.4421                                             \n",
      "Epoch 005 | Train Loss: 24.0423 Acc: 0.3276 | Val Loss: 9.3156 Acc: 0.4484                                              \n",
      "Epoch 006 | Train Loss: 20.6001 Acc: 0.3342 | Val Loss: 7.4350 Acc: 0.4397                                              \n",
      "Epoch 007 | Train Loss: 18.1731 Acc: 0.3409 | Val Loss: 5.0240 Acc: 0.3854                                              \n",
      "Epoch 008 | Train Loss: 16.1227 Acc: 0.3515 | Val Loss: 4.1047 Acc: 0.3669                                              \n",
      "Epoch 009 | Train Loss: 14.0684 Acc: 0.3553 | Val Loss: 3.5514 Acc: 0.2991                                              \n",
      "Epoch 010 | Train Loss: 12.5912 Acc: 0.3611 | Val Loss: 3.4194 Acc: 0.3448                                              \n",
      "Epoch 011 | Train Loss: 11.5049 Acc: 0.3744 | Val Loss: 3.6269 Acc: 0.3424                                              \n",
      "Epoch 012 | Train Loss: 10.5538 Acc: 0.3805 | Val Loss: 3.5511 Acc: 0.3791                                              \n",
      "Epoch 013 | Train Loss: 9.7193 Acc: 0.3869 | Val Loss: 3.4432 Acc: 0.3516                                               \n",
      "Epoch 014 | Train Loss: 8.8390 Acc: 0.3916 | Val Loss: 2.7598 Acc: 0.3615                                               \n",
      "Epoch 015 | Train Loss: 8.2833 Acc: 0.3991 | Val Loss: 2.8652 Acc: 0.3979                                               \n",
      "Epoch 016 | Train Loss: 7.8768 Acc: 0.3930 | Val Loss: 2.2998 Acc: 0.3773                                               \n",
      "Epoch 017 | Train Loss: 7.5187 Acc: 0.4027 | Val Loss: 1.9329 Acc: 0.4412                                               \n",
      "Epoch 018 | Train Loss: 6.9861 Acc: 0.4183 | Val Loss: 1.9702 Acc: 0.4573                                               \n",
      "Epoch 019 | Train Loss: 6.6190 Acc: 0.4210 | Val Loss: 2.1500 Acc: 0.4418                                               \n",
      "Epoch 020 | Train Loss: 6.3981 Acc: 0.4247 | Val Loss: 1.8931 Acc: 0.4651                                               \n",
      "Epoch 021 | Train Loss: 5.8619 Acc: 0.4332 | Val Loss: 1.8542 Acc: 0.5206                                               \n",
      "Epoch 022 | Train Loss: 5.7366 Acc: 0.4368 | Val Loss: 1.5962 Acc: 0.5385                                               \n",
      "Epoch 023 | Train Loss: 5.5884 Acc: 0.4400 | Val Loss: 1.9712 Acc: 0.5096                                               \n",
      "Epoch 024 | Train Loss: 5.3015 Acc: 0.4404 | Val Loss: 1.5974 Acc: 0.5812                                               \n",
      "Epoch 025 | Train Loss: 5.1792 Acc: 0.4426 | Val Loss: 1.4281 Acc: 0.6167                                               \n",
      "Epoch 026 | Train Loss: 4.9332 Acc: 0.4529 | Val Loss: 1.5046 Acc: 0.5779                                               \n",
      "Epoch 027 | Train Loss: 4.7155 Acc: 0.4509 | Val Loss: 1.5578 Acc: 0.5615                                               \n",
      "Epoch 028 | Train Loss: 4.6376 Acc: 0.4569 | Val Loss: 1.6615 Acc: 0.5710                                               \n",
      "Epoch 029 | Train Loss: 4.5459 Acc: 0.4638 | Val Loss: 1.5102 Acc: 0.5899                                               \n",
      "Epoch 030 | Train Loss: 4.1381 Acc: 0.4737 | Val Loss: 1.3003 Acc: 0.6075                                               \n",
      "Epoch 031 | Train Loss: 4.1576 Acc: 0.4804 | Val Loss: 1.2778 Acc: 0.6054                                               \n",
      "Epoch 032 | Train Loss: 3.9724 Acc: 0.4760 | Val Loss: 1.4202 Acc: 0.5758                                               \n",
      "Epoch 033 | Train Loss: 3.8829 Acc: 0.4823 | Val Loss: 1.2176 Acc: 0.5988                                               \n",
      "Epoch 034 | Train Loss: 3.6221 Acc: 0.4891 | Val Loss: 1.4206 Acc: 0.5681                                               \n",
      "Epoch 035 | Train Loss: 3.6337 Acc: 0.4894 | Val Loss: 1.1481 Acc: 0.5916                                               \n",
      "Epoch 036 | Train Loss: 3.5240 Acc: 0.4897 | Val Loss: 1.1466 Acc: 0.6278                                               \n",
      "Epoch 037 | Train Loss: 3.4078 Acc: 0.4974 | Val Loss: 1.1232 Acc: 0.6340                                               \n",
      "Epoch 038 | Train Loss: 3.2367 Acc: 0.5039 | Val Loss: 1.2116 Acc: 0.5896                                               \n",
      "Epoch 039 | Train Loss: 3.1442 Acc: 0.5013 | Val Loss: 1.1085 Acc: 0.6445                                               \n",
      "Epoch 040 | Train Loss: 3.0613 Acc: 0.5068 | Val Loss: 1.1170 Acc: 0.6382                                               \n",
      "Epoch 041 | Train Loss: 2.9910 Acc: 0.5112 | Val Loss: 1.0169 Acc: 0.6340                                               \n",
      "Epoch 042 | Train Loss: 2.9210 Acc: 0.5232 | Val Loss: 0.9946 Acc: 0.6860                                               \n",
      "Epoch 043 | Train Loss: 2.8132 Acc: 0.5275 | Val Loss: 0.9656 Acc: 0.6499                                               \n",
      "Epoch 044 | Train Loss: 2.7899 Acc: 0.5309 | Val Loss: 1.0353 Acc: 0.6540                                               \n",
      "Epoch 045 | Train Loss: 2.6935 Acc: 0.5343 | Val Loss: 0.9963 Acc: 0.6328                                               \n",
      "Epoch 046 | Train Loss: 2.6089 Acc: 0.5427 | Val Loss: 0.9724 Acc: 0.6561                                               \n",
      "Epoch 047 | Train Loss: 2.5225 Acc: 0.5494 | Val Loss: 1.0783 Acc: 0.6319                                               \n",
      "Epoch 048 | Train Loss: 2.4264 Acc: 0.5588 | Val Loss: 1.1779 Acc: 0.6472                                               \n",
      "Epoch 049 | Train Loss: 2.3836 Acc: 0.5563 | Val Loss: 0.9173 Acc: 0.6710                                               \n",
      "Epoch 050 | Train Loss: 2.3824 Acc: 0.5601 | Val Loss: 0.9416 Acc: 0.6821                                               \n",
      "Epoch 051 | Train Loss: 2.2572 Acc: 0.5705 | Val Loss: 0.9095 Acc: 0.6833                                               \n",
      "Epoch 052 | Train Loss: 2.2496 Acc: 0.5682 | Val Loss: 0.8842 Acc: 0.7027                                               \n",
      "Epoch 053 | Train Loss: 2.1652 Acc: 0.5790 | Val Loss: 0.9669 Acc: 0.6731                                               \n",
      "Epoch 054 | Train Loss: 2.0668 Acc: 0.5862 | Val Loss: 0.7901 Acc: 0.7093                                               \n",
      "Epoch 055 | Train Loss: 2.1035 Acc: 0.5894 | Val Loss: 0.7874 Acc: 0.7173                                               \n",
      "Epoch 056 | Train Loss: 2.0414 Acc: 0.5944 | Val Loss: 0.8313 Acc: 0.6988                                               \n",
      "Epoch 057 | Train Loss: 2.0188 Acc: 0.5987 | Val Loss: 0.8089 Acc: 0.7236                                               \n",
      "Epoch 058 | Train Loss: 1.9214 Acc: 0.5934 | Val Loss: 0.9267 Acc: 0.7251                                               \n",
      "Epoch 059 | Train Loss: 1.9132 Acc: 0.6041 | Val Loss: 0.8004 Acc: 0.7290                                               \n",
      "Epoch 060 | Train Loss: 1.8670 Acc: 0.6140 | Val Loss: 0.7316 Acc: 0.7299                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 36, 'cnn_dense': 256, 'cnn_dropout': 0.35949774798575745, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 32, 'cnn_kernels_2': 64, 'learning_rate': 5.079364620243633e-05, 'lstm_dense': 128, 'lstm_hidden_size': 64, 'lstm_layers': 6, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 4.1554 Acc: 0.4232 | Val Loss: 0.9909 Acc: 0.5358                                               \n",
      "Epoch 002 | Train Loss: 1.0045 Acc: 0.4665 | Val Loss: 0.8946 Acc: 0.5027                                               \n",
      "Epoch 003 | Train Loss: 0.9384 Acc: 0.4974 | Val Loss: 0.8607 Acc: 0.6110                                               \n",
      "Epoch 004 | Train Loss: 0.9101 Acc: 0.5252 | Val Loss: 0.8406 Acc: 0.5696                                               \n",
      "Epoch 005 | Train Loss: 0.8880 Acc: 0.5331 | Val Loss: 0.8163 Acc: 0.5737                                               \n",
      "Epoch 006 | Train Loss: 0.8763 Acc: 0.5393 | Val Loss: 0.7954 Acc: 0.6167                                               \n",
      "Epoch 007 | Train Loss: 0.8715 Acc: 0.5438 | Val Loss: 0.7988 Acc: 0.6481                                               \n",
      "Epoch 008 | Train Loss: 0.8611 Acc: 0.5615 | Val Loss: 0.7889 Acc: 0.6149                                               \n",
      "Epoch 009 | Train Loss: 0.8558 Acc: 0.5583 | Val Loss: 0.7829 Acc: 0.6618                                               \n",
      "Epoch 010 | Train Loss: 0.8499 Acc: 0.5757 | Val Loss: 0.7721 Acc: 0.6173                                               \n",
      "Epoch 011 | Train Loss: 0.8487 Acc: 0.5688 | Val Loss: 0.7845 Acc: 0.6403                                               \n",
      "Epoch 012 | Train Loss: 0.8423 Acc: 0.5810 | Val Loss: 0.7482 Acc: 0.6519                                               \n",
      "Epoch 013 | Train Loss: 0.8392 Acc: 0.5863 | Val Loss: 0.7638 Acc: 0.6424                                               \n",
      "Epoch 014 | Train Loss: 0.8399 Acc: 0.5842 | Val Loss: 0.7513 Acc: 0.6451                                               \n",
      "Epoch 015 | Train Loss: 0.8338 Acc: 0.5932 | Val Loss: 0.7410 Acc: 0.6457                                               \n",
      "Epoch 016 | Train Loss: 0.8346 Acc: 0.5874 | Val Loss: 0.7425 Acc: 0.6397                                               \n",
      "Epoch 017 | Train Loss: 0.8302 Acc: 0.5898 | Val Loss: 0.7314 Acc: 0.6716                                               \n",
      "Epoch 018 | Train Loss: 0.8220 Acc: 0.5974 | Val Loss: 0.7690 Acc: 0.6030                                               \n",
      "Epoch 019 | Train Loss: 0.8196 Acc: 0.5997 | Val Loss: 0.7133 Acc: 0.7039                                               \n",
      "Epoch 020 | Train Loss: 0.8188 Acc: 0.6049 | Val Loss: 0.7111 Acc: 0.7042                                               \n",
      "Epoch 021 | Train Loss: 0.8131 Acc: 0.6053 | Val Loss: 0.7077 Acc: 0.6875                                               \n",
      "Epoch 022 | Train Loss: 0.8074 Acc: 0.6133 | Val Loss: 0.7353 Acc: 0.6424                                               \n",
      "Epoch 023 | Train Loss: 0.8064 Acc: 0.6112 | Val Loss: 0.7408 Acc: 0.6415                                               \n",
      "Epoch 024 | Train Loss: 0.8062 Acc: 0.6101 | Val Loss: 0.6899 Acc: 0.6976                                               \n",
      "Epoch 025 | Train Loss: 0.7995 Acc: 0.6140 | Val Loss: 0.6934 Acc: 0.6973                                               \n",
      "Epoch 026 | Train Loss: 0.7958 Acc: 0.6130 | Val Loss: 0.6857 Acc: 0.6636                                               \n",
      "Epoch 027 | Train Loss: 0.7919 Acc: 0.6221 | Val Loss: 0.7015 Acc: 0.7349                                               \n",
      "Epoch 028 | Train Loss: 0.7937 Acc: 0.6174 | Val Loss: 0.7045 Acc: 0.6681                                               \n",
      "Epoch 029 | Train Loss: 0.7848 Acc: 0.6150 | Val Loss: 0.6777 Acc: 0.7337                                               \n",
      "Epoch 030 | Train Loss: 0.7925 Acc: 0.6142 | Val Loss: 0.6769 Acc: 0.7042                                               \n",
      "Epoch 031 | Train Loss: 0.7816 Acc: 0.6274 | Val Loss: 0.6772 Acc: 0.6922                                               \n",
      "Epoch 032 | Train Loss: 0.7920 Acc: 0.6169 | Val Loss: 0.6622 Acc: 0.7093                                               \n",
      "Epoch 033 | Train Loss: 0.7796 Acc: 0.6244 | Val Loss: 0.6547 Acc: 0.7164                                               \n",
      "Epoch 034 | Train Loss: 0.7785 Acc: 0.6262 | Val Loss: 0.6577 Acc: 0.7000                                               \n",
      "Epoch 035 | Train Loss: 0.7769 Acc: 0.6210 | Val Loss: 0.6685 Acc: 0.6624                                               \n",
      "Epoch 036 | Train Loss: 0.7686 Acc: 0.6265 | Val Loss: 0.6485 Acc: 0.6979                                               \n",
      "Epoch 037 | Train Loss: 0.7738 Acc: 0.6322 | Val Loss: 0.6770 Acc: 0.6890                                               \n",
      "Epoch 038 | Train Loss: 0.7753 Acc: 0.6318 | Val Loss: 0.6359 Acc: 0.7146                                               \n",
      "Epoch 039 | Train Loss: 0.7621 Acc: 0.6406 | Val Loss: 0.6339 Acc: 0.7278                                               \n",
      "Epoch 040 | Train Loss: 0.7604 Acc: 0.6359 | Val Loss: 0.6754 Acc: 0.6937                                               \n",
      "Epoch 041 | Train Loss: 0.7635 Acc: 0.6395 | Val Loss: 0.6432 Acc: 0.7143                                               \n",
      "Epoch 042 | Train Loss: 0.7680 Acc: 0.6377 | Val Loss: 0.6265 Acc: 0.7731                                               \n",
      "Epoch 043 | Train Loss: 0.7709 Acc: 0.6226 | Val Loss: 0.6228 Acc: 0.7203                                               \n",
      "Epoch 044 | Train Loss: 0.7560 Acc: 0.6374 | Val Loss: 0.6349 Acc: 0.7457                                               \n",
      "Epoch 045 | Train Loss: 0.7566 Acc: 0.6409 | Val Loss: 0.6207 Acc: 0.7248                                               \n",
      "Epoch 046 | Train Loss: 0.7542 Acc: 0.6336 | Val Loss: 0.6434 Acc: 0.7015                                               \n",
      "Epoch 047 | Train Loss: 0.7506 Acc: 0.6393 | Val Loss: 0.6432 Acc: 0.7075                                               \n",
      "Epoch 048 | Train Loss: 0.7586 Acc: 0.6383 | Val Loss: 0.6182 Acc: 0.6994                                               \n",
      "Epoch 049 | Train Loss: 0.7431 Acc: 0.6477 | Val Loss: 0.6248 Acc: 0.7000                                               \n",
      "Epoch 050 | Train Loss: 0.7560 Acc: 0.6419 | Val Loss: 0.6453 Acc: 0.7400                                               \n",
      "Epoch 051 | Train Loss: 0.7482 Acc: 0.6465 | Val Loss: 0.6465 Acc: 0.7242                                               \n",
      "Epoch 052 | Train Loss: 0.7485 Acc: 0.6425 | Val Loss: 0.6232 Acc: 0.6615                                               \n",
      "Epoch 053 | Train Loss: 0.7357 Acc: 0.6495 | Val Loss: 0.6145 Acc: 0.6716                                               \n",
      "Epoch 054 | Train Loss: 0.7331 Acc: 0.6516 | Val Loss: 0.5988 Acc: 0.6907                                               \n",
      "Epoch 055 | Train Loss: 0.7431 Acc: 0.6514 | Val Loss: 0.6252 Acc: 0.7340                                               \n",
      "Epoch 056 | Train Loss: 0.7433 Acc: 0.6525 | Val Loss: 0.6087 Acc: 0.7215                                               \n",
      "Epoch 057 | Train Loss: 0.7309 Acc: 0.6570 | Val Loss: 0.5878 Acc: 0.7543                                               \n",
      "Epoch 058 | Train Loss: 0.7363 Acc: 0.6546 | Val Loss: 0.6016 Acc: 0.7015                                               \n",
      "Epoch 059 | Train Loss: 0.7418 Acc: 0.6470 | Val Loss: 0.6127 Acc: 0.7278                                               \n",
      "Epoch 060 | Train Loss: 0.7324 Acc: 0.6535 | Val Loss: 0.5978 Acc: 0.7433                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 256, 'cnn_dropout': 0.28580831770321496, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 64, 'learning_rate': 0.00021649871464847646, 'lstm_dense': 128, 'lstm_hidden_size': 32, 'lstm_layers': 6, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 22.1955 Acc: 0.3708 | Val Loss: 4.3890 Acc: 0.5376                                              \n",
      "Epoch 002 | Train Loss: 6.4611 Acc: 0.4234 | Val Loss: 2.1209 Acc: 0.5803                                               \n",
      "Epoch 003 | Train Loss: 3.9570 Acc: 0.4682 | Val Loss: 1.5476 Acc: 0.5627                                               \n",
      "Epoch 004 | Train Loss: 2.6031 Acc: 0.5012 | Val Loss: 0.9699 Acc: 0.6400                                               \n",
      "Epoch 005 | Train Loss: 1.9527 Acc: 0.5335 | Val Loss: 0.8587 Acc: 0.6445                                               \n",
      "Epoch 006 | Train Loss: 1.5402 Acc: 0.5615 | Val Loss: 1.1792 Acc: 0.5591                                               \n",
      "Epoch 007 | Train Loss: 1.2550 Acc: 0.6009 | Val Loss: 0.6499 Acc: 0.7424                                               \n",
      "Epoch 008 | Train Loss: 1.0706 Acc: 0.6397 | Val Loss: 0.9153 Acc: 0.6767                                               \n",
      "Epoch 009 | Train Loss: 0.9211 Acc: 0.6715 | Val Loss: 0.5835 Acc: 0.7582                                               \n",
      "Epoch 010 | Train Loss: 0.8046 Acc: 0.7086 | Val Loss: 1.0370 Acc: 0.6573                                               \n",
      "Epoch 011 | Train Loss: 0.7102 Acc: 0.7436 | Val Loss: 0.5144 Acc: 0.7925                                               \n",
      "Epoch 012 | Train Loss: 0.6170 Acc: 0.7807 | Val Loss: 0.6845 Acc: 0.7561                                               \n",
      "Epoch 013 | Train Loss: 0.5367 Acc: 0.8157 | Val Loss: 0.3998 Acc: 0.8496                                               \n",
      "Epoch 014 | Train Loss: 0.4861 Acc: 0.8390 | Val Loss: 0.3030 Acc: 0.8913                                               \n",
      "Epoch 015 | Train Loss: 0.4240 Acc: 0.8606 | Val Loss: 0.3058 Acc: 0.8946                                               \n",
      "Epoch 016 | Train Loss: 0.3747 Acc: 0.8785 | Val Loss: 0.2181 Acc: 0.9406                                               \n",
      "Epoch 017 | Train Loss: 0.3374 Acc: 0.8934 | Val Loss: 0.3368 Acc: 0.8770                                               \n",
      "Epoch 018 | Train Loss: 0.3041 Acc: 0.9024 | Val Loss: 0.2028 Acc: 0.9224                                               \n",
      "Epoch 019 | Train Loss: 0.2675 Acc: 0.9177 | Val Loss: 0.1896 Acc: 0.9319                                               \n",
      "Epoch 020 | Train Loss: 0.2340 Acc: 0.9287 | Val Loss: 0.2326 Acc: 0.9149                                               \n",
      "Epoch 021 | Train Loss: 0.2206 Acc: 0.9310 | Val Loss: 0.1846 Acc: 0.9310                                               \n",
      "Epoch 022 | Train Loss: 0.1941 Acc: 0.9380 | Val Loss: 0.1658 Acc: 0.9412                                               \n",
      "Epoch 023 | Train Loss: 0.1778 Acc: 0.9430 | Val Loss: 0.2456 Acc: 0.9110                                               \n",
      "Epoch 024 | Train Loss: 0.1588 Acc: 0.9531 | Val Loss: 0.1762 Acc: 0.9469                                               \n",
      "Epoch 025 | Train Loss: 0.1406 Acc: 0.9559 | Val Loss: 0.1468 Acc: 0.9534                                               \n",
      "Epoch 026 | Train Loss: 0.1259 Acc: 0.9619 | Val Loss: 0.1466 Acc: 0.9463                                               \n",
      "Epoch 027 | Train Loss: 0.1155 Acc: 0.9640 | Val Loss: 0.1118 Acc: 0.9684                                               \n",
      "Epoch 028 | Train Loss: 0.1042 Acc: 0.9692 | Val Loss: 0.1502 Acc: 0.9510                                               \n",
      "Epoch 029 | Train Loss: 0.0962 Acc: 0.9704 | Val Loss: 0.0968 Acc: 0.9749                                               \n",
      "Epoch 030 | Train Loss: 0.0944 Acc: 0.9720 | Val Loss: 0.1222 Acc: 0.9663                                               \n",
      "Epoch 031 | Train Loss: 0.0802 Acc: 0.9753 | Val Loss: 0.0927 Acc: 0.9699                                               \n",
      "Epoch 032 | Train Loss: 0.0791 Acc: 0.9773 | Val Loss: 0.0922 Acc: 0.9734                                               \n",
      "Epoch 033 | Train Loss: 0.0694 Acc: 0.9799 | Val Loss: 0.0774 Acc: 0.9761                                               \n",
      "Epoch 034 | Train Loss: 0.0647 Acc: 0.9810 | Val Loss: 0.1178 Acc: 0.9606                                               \n",
      "Epoch 035 | Train Loss: 0.0654 Acc: 0.9798 | Val Loss: 0.1289 Acc: 0.9666                                               \n",
      "Epoch 036 | Train Loss: 0.0553 Acc: 0.9840 | Val Loss: 0.0707 Acc: 0.9794                                               \n",
      "Epoch 037 | Train Loss: 0.0473 Acc: 0.9853 | Val Loss: 0.1468 Acc: 0.9478                                               \n",
      "Epoch 038 | Train Loss: 0.0538 Acc: 0.9842 | Val Loss: 0.1191 Acc: 0.9690                                               \n",
      "Epoch 039 | Train Loss: 0.0457 Acc: 0.9873 | Val Loss: 0.0741 Acc: 0.9815                                               \n",
      "Epoch 040 | Train Loss: 0.0453 Acc: 0.9857 | Val Loss: 0.1231 Acc: 0.9701                                               \n",
      "Epoch 041 | Train Loss: 0.0471 Acc: 0.9867 | Val Loss: 0.0724 Acc: 0.9797                                               \n",
      "Epoch 042 | Train Loss: 0.0451 Acc: 0.9873 | Val Loss: 0.0796 Acc: 0.9800                                               \n",
      "Epoch 043 | Train Loss: 0.0419 Acc: 0.9874 | Val Loss: 0.0861 Acc: 0.9767                                               \n",
      "Epoch 044 | Train Loss: 0.0347 Acc: 0.9889 | Val Loss: 0.0571 Acc: 0.9833                                               \n",
      "Epoch 045 | Train Loss: 0.0345 Acc: 0.9893 | Val Loss: 0.0713 Acc: 0.9806                                               \n",
      "Epoch 046 | Train Loss: 0.0351 Acc: 0.9906 | Val Loss: 0.0963 Acc: 0.9785                                               \n",
      "Epoch 047 | Train Loss: 0.0377 Acc: 0.9893 | Val Loss: 0.0971 Acc: 0.9782                                               \n",
      "Epoch 048 | Train Loss: 0.0379 Acc: 0.9886 | Val Loss: 0.0877 Acc: 0.9794                                               \n",
      "Epoch 049 | Train Loss: 0.0296 Acc: 0.9913 | Val Loss: 0.0953 Acc: 0.9704                                               \n",
      "Epoch 050 | Train Loss: 0.0316 Acc: 0.9901 | Val Loss: 0.0788 Acc: 0.9800                                               \n",
      "Epoch 051 | Train Loss: 0.0293 Acc: 0.9915 | Val Loss: 0.0732 Acc: 0.9821                                               \n",
      "Epoch 052 | Train Loss: 0.0290 Acc: 0.9920 | Val Loss: 0.0814 Acc: 0.9800                                               \n",
      "Epoch 053 | Train Loss: 0.0323 Acc: 0.9909 | Val Loss: 0.0744 Acc: 0.9815                                               \n",
      "Epoch 054 | Train Loss: 0.0254 Acc: 0.9931 | Val Loss: 0.0426 Acc: 0.9881                                               \n",
      "Epoch 055 | Train Loss: 0.0264 Acc: 0.9914 | Val Loss: 0.0601 Acc: 0.9839                                               \n",
      "Epoch 056 | Train Loss: 0.0285 Acc: 0.9924 | Val Loss: 0.0711 Acc: 0.9791                                               \n",
      "Epoch 057 | Train Loss: 0.0312 Acc: 0.9910 | Val Loss: 0.0825 Acc: 0.9806                                               \n",
      "Epoch 058 | Train Loss: 0.0257 Acc: 0.9918 | Val Loss: 0.0623 Acc: 0.9842                                               \n",
      "Epoch 059 | Train Loss: 0.0247 Acc: 0.9928 | Val Loss: 0.1272 Acc: 0.9758                                               \n",
      "Epoch 060 | Train Loss: 0.0279 Acc: 0.9926 | Val Loss: 0.0755 Acc: 0.9809                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 64, 'cnn_dropout': 0.450542302190394, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 96, 'learning_rate': 0.00031251251845932054, 'lstm_dense': 128, 'lstm_hidden_size': 128, 'lstm_layers': 6, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 7.0982 Acc: 0.2655 | Val Loss: 1.3529 Acc: 0.4490                                               \n",
      "Epoch 002 | Train Loss: 1.3362 Acc: 0.4470 | Val Loss: 1.3166 Acc: 0.4490                                               \n",
      "Epoch 003 | Train Loss: 1.3065 Acc: 0.4471 | Val Loss: 1.2919 Acc: 0.4490                                               \n",
      "Epoch 004 | Train Loss: 1.2866 Acc: 0.4470 | Val Loss: 1.2755 Acc: 0.4490                                               \n",
      "Epoch 005 | Train Loss: 1.2730 Acc: 0.4470 | Val Loss: 1.2644 Acc: 0.4490                                               \n",
      "Epoch 006 | Train Loss: 1.2640 Acc: 0.4468 | Val Loss: 1.2564 Acc: 0.4490                                               \n",
      "Epoch 007 | Train Loss: 1.2568 Acc: 0.4473 | Val Loss: 1.2506 Acc: 0.4490                                               \n",
      "Epoch 008 | Train Loss: 1.2517 Acc: 0.4473 | Val Loss: 1.2461 Acc: 0.4490                                               \n",
      "Epoch 009 | Train Loss: 1.2478 Acc: 0.4473 | Val Loss: 1.2426 Acc: 0.4490                                               \n",
      "Epoch 010 | Train Loss: 1.2447 Acc: 0.4473 | Val Loss: 1.2397 Acc: 0.4490                                               \n",
      "Epoch 011 | Train Loss: 1.2421 Acc: 0.4473 | Val Loss: 1.2373 Acc: 0.4490                                               \n",
      "Epoch 012 | Train Loss: 1.2402 Acc: 0.4473 | Val Loss: 1.2354 Acc: 0.4490                                               \n",
      "Epoch 013 | Train Loss: 1.2386 Acc: 0.4472 | Val Loss: 1.2338 Acc: 0.4490                                               \n",
      "Epoch 014 | Train Loss: 1.2372 Acc: 0.4472 | Val Loss: 1.2324 Acc: 0.4490                                               \n",
      "Epoch 015 | Train Loss: 1.2357 Acc: 0.4473 | Val Loss: 1.2313 Acc: 0.4490                                               \n",
      "Epoch 016 | Train Loss: 1.2351 Acc: 0.4472 | Val Loss: 1.2303 Acc: 0.4490                                               \n",
      "Epoch 017 | Train Loss: 1.2336 Acc: 0.4474 | Val Loss: 1.2294 Acc: 0.4490                                               \n",
      "Epoch 018 | Train Loss: 1.2333 Acc: 0.4473 | Val Loss: 1.2287 Acc: 0.4490                                               \n",
      "Epoch 019 | Train Loss: 1.2325 Acc: 0.4473 | Val Loss: 1.2281 Acc: 0.4490                                               \n",
      "Epoch 020 | Train Loss: 1.2322 Acc: 0.4473 | Val Loss: 1.2276 Acc: 0.4490                                               \n",
      "Epoch 021 | Train Loss: 1.2317 Acc: 0.4473 | Val Loss: 1.2271 Acc: 0.4490                                               \n",
      "Epoch 022 | Train Loss: 1.2313 Acc: 0.4473 | Val Loss: 1.2268 Acc: 0.4490                                               \n",
      "Epoch 023 | Train Loss: 1.2312 Acc: 0.4472 | Val Loss: 1.2264 Acc: 0.4490                                               \n",
      "Epoch 024 | Train Loss: 1.2309 Acc: 0.4472 | Val Loss: 1.2261 Acc: 0.4490                                               \n",
      "Epoch 025 | Train Loss: 1.2301 Acc: 0.4474 | Val Loss: 1.2259 Acc: 0.4490                                               \n",
      "Epoch 026 | Train Loss: 1.2302 Acc: 0.4473 | Val Loss: 1.2257 Acc: 0.4490                                               \n",
      "Epoch 027 | Train Loss: 1.2301 Acc: 0.4473 | Val Loss: 1.2255 Acc: 0.4490                                               \n",
      "Epoch 028 | Train Loss: 1.2299 Acc: 0.4473 | Val Loss: 1.2253 Acc: 0.4490                                               \n",
      "Epoch 029 | Train Loss: 1.2300 Acc: 0.4472 | Val Loss: 1.2252 Acc: 0.4490                                               \n",
      "Epoch 030 | Train Loss: 1.2295 Acc: 0.4473 | Val Loss: 1.2251 Acc: 0.4490                                               \n",
      "Epoch 031 | Train Loss: 1.2292 Acc: 0.4474 | Val Loss: 1.2250 Acc: 0.4490                                               \n",
      "Epoch 032 | Train Loss: 1.2291 Acc: 0.4474 | Val Loss: 1.2249 Acc: 0.4490                                               \n",
      "Epoch 033 | Train Loss: 1.2296 Acc: 0.4472 | Val Loss: 1.2248 Acc: 0.4490                                               \n",
      "Epoch 034 | Train Loss: 1.2294 Acc: 0.4473 | Val Loss: 1.2247 Acc: 0.4490                                               \n",
      "Epoch 035 | Train Loss: 1.2295 Acc: 0.4472 | Val Loss: 1.2247 Acc: 0.4490                                               \n",
      "Epoch 036 | Train Loss: 1.2291 Acc: 0.4473 | Val Loss: 1.2246 Acc: 0.4490                                               \n",
      "Epoch 037 | Train Loss: 1.2292 Acc: 0.4473 | Val Loss: 1.2246 Acc: 0.4490                                               \n",
      "Epoch 038 | Train Loss: 1.2290 Acc: 0.4473 | Val Loss: 1.2245 Acc: 0.4490                                               \n",
      "Epoch 039 | Train Loss: 1.2290 Acc: 0.4473 | Val Loss: 1.2245 Acc: 0.4490                                               \n",
      "Epoch 040 | Train Loss: 1.2287 Acc: 0.4474 | Val Loss: 1.2244 Acc: 0.4490                                               \n",
      "Epoch 041 | Train Loss: 1.2287 Acc: 0.4474 | Val Loss: 1.2244 Acc: 0.4490                                               \n",
      "Epoch 042 | Train Loss: 1.2291 Acc: 0.4473 | Val Loss: 1.2244 Acc: 0.4490                                               \n",
      "Epoch 043 | Train Loss: 1.2293 Acc: 0.4472 | Val Loss: 1.2244 Acc: 0.4490                                               \n",
      "Epoch 044 | Train Loss: 1.2288 Acc: 0.4473 | Val Loss: 1.2243 Acc: 0.4490                                               \n",
      "Epoch 045 | Train Loss: 1.2298 Acc: 0.4470 | Val Loss: 1.2243 Acc: 0.4490                                               \n",
      "Epoch 046 | Train Loss: 1.2292 Acc: 0.4472 | Val Loss: 1.2243 Acc: 0.4490                                               \n",
      "Epoch 047 | Train Loss: 1.2288 Acc: 0.4473 | Val Loss: 1.2243 Acc: 0.4490                                               \n",
      "Epoch 048 | Train Loss: 1.2288 Acc: 0.4473 | Val Loss: 1.2243 Acc: 0.4490                                               \n",
      "Epoch 049 | Train Loss: 1.2287 Acc: 0.4474 | Val Loss: 1.2243 Acc: 0.4490                                               \n",
      "Epoch 050 | Train Loss: 1.2288 Acc: 0.4473 | Val Loss: 1.2243 Acc: 0.4490                                               \n",
      "Epoch 051 | Train Loss: 1.2287 Acc: 0.4474 | Val Loss: 1.2243 Acc: 0.4490                                               \n",
      "Epoch 052 | Train Loss: 1.2290 Acc: 0.4473 | Val Loss: 1.2243 Acc: 0.4490                                               \n",
      "Epoch 053 | Train Loss: 1.2292 Acc: 0.4472 | Val Loss: 1.2242 Acc: 0.4490                                               \n",
      "Epoch 054 | Train Loss: 1.2290 Acc: 0.4473 | Val Loss: 1.2242 Acc: 0.4490                                               \n",
      "Epoch 055 | Train Loss: 1.2290 Acc: 0.4473 | Val Loss: 1.2242 Acc: 0.4490                                               \n",
      "Epoch 056 | Train Loss: 1.2287 Acc: 0.4474 | Val Loss: 1.2242 Acc: 0.4490                                               \n",
      "Epoch 057 | Train Loss: 1.2290 Acc: 0.4473 | Val Loss: 1.2242 Acc: 0.4490                                               \n",
      "Epoch 058 | Train Loss: 1.2291 Acc: 0.4472 | Val Loss: 1.2242 Acc: 0.4490                                               \n",
      "Epoch 059 | Train Loss: 1.2291 Acc: 0.4472 | Val Loss: 1.2242 Acc: 0.4490                                               \n",
      "Epoch 060 | Train Loss: 1.2288 Acc: 0.4473 | Val Loss: 1.2242 Acc: 0.4490                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 256, 'cnn_dropout': 0.4944305896069884, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 64, 'learning_rate': 1.7399647388858742e-05, 'lstm_dense': 256, 'lstm_hidden_size': 32, 'lstm_layers': 2, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 334.8246 Acc: 0.2596 | Val Loss: 200.8994 Acc: 0.2743                                           \n",
      "Epoch 002 | Train Loss: 216.5826 Acc: 0.2559 | Val Loss: 100.6606 Acc: 0.2737                                           \n",
      "Epoch 003 | Train Loss: 130.7246 Acc: 0.2823 | Val Loss: 36.2564 Acc: 0.4027                                            \n",
      "Epoch 004 | Train Loss: 88.9816 Acc: 0.3096 | Val Loss: 41.7826 Acc: 0.3937                                             \n",
      "Epoch 005 | Train Loss: 76.7184 Acc: 0.3299 | Val Loss: 41.0036 Acc: 0.4284                                             \n",
      "Epoch 006 | Train Loss: 67.4097 Acc: 0.3408 | Val Loss: 36.0854 Acc: 0.4194                                             \n",
      "Epoch 007 | Train Loss: 59.2597 Acc: 0.3476 | Val Loss: 31.6561 Acc: 0.4257                                             \n",
      "Epoch 008 | Train Loss: 53.8174 Acc: 0.3529 | Val Loss: 26.9176 Acc: 0.4304                                             \n",
      "Epoch 009 | Train Loss: 48.9426 Acc: 0.3476 | Val Loss: 22.9730 Acc: 0.4182                                             \n",
      "Epoch 010 | Train Loss: 43.2729 Acc: 0.3545 | Val Loss: 19.1635 Acc: 0.3931                                             \n",
      "Epoch 011 | Train Loss: 39.3764 Acc: 0.3542 | Val Loss: 16.5458 Acc: 0.4146                                             \n",
      "Epoch 012 | Train Loss: 35.5084 Acc: 0.3527 | Val Loss: 14.2857 Acc: 0.4027                                             \n",
      "Epoch 013 | Train Loss: 32.8417 Acc: 0.3606 | Val Loss: 13.1253 Acc: 0.3749                                             \n",
      "Epoch 014 | Train Loss: 30.2042 Acc: 0.3654 | Val Loss: 11.3520 Acc: 0.3660                                             \n",
      "Epoch 015 | Train Loss: 28.5786 Acc: 0.3618 | Val Loss: 10.5853 Acc: 0.3782                                             \n",
      "Epoch 016 | Train Loss: 26.3245 Acc: 0.3668 | Val Loss: 9.6314 Acc: 0.3436                                              \n",
      "Epoch 017 | Train Loss: 25.2384 Acc: 0.3606 | Val Loss: 8.9189 Acc: 0.3719                                              \n",
      "Epoch 018 | Train Loss: 23.2780 Acc: 0.3688 | Val Loss: 8.2064 Acc: 0.3609                                              \n",
      "Epoch 019 | Train Loss: 22.0474 Acc: 0.3710 | Val Loss: 7.7336 Acc: 0.3737                                              \n",
      "Epoch 020 | Train Loss: 20.9302 Acc: 0.3685 | Val Loss: 7.2758 Acc: 0.3391                                              \n",
      "Epoch 021 | Train Loss: 19.9372 Acc: 0.3709 | Val Loss: 6.6093 Acc: 0.3773                                              \n",
      "Epoch 022 | Train Loss: 18.3644 Acc: 0.3803 | Val Loss: 6.2260 Acc: 0.3675                                              \n",
      "Epoch 023 | Train Loss: 18.0172 Acc: 0.3735 | Val Loss: 6.2021 Acc: 0.3510                                              \n",
      "Epoch 024 | Train Loss: 17.0035 Acc: 0.3759 | Val Loss: 5.1429 Acc: 0.3839                                              \n",
      "Epoch 025 | Train Loss: 16.3941 Acc: 0.3788 | Val Loss: 4.9806 Acc: 0.3782                                              \n",
      "Epoch 026 | Train Loss: 15.4712 Acc: 0.3794 | Val Loss: 5.1635 Acc: 0.3245                                              \n",
      "Epoch 027 | Train Loss: 14.8051 Acc: 0.3801 | Val Loss: 4.8250 Acc: 0.3842                                              \n",
      "Epoch 028 | Train Loss: 14.0247 Acc: 0.3867 | Val Loss: 4.2426 Acc: 0.3684                                              \n",
      "Epoch 029 | Train Loss: 13.3001 Acc: 0.3858 | Val Loss: 3.6976 Acc: 0.4275                                              \n",
      "Epoch 030 | Train Loss: 13.0562 Acc: 0.3888 | Val Loss: 3.9463 Acc: 0.3666                                              \n",
      "Epoch 031 | Train Loss: 12.5229 Acc: 0.3879 | Val Loss: 3.8948 Acc: 0.3907                                              \n",
      "Epoch 032 | Train Loss: 11.9030 Acc: 0.3895 | Val Loss: 3.7166 Acc: 0.3976                                              \n",
      "Epoch 033 | Train Loss: 11.4366 Acc: 0.3898 | Val Loss: 3.2799 Acc: 0.4307                                              \n",
      "Epoch 034 | Train Loss: 10.7545 Acc: 0.3945 | Val Loss: 3.1961 Acc: 0.4173                                              \n",
      "Epoch 035 | Train Loss: 10.4029 Acc: 0.3956 | Val Loss: 2.9868 Acc: 0.4558                                              \n",
      "Epoch 036 | Train Loss: 10.0568 Acc: 0.3957 | Val Loss: 2.9638 Acc: 0.4251                                              \n",
      "Epoch 037 | Train Loss: 9.7964 Acc: 0.3920 | Val Loss: 2.7782 Acc: 0.4179                                               \n",
      "Epoch 038 | Train Loss: 9.3026 Acc: 0.4010 | Val Loss: 2.7192 Acc: 0.4140                                               \n",
      "Epoch 039 | Train Loss: 9.1750 Acc: 0.3934 | Val Loss: 2.7991 Acc: 0.4645                                               \n",
      "Epoch 040 | Train Loss: 8.7666 Acc: 0.3934 | Val Loss: 2.4295 Acc: 0.4773                                               \n",
      "Epoch 041 | Train Loss: 8.3376 Acc: 0.4039 | Val Loss: 2.4986 Acc: 0.4403                                               \n",
      "Epoch 042 | Train Loss: 8.0727 Acc: 0.3975 | Val Loss: 2.2910 Acc: 0.4809                                               \n",
      "Epoch 043 | Train Loss: 7.9263 Acc: 0.3975 | Val Loss: 2.2641 Acc: 0.4919                                               \n",
      "Epoch 044 | Train Loss: 7.4680 Acc: 0.4032 | Val Loss: 2.1943 Acc: 0.5012                                               \n",
      "Epoch 045 | Train Loss: 7.1202 Acc: 0.4082 | Val Loss: 2.1724 Acc: 0.4570                                               \n",
      "Epoch 046 | Train Loss: 6.7426 Acc: 0.4095 | Val Loss: 2.1145 Acc: 0.4528                                               \n",
      "Epoch 047 | Train Loss: 6.6821 Acc: 0.4170 | Val Loss: 2.0036 Acc: 0.5322                                               \n",
      "Epoch 048 | Train Loss: 6.3816 Acc: 0.4128 | Val Loss: 1.9100 Acc: 0.5221                                               \n",
      "Epoch 049 | Train Loss: 6.1877 Acc: 0.4144 | Val Loss: 1.8871 Acc: 0.4869                                               \n",
      "Epoch 050 | Train Loss: 5.9363 Acc: 0.4101 | Val Loss: 1.8847 Acc: 0.4937                                               \n",
      "Epoch 051 | Train Loss: 5.9134 Acc: 0.4150 | Val Loss: 1.8285 Acc: 0.5018                                               \n",
      "Epoch 052 | Train Loss: 5.5438 Acc: 0.4181 | Val Loss: 1.7198 Acc: 0.5182                                               \n",
      "Epoch 053 | Train Loss: 5.4265 Acc: 0.4214 | Val Loss: 1.6865 Acc: 0.4833                                               \n",
      "Epoch 054 | Train Loss: 5.2888 Acc: 0.4222 | Val Loss: 1.6266 Acc: 0.5048                                               \n",
      "Epoch 055 | Train Loss: 5.0770 Acc: 0.4203 | Val Loss: 1.6553 Acc: 0.4743                                               \n",
      "Epoch 056 | Train Loss: 4.9347 Acc: 0.4231 | Val Loss: 1.6359 Acc: 0.4812                                               \n",
      "Epoch 057 | Train Loss: 4.7629 Acc: 0.4214 | Val Loss: 1.4720 Acc: 0.5206                                               \n",
      "Epoch 058 | Train Loss: 4.7025 Acc: 0.4273 | Val Loss: 1.5353 Acc: 0.5328                                               \n",
      "Epoch 059 | Train Loss: 4.5417 Acc: 0.4286 | Val Loss: 1.4519 Acc: 0.5830                                               \n",
      "Epoch 060 | Train Loss: 4.4060 Acc: 0.4350 | Val Loss: 1.6295 Acc: 0.4824                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 256, 'cnn_dropout': 0.511978505344807, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 16, 'cnn_kernels_2': 32, 'learning_rate': 1.0888650144559011e-05, 'lstm_dense': 32, 'lstm_hidden_size': 96, 'lstm_layers': 1, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 104.4189 Acc: 0.2791 | Val Loss: 55.0756 Acc: 0.4421                                            \n",
      "Epoch 002 | Train Loss: 76.2900 Acc: 0.3158 | Val Loss: 48.6815 Acc: 0.4421                                             \n",
      "Epoch 003 | Train Loss: 63.5838 Acc: 0.3302 | Val Loss: 40.1307 Acc: 0.4364                                             \n",
      "Epoch 004 | Train Loss: 53.9393 Acc: 0.3317 | Val Loss: 32.8080 Acc: 0.4364                                             \n",
      "Epoch 005 | Train Loss: 47.8427 Acc: 0.3445 | Val Loss: 29.5241 Acc: 0.4430                                             \n",
      "Epoch 006 | Train Loss: 42.6863 Acc: 0.3421 | Val Loss: 25.9890 Acc: 0.4376                                             \n",
      "Epoch 007 | Train Loss: 38.3100 Acc: 0.3476 | Val Loss: 24.7704 Acc: 0.4376                                             \n",
      "Epoch 008 | Train Loss: 32.9627 Acc: 0.3553 | Val Loss: 22.4981 Acc: 0.4376                                             \n",
      "Epoch 009 | Train Loss: 30.3116 Acc: 0.3526 | Val Loss: 20.8507 Acc: 0.4376                                             \n",
      "Epoch 010 | Train Loss: 26.9930 Acc: 0.3548 | Val Loss: 18.7565 Acc: 0.4313                                             \n",
      "Epoch 011 | Train Loss: 24.3120 Acc: 0.3575 | Val Loss: 16.9099 Acc: 0.4313                                             \n",
      "Epoch 012 | Train Loss: 22.0774 Acc: 0.3583 | Val Loss: 14.6166 Acc: 0.4376                                             \n",
      "Epoch 013 | Train Loss: 20.0111 Acc: 0.3717 | Val Loss: 12.6646 Acc: 0.4376                                             \n",
      "Epoch 014 | Train Loss: 17.9909 Acc: 0.3685 | Val Loss: 11.9652 Acc: 0.4376                                             \n",
      "Epoch 015 | Train Loss: 16.8931 Acc: 0.3765 | Val Loss: 10.4622 Acc: 0.4376                                             \n",
      "Epoch 016 | Train Loss: 15.3858 Acc: 0.3835 | Val Loss: 9.2264 Acc: 0.4355                                              \n",
      "Epoch 017 | Train Loss: 14.3839 Acc: 0.3874 | Val Loss: 8.4534 Acc: 0.4507                                              \n",
      "Epoch 018 | Train Loss: 13.0229 Acc: 0.3870 | Val Loss: 7.8876 Acc: 0.4355                                              \n",
      "Epoch 019 | Train Loss: 12.1693 Acc: 0.3884 | Val Loss: 6.3754 Acc: 0.4355                                              \n",
      "Epoch 020 | Train Loss: 11.4083 Acc: 0.3968 | Val Loss: 5.7853 Acc: 0.4325                                              \n",
      "Epoch 021 | Train Loss: 10.5393 Acc: 0.4035 | Val Loss: 4.7878 Acc: 0.4415                                              \n",
      "Epoch 022 | Train Loss: 9.8017 Acc: 0.4062 | Val Loss: 4.5591 Acc: 0.4415                                               \n",
      "Epoch 023 | Train Loss: 9.3257 Acc: 0.4161 | Val Loss: 4.6299 Acc: 0.4513                                               \n",
      "Epoch 024 | Train Loss: 8.8839 Acc: 0.4200 | Val Loss: 4.1621 Acc: 0.4534                                               \n",
      "Epoch 025 | Train Loss: 8.4729 Acc: 0.4191 | Val Loss: 3.8747 Acc: 0.4522                                               \n",
      "Epoch 026 | Train Loss: 8.1959 Acc: 0.4218 | Val Loss: 3.2143 Acc: 0.4597                                               \n",
      "Epoch 027 | Train Loss: 7.6317 Acc: 0.4232 | Val Loss: 3.0076 Acc: 0.4651                                               \n",
      "Epoch 028 | Train Loss: 7.3831 Acc: 0.4264 | Val Loss: 2.9165 Acc: 0.4573                                               \n",
      "Epoch 029 | Train Loss: 6.9388 Acc: 0.4397 | Val Loss: 3.2120 Acc: 0.4558                                               \n",
      "Epoch 030 | Train Loss: 6.6808 Acc: 0.4327 | Val Loss: 2.7157 Acc: 0.4827                                               \n",
      "Epoch 031 | Train Loss: 6.5804 Acc: 0.4299 | Val Loss: 2.3393 Acc: 0.5149                                               \n",
      "Epoch 032 | Train Loss: 6.2174 Acc: 0.4332 | Val Loss: 2.5239 Acc: 0.4645                                               \n",
      "Epoch 033 | Train Loss: 6.0840 Acc: 0.4371 | Val Loss: 2.2264 Acc: 0.5322                                               \n",
      "Epoch 034 | Train Loss: 6.0949 Acc: 0.4317 | Val Loss: 2.4840 Acc: 0.4687                                               \n",
      "Epoch 035 | Train Loss: 5.6818 Acc: 0.4464 | Val Loss: 2.0413 Acc: 0.5319                                               \n",
      "Epoch 036 | Train Loss: 5.4380 Acc: 0.4452 | Val Loss: 2.1692 Acc: 0.4958                                               \n",
      "Epoch 037 | Train Loss: 5.1385 Acc: 0.4543 | Val Loss: 1.9498 Acc: 0.5006                                               \n",
      "Epoch 038 | Train Loss: 4.9981 Acc: 0.4565 | Val Loss: 1.7357 Acc: 0.4791                                               \n",
      "Epoch 039 | Train Loss: 4.9734 Acc: 0.4545 | Val Loss: 1.9909 Acc: 0.5000                                               \n",
      "Epoch 040 | Train Loss: 4.8451 Acc: 0.4558 | Val Loss: 1.8399 Acc: 0.5009                                               \n",
      "Epoch 041 | Train Loss: 4.7936 Acc: 0.4513 | Val Loss: 1.8012 Acc: 0.5328                                               \n",
      "Epoch 042 | Train Loss: 4.4932 Acc: 0.4591 | Val Loss: 1.6050 Acc: 0.5033                                               \n",
      "Epoch 043 | Train Loss: 4.4641 Acc: 0.4632 | Val Loss: 1.7872 Acc: 0.5203                                               \n",
      "Epoch 044 | Train Loss: 4.2479 Acc: 0.4637 | Val Loss: 1.5979 Acc: 0.4860                                               \n",
      "Epoch 045 | Train Loss: 4.2100 Acc: 0.4680 | Val Loss: 1.5512 Acc: 0.5301                                               \n",
      "Epoch 046 | Train Loss: 4.0093 Acc: 0.4729 | Val Loss: 1.4776 Acc: 0.5472                                               \n",
      "Epoch 047 | Train Loss: 3.9415 Acc: 0.4708 | Val Loss: 1.4360 Acc: 0.5313                                               \n",
      "Epoch 048 | Train Loss: 3.9009 Acc: 0.4675 | Val Loss: 1.3769 Acc: 0.5382                                               \n",
      "Epoch 049 | Train Loss: 3.7463 Acc: 0.4829 | Val Loss: 1.3319 Acc: 0.5687                                               \n",
      "Epoch 050 | Train Loss: 3.6211 Acc: 0.4708 | Val Loss: 1.3278 Acc: 0.5591                                               \n",
      "Epoch 051 | Train Loss: 3.5448 Acc: 0.4740 | Val Loss: 1.3135 Acc: 0.5319                                               \n",
      "Epoch 052 | Train Loss: 3.4672 Acc: 0.4789 | Val Loss: 1.2322 Acc: 0.5433                                               \n",
      "Epoch 053 | Train Loss: 3.4377 Acc: 0.4812 | Val Loss: 1.2579 Acc: 0.5439                                               \n",
      "Epoch 054 | Train Loss: 3.2291 Acc: 0.4862 | Val Loss: 1.2573 Acc: 0.5448                                               \n",
      "Epoch 055 | Train Loss: 3.2285 Acc: 0.4848 | Val Loss: 1.1480 Acc: 0.5630                                               \n",
      "Epoch 056 | Train Loss: 3.0868 Acc: 0.4950 | Val Loss: 1.1391 Acc: 0.5618                                               \n",
      "Epoch 057 | Train Loss: 3.0773 Acc: 0.4958 | Val Loss: 1.1280 Acc: 0.5570                                               \n",
      "Epoch 058 | Train Loss: 3.0068 Acc: 0.4990 | Val Loss: 1.1111 Acc: 0.5609                                               \n",
      "Epoch 059 | Train Loss: 2.8319 Acc: 0.5059 | Val Loss: 1.0719 Acc: 0.5788                                               \n",
      "Epoch 060 | Train Loss: 2.8086 Acc: 0.5032 | Val Loss: 1.0195 Acc: 0.5707                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 64, 'cnn_dropout': 0.3888240868239543, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 48, 'cnn_kernels_2': 32, 'learning_rate': 1.5554247957322942e-05, 'lstm_dense': 64, 'lstm_hidden_size': 64, 'lstm_layers': 6, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 61.8795 Acc: 0.2980 | Val Loss: 24.8045 Acc: 0.3725                                             \n",
      "Epoch 002 | Train Loss: 36.5349 Acc: 0.3058 | Val Loss: 10.2565 Acc: 0.3815                                             \n",
      "Epoch 003 | Train Loss: 27.1440 Acc: 0.3215 | Val Loss: 7.6430 Acc: 0.3203                                              \n",
      "Epoch 004 | Train Loss: 21.2331 Acc: 0.3520 | Val Loss: 6.2972 Acc: 0.2973                                              \n",
      "Epoch 005 | Train Loss: 17.3335 Acc: 0.3538 | Val Loss: 5.2326 Acc: 0.2809                                              \n",
      "Epoch 006 | Train Loss: 13.7272 Acc: 0.3693 | Val Loss: 4.0684 Acc: 0.3030                                              \n",
      "Epoch 007 | Train Loss: 11.5967 Acc: 0.3821 | Val Loss: 3.0962 Acc: 0.3687                                              \n",
      "Epoch 008 | Train Loss: 9.8914 Acc: 0.3973 | Val Loss: 2.7856 Acc: 0.3925                                               \n",
      "Epoch 009 | Train Loss: 8.6618 Acc: 0.3999 | Val Loss: 2.7059 Acc: 0.4230                                               \n",
      "Epoch 010 | Train Loss: 7.8850 Acc: 0.3902 | Val Loss: 2.5970 Acc: 0.4212                                               \n",
      "Epoch 011 | Train Loss: 7.0344 Acc: 0.4010 | Val Loss: 2.3973 Acc: 0.4636                                               \n",
      "Epoch 012 | Train Loss: 6.5848 Acc: 0.4061 | Val Loss: 2.4580 Acc: 0.4499                                               \n",
      "Epoch 013 | Train Loss: 6.0101 Acc: 0.4161 | Val Loss: 2.1437 Acc: 0.4791                                               \n",
      "Epoch 014 | Train Loss: 5.4517 Acc: 0.4194 | Val Loss: 1.9562 Acc: 0.4955                                               \n",
      "Epoch 015 | Train Loss: 4.9851 Acc: 0.4223 | Val Loss: 1.7512 Acc: 0.5203                                               \n",
      "Epoch 016 | Train Loss: 4.6528 Acc: 0.4322 | Val Loss: 1.5579 Acc: 0.5101                                               \n",
      "Epoch 017 | Train Loss: 4.3041 Acc: 0.4353 | Val Loss: 1.4331 Acc: 0.5101                                               \n",
      "Epoch 018 | Train Loss: 3.8921 Acc: 0.4492 | Val Loss: 1.3688 Acc: 0.5254                                               \n",
      "Epoch 019 | Train Loss: 3.7402 Acc: 0.4567 | Val Loss: 1.3861 Acc: 0.5445                                               \n",
      "Epoch 020 | Train Loss: 3.5520 Acc: 0.4551 | Val Loss: 1.3188 Acc: 0.5451                                               \n",
      "Epoch 021 | Train Loss: 3.2950 Acc: 0.4587 | Val Loss: 1.2432 Acc: 0.5704                                               \n",
      "Epoch 022 | Train Loss: 3.0535 Acc: 0.4715 | Val Loss: 1.3103 Acc: 0.5618                                               \n",
      "Epoch 023 | Train Loss: 2.8870 Acc: 0.4762 | Val Loss: 1.2914 Acc: 0.5537                                               \n",
      "Epoch 024 | Train Loss: 2.7030 Acc: 0.4873 | Val Loss: 1.2462 Acc: 0.5693                                               \n",
      "Epoch 025 | Train Loss: 2.6037 Acc: 0.4983 | Val Loss: 1.0727 Acc: 0.5869                                               \n",
      "Epoch 026 | Train Loss: 2.4642 Acc: 0.4994 | Val Loss: 1.0475 Acc: 0.6104                                               \n",
      "Epoch 027 | Train Loss: 2.3177 Acc: 0.5149 | Val Loss: 1.0268 Acc: 0.6200                                               \n",
      "Epoch 028 | Train Loss: 2.1902 Acc: 0.5206 | Val Loss: 0.9994 Acc: 0.6113                                               \n",
      "Epoch 029 | Train Loss: 2.1060 Acc: 0.5262 | Val Loss: 1.0417 Acc: 0.6269                                               \n",
      "Epoch 030 | Train Loss: 2.0237 Acc: 0.5353 | Val Loss: 1.0836 Acc: 0.6224                                               \n",
      "Epoch 031 | Train Loss: 1.9324 Acc: 0.5421 | Val Loss: 1.0790 Acc: 0.6200                                               \n",
      "Epoch 032 | Train Loss: 1.9405 Acc: 0.5428 | Val Loss: 0.8735 Acc: 0.6666                                               \n",
      "Epoch 033 | Train Loss: 1.8460 Acc: 0.5485 | Val Loss: 1.1272 Acc: 0.6134                                               \n",
      "Epoch 034 | Train Loss: 1.7928 Acc: 0.5629 | Val Loss: 1.1471 Acc: 0.6218                                               \n",
      "Epoch 035 | Train Loss: 1.6952 Acc: 0.5588 | Val Loss: 0.8480 Acc: 0.6693                                               \n",
      "Epoch 036 | Train Loss: 1.6370 Acc: 0.5711 | Val Loss: 1.0618 Acc: 0.6325                                               \n",
      "Epoch 037 | Train Loss: 1.6338 Acc: 0.5734 | Val Loss: 0.9028 Acc: 0.6510                                               \n",
      "Epoch 038 | Train Loss: 1.5380 Acc: 0.5841 | Val Loss: 0.9739 Acc: 0.6549                                               \n",
      "Epoch 039 | Train Loss: 1.4844 Acc: 0.5897 | Val Loss: 0.9518 Acc: 0.6722                                               \n",
      "Epoch 040 | Train Loss: 1.4606 Acc: 0.5948 | Val Loss: 1.0724 Acc: 0.6409                                               \n",
      "Epoch 041 | Train Loss: 1.3592 Acc: 0.6055 | Val Loss: 0.9405 Acc: 0.6567                                               \n",
      "Epoch 042 | Train Loss: 1.3541 Acc: 0.6060 | Val Loss: 0.8398 Acc: 0.6606                                               \n",
      "Epoch 043 | Train Loss: 1.3514 Acc: 0.6088 | Val Loss: 0.7927 Acc: 0.6558                                               \n",
      "Epoch 044 | Train Loss: 1.2987 Acc: 0.6176 | Val Loss: 0.8342 Acc: 0.6940                                               \n",
      "Epoch 045 | Train Loss: 1.3135 Acc: 0.6148 | Val Loss: 0.7457 Acc: 0.7003                                               \n",
      "Epoch 046 | Train Loss: 1.2336 Acc: 0.6219 | Val Loss: 0.7242 Acc: 0.6522                                               \n",
      "Epoch 047 | Train Loss: 1.1884 Acc: 0.6284 | Val Loss: 0.8662 Acc: 0.6785                                               \n",
      "Epoch 048 | Train Loss: 1.2112 Acc: 0.6277 | Val Loss: 0.8729 Acc: 0.6863                                               \n",
      "Epoch 049 | Train Loss: 1.1784 Acc: 0.6383 | Val Loss: 0.7222 Acc: 0.6910                                               \n",
      "Epoch 050 | Train Loss: 1.0994 Acc: 0.6426 | Val Loss: 0.7586 Acc: 0.6654                                               \n",
      "Epoch 051 | Train Loss: 1.0971 Acc: 0.6426 | Val Loss: 0.7093 Acc: 0.7185                                               \n",
      "Epoch 052 | Train Loss: 1.0929 Acc: 0.6380 | Val Loss: 0.6962 Acc: 0.7272                                               \n",
      "Epoch 053 | Train Loss: 1.0575 Acc: 0.6478 | Val Loss: 0.8517 Acc: 0.7054                                               \n",
      "Epoch 054 | Train Loss: 1.0289 Acc: 0.6536 | Val Loss: 0.6744 Acc: 0.6893                                               \n",
      "Epoch 055 | Train Loss: 0.9992 Acc: 0.6621 | Val Loss: 0.7040 Acc: 0.7221                                               \n",
      "Epoch 056 | Train Loss: 1.0037 Acc: 0.6579 | Val Loss: 0.6917 Acc: 0.7024                                               \n",
      "Epoch 057 | Train Loss: 0.9723 Acc: 0.6626 | Val Loss: 0.5900 Acc: 0.7376                                               \n",
      "Epoch 058 | Train Loss: 0.9698 Acc: 0.6668 | Val Loss: 0.9171 Acc: 0.6937                                               \n",
      "Epoch 059 | Train Loss: 0.9119 Acc: 0.6744 | Val Loss: 0.6814 Acc: 0.7200                                               \n",
      "Epoch 060 | Train Loss: 0.9463 Acc: 0.6710 | Val Loss: 0.6369 Acc: 0.7137                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 32, 'cnn_dropout': 0.41814653842266153, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 96, 'learning_rate': 7.455847961205369e-05, 'lstm_dense': 256, 'lstm_hidden_size': 128, 'lstm_layers': 3, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 28.7072 Acc: 0.3558 | Val Loss: 6.8512 Acc: 0.4633                                              \n",
      "Epoch 002 | Train Loss: 13.2451 Acc: 0.4029 | Val Loss: 4.3467 Acc: 0.5439                                              \n",
      "Epoch 003 | Train Loss: 9.2285 Acc: 0.4327 | Val Loss: 4.2919 Acc: 0.5534                                               \n",
      "Epoch 004 | Train Loss: 6.9470 Acc: 0.4668 | Val Loss: 3.0690 Acc: 0.5797                                               \n",
      "Epoch 005 | Train Loss: 5.6268 Acc: 0.4850 | Val Loss: 2.9521 Acc: 0.5687                                               \n",
      "Epoch 006 | Train Loss: 4.7474 Acc: 0.5116 | Val Loss: 1.7900 Acc: 0.6164                                               \n",
      "Epoch 007 | Train Loss: 4.0557 Acc: 0.5282 | Val Loss: 1.9768 Acc: 0.6233                                               \n",
      "Epoch 008 | Train Loss: 3.5235 Acc: 0.5365 | Val Loss: 1.9319 Acc: 0.6385                                               \n",
      "Epoch 009 | Train Loss: 3.0987 Acc: 0.5680 | Val Loss: 1.3439 Acc: 0.6734                                               \n",
      "Epoch 010 | Train Loss: 2.7493 Acc: 0.5865 | Val Loss: 1.1533 Acc: 0.7146                                               \n",
      "Epoch 011 | Train Loss: 2.4661 Acc: 0.5944 | Val Loss: 1.2733 Acc: 0.7036                                               \n",
      "Epoch 012 | Train Loss: 2.2080 Acc: 0.6060 | Val Loss: 1.1522 Acc: 0.7155                                               \n",
      "Epoch 013 | Train Loss: 1.9601 Acc: 0.6288 | Val Loss: 1.0733 Acc: 0.7164                                               \n",
      "Epoch 014 | Train Loss: 1.8602 Acc: 0.6333 | Val Loss: 0.9578 Acc: 0.7412                                               \n",
      "Epoch 015 | Train Loss: 1.6607 Acc: 0.6470 | Val Loss: 1.1680 Acc: 0.6818                                               \n",
      "Epoch 016 | Train Loss: 1.5430 Acc: 0.6574 | Val Loss: 0.7987 Acc: 0.7125                                               \n",
      "Epoch 017 | Train Loss: 1.4333 Acc: 0.6689 | Val Loss: 0.7307 Acc: 0.7701                                               \n",
      "Epoch 018 | Train Loss: 1.3627 Acc: 0.6803 | Val Loss: 1.0344 Acc: 0.7185                                               \n",
      "Epoch 019 | Train Loss: 1.2547 Acc: 0.6984 | Val Loss: 0.7202 Acc: 0.7513                                               \n",
      "Epoch 020 | Train Loss: 1.1645 Acc: 0.7026 | Val Loss: 0.9151 Acc: 0.7543                                               \n",
      "Epoch 021 | Train Loss: 1.0975 Acc: 0.7199 | Val Loss: 0.6802 Acc: 0.7528                                               \n",
      "Epoch 022 | Train Loss: 0.9707 Acc: 0.7378 | Val Loss: 0.9610 Acc: 0.7290                                               \n",
      "Epoch 023 | Train Loss: 0.9060 Acc: 0.7573 | Val Loss: 0.5760 Acc: 0.8027                                               \n",
      "Epoch 024 | Train Loss: 0.8446 Acc: 0.7625 | Val Loss: 0.4219 Acc: 0.8454                                               \n",
      "Epoch 025 | Train Loss: 0.7821 Acc: 0.7783 | Val Loss: 0.5742 Acc: 0.7994                                               \n",
      "Epoch 026 | Train Loss: 0.7272 Acc: 0.7887 | Val Loss: 0.3216 Acc: 0.8806                                               \n",
      "Epoch 027 | Train Loss: 0.6983 Acc: 0.8004 | Val Loss: 0.5302 Acc: 0.8230                                               \n",
      "Epoch 028 | Train Loss: 0.5883 Acc: 0.8234 | Val Loss: 0.3809 Acc: 0.8609                                               \n",
      "Epoch 029 | Train Loss: 0.5937 Acc: 0.8283 | Val Loss: 0.4896 Acc: 0.8370                                               \n",
      "Epoch 030 | Train Loss: 0.5660 Acc: 0.8326 | Val Loss: 0.4619 Acc: 0.8451                                               \n",
      "Epoch 031 | Train Loss: 0.4946 Acc: 0.8489 | Val Loss: 0.4847 Acc: 0.8433                                               \n",
      "Epoch 032 | Train Loss: 0.4617 Acc: 0.8563 | Val Loss: 0.3926 Acc: 0.8618                                               \n",
      "Epoch 033 | Train Loss: 0.4410 Acc: 0.8671 | Val Loss: 0.3127 Acc: 0.8955                                               \n",
      "Epoch 034 | Train Loss: 0.3951 Acc: 0.8775 | Val Loss: 0.4546 Acc: 0.8531                                               \n",
      "Epoch 035 | Train Loss: 0.3870 Acc: 0.8820 | Val Loss: 0.2925 Acc: 0.8884                                               \n",
      "Epoch 036 | Train Loss: 0.3529 Acc: 0.8944 | Val Loss: 0.2189 Acc: 0.9236                                               \n",
      "Epoch 037 | Train Loss: 0.3545 Acc: 0.8879 | Val Loss: 0.2536 Acc: 0.9000                                               \n",
      "Epoch 038 | Train Loss: 0.3185 Acc: 0.9013 | Val Loss: 0.3262 Acc: 0.8901                                               \n",
      "Epoch 039 | Train Loss: 0.2993 Acc: 0.9029 | Val Loss: 0.3722 Acc: 0.8812                                               \n",
      "Epoch 040 | Train Loss: 0.2886 Acc: 0.9100 | Val Loss: 0.2314 Acc: 0.9167                                               \n",
      "Epoch 041 | Train Loss: 0.2698 Acc: 0.9163 | Val Loss: 0.4090 Acc: 0.8684                                               \n",
      "Epoch 042 | Train Loss: 0.2528 Acc: 0.9187 | Val Loss: 0.2082 Acc: 0.9296                                               \n",
      "Epoch 043 | Train Loss: 0.2460 Acc: 0.9240 | Val Loss: 0.2117 Acc: 0.9233                                               \n",
      "Epoch 044 | Train Loss: 0.2534 Acc: 0.9225 | Val Loss: 0.2023 Acc: 0.9260                                               \n",
      "Epoch 045 | Train Loss: 0.2158 Acc: 0.9308 | Val Loss: 0.1618 Acc: 0.9430                                               \n",
      "Epoch 046 | Train Loss: 0.2139 Acc: 0.9337 | Val Loss: 0.2776 Acc: 0.9200                                               \n",
      "Epoch 047 | Train Loss: 0.2154 Acc: 0.9323 | Val Loss: 0.1695 Acc: 0.9424                                               \n",
      "Epoch 048 | Train Loss: 0.2156 Acc: 0.9361 | Val Loss: 0.1887 Acc: 0.9382                                               \n",
      "Epoch 049 | Train Loss: 0.1825 Acc: 0.9394 | Val Loss: 0.2312 Acc: 0.9287                                               \n",
      "Epoch 050 | Train Loss: 0.1853 Acc: 0.9428 | Val Loss: 0.1228 Acc: 0.9540                                               \n",
      "Epoch 051 | Train Loss: 0.1825 Acc: 0.9439 | Val Loss: 0.1813 Acc: 0.9424                                               \n",
      "Epoch 052 | Train Loss: 0.1657 Acc: 0.9468 | Val Loss: 0.2855 Acc: 0.9000                                               \n",
      "Epoch 053 | Train Loss: 0.1555 Acc: 0.9519 | Val Loss: 0.1353 Acc: 0.9555                                               \n",
      "Epoch 054 | Train Loss: 0.1566 Acc: 0.9500 | Val Loss: 0.2726 Acc: 0.9194                                               \n",
      "Epoch 055 | Train Loss: 0.1619 Acc: 0.9492 | Val Loss: 0.1807 Acc: 0.9406                                               \n",
      "Epoch 056 | Train Loss: 0.1560 Acc: 0.9519 | Val Loss: 0.1157 Acc: 0.9606                                               \n",
      "Epoch 057 | Train Loss: 0.1448 Acc: 0.9558 | Val Loss: 0.1527 Acc: 0.9496                                               \n",
      "Epoch 058 | Train Loss: 0.1414 Acc: 0.9568 | Val Loss: 0.1088 Acc: 0.9657                                               \n",
      "Epoch 059 | Train Loss: 0.1399 Acc: 0.9564 | Val Loss: 0.1295 Acc: 0.9576                                               \n",
      "Epoch 060 | Train Loss: 0.1290 Acc: 0.9601 | Val Loss: 0.1389 Acc: 0.9582                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 36, 'cnn_dense': 32, 'cnn_dropout': 0.40201838893534486, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 16, 'learning_rate': 0.00011097694404491729, 'lstm_dense': 32, 'lstm_hidden_size': 96, 'lstm_layers': 2, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 4.2596 Acc: 0.4347 | Val Loss: 1.1508 Acc: 0.4528                                               \n",
      "Epoch 002 | Train Loss: 1.1119 Acc: 0.4586 | Val Loss: 0.9699 Acc: 0.5406                                               \n",
      "Epoch 003 | Train Loss: 1.0757 Acc: 0.4800 | Val Loss: 0.9619 Acc: 0.5128                                               \n",
      "Epoch 004 | Train Loss: 1.0194 Acc: 0.4835 | Val Loss: 1.0369 Acc: 0.5140                                               \n",
      "Epoch 005 | Train Loss: 0.9982 Acc: 0.4957 | Val Loss: 0.8862 Acc: 0.5481                                               \n",
      "Epoch 006 | Train Loss: 0.9646 Acc: 0.5147 | Val Loss: 0.9012 Acc: 0.5301                                               \n",
      "Epoch 007 | Train Loss: 0.9657 Acc: 0.5259 | Val Loss: 0.8446 Acc: 0.5406                                               \n",
      "Epoch 008 | Train Loss: 0.9548 Acc: 0.5238 | Val Loss: 0.8378 Acc: 0.5961                                               \n",
      "Epoch 009 | Train Loss: 0.9388 Acc: 0.5265 | Val Loss: 0.8282 Acc: 0.6027                                               \n",
      "Epoch 010 | Train Loss: 0.9442 Acc: 0.5300 | Val Loss: 0.8471 Acc: 0.5743                                               \n",
      "Epoch 011 | Train Loss: 0.9345 Acc: 0.5377 | Val Loss: 0.8692 Acc: 0.5319                                               \n",
      "Epoch 012 | Train Loss: 0.9388 Acc: 0.5412 | Val Loss: 0.8269 Acc: 0.5693                                               \n",
      "Epoch 013 | Train Loss: 0.9323 Acc: 0.5376 | Val Loss: 0.8309 Acc: 0.5985                                               \n",
      "Epoch 014 | Train Loss: 0.9212 Acc: 0.5421 | Val Loss: 0.8077 Acc: 0.5854                                               \n",
      "Epoch 015 | Train Loss: 0.9260 Acc: 0.5344 | Val Loss: 0.8202 Acc: 0.6236                                               \n",
      "Epoch 016 | Train Loss: 0.9148 Acc: 0.5358 | Val Loss: 0.8174 Acc: 0.6340                                               \n",
      "Epoch 017 | Train Loss: 0.9197 Acc: 0.5400 | Val Loss: 0.8198 Acc: 0.5851                                               \n",
      "Epoch 018 | Train Loss: 0.8989 Acc: 0.5539 | Val Loss: 0.8508 Acc: 0.6263                                               \n",
      "Epoch 019 | Train Loss: 0.8956 Acc: 0.5534 | Val Loss: 0.7715 Acc: 0.6218                                               \n",
      "Epoch 020 | Train Loss: 0.8916 Acc: 0.5452 | Val Loss: 0.7985 Acc: 0.6087                                               \n",
      "Epoch 021 | Train Loss: 0.9059 Acc: 0.5465 | Val Loss: 0.7917 Acc: 0.6203                                               \n",
      "Epoch 022 | Train Loss: 0.9047 Acc: 0.5506 | Val Loss: 0.7961 Acc: 0.6087                                               \n",
      "Epoch 023 | Train Loss: 0.8986 Acc: 0.5470 | Val Loss: 0.7793 Acc: 0.6460                                               \n",
      "Epoch 024 | Train Loss: 0.8765 Acc: 0.5614 | Val Loss: 0.7808 Acc: 0.6931                                               \n",
      "Epoch 025 | Train Loss: 0.8672 Acc: 0.5713 | Val Loss: 0.8239 Acc: 0.6767                                               \n",
      "Epoch 026 | Train Loss: 0.9332 Acc: 0.5241 | Val Loss: 0.8595 Acc: 0.6110                                               \n",
      "Epoch 027 | Train Loss: 0.8982 Acc: 0.5494 | Val Loss: 0.7839 Acc: 0.6125                                               \n",
      "Epoch 028 | Train Loss: 0.8950 Acc: 0.5568 | Val Loss: 0.8208 Acc: 0.5901                                               \n",
      "Epoch 029 | Train Loss: 0.8874 Acc: 0.5673 | Val Loss: 0.7652 Acc: 0.6340                                               \n",
      "Epoch 030 | Train Loss: 0.8779 Acc: 0.5621 | Val Loss: 0.7362 Acc: 0.6391                                               \n",
      "Epoch 031 | Train Loss: 0.8797 Acc: 0.5613 | Val Loss: 0.7651 Acc: 0.6275                                               \n",
      "Epoch 032 | Train Loss: 0.8674 Acc: 0.5696 | Val Loss: 0.7644 Acc: 0.6839                                               \n",
      "Epoch 033 | Train Loss: 0.8743 Acc: 0.5601 | Val Loss: 0.7814 Acc: 0.6442                                               \n",
      "Epoch 034 | Train Loss: 0.8768 Acc: 0.5627 | Val Loss: 0.7601 Acc: 0.6325                                               \n",
      "Epoch 035 | Train Loss: 0.8647 Acc: 0.5738 | Val Loss: 0.7982 Acc: 0.5860                                               \n",
      "Epoch 036 | Train Loss: 0.8678 Acc: 0.5643 | Val Loss: 0.7378 Acc: 0.6585                                               \n",
      "Epoch 037 | Train Loss: 0.8704 Acc: 0.5607 | Val Loss: 0.7597 Acc: 0.6364                                               \n",
      "Epoch 038 | Train Loss: 0.8778 Acc: 0.5547 | Val Loss: 0.7632 Acc: 0.6278                                               \n",
      "Epoch 039 | Train Loss: 0.8667 Acc: 0.5644 | Val Loss: 0.7417 Acc: 0.6693                                               \n",
      "Epoch 040 | Train Loss: 0.8577 Acc: 0.5655 | Val Loss: 0.7361 Acc: 0.6546                                               \n",
      "Epoch 041 | Train Loss: 0.8460 Acc: 0.5730 | Val Loss: 0.7406 Acc: 0.6478                                               \n",
      "Epoch 042 | Train Loss: 0.8589 Acc: 0.5721 | Val Loss: 0.7375 Acc: 0.6281                                               \n",
      "Epoch 043 | Train Loss: 0.8656 Acc: 0.5663 | Val Loss: 0.7485 Acc: 0.6696                                               \n",
      "Epoch 044 | Train Loss: 0.8487 Acc: 0.5771 | Val Loss: 0.7140 Acc: 0.6788                                               \n",
      "Epoch 045 | Train Loss: 0.8540 Acc: 0.5704 | Val Loss: 0.7396 Acc: 0.6558                                               \n",
      "Epoch 046 | Train Loss: 0.8480 Acc: 0.5730 | Val Loss: 0.7438 Acc: 0.6382                                               \n",
      "Epoch 047 | Train Loss: 0.8529 Acc: 0.5730 | Val Loss: 0.8023 Acc: 0.6501                                               \n",
      "Epoch 048 | Train Loss: 0.9157 Acc: 0.5394 | Val Loss: 0.8909 Acc: 0.5785                                               \n",
      "Epoch 049 | Train Loss: 0.9133 Acc: 0.5451 | Val Loss: 0.7809 Acc: 0.6451                                               \n",
      "Epoch 050 | Train Loss: 0.9215 Acc: 0.5341 | Val Loss: 0.7836 Acc: 0.6293                                               \n",
      "Epoch 051 | Train Loss: 0.8955 Acc: 0.5432 | Val Loss: 0.7562 Acc: 0.6576                                               \n",
      "Epoch 052 | Train Loss: 0.9050 Acc: 0.5362 | Val Loss: 0.7964 Acc: 0.6475                                               \n",
      "Epoch 053 | Train Loss: 0.8830 Acc: 0.5572 | Val Loss: 0.7564 Acc: 0.6594                                               \n",
      "Epoch 054 | Train Loss: 0.8796 Acc: 0.5551 | Val Loss: 0.8380 Acc: 0.5785                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 128, 'cnn_dropout': 0.2915089059687573, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 16, 'learning_rate': 0.0008735788909485802, 'lstm_dense': 64, 'lstm_hidden_size': 32, 'lstm_layers': 6, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 8.0813 Acc: 0.4285 | Val Loss: 3.7315 Acc: 0.5084                                               \n",
      "Epoch 002 | Train Loss: 1.2771 Acc: 0.5507 | Val Loss: 0.9203 Acc: 0.5839                                               \n",
      "Epoch 003 | Train Loss: 0.8455 Acc: 0.6210 | Val Loss: 1.5522 Acc: 0.5442                                               \n",
      "Epoch 004 | Train Loss: 0.7748 Acc: 0.6498 | Val Loss: 0.6996 Acc: 0.6890                                               \n",
      "Epoch 005 | Train Loss: 0.6948 Acc: 0.6753 | Val Loss: 0.5142 Acc: 0.7928                                               \n",
      "Epoch 006 | Train Loss: 0.6305 Acc: 0.7118 | Val Loss: 0.5530 Acc: 0.7687                                               \n",
      "Epoch 007 | Train Loss: 0.6061 Acc: 0.7322 | Val Loss: 0.5739 Acc: 0.7343                                               \n",
      "Epoch 008 | Train Loss: 0.5729 Acc: 0.7521 | Val Loss: 0.4960 Acc: 0.7922                                               \n",
      "Epoch 009 | Train Loss: 0.5541 Acc: 0.7563 | Val Loss: 0.5050 Acc: 0.7818                                               \n",
      "Epoch 010 | Train Loss: 0.5366 Acc: 0.7666 | Val Loss: 0.5476 Acc: 0.7690                                               \n",
      "Epoch 011 | Train Loss: 0.5116 Acc: 0.7796 | Val Loss: 0.7142 Acc: 0.7122                                               \n",
      "Epoch 012 | Train Loss: 0.4782 Acc: 0.7954 | Val Loss: 0.4195 Acc: 0.8281                                               \n",
      "Epoch 013 | Train Loss: 0.5055 Acc: 0.7880 | Val Loss: 0.5988 Acc: 0.7409                                               \n",
      "Epoch 014 | Train Loss: 0.4640 Acc: 0.8080 | Val Loss: 0.5711 Acc: 0.7122                                               \n",
      "Epoch 015 | Train Loss: 0.4506 Acc: 0.8095 | Val Loss: 0.5138 Acc: 0.7627                                               \n",
      "Epoch 016 | Train Loss: 0.4324 Acc: 0.8239 | Val Loss: 0.3180 Acc: 0.8588                                               \n",
      "Epoch 017 | Train Loss: 0.4257 Acc: 0.8292 | Val Loss: 0.3718 Acc: 0.8555                                               \n",
      "Epoch 018 | Train Loss: 0.4140 Acc: 0.8289 | Val Loss: 0.3981 Acc: 0.8430                                               \n",
      "Epoch 019 | Train Loss: 0.4048 Acc: 0.8319 | Val Loss: 0.3084 Acc: 0.8743                                               \n",
      "Epoch 020 | Train Loss: 0.4009 Acc: 0.8379 | Val Loss: 0.4576 Acc: 0.8376                                               \n",
      "Epoch 021 | Train Loss: 0.4010 Acc: 0.8395 | Val Loss: 0.4118 Acc: 0.8588                                               \n",
      "Epoch 022 | Train Loss: 0.3966 Acc: 0.8413 | Val Loss: 0.3863 Acc: 0.8140                                               \n",
      "Epoch 023 | Train Loss: 0.3920 Acc: 0.8466 | Val Loss: 0.3087 Acc: 0.8666                                               \n",
      "Epoch 024 | Train Loss: 0.3876 Acc: 0.8430 | Val Loss: 0.3831 Acc: 0.8394                                               \n",
      "Epoch 025 | Train Loss: 0.3714 Acc: 0.8501 | Val Loss: 0.5171 Acc: 0.7818                                               \n",
      "Epoch 026 | Train Loss: 0.4169 Acc: 0.8263 | Val Loss: 0.3122 Acc: 0.8552                                               \n",
      "Epoch 027 | Train Loss: 0.3734 Acc: 0.8521 | Val Loss: 0.2456 Acc: 0.8779                                               \n",
      "Epoch 028 | Train Loss: 0.3807 Acc: 0.8492 | Val Loss: 0.3462 Acc: 0.8176                                               \n",
      "Epoch 029 | Train Loss: 0.3646 Acc: 0.8543 | Val Loss: 0.7217 Acc: 0.7699                                               \n",
      "Epoch 030 | Train Loss: 0.3631 Acc: 0.8526 | Val Loss: 0.3568 Acc: 0.8373                                               \n",
      "Epoch 031 | Train Loss: 0.3625 Acc: 0.8536 | Val Loss: 0.2552 Acc: 0.9119                                               \n",
      "Epoch 032 | Train Loss: 0.3559 Acc: 0.8587 | Val Loss: 0.4926 Acc: 0.8081                                               \n",
      "Epoch 033 | Train Loss: 0.3595 Acc: 0.8559 | Val Loss: 0.5608 Acc: 0.7528                                               \n",
      "Epoch 034 | Train Loss: 0.4024 Acc: 0.8386 | Val Loss: 0.3526 Acc: 0.8349                                               \n",
      "Epoch 035 | Train Loss: 0.3889 Acc: 0.8415 | Val Loss: 0.2577 Acc: 0.8636                                               \n",
      "Epoch 036 | Train Loss: 0.3956 Acc: 0.8379 | Val Loss: 0.4376 Acc: 0.8051                                               \n",
      "Epoch 037 | Train Loss: 0.3792 Acc: 0.8441 | Val Loss: 0.3197 Acc: 0.8182                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 64, 'cnn_dropout': 0.33103096328929965, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 64, 'learning_rate': 3.069590123518817e-05, 'lstm_dense': 64, 'lstm_hidden_size': 64, 'lstm_layers': 3, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 6.0528 Acc: 0.3302 | Val Loss: 1.1714 Acc: 0.3018                                               \n",
      "Epoch 002 | Train Loss: 1.2076 Acc: 0.3985 | Val Loss: 1.0871 Acc: 0.3896                                               \n",
      "Epoch 003 | Train Loss: 1.1414 Acc: 0.4147 | Val Loss: 1.0626 Acc: 0.4334                                               \n",
      "Epoch 004 | Train Loss: 1.1167 Acc: 0.4133 | Val Loss: 1.0784 Acc: 0.4113                                               \n",
      "Epoch 005 | Train Loss: 1.0831 Acc: 0.4174 | Val Loss: 1.0253 Acc: 0.3242                                               \n",
      "Epoch 006 | Train Loss: 1.0604 Acc: 0.4338 | Val Loss: 0.9957 Acc: 0.3872                                               \n",
      "Epoch 007 | Train Loss: 1.0356 Acc: 0.4415 | Val Loss: 0.9717 Acc: 0.4358                                               \n",
      "Epoch 008 | Train Loss: 1.0290 Acc: 0.4455 | Val Loss: 0.9542 Acc: 0.4358                                               \n",
      "Epoch 009 | Train Loss: 0.9955 Acc: 0.4479 | Val Loss: 0.9787 Acc: 0.4060                                               \n",
      "Epoch 010 | Train Loss: 0.9955 Acc: 0.4485 | Val Loss: 0.9277 Acc: 0.4293                                               \n",
      "Epoch 011 | Train Loss: 0.9824 Acc: 0.4526 | Val Loss: 0.9172 Acc: 0.4340                                               \n",
      "Epoch 012 | Train Loss: 0.9748 Acc: 0.4482 | Val Loss: 0.9464 Acc: 0.4421                                               \n",
      "Epoch 013 | Train Loss: 0.9763 Acc: 0.4619 | Val Loss: 0.9256 Acc: 0.4301                                               \n",
      "Epoch 014 | Train Loss: 0.9725 Acc: 0.4558 | Val Loss: 0.9042 Acc: 0.4415                                               \n",
      "Epoch 015 | Train Loss: 0.9754 Acc: 0.4506 | Val Loss: 0.9672 Acc: 0.4457                                               \n",
      "Epoch 016 | Train Loss: 0.9666 Acc: 0.4614 | Val Loss: 0.9140 Acc: 0.4537                                               \n",
      "Epoch 017 | Train Loss: 0.9661 Acc: 0.4494 | Val Loss: 0.8938 Acc: 0.4340                                               \n",
      "Epoch 018 | Train Loss: 0.9653 Acc: 0.4550 | Val Loss: 0.9303 Acc: 0.4340                                               \n",
      "Epoch 019 | Train Loss: 0.9607 Acc: 0.4601 | Val Loss: 1.0175 Acc: 0.4445                                               \n",
      "Epoch 020 | Train Loss: 0.9374 Acc: 0.4571 | Val Loss: 0.8910 Acc: 0.4349                                               \n",
      "Epoch 021 | Train Loss: 0.9514 Acc: 0.4668 | Val Loss: 0.8923 Acc: 0.4534                                               \n",
      "Epoch 022 | Train Loss: 0.9362 Acc: 0.4719 | Val Loss: 0.8914 Acc: 0.4146                                               \n",
      "Epoch 023 | Train Loss: 0.9385 Acc: 0.4611 | Val Loss: 0.8828 Acc: 0.4137                                               \n",
      "Epoch 024 | Train Loss: 0.9293 Acc: 0.4732 | Val Loss: 0.8854 Acc: 0.4537                                               \n",
      "Epoch 025 | Train Loss: 0.9326 Acc: 0.4654 | Val Loss: 0.8812 Acc: 0.4516                                               \n",
      "Epoch 026 | Train Loss: 0.9175 Acc: 0.4718 | Val Loss: 0.8721 Acc: 0.4301                                               \n",
      "Epoch 027 | Train Loss: 0.9328 Acc: 0.4673 | Val Loss: 0.9103 Acc: 0.4728                                               \n",
      "Epoch 028 | Train Loss: 0.9262 Acc: 0.4715 | Val Loss: 0.8962 Acc: 0.4704                                               \n",
      "Epoch 029 | Train Loss: 0.9264 Acc: 0.4791 | Val Loss: 0.8663 Acc: 0.4573                                               \n",
      "Epoch 030 | Train Loss: 0.9104 Acc: 0.4777 | Val Loss: 0.8835 Acc: 0.4194                                               \n",
      "Epoch 031 | Train Loss: 0.9164 Acc: 0.4839 | Val Loss: 0.9194 Acc: 0.4728                                               \n",
      "Epoch 032 | Train Loss: 0.9278 Acc: 0.4856 | Val Loss: 0.8477 Acc: 0.5322                                               \n",
      "Epoch 033 | Train Loss: 0.9029 Acc: 0.4881 | Val Loss: 0.8404 Acc: 0.4666                                               \n",
      "Epoch 034 | Train Loss: 0.9014 Acc: 0.4922 | Val Loss: 0.8737 Acc: 0.4645                                               \n",
      "Epoch 035 | Train Loss: 0.8917 Acc: 0.5000 | Val Loss: 0.8218 Acc: 0.5182                                               \n",
      "Epoch 036 | Train Loss: 0.8845 Acc: 0.5044 | Val Loss: 0.8339 Acc: 0.5955                                               \n",
      "Epoch 037 | Train Loss: 0.8748 Acc: 0.5117 | Val Loss: 0.8210 Acc: 0.5191                                               \n",
      "Epoch 038 | Train Loss: 0.8745 Acc: 0.5124 | Val Loss: 0.8158 Acc: 0.5367                                               \n",
      "Epoch 039 | Train Loss: 0.8693 Acc: 0.5182 | Val Loss: 0.8257 Acc: 0.6275                                               \n",
      "Epoch 040 | Train Loss: 0.8579 Acc: 0.5130 | Val Loss: 0.8009 Acc: 0.4821                                               \n",
      "Epoch 041 | Train Loss: 0.8542 Acc: 0.5203 | Val Loss: 0.8301 Acc: 0.6024                                               \n",
      "Epoch 042 | Train Loss: 0.8583 Acc: 0.5200 | Val Loss: 0.8099 Acc: 0.5749                                               \n",
      "Epoch 043 | Train Loss: 0.8580 Acc: 0.5220 | Val Loss: 0.7927 Acc: 0.5221                                               \n",
      "Epoch 044 | Train Loss: 0.8445 Acc: 0.5306 | Val Loss: 0.7943 Acc: 0.5952                                               \n",
      "Epoch 045 | Train Loss: 0.8474 Acc: 0.5322 | Val Loss: 0.8123 Acc: 0.6301                                               \n",
      "Epoch 046 | Train Loss: 0.8445 Acc: 0.5308 | Val Loss: 0.7760 Acc: 0.6358                                               \n",
      "Epoch 047 | Train Loss: 0.8327 Acc: 0.5389 | Val Loss: 0.7975 Acc: 0.6307                                               \n",
      "Epoch 048 | Train Loss: 0.8378 Acc: 0.5289 | Val Loss: 0.7904 Acc: 0.5612                                               \n",
      "Epoch 049 | Train Loss: 0.8363 Acc: 0.5426 | Val Loss: 0.7808 Acc: 0.5615                                               \n",
      "Epoch 050 | Train Loss: 0.8355 Acc: 0.5430 | Val Loss: 0.7682 Acc: 0.5639                                               \n",
      "Epoch 051 | Train Loss: 0.8301 Acc: 0.5447 | Val Loss: 0.7624 Acc: 0.5794                                               \n",
      "Epoch 052 | Train Loss: 0.8332 Acc: 0.5521 | Val Loss: 0.7633 Acc: 0.5173                                               \n",
      "Epoch 053 | Train Loss: 0.8312 Acc: 0.5502 | Val Loss: 0.7843 Acc: 0.6376                                               \n",
      "Epoch 054 | Train Loss: 0.8241 Acc: 0.5544 | Val Loss: 0.7962 Acc: 0.6403                                               \n",
      "Epoch 055 | Train Loss: 0.8282 Acc: 0.5474 | Val Loss: 0.7803 Acc: 0.6307                                               \n",
      "Epoch 056 | Train Loss: 0.8110 Acc: 0.5616 | Val Loss: 0.7569 Acc: 0.5719                                               \n",
      "Epoch 057 | Train Loss: 0.8152 Acc: 0.5663 | Val Loss: 0.7669 Acc: 0.5391                                               \n",
      "Epoch 058 | Train Loss: 0.8158 Acc: 0.5633 | Val Loss: 0.7434 Acc: 0.5797                                               \n",
      "Epoch 059 | Train Loss: 0.8113 Acc: 0.5646 | Val Loss: 0.7519 Acc: 0.6200                                               \n",
      "Epoch 060 | Train Loss: 0.8089 Acc: 0.5714 | Val Loss: 0.7530 Acc: 0.6245                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 32, 'cnn_dropout': 0.3580673705478318, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 16, 'cnn_kernels_2': 64, 'learning_rate': 1.7311868524018822e-05, 'lstm_dense': 128, 'lstm_hidden_size': 32, 'lstm_layers': 4, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 48.8102 Acc: 0.3045 | Val Loss: 14.8613 Acc: 0.2848                                             \n",
      "Epoch 002 | Train Loss: 26.1655 Acc: 0.3236 | Val Loss: 8.1406 Acc: 0.4069                                              \n",
      "Epoch 003 | Train Loss: 16.7102 Acc: 0.3370 | Val Loss: 5.0646 Acc: 0.5394                                              \n",
      "Epoch 004 | Train Loss: 12.2026 Acc: 0.3606 | Val Loss: 3.8585 Acc: 0.5412                                              \n",
      "Epoch 005 | Train Loss: 9.5823 Acc: 0.3723 | Val Loss: 2.6712 Acc: 0.5185                                               \n",
      "Epoch 006 | Train Loss: 7.8654 Acc: 0.3842 | Val Loss: 2.2849 Acc: 0.5358                                               \n",
      "Epoch 007 | Train Loss: 6.9325 Acc: 0.3859 | Val Loss: 2.0977 Acc: 0.4597                                               \n",
      "Epoch 008 | Train Loss: 5.8676 Acc: 0.3988 | Val Loss: 2.0263 Acc: 0.4621                                               \n",
      "Epoch 009 | Train Loss: 5.2235 Acc: 0.3941 | Val Loss: 1.8662 Acc: 0.5090                                               \n",
      "Epoch 010 | Train Loss: 4.7846 Acc: 0.3955 | Val Loss: 1.7468 Acc: 0.5119                                               \n",
      "Epoch 011 | Train Loss: 4.2794 Acc: 0.4096 | Val Loss: 1.7120 Acc: 0.4812                                               \n",
      "Epoch 012 | Train Loss: 3.8588 Acc: 0.4162 | Val Loss: 1.5212 Acc: 0.5540                                               \n",
      "Epoch 013 | Train Loss: 3.5237 Acc: 0.4228 | Val Loss: 1.4139 Acc: 0.4800                                               \n",
      "Epoch 014 | Train Loss: 3.1824 Acc: 0.4290 | Val Loss: 1.3713 Acc: 0.5824                                               \n",
      "Epoch 015 | Train Loss: 2.9953 Acc: 0.4399 | Val Loss: 1.2727 Acc: 0.5881                                               \n",
      "Epoch 016 | Train Loss: 2.7195 Acc: 0.4523 | Val Loss: 1.1578 Acc: 0.5934                                               \n",
      "Epoch 017 | Train Loss: 2.4955 Acc: 0.4634 | Val Loss: 1.2007 Acc: 0.5648                                               \n",
      "Epoch 018 | Train Loss: 2.3713 Acc: 0.4710 | Val Loss: 1.0892 Acc: 0.5719                                               \n",
      "Epoch 019 | Train Loss: 2.2757 Acc: 0.4758 | Val Loss: 1.1355 Acc: 0.6096                                               \n",
      "Epoch 020 | Train Loss: 2.0694 Acc: 0.4844 | Val Loss: 1.0817 Acc: 0.5979                                               \n",
      "Epoch 021 | Train Loss: 1.9821 Acc: 0.4826 | Val Loss: 0.9404 Acc: 0.6090                                               \n",
      "Epoch 022 | Train Loss: 1.8879 Acc: 0.4966 | Val Loss: 1.1797 Acc: 0.5684                                               \n",
      "Epoch 023 | Train Loss: 1.6932 Acc: 0.5006 | Val Loss: 0.9682 Acc: 0.6325                                               \n",
      "Epoch 024 | Train Loss: 1.6309 Acc: 0.5140 | Val Loss: 1.0977 Acc: 0.5916                                               \n",
      "Epoch 025 | Train Loss: 1.5534 Acc: 0.5129 | Val Loss: 0.9296 Acc: 0.6206                                               \n",
      "Epoch 026 | Train Loss: 1.4812 Acc: 0.5321 | Val Loss: 0.9592 Acc: 0.6063                                               \n",
      "Epoch 027 | Train Loss: 1.4548 Acc: 0.5334 | Val Loss: 0.8226 Acc: 0.6367                                               \n",
      "Epoch 028 | Train Loss: 1.3730 Acc: 0.5407 | Val Loss: 0.8760 Acc: 0.6391                                               \n",
      "Epoch 029 | Train Loss: 1.3075 Acc: 0.5449 | Val Loss: 0.8555 Acc: 0.6245                                               \n",
      "Epoch 030 | Train Loss: 1.2434 Acc: 0.5532 | Val Loss: 0.9089 Acc: 0.6104                                               \n",
      "Epoch 031 | Train Loss: 1.1827 Acc: 0.5614 | Val Loss: 0.8899 Acc: 0.6134                                               \n",
      "Epoch 032 | Train Loss: 1.1852 Acc: 0.5647 | Val Loss: 0.7655 Acc: 0.6558                                               \n",
      "Epoch 033 | Train Loss: 1.1232 Acc: 0.5863 | Val Loss: 0.8323 Acc: 0.6409                                               \n",
      "Epoch 034 | Train Loss: 1.1268 Acc: 0.5801 | Val Loss: 0.8552 Acc: 0.6194                                               \n",
      "Epoch 035 | Train Loss: 1.0646 Acc: 0.5886 | Val Loss: 0.8294 Acc: 0.6424                                               \n",
      "Epoch 036 | Train Loss: 1.0499 Acc: 0.5913 | Val Loss: 0.8946 Acc: 0.6143                                               \n",
      "Epoch 037 | Train Loss: 1.0242 Acc: 0.6000 | Val Loss: 0.8385 Acc: 0.6293                                               \n",
      "Epoch 038 | Train Loss: 1.0059 Acc: 0.6086 | Val Loss: 0.8297 Acc: 0.6355                                               \n",
      "Epoch 039 | Train Loss: 0.9850 Acc: 0.6077 | Val Loss: 0.7686 Acc: 0.6501                                               \n",
      "Epoch 040 | Train Loss: 0.9742 Acc: 0.6170 | Val Loss: 0.8184 Acc: 0.6131                                               \n",
      "Epoch 041 | Train Loss: 0.9501 Acc: 0.6209 | Val Loss: 0.9106 Acc: 0.6242                                               \n",
      "Epoch 042 | Train Loss: 0.9539 Acc: 0.6241 | Val Loss: 0.7277 Acc: 0.6645                                               \n",
      "Epoch 043 | Train Loss: 0.9227 Acc: 0.6327 | Val Loss: 0.7588 Acc: 0.6307                                               \n",
      "Epoch 044 | Train Loss: 0.9162 Acc: 0.6342 | Val Loss: 0.7604 Acc: 0.6669                                               \n",
      "Epoch 045 | Train Loss: 0.8832 Acc: 0.6463 | Val Loss: 0.7358 Acc: 0.6791                                               \n",
      "Epoch 046 | Train Loss: 0.8877 Acc: 0.6446 | Val Loss: 0.6753 Acc: 0.6970                                               \n",
      "Epoch 047 | Train Loss: 0.8551 Acc: 0.6526 | Val Loss: 0.8029 Acc: 0.6546                                               \n",
      "Epoch 048 | Train Loss: 0.8376 Acc: 0.6584 | Val Loss: 0.7803 Acc: 0.6487                                               \n",
      "Epoch 049 | Train Loss: 0.8266 Acc: 0.6627 | Val Loss: 0.7534 Acc: 0.6645                                               \n",
      "Epoch 050 | Train Loss: 0.8276 Acc: 0.6650 | Val Loss: 0.6996 Acc: 0.6821                                               \n",
      "Epoch 051 | Train Loss: 0.8158 Acc: 0.6639 | Val Loss: 0.7263 Acc: 0.6806                                               \n",
      "Epoch 052 | Train Loss: 0.8054 Acc: 0.6711 | Val Loss: 0.7611 Acc: 0.6699                                               \n",
      "Epoch 053 | Train Loss: 0.7884 Acc: 0.6764 | Val Loss: 0.8600 Acc: 0.6337                                               \n",
      "Epoch 054 | Train Loss: 0.7848 Acc: 0.6781 | Val Loss: 0.7437 Acc: 0.6782                                               \n",
      "Epoch 055 | Train Loss: 0.7694 Acc: 0.6818 | Val Loss: 0.7176 Acc: 0.6791                                               \n",
      "Epoch 056 | Train Loss: 0.7750 Acc: 0.6725 | Val Loss: 0.7160 Acc: 0.6755                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 32, 'cnn_dropout': 0.6147286704043524, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 32, 'cnn_kernels_2': 64, 'learning_rate': 0.0017497503552416375, 'lstm_dense': 128, 'lstm_hidden_size': 128, 'lstm_layers': 2, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 11.4219 Acc: 0.4258 | Val Loss: 2.9652 Acc: 0.4904                                              \n",
      "Epoch 002 | Train Loss: 2.3209 Acc: 0.5118 | Val Loss: 1.0542 Acc: 0.5946                                               \n",
      "Epoch 003 | Train Loss: 1.4369 Acc: 0.5384 | Val Loss: 0.9243 Acc: 0.5916                                               \n",
      "Epoch 004 | Train Loss: 1.1695 Acc: 0.5638 | Val Loss: 0.8941 Acc: 0.5979                                               \n",
      "Epoch 005 | Train Loss: 0.9375 Acc: 0.5955 | Val Loss: 0.8716 Acc: 0.6182                                               \n",
      "Epoch 006 | Train Loss: 0.8305 Acc: 0.6300 | Val Loss: 0.6211 Acc: 0.7373                                               \n",
      "Epoch 007 | Train Loss: 0.7368 Acc: 0.6648 | Val Loss: 0.6643 Acc: 0.6746                                               \n",
      "Epoch 008 | Train Loss: 0.6883 Acc: 0.6839 | Val Loss: 0.6306 Acc: 0.7134                                               \n",
      "Epoch 009 | Train Loss: 0.6458 Acc: 0.7183 | Val Loss: 0.5396 Acc: 0.7824                                               \n",
      "Epoch 010 | Train Loss: 0.5676 Acc: 0.7589 | Val Loss: 0.4825 Acc: 0.7815                                               \n",
      "Epoch 011 | Train Loss: 0.5363 Acc: 0.7771 | Val Loss: 0.4840 Acc: 0.8036                                               \n",
      "Epoch 012 | Train Loss: 0.4561 Acc: 0.8139 | Val Loss: 0.3842 Acc: 0.8639                                               \n",
      "Epoch 013 | Train Loss: 0.4197 Acc: 0.8283 | Val Loss: 0.5026 Acc: 0.7818                                               \n",
      "Epoch 014 | Train Loss: 0.3858 Acc: 0.8482 | Val Loss: 0.3390 Acc: 0.8675                                               \n",
      "Epoch 015 | Train Loss: 0.3628 Acc: 0.8598 | Val Loss: 0.2646 Acc: 0.9018                                               \n",
      "Epoch 016 | Train Loss: 0.3386 Acc: 0.8725 | Val Loss: 0.3783 Acc: 0.8469                                               \n",
      "Epoch 017 | Train Loss: 0.3117 Acc: 0.8811 | Val Loss: 0.2489 Acc: 0.9003                                               \n",
      "Epoch 018 | Train Loss: 0.2910 Acc: 0.8912 | Val Loss: 0.3918 Acc: 0.8418                                               \n",
      "Epoch 019 | Train Loss: 0.2894 Acc: 0.8930 | Val Loss: 0.2864 Acc: 0.8961                                               \n",
      "Epoch 020 | Train Loss: 0.2376 Acc: 0.9113 | Val Loss: 0.3249 Acc: 0.8731                                               \n",
      "Epoch 021 | Train Loss: 0.2516 Acc: 0.9045 | Val Loss: 0.3699 Acc: 0.8540                                               \n",
      "Epoch 022 | Train Loss: 0.2437 Acc: 0.9085 | Val Loss: 0.2164 Acc: 0.9200                                               \n",
      "Epoch 023 | Train Loss: 0.2488 Acc: 0.9104 | Val Loss: 0.2690 Acc: 0.8976                                               \n",
      "Epoch 024 | Train Loss: 0.2273 Acc: 0.9171 | Val Loss: 0.2373 Acc: 0.9116                                               \n",
      "Epoch 025 | Train Loss: 0.2167 Acc: 0.9241 | Val Loss: 0.2580 Acc: 0.9173                                               \n",
      "Epoch 026 | Train Loss: 0.2158 Acc: 0.9237 | Val Loss: 0.2043 Acc: 0.9242                                               \n",
      "Epoch 027 | Train Loss: 0.1911 Acc: 0.9334 | Val Loss: 0.2451 Acc: 0.9119                                               \n",
      "Epoch 028 | Train Loss: 0.1924 Acc: 0.9313 | Val Loss: 0.1998 Acc: 0.9254                                               \n",
      "Epoch 029 | Train Loss: 0.1975 Acc: 0.9284 | Val Loss: 0.1945 Acc: 0.9155                                               \n",
      "Epoch 030 | Train Loss: 0.1920 Acc: 0.9316 | Val Loss: 0.2698 Acc: 0.8979                                               \n",
      "Epoch 031 | Train Loss: 0.2195 Acc: 0.9191 | Val Loss: 0.2891 Acc: 0.8955                                               \n",
      "Epoch 032 | Train Loss: 0.2062 Acc: 0.9257 | Val Loss: 0.2906 Acc: 0.8815                                               \n",
      "Epoch 033 | Train Loss: 0.2227 Acc: 0.9216 | Val Loss: 0.2185 Acc: 0.9218                                               \n",
      "Epoch 034 | Train Loss: 0.2155 Acc: 0.9237 | Val Loss: 0.2335 Acc: 0.9087                                               \n",
      "Epoch 035 | Train Loss: 0.1742 Acc: 0.9390 | Val Loss: 0.2140 Acc: 0.9099                                               \n",
      "Epoch 036 | Train Loss: 0.1918 Acc: 0.9342 | Val Loss: 0.1614 Acc: 0.9403                                               \n",
      "Epoch 037 | Train Loss: 0.1665 Acc: 0.9387 | Val Loss: 0.1832 Acc: 0.9272                                               \n",
      "Epoch 038 | Train Loss: 0.1874 Acc: 0.9345 | Val Loss: 0.1229 Acc: 0.9612                                               \n",
      "Epoch 039 | Train Loss: 0.1766 Acc: 0.9379 | Val Loss: 0.2122 Acc: 0.9224                                               \n",
      "Epoch 040 | Train Loss: 0.1587 Acc: 0.9411 | Val Loss: 0.1789 Acc: 0.9430                                               \n",
      "Epoch 041 | Train Loss: 0.1545 Acc: 0.9460 | Val Loss: 0.2279 Acc: 0.9006                                               \n",
      "Epoch 042 | Train Loss: 0.1546 Acc: 0.9460 | Val Loss: 0.2353 Acc: 0.9257                                               \n",
      "Epoch 043 | Train Loss: 0.1908 Acc: 0.9348 | Val Loss: 0.1838 Acc: 0.9254                                               \n",
      "Epoch 044 | Train Loss: 0.1525 Acc: 0.9444 | Val Loss: 0.2159 Acc: 0.9155                                               \n",
      "Epoch 045 | Train Loss: 0.1647 Acc: 0.9422 | Val Loss: 0.1344 Acc: 0.9436                                               \n",
      "Epoch 046 | Train Loss: 0.1457 Acc: 0.9474 | Val Loss: 0.2155 Acc: 0.9116                                               \n",
      "Epoch 047 | Train Loss: 0.1615 Acc: 0.9449 | Val Loss: 0.2137 Acc: 0.9194                                               \n",
      "Epoch 048 | Train Loss: 0.1457 Acc: 0.9503 | Val Loss: 0.1948 Acc: 0.9352                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 128, 'cnn_dropout': 0.07611794886579332, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 48, 'cnn_kernels_2': 32, 'learning_rate': 0.004885741918578917, 'lstm_dense': 32, 'lstm_hidden_size': 96, 'lstm_layers': 6, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 7.4422 Acc: 0.4851 | Val Loss: 0.9780 Acc: 0.5973                                               \n",
      "Epoch 002 | Train Loss: 0.8870 Acc: 0.5996 | Val Loss: 0.7693 Acc: 0.6057                                               \n",
      "Epoch 003 | Train Loss: 0.7768 Acc: 0.6405 | Val Loss: 0.7733 Acc: 0.5719                                               \n",
      "Epoch 004 | Train Loss: 0.7097 Acc: 0.6795 | Val Loss: 0.6668 Acc: 0.6922                                               \n",
      "Epoch 005 | Train Loss: 0.6858 Acc: 0.6941 | Val Loss: 0.6216 Acc: 0.7081                                               \n",
      "Epoch 006 | Train Loss: 0.6467 Acc: 0.7139 | Val Loss: 0.7462 Acc: 0.6287                                               \n",
      "Epoch 007 | Train Loss: 0.6118 Acc: 0.7295 | Val Loss: 0.6580 Acc: 0.7084                                               \n",
      "Epoch 008 | Train Loss: 0.6122 Acc: 0.7209 | Val Loss: 0.6575 Acc: 0.6743                                               \n",
      "Epoch 009 | Train Loss: 0.6819 Acc: 0.6871 | Val Loss: 0.6280 Acc: 0.7113                                               \n",
      "Epoch 010 | Train Loss: 0.6472 Acc: 0.6964 | Val Loss: 0.6527 Acc: 0.6878                                               \n",
      "Epoch 011 | Train Loss: 0.6455 Acc: 0.7051 | Val Loss: 0.6534 Acc: 0.6660                                               \n",
      "Epoch 012 | Train Loss: 0.6425 Acc: 0.6946 | Val Loss: 0.6921 Acc: 0.6910                                               \n",
      "Epoch 013 | Train Loss: 0.7174 Acc: 0.6612 | Val Loss: 0.6632 Acc: 0.7278                                               \n",
      "Epoch 014 | Train Loss: 0.6942 Acc: 0.6700 | Val Loss: 0.7302 Acc: 0.6719                                               \n",
      "Epoch 015 | Train Loss: 0.6629 Acc: 0.6875 | Val Loss: 0.6598 Acc: 0.7185                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 36, 'cnn_dense': 256, 'cnn_dropout': 0.3034432954742866, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 96, 'learning_rate': 4.45520908963029e-05, 'lstm_dense': 128, 'lstm_hidden_size': 32, 'lstm_layers': 6, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 100.9584 Acc: 0.3268 | Val Loss: 16.8502 Acc: 0.4642                                            \n",
      "Epoch 002 | Train Loss: 29.6071 Acc: 0.3792 | Val Loss: 7.9735 Acc: 0.5048                                              \n",
      "Epoch 003 | Train Loss: 17.7609 Acc: 0.4104 | Val Loss: 5.1165 Acc: 0.4639                                              \n",
      "Epoch 004 | Train Loss: 13.4675 Acc: 0.4213 | Val Loss: 4.1206 Acc: 0.4827                                              \n",
      "Epoch 005 | Train Loss: 11.0217 Acc: 0.4264 | Val Loss: 3.6241 Acc: 0.4910                                              \n",
      "Epoch 006 | Train Loss: 8.8925 Acc: 0.4441 | Val Loss: 2.9406 Acc: 0.5400                                               \n",
      "Epoch 007 | Train Loss: 7.7286 Acc: 0.4561 | Val Loss: 2.3546 Acc: 0.5588                                               \n",
      "Epoch 008 | Train Loss: 6.4044 Acc: 0.4805 | Val Loss: 2.2340 Acc: 0.5555                                               \n",
      "Epoch 009 | Train Loss: 5.5580 Acc: 0.4876 | Val Loss: 1.7590 Acc: 0.5743                                               \n",
      "Epoch 010 | Train Loss: 4.9789 Acc: 0.5052 | Val Loss: 1.7577 Acc: 0.6027                                               \n",
      "Epoch 011 | Train Loss: 4.4743 Acc: 0.5232 | Val Loss: 1.7208 Acc: 0.6475                                               \n",
      "Epoch 012 | Train Loss: 4.0335 Acc: 0.5286 | Val Loss: 1.7033 Acc: 0.6627                                               \n",
      "Epoch 013 | Train Loss: 3.5623 Acc: 0.5530 | Val Loss: 1.3655 Acc: 0.6830                                               \n",
      "Epoch 014 | Train Loss: 3.0675 Acc: 0.5741 | Val Loss: 1.4062 Acc: 0.7131                                               \n",
      "Epoch 015 | Train Loss: 2.7780 Acc: 0.5861 | Val Loss: 1.3278 Acc: 0.6866                                               \n",
      "Epoch 016 | Train Loss: 2.5016 Acc: 0.6148 | Val Loss: 1.1639 Acc: 0.7221                                               \n",
      "Epoch 017 | Train Loss: 2.2916 Acc: 0.6315 | Val Loss: 1.1528 Acc: 0.7397                                               \n",
      "Epoch 018 | Train Loss: 2.0348 Acc: 0.6535 | Val Loss: 0.8854 Acc: 0.7600                                               \n",
      "Epoch 019 | Train Loss: 1.8074 Acc: 0.6746 | Val Loss: 0.7776 Acc: 0.7782                                               \n",
      "Epoch 020 | Train Loss: 1.5911 Acc: 0.6899 | Val Loss: 0.7662 Acc: 0.7779                                               \n",
      "Epoch 021 | Train Loss: 1.5028 Acc: 0.7024 | Val Loss: 0.9466 Acc: 0.7531                                               \n",
      "Epoch 022 | Train Loss: 1.3181 Acc: 0.7236 | Val Loss: 0.6954 Acc: 0.8057                                               \n",
      "Epoch 023 | Train Loss: 1.2604 Acc: 0.7398 | Val Loss: 0.5611 Acc: 0.8361                                               \n",
      "Epoch 024 | Train Loss: 1.0928 Acc: 0.7526 | Val Loss: 0.6370 Acc: 0.8179                                               \n",
      "Epoch 025 | Train Loss: 1.0335 Acc: 0.7650 | Val Loss: 0.5886 Acc: 0.8325                                               \n",
      "Epoch 026 | Train Loss: 0.9469 Acc: 0.7736 | Val Loss: 0.5331 Acc: 0.8591                                               \n",
      "Epoch 027 | Train Loss: 0.8507 Acc: 0.7902 | Val Loss: 0.6049 Acc: 0.8069                                               \n",
      "Epoch 028 | Train Loss: 0.8259 Acc: 0.7954 | Val Loss: 0.5214 Acc: 0.8672                                               \n",
      "Epoch 029 | Train Loss: 0.7636 Acc: 0.8061 | Val Loss: 0.4558 Acc: 0.8507                                               \n",
      "Epoch 030 | Train Loss: 0.6570 Acc: 0.8251 | Val Loss: 0.6444 Acc: 0.8090                                               \n",
      "Epoch 031 | Train Loss: 0.6151 Acc: 0.8330 | Val Loss: 0.5024 Acc: 0.8475                                               \n",
      "Epoch 032 | Train Loss: 0.5827 Acc: 0.8409 | Val Loss: 0.4959 Acc: 0.8370                                               \n",
      "Epoch 033 | Train Loss: 0.5542 Acc: 0.8438 | Val Loss: 0.3626 Acc: 0.8940                                               \n",
      "Epoch 034 | Train Loss: 0.5153 Acc: 0.8554 | Val Loss: 0.4304 Acc: 0.8519                                               \n",
      "Epoch 035 | Train Loss: 0.4887 Acc: 0.8556 | Val Loss: 0.3305 Acc: 0.8890                                               \n",
      "Epoch 036 | Train Loss: 0.4436 Acc: 0.8707 | Val Loss: 0.4022 Acc: 0.8896                                               \n",
      "Epoch 037 | Train Loss: 0.4263 Acc: 0.8719 | Val Loss: 0.3406 Acc: 0.8916                                               \n",
      "Epoch 038 | Train Loss: 0.4036 Acc: 0.8788 | Val Loss: 0.3799 Acc: 0.8755                                               \n",
      "Epoch 039 | Train Loss: 0.3734 Acc: 0.8895 | Val Loss: 0.3299 Acc: 0.8863                                               \n",
      "Epoch 040 | Train Loss: 0.3483 Acc: 0.8913 | Val Loss: 0.3173 Acc: 0.8875                                               \n",
      "Epoch 041 | Train Loss: 0.3210 Acc: 0.9036 | Val Loss: 0.2944 Acc: 0.8970                                               \n",
      "Epoch 042 | Train Loss: 0.3204 Acc: 0.9043 | Val Loss: 0.3370 Acc: 0.9066                                               \n",
      "Epoch 043 | Train Loss: 0.2909 Acc: 0.9132 | Val Loss: 0.2794 Acc: 0.9057                                               \n",
      "Epoch 044 | Train Loss: 0.2614 Acc: 0.9201 | Val Loss: 0.3529 Acc: 0.8770                                               \n",
      "Epoch 045 | Train Loss: 0.2672 Acc: 0.9219 | Val Loss: 0.4414 Acc: 0.8633                                               \n",
      "Epoch 046 | Train Loss: 0.2645 Acc: 0.9183 | Val Loss: 0.3106 Acc: 0.9033                                               \n",
      "Epoch 047 | Train Loss: 0.2308 Acc: 0.9292 | Val Loss: 0.3046 Acc: 0.9021                                               \n",
      "Epoch 048 | Train Loss: 0.2295 Acc: 0.9311 | Val Loss: 0.5265 Acc: 0.8460                                               \n",
      "Epoch 049 | Train Loss: 0.2165 Acc: 0.9351 | Val Loss: 0.3875 Acc: 0.8639                                               \n",
      "Epoch 050 | Train Loss: 0.2006 Acc: 0.9386 | Val Loss: 0.3821 Acc: 0.8940                                               \n",
      "Epoch 051 | Train Loss: 0.2032 Acc: 0.9373 | Val Loss: 0.3105 Acc: 0.9060                                               \n",
      "Epoch 052 | Train Loss: 0.2016 Acc: 0.9390 | Val Loss: 0.3640 Acc: 0.8967                                               \n",
      "Epoch 053 | Train Loss: 0.1726 Acc: 0.9467 | Val Loss: 0.2527 Acc: 0.9251                                               \n",
      "Epoch 054 | Train Loss: 0.1757 Acc: 0.9481 | Val Loss: 0.2703 Acc: 0.9149                                               \n",
      "Epoch 055 | Train Loss: 0.1782 Acc: 0.9495 | Val Loss: 0.4470 Acc: 0.8767                                               \n",
      "Epoch 056 | Train Loss: 0.1762 Acc: 0.9503 | Val Loss: 0.2548 Acc: 0.9161                                               \n",
      "Epoch 057 | Train Loss: 0.1479 Acc: 0.9560 | Val Loss: 0.4630 Acc: 0.8579                                               \n",
      "Epoch 058 | Train Loss: 0.1473 Acc: 0.9571 | Val Loss: 0.3536 Acc: 0.8928                                               \n",
      "Epoch 059 | Train Loss: 0.1458 Acc: 0.9590 | Val Loss: 0.3548 Acc: 0.8901                                               \n",
      "Epoch 060 | Train Loss: 0.1186 Acc: 0.9663 | Val Loss: 0.3063 Acc: 0.9143                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 256, 'cnn_dropout': 0.6117904720262127, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 32, 'cnn_kernels_2': 96, 'learning_rate': 0.0010469633160326574, 'lstm_dense': 256, 'lstm_hidden_size': 128, 'lstm_layers': 1, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 5.9511 Acc: 0.4484 | Val Loss: 1.2080 Acc: 0.4591                                               \n",
      "Epoch 002 | Train Loss: 1.1792 Acc: 0.4830 | Val Loss: 1.1485 Acc: 0.5081                                               \n",
      "Epoch 003 | Train Loss: 1.1392 Acc: 0.5014 | Val Loss: 1.1216 Acc: 0.5155                                               \n",
      "Epoch 004 | Train Loss: 1.0947 Acc: 0.5191 | Val Loss: 1.0685 Acc: 0.5209                                               \n",
      "Epoch 005 | Train Loss: 1.0139 Acc: 0.5493 | Val Loss: 0.9827 Acc: 0.5758                                               \n",
      "Epoch 006 | Train Loss: 0.9156 Acc: 0.6169 | Val Loss: 0.8969 Acc: 0.6331                                               \n",
      "Epoch 007 | Train Loss: 0.8607 Acc: 0.6388 | Val Loss: 0.8077 Acc: 0.6707                                               \n",
      "Epoch 008 | Train Loss: 0.7746 Acc: 0.6888 | Val Loss: 0.6958 Acc: 0.7221                                               \n",
      "Epoch 009 | Train Loss: 0.6770 Acc: 0.7362 | Val Loss: 0.6288 Acc: 0.7549                                               \n",
      "Epoch 010 | Train Loss: 0.5983 Acc: 0.7682 | Val Loss: 0.5650 Acc: 0.7907                                               \n",
      "Epoch 011 | Train Loss: 0.5438 Acc: 0.7910 | Val Loss: 0.5419 Acc: 0.7958                                               \n",
      "Epoch 012 | Train Loss: 0.4935 Acc: 0.8158 | Val Loss: 0.4331 Acc: 0.8409                                               \n",
      "Epoch 013 | Train Loss: 0.4350 Acc: 0.8399 | Val Loss: 0.4249 Acc: 0.8475                                               \n",
      "Epoch 014 | Train Loss: 0.3995 Acc: 0.8535 | Val Loss: 0.4517 Acc: 0.8376                                               \n",
      "Epoch 015 | Train Loss: 0.3557 Acc: 0.8750 | Val Loss: 0.4339 Acc: 0.8490                                               \n",
      "Epoch 016 | Train Loss: 0.3303 Acc: 0.8780 | Val Loss: 0.3835 Acc: 0.8630                                               \n",
      "Epoch 017 | Train Loss: 0.3029 Acc: 0.8923 | Val Loss: 0.3254 Acc: 0.8839                                               \n",
      "Epoch 018 | Train Loss: 0.2673 Acc: 0.9033 | Val Loss: 0.2797 Acc: 0.9027                                               \n",
      "Epoch 019 | Train Loss: 0.2473 Acc: 0.9127 | Val Loss: 0.2524 Acc: 0.9107                                               \n",
      "Epoch 020 | Train Loss: 0.2285 Acc: 0.9198 | Val Loss: 0.2857 Acc: 0.9078                                               \n",
      "Epoch 021 | Train Loss: 0.2151 Acc: 0.9260 | Val Loss: 0.2543 Acc: 0.9128                                               \n",
      "Epoch 022 | Train Loss: 0.2151 Acc: 0.9218 | Val Loss: 0.3576 Acc: 0.8815                                               \n",
      "Epoch 023 | Train Loss: 0.1772 Acc: 0.9352 | Val Loss: 0.2906 Acc: 0.9033                                               \n",
      "Epoch 024 | Train Loss: 0.1780 Acc: 0.9390 | Val Loss: 0.2504 Acc: 0.9152                                               \n",
      "Epoch 025 | Train Loss: 0.1763 Acc: 0.9375 | Val Loss: 0.2339 Acc: 0.9239                                               \n",
      "Epoch 026 | Train Loss: 0.1761 Acc: 0.9376 | Val Loss: 0.2241 Acc: 0.9257                                               \n",
      "Epoch 027 | Train Loss: 0.1571 Acc: 0.9429 | Val Loss: 0.2098 Acc: 0.9304                                               \n",
      "Epoch 028 | Train Loss: 0.1425 Acc: 0.9495 | Val Loss: 0.2120 Acc: 0.9290                                               \n",
      "Epoch 029 | Train Loss: 0.1478 Acc: 0.9480 | Val Loss: 0.2762 Acc: 0.9128                                               \n",
      "Epoch 030 | Train Loss: 0.1301 Acc: 0.9557 | Val Loss: 0.2329 Acc: 0.9284                                               \n",
      "Epoch 031 | Train Loss: 0.1205 Acc: 0.9581 | Val Loss: 0.3200 Acc: 0.8973                                               \n",
      "Epoch 032 | Train Loss: 0.1131 Acc: 0.9607 | Val Loss: 0.2224 Acc: 0.9287                                               \n",
      "Epoch 033 | Train Loss: 0.1004 Acc: 0.9651 | Val Loss: 0.1779 Acc: 0.9406                                               \n",
      "Epoch 034 | Train Loss: 0.0982 Acc: 0.9646 | Val Loss: 0.2237 Acc: 0.9290                                               \n",
      "Epoch 035 | Train Loss: 0.0967 Acc: 0.9675 | Val Loss: 0.2500 Acc: 0.9218                                               \n",
      "Epoch 036 | Train Loss: 0.0896 Acc: 0.9691 | Val Loss: 0.2408 Acc: 0.9290                                               \n",
      "Epoch 037 | Train Loss: 0.0955 Acc: 0.9664 | Val Loss: 0.2084 Acc: 0.9334                                               \n",
      "Epoch 038 | Train Loss: 0.0833 Acc: 0.9704 | Val Loss: 0.1959 Acc: 0.9397                                               \n",
      "Epoch 039 | Train Loss: 0.0821 Acc: 0.9721 | Val Loss: 0.2174 Acc: 0.9334                                               \n",
      "Epoch 040 | Train Loss: 0.0866 Acc: 0.9708 | Val Loss: 0.1867 Acc: 0.9433                                               \n",
      "Epoch 041 | Train Loss: 0.0861 Acc: 0.9702 | Val Loss: 0.1643 Acc: 0.9457                                               \n",
      "Epoch 042 | Train Loss: 0.0742 Acc: 0.9752 | Val Loss: 0.2687 Acc: 0.9167                                               \n",
      "Epoch 043 | Train Loss: 0.0780 Acc: 0.9724 | Val Loss: 0.2724 Acc: 0.9119                                               \n",
      "Epoch 044 | Train Loss: 0.0803 Acc: 0.9710 | Val Loss: 0.1764 Acc: 0.9493                                               \n",
      "Epoch 045 | Train Loss: 0.0783 Acc: 0.9737 | Val Loss: 0.2096 Acc: 0.9367                                               \n",
      "Epoch 046 | Train Loss: 0.0707 Acc: 0.9745 | Val Loss: 0.1509 Acc: 0.9546                                               \n",
      "Epoch 047 | Train Loss: 0.0720 Acc: 0.9749 | Val Loss: 0.2091 Acc: 0.9349                                               \n",
      "Epoch 048 | Train Loss: 0.0681 Acc: 0.9767 | Val Loss: 0.3206 Acc: 0.9003                                               \n",
      "Epoch 049 | Train Loss: 0.0711 Acc: 0.9761 | Val Loss: 0.2051 Acc: 0.9379                                               \n",
      "Epoch 050 | Train Loss: 0.0653 Acc: 0.9777 | Val Loss: 0.2091 Acc: 0.9391                                               \n",
      "Epoch 051 | Train Loss: 0.0663 Acc: 0.9759 | Val Loss: 0.2658 Acc: 0.9224                                               \n",
      "Epoch 052 | Train Loss: 0.1182 Acc: 0.9595 | Val Loss: 0.2588 Acc: 0.9179                                               \n",
      "Epoch 053 | Train Loss: 0.0696 Acc: 0.9751 | Val Loss: 0.1574 Acc: 0.9531                                               \n",
      "Epoch 054 | Train Loss: 0.0613 Acc: 0.9774 | Val Loss: 0.1630 Acc: 0.9534                                               \n",
      "Epoch 055 | Train Loss: 0.0632 Acc: 0.9766 | Val Loss: 0.1772 Acc: 0.9460                                               \n",
      "Epoch 056 | Train Loss: 0.0528 Acc: 0.9822 | Val Loss: 0.1907 Acc: 0.9415                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 32, 'cnn_dropout': 0.6234383860814121, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 16, 'learning_rate': 0.00011681531652268983, 'lstm_dense': 32, 'lstm_hidden_size': 96, 'lstm_layers': 2, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 15.3625 Acc: 0.3661 | Val Loss: 5.6544 Acc: 0.4143                                              \n",
      "Epoch 002 | Train Loss: 5.6394 Acc: 0.4192 | Val Loss: 2.1034 Acc: 0.4991                                               \n",
      "Epoch 003 | Train Loss: 3.4545 Acc: 0.4506 | Val Loss: 2.3111 Acc: 0.4704                                               \n",
      "Epoch 004 | Train Loss: 2.5382 Acc: 0.4805 | Val Loss: 1.2909 Acc: 0.6182                                               \n",
      "Epoch 005 | Train Loss: 1.9346 Acc: 0.5224 | Val Loss: 1.1181 Acc: 0.6018                                               \n",
      "Epoch 006 | Train Loss: 1.5495 Acc: 0.5422 | Val Loss: 1.6171 Acc: 0.5478                                               \n",
      "Epoch 007 | Train Loss: 1.3223 Acc: 0.5666 | Val Loss: 1.1802 Acc: 0.5904                                               \n",
      "Epoch 008 | Train Loss: 1.1559 Acc: 0.5905 | Val Loss: 1.0841 Acc: 0.5621                                               \n",
      "Epoch 009 | Train Loss: 1.0276 Acc: 0.6044 | Val Loss: 1.1622 Acc: 0.5418                                               \n",
      "Epoch 010 | Train Loss: 0.9069 Acc: 0.6333 | Val Loss: 0.8713 Acc: 0.6612                                               \n",
      "Epoch 011 | Train Loss: 0.8548 Acc: 0.6532 | Val Loss: 1.0163 Acc: 0.6445                                               \n",
      "Epoch 012 | Train Loss: 0.7890 Acc: 0.6727 | Val Loss: 0.7252 Acc: 0.6758                                               \n",
      "Epoch 013 | Train Loss: 0.7354 Acc: 0.6926 | Val Loss: 0.9158 Acc: 0.5979                                               \n",
      "Epoch 014 | Train Loss: 0.6900 Acc: 0.7094 | Val Loss: 0.7136 Acc: 0.6275                                               \n",
      "Epoch 015 | Train Loss: 0.6469 Acc: 0.7278 | Val Loss: 0.6609 Acc: 0.6946                                               \n",
      "Epoch 016 | Train Loss: 0.6056 Acc: 0.7456 | Val Loss: 0.7940 Acc: 0.6848                                               \n",
      "Epoch 017 | Train Loss: 0.5727 Acc: 0.7606 | Val Loss: 0.4840 Acc: 0.7952                                               \n",
      "Epoch 018 | Train Loss: 0.5401 Acc: 0.7765 | Val Loss: 0.6158 Acc: 0.7093                                               \n",
      "Epoch 019 | Train Loss: 0.5096 Acc: 0.7933 | Val Loss: 0.5096 Acc: 0.7758                                               \n",
      "Epoch 020 | Train Loss: 0.4838 Acc: 0.8048 | Val Loss: 0.4460 Acc: 0.8185                                               \n",
      "Epoch 021 | Train Loss: 0.4554 Acc: 0.8172 | Val Loss: 0.4000 Acc: 0.8513                                               \n",
      "Epoch 022 | Train Loss: 0.4257 Acc: 0.8331 | Val Loss: 0.3950 Acc: 0.8400                                               \n",
      "Epoch 023 | Train Loss: 0.4167 Acc: 0.8348 | Val Loss: 0.3173 Acc: 0.8809                                               \n",
      "Epoch 024 | Train Loss: 0.3897 Acc: 0.8484 | Val Loss: 0.3448 Acc: 0.8564                                               \n",
      "Epoch 025 | Train Loss: 0.3601 Acc: 0.8610 | Val Loss: 0.3138 Acc: 0.8761                                               \n",
      "Epoch 026 | Train Loss: 0.3533 Acc: 0.8658 | Val Loss: 0.3536 Acc: 0.8516                                               \n",
      "Epoch 027 | Train Loss: 0.3350 Acc: 0.8747 | Val Loss: 0.2848 Acc: 0.8860                                               \n",
      "Epoch 028 | Train Loss: 0.3225 Acc: 0.8778 | Val Loss: 0.2309 Acc: 0.9128                                               \n",
      "Epoch 029 | Train Loss: 0.3096 Acc: 0.8813 | Val Loss: 0.2614 Acc: 0.8973                                               \n",
      "Epoch 030 | Train Loss: 0.2899 Acc: 0.8917 | Val Loss: 0.2455 Acc: 0.9024                                               \n",
      "Epoch 031 | Train Loss: 0.2797 Acc: 0.8987 | Val Loss: 0.1816 Acc: 0.9328                                               \n",
      "Epoch 032 | Train Loss: 0.2566 Acc: 0.9051 | Val Loss: 0.2598 Acc: 0.8958                                               \n",
      "Epoch 033 | Train Loss: 0.2470 Acc: 0.9098 | Val Loss: 0.2160 Acc: 0.9146                                               \n",
      "Epoch 034 | Train Loss: 0.2393 Acc: 0.9142 | Val Loss: 0.2168 Acc: 0.9152                                               \n",
      "Epoch 035 | Train Loss: 0.2226 Acc: 0.9187 | Val Loss: 0.1972 Acc: 0.9251                                               \n",
      "Epoch 036 | Train Loss: 0.2114 Acc: 0.9264 | Val Loss: 0.1874 Acc: 0.9272                                               \n",
      "Epoch 037 | Train Loss: 0.2041 Acc: 0.9269 | Val Loss: 0.1614 Acc: 0.9379                                               \n",
      "Epoch 038 | Train Loss: 0.1838 Acc: 0.9364 | Val Loss: 0.1598 Acc: 0.9382                                               \n",
      "Epoch 039 | Train Loss: 0.1813 Acc: 0.9354 | Val Loss: 0.2278 Acc: 0.9081                                               \n",
      "Epoch 040 | Train Loss: 0.1826 Acc: 0.9359 | Val Loss: 0.1808 Acc: 0.9343                                               \n",
      "Epoch 041 | Train Loss: 0.1727 Acc: 0.9390 | Val Loss: 0.1313 Acc: 0.9507                                               \n",
      "Epoch 042 | Train Loss: 0.1623 Acc: 0.9423 | Val Loss: 0.1334 Acc: 0.9490                                               \n",
      "Epoch 043 | Train Loss: 0.1620 Acc: 0.9440 | Val Loss: 0.1098 Acc: 0.9606                                               \n",
      "Epoch 044 | Train Loss: 0.1492 Acc: 0.9478 | Val Loss: 0.1458 Acc: 0.9478                                               \n",
      "Epoch 045 | Train Loss: 0.1489 Acc: 0.9484 | Val Loss: 0.1574 Acc: 0.9385                                               \n",
      "Epoch 046 | Train Loss: 0.1459 Acc: 0.9502 | Val Loss: 0.1147 Acc: 0.9555                                               \n",
      "Epoch 047 | Train Loss: 0.1349 Acc: 0.9544 | Val Loss: 0.1280 Acc: 0.9519                                               \n",
      "Epoch 048 | Train Loss: 0.1373 Acc: 0.9536 | Val Loss: 0.0964 Acc: 0.9633                                               \n",
      "Epoch 049 | Train Loss: 0.1252 Acc: 0.9578 | Val Loss: 0.1168 Acc: 0.9585                                               \n",
      "Epoch 050 | Train Loss: 0.1328 Acc: 0.9556 | Val Loss: 0.0928 Acc: 0.9645                                               \n",
      "Epoch 051 | Train Loss: 0.1162 Acc: 0.9609 | Val Loss: 0.1536 Acc: 0.9394                                               \n",
      "Epoch 052 | Train Loss: 0.1160 Acc: 0.9612 | Val Loss: 0.0973 Acc: 0.9645                                               \n",
      "Epoch 053 | Train Loss: 0.1120 Acc: 0.9622 | Val Loss: 0.0831 Acc: 0.9687                                               \n",
      "Epoch 054 | Train Loss: 0.1134 Acc: 0.9611 | Val Loss: 0.1655 Acc: 0.9373                                               \n",
      "Epoch 055 | Train Loss: 0.1128 Acc: 0.9622 | Val Loss: 0.0867 Acc: 0.9678                                               \n",
      "Epoch 056 | Train Loss: 0.1014 Acc: 0.9658 | Val Loss: 0.0915 Acc: 0.9648                                               \n",
      "Epoch 057 | Train Loss: 0.0978 Acc: 0.9675 | Val Loss: 0.1738 Acc: 0.9328                                               \n",
      "Epoch 058 | Train Loss: 0.0966 Acc: 0.9682 | Val Loss: 0.0906 Acc: 0.9615                                               \n",
      "Epoch 059 | Train Loss: 0.1019 Acc: 0.9646 | Val Loss: 0.0644 Acc: 0.9773                                               \n",
      "Epoch 060 | Train Loss: 0.0893 Acc: 0.9711 | Val Loss: 0.0679 Acc: 0.9752                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 128, 'cnn_dropout': 0.18137504875272353, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 32, 'cnn_kernels_2': 16, 'learning_rate': 2.2410170676601078e-05, 'lstm_dense': 256, 'lstm_hidden_size': 32, 'lstm_layers': 1, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 110.9471 Acc: 0.2854 | Val Loss: 14.1212 Acc: 0.4501                                            \n",
      "Epoch 002 | Train Loss: 67.1501 Acc: 0.3367 | Val Loss: 9.3392 Acc: 0.4875                                              \n",
      "Epoch 003 | Train Loss: 52.4435 Acc: 0.3477 | Val Loss: 5.7875 Acc: 0.4585                                              \n",
      "Epoch 004 | Train Loss: 40.8850 Acc: 0.3559 | Val Loss: 4.2662 Acc: 0.4949                                              \n",
      "Epoch 005 | Train Loss: 31.6945 Acc: 0.3668 | Val Loss: 4.1973 Acc: 0.4654                                              \n",
      "Epoch 006 | Train Loss: 25.1579 Acc: 0.3711 | Val Loss: 4.7838 Acc: 0.4636                                              \n",
      "Epoch 007 | Train Loss: 21.1568 Acc: 0.3795 | Val Loss: 4.2210 Acc: 0.4845                                              \n",
      "Epoch 008 | Train Loss: 17.9615 Acc: 0.3868 | Val Loss: 3.5389 Acc: 0.5066                                              \n",
      "Epoch 009 | Train Loss: 15.3787 Acc: 0.3959 | Val Loss: 4.3015 Acc: 0.4970                                              \n",
      "Epoch 010 | Train Loss: 13.7398 Acc: 0.4012 | Val Loss: 4.5764 Acc: 0.4875                                              \n",
      "Epoch 011 | Train Loss: 12.4072 Acc: 0.4050 | Val Loss: 3.2631 Acc: 0.5146                                              \n",
      "Epoch 012 | Train Loss: 11.5711 Acc: 0.4070 | Val Loss: 3.4424 Acc: 0.5215                                              \n",
      "Epoch 013 | Train Loss: 10.2034 Acc: 0.4120 | Val Loss: 3.4313 Acc: 0.5325                                              \n",
      "Epoch 014 | Train Loss: 9.2383 Acc: 0.4259 | Val Loss: 3.1283 Acc: 0.5451                                               \n",
      "Epoch 015 | Train Loss: 8.6969 Acc: 0.4207 | Val Loss: 2.5998 Acc: 0.5579                                               \n",
      "Epoch 016 | Train Loss: 8.0390 Acc: 0.4231 | Val Loss: 2.3592 Acc: 0.5549                                               \n",
      "Epoch 017 | Train Loss: 7.6760 Acc: 0.4370 | Val Loss: 2.3774 Acc: 0.5081                                               \n",
      "Epoch 018 | Train Loss: 6.9118 Acc: 0.4395 | Val Loss: 2.2561 Acc: 0.5612                                               \n",
      "Epoch 019 | Train Loss: 6.2848 Acc: 0.4445 | Val Loss: 2.3511 Acc: 0.5696                                               \n",
      "Epoch 020 | Train Loss: 5.9526 Acc: 0.4574 | Val Loss: 2.0703 Acc: 0.5842                                               \n",
      "Epoch 021 | Train Loss: 5.4792 Acc: 0.4501 | Val Loss: 2.0179 Acc: 0.5394                                               \n",
      "Epoch 022 | Train Loss: 5.2656 Acc: 0.4578 | Val Loss: 1.8921 Acc: 0.5681                                               \n",
      "Epoch 023 | Train Loss: 4.9361 Acc: 0.4566 | Val Loss: 1.6236 Acc: 0.5827                                               \n",
      "Epoch 024 | Train Loss: 4.6128 Acc: 0.4598 | Val Loss: 1.5193 Acc: 0.5725                                               \n",
      "Epoch 025 | Train Loss: 4.2686 Acc: 0.4741 | Val Loss: 1.3976 Acc: 0.5731                                               \n",
      "Epoch 026 | Train Loss: 4.0560 Acc: 0.4723 | Val Loss: 1.3367 Acc: 0.5803                                               \n",
      "Epoch 027 | Train Loss: 3.9543 Acc: 0.4705 | Val Loss: 1.1866 Acc: 0.6218                                               \n",
      "Epoch 028 | Train Loss: 3.6925 Acc: 0.4763 | Val Loss: 1.1395 Acc: 0.6233                                               \n",
      "Epoch 029 | Train Loss: 3.5279 Acc: 0.4768 | Val Loss: 1.1568 Acc: 0.6066                                               \n",
      "Epoch 030 | Train Loss: 3.3055 Acc: 0.4862 | Val Loss: 1.1808 Acc: 0.5913                                               \n",
      "Epoch 031 | Train Loss: 3.1857 Acc: 0.4902 | Val Loss: 1.0013 Acc: 0.6427                                               \n",
      "Epoch 032 | Train Loss: 3.0520 Acc: 0.4949 | Val Loss: 1.0050 Acc: 0.6525                                               \n",
      "Epoch 033 | Train Loss: 2.8899 Acc: 0.4943 | Val Loss: 1.0253 Acc: 0.6499                                               \n",
      "Epoch 034 | Train Loss: 2.7369 Acc: 0.4990 | Val Loss: 0.9087 Acc: 0.6633                                               \n",
      "Epoch 035 | Train Loss: 2.6594 Acc: 0.5079 | Val Loss: 0.9026 Acc: 0.6818                                               \n",
      "Epoch 036 | Train Loss: 2.5668 Acc: 0.5058 | Val Loss: 0.8698 Acc: 0.6543                                               \n",
      "Epoch 037 | Train Loss: 2.5216 Acc: 0.5032 | Val Loss: 0.8501 Acc: 0.6525                                               \n",
      "Epoch 038 | Train Loss: 2.3707 Acc: 0.5139 | Val Loss: 0.8961 Acc: 0.6113                                               \n",
      "Epoch 039 | Train Loss: 2.3461 Acc: 0.5170 | Val Loss: 0.8357 Acc: 0.6370                                               \n",
      "Epoch 040 | Train Loss: 2.1405 Acc: 0.5177 | Val Loss: 0.8247 Acc: 0.6501                                               \n",
      "Epoch 041 | Train Loss: 2.1251 Acc: 0.5193 | Val Loss: 0.8189 Acc: 0.6340                                               \n",
      "Epoch 042 | Train Loss: 1.9593 Acc: 0.5341 | Val Loss: 0.8072 Acc: 0.6549                                               \n",
      "Epoch 043 | Train Loss: 2.0260 Acc: 0.5247 | Val Loss: 0.8276 Acc: 0.6612                                               \n",
      "Epoch 044 | Train Loss: 1.9390 Acc: 0.5323 | Val Loss: 0.7993 Acc: 0.6436                                               \n",
      "Epoch 045 | Train Loss: 1.8526 Acc: 0.5327 | Val Loss: 0.7830 Acc: 0.6854                                               \n",
      "Epoch 046 | Train Loss: 1.8047 Acc: 0.5305 | Val Loss: 0.7819 Acc: 0.6794                                               \n",
      "Epoch 047 | Train Loss: 1.7689 Acc: 0.5397 | Val Loss: 0.8112 Acc: 0.6540                                               \n",
      "Epoch 048 | Train Loss: 1.7216 Acc: 0.5406 | Val Loss: 0.7603 Acc: 0.6722                                               \n",
      "Epoch 049 | Train Loss: 1.6685 Acc: 0.5498 | Val Loss: 0.7743 Acc: 0.6767                                               \n",
      "Epoch 050 | Train Loss: 1.6318 Acc: 0.5477 | Val Loss: 0.8149 Acc: 0.6624                                               \n",
      "Epoch 051 | Train Loss: 1.5437 Acc: 0.5546 | Val Loss: 0.8084 Acc: 0.6669                                               \n",
      "Epoch 052 | Train Loss: 1.5033 Acc: 0.5550 | Val Loss: 0.7572 Acc: 0.6991                                               \n",
      "Epoch 053 | Train Loss: 1.4969 Acc: 0.5612 | Val Loss: 0.7828 Acc: 0.6979                                               \n",
      "Epoch 054 | Train Loss: 1.4799 Acc: 0.5571 | Val Loss: 0.7282 Acc: 0.7191                                               \n",
      "Epoch 055 | Train Loss: 1.4508 Acc: 0.5583 | Val Loss: 0.7268 Acc: 0.7170                                               \n",
      "Epoch 056 | Train Loss: 1.3952 Acc: 0.5594 | Val Loss: 0.7067 Acc: 0.7245                                               \n",
      "Epoch 057 | Train Loss: 1.3768 Acc: 0.5691 | Val Loss: 0.7219 Acc: 0.7090                                               \n",
      "Epoch 058 | Train Loss: 1.3855 Acc: 0.5668 | Val Loss: 0.7360 Acc: 0.7197                                               \n",
      "Epoch 059 | Train Loss: 1.3114 Acc: 0.5695 | Val Loss: 0.8298 Acc: 0.6672                                               \n",
      "Epoch 060 | Train Loss: 1.2856 Acc: 0.5787 | Val Loss: 0.7546 Acc: 0.6484                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 36, 'cnn_dense': 256, 'cnn_dropout': 0.055774755214836776, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 32, 'learning_rate': 0.0024510430352706757, 'lstm_dense': 128, 'lstm_hidden_size': 128, 'lstm_layers': 6, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 5.1806 Acc: 0.4922 | Val Loss: 0.7973 Acc: 0.6110                                               \n",
      "Epoch 002 | Train Loss: 0.9135 Acc: 0.5980 | Val Loss: 0.6162 Acc: 0.7069                                               \n",
      "Epoch 003 | Train Loss: 0.7072 Acc: 0.6668 | Val Loss: 0.6065 Acc: 0.7137                                               \n",
      "Epoch 004 | Train Loss: 0.6503 Acc: 0.6992 | Val Loss: 0.6081 Acc: 0.7039                                               \n",
      "Epoch 005 | Train Loss: 0.6282 Acc: 0.7107 | Val Loss: 0.6885 Acc: 0.6409                                               \n",
      "Epoch 006 | Train Loss: 0.6097 Acc: 0.7217 | Val Loss: 0.5403 Acc: 0.7499                                               \n",
      "Epoch 007 | Train Loss: 0.5930 Acc: 0.7275 | Val Loss: 0.6678 Acc: 0.6301                                               \n",
      "Epoch 008 | Train Loss: 0.6664 Acc: 0.6998 | Val Loss: 0.7707 Acc: 0.6346                                               \n",
      "Epoch 009 | Train Loss: 0.6538 Acc: 0.6944 | Val Loss: 0.7416 Acc: 0.5704                                               \n",
      "Epoch 010 | Train Loss: 0.6986 Acc: 0.6809 | Val Loss: 0.7599 Acc: 0.6281                                               \n",
      "Epoch 011 | Train Loss: 0.6623 Acc: 0.6956 | Val Loss: 0.6780 Acc: 0.6827                                               \n",
      "Epoch 012 | Train Loss: 0.6382 Acc: 0.7051 | Val Loss: 0.7337 Acc: 0.7072                                               \n",
      "Epoch 013 | Train Loss: 0.6334 Acc: 0.7099 | Val Loss: 0.8535 Acc: 0.6194                                               \n",
      "Epoch 014 | Train Loss: 0.6769 Acc: 0.6836 | Val Loss: 0.9058 Acc: 0.5466                                               \n",
      "Epoch 015 | Train Loss: 0.6702 Acc: 0.6940 | Val Loss: 0.7249 Acc: 0.6875                                               \n",
      "Epoch 016 | Train Loss: 0.6636 Acc: 0.6939 | Val Loss: 0.7684 Acc: 0.6257                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 256, 'cnn_dropout': 0.2293251467456617, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 16, 'learning_rate': 5.6197690636196857e-05, 'lstm_dense': 32, 'lstm_hidden_size': 96, 'lstm_layers': 1, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 23.0865 Acc: 0.3505 | Val Loss: 5.8030 Acc: 0.3681                                              \n",
      "Epoch 002 | Train Loss: 8.5679 Acc: 0.3990 | Val Loss: 3.2100 Acc: 0.4696                                               \n",
      "Epoch 003 | Train Loss: 5.4978 Acc: 0.4287 | Val Loss: 1.8367 Acc: 0.5570                                               \n",
      "Epoch 004 | Train Loss: 3.8360 Acc: 0.4601 | Val Loss: 1.1260 Acc: 0.6615                                               \n",
      "Epoch 005 | Train Loss: 2.9050 Acc: 0.4792 | Val Loss: 1.1335 Acc: 0.6224                                               \n",
      "Epoch 006 | Train Loss: 2.2462 Acc: 0.5067 | Val Loss: 0.8829 Acc: 0.6507                                               \n",
      "Epoch 007 | Train Loss: 1.8333 Acc: 0.5191 | Val Loss: 1.0560 Acc: 0.6033                                               \n",
      "Epoch 008 | Train Loss: 1.5842 Acc: 0.5332 | Val Loss: 0.9291 Acc: 0.6328                                               \n",
      "Epoch 009 | Train Loss: 1.3176 Acc: 0.5485 | Val Loss: 0.8003 Acc: 0.6367                                               \n",
      "Epoch 010 | Train Loss: 1.2312 Acc: 0.5676 | Val Loss: 0.7179 Acc: 0.6570                                               \n",
      "Epoch 011 | Train Loss: 1.0835 Acc: 0.5827 | Val Loss: 0.8103 Acc: 0.6525                                               \n",
      "Epoch 012 | Train Loss: 1.0381 Acc: 0.5897 | Val Loss: 0.6295 Acc: 0.7304                                               \n",
      "Epoch 013 | Train Loss: 0.9658 Acc: 0.6067 | Val Loss: 0.6279 Acc: 0.7310                                               \n",
      "Epoch 014 | Train Loss: 0.9290 Acc: 0.6110 | Val Loss: 0.6485 Acc: 0.6949                                               \n",
      "Epoch 015 | Train Loss: 0.8643 Acc: 0.6283 | Val Loss: 0.6549 Acc: 0.6994                                               \n",
      "Epoch 016 | Train Loss: 0.8535 Acc: 0.6361 | Val Loss: 0.6107 Acc: 0.7597                                               \n",
      "Epoch 017 | Train Loss: 0.8076 Acc: 0.6472 | Val Loss: 0.5613 Acc: 0.7490                                               \n",
      "Epoch 018 | Train Loss: 0.7699 Acc: 0.6602 | Val Loss: 0.5616 Acc: 0.7743                                               \n",
      "Epoch 019 | Train Loss: 0.7492 Acc: 0.6698 | Val Loss: 0.5533 Acc: 0.7579                                               \n",
      "Epoch 020 | Train Loss: 0.7172 Acc: 0.6889 | Val Loss: 0.6073 Acc: 0.7167                                               \n",
      "Epoch 021 | Train Loss: 0.6911 Acc: 0.6988 | Val Loss: 0.5327 Acc: 0.7821                                               \n",
      "Epoch 022 | Train Loss: 0.6629 Acc: 0.7107 | Val Loss: 0.5246 Acc: 0.7570                                               \n",
      "Epoch 023 | Train Loss: 0.6433 Acc: 0.7208 | Val Loss: 0.4847 Acc: 0.8024                                               \n",
      "Epoch 024 | Train Loss: 0.6300 Acc: 0.7270 | Val Loss: 0.5148 Acc: 0.7815                                               \n",
      "Epoch 025 | Train Loss: 0.6091 Acc: 0.7421 | Val Loss: 0.4726 Acc: 0.8000                                               \n",
      "Epoch 026 | Train Loss: 0.5996 Acc: 0.7463 | Val Loss: 0.4697 Acc: 0.8003                                               \n",
      "Epoch 027 | Train Loss: 0.5648 Acc: 0.7621 | Val Loss: 0.4936 Acc: 0.7958                                               \n",
      "Epoch 028 | Train Loss: 0.5541 Acc: 0.7677 | Val Loss: 0.4611 Acc: 0.7901                                               \n",
      "Epoch 029 | Train Loss: 0.5456 Acc: 0.7743 | Val Loss: 0.4654 Acc: 0.8033                                               \n",
      "Epoch 030 | Train Loss: 0.5268 Acc: 0.7853 | Val Loss: 0.5627 Acc: 0.7439                                               \n",
      "Epoch 031 | Train Loss: 0.5016 Acc: 0.7967 | Val Loss: 0.4409 Acc: 0.8242                                               \n",
      "Epoch 032 | Train Loss: 0.4694 Acc: 0.8144 | Val Loss: 0.4439 Acc: 0.8000                                               \n",
      "Epoch 033 | Train Loss: 0.4525 Acc: 0.8184 | Val Loss: 0.4004 Acc: 0.8334                                               \n",
      "Epoch 034 | Train Loss: 0.4268 Acc: 0.8319 | Val Loss: 0.3273 Acc: 0.8684                                               \n",
      "Epoch 035 | Train Loss: 0.3952 Acc: 0.8464 | Val Loss: 0.3488 Acc: 0.8627                                               \n",
      "Epoch 036 | Train Loss: 0.3768 Acc: 0.8548 | Val Loss: 0.2850 Acc: 0.8904                                               \n",
      "Epoch 037 | Train Loss: 0.3467 Acc: 0.8666 | Val Loss: 0.2645 Acc: 0.8988                                               \n",
      "Epoch 038 | Train Loss: 0.3253 Acc: 0.8792 | Val Loss: 0.2916 Acc: 0.8830                                               \n",
      "Epoch 039 | Train Loss: 0.3051 Acc: 0.8867 | Val Loss: 0.2218 Acc: 0.9185                                               \n",
      "Epoch 040 | Train Loss: 0.2821 Acc: 0.8960 | Val Loss: 0.2386 Acc: 0.9066                                               \n",
      "Epoch 041 | Train Loss: 0.2672 Acc: 0.9020 | Val Loss: 0.2036 Acc: 0.9281                                               \n",
      "Epoch 042 | Train Loss: 0.2494 Acc: 0.9086 | Val Loss: 0.2090 Acc: 0.9224                                               \n",
      "Epoch 043 | Train Loss: 0.2307 Acc: 0.9133 | Val Loss: 0.2035 Acc: 0.9236                                               \n",
      "Epoch 044 | Train Loss: 0.2157 Acc: 0.9231 | Val Loss: 0.1780 Acc: 0.9340                                               \n",
      "Epoch 045 | Train Loss: 0.2068 Acc: 0.9236 | Val Loss: 0.1717 Acc: 0.9391                                               \n",
      "Epoch 046 | Train Loss: 0.1980 Acc: 0.9272 | Val Loss: 0.1652 Acc: 0.9412                                               \n",
      "Epoch 047 | Train Loss: 0.1839 Acc: 0.9349 | Val Loss: 0.2277 Acc: 0.9146                                               \n",
      "Epoch 048 | Train Loss: 0.1756 Acc: 0.9366 | Val Loss: 0.1581 Acc: 0.9439                                               \n",
      "Epoch 049 | Train Loss: 0.1679 Acc: 0.9408 | Val Loss: 0.1846 Acc: 0.9358                                               \n",
      "Epoch 050 | Train Loss: 0.1562 Acc: 0.9448 | Val Loss: 0.1453 Acc: 0.9501                                               \n",
      "Epoch 051 | Train Loss: 0.1511 Acc: 0.9471 | Val Loss: 0.1530 Acc: 0.9445                                               \n",
      "Epoch 052 | Train Loss: 0.1403 Acc: 0.9495 | Val Loss: 0.1313 Acc: 0.9558                                               \n",
      "Epoch 053 | Train Loss: 0.1324 Acc: 0.9537 | Val Loss: 0.1332 Acc: 0.9531                                               \n",
      "Epoch 054 | Train Loss: 0.1305 Acc: 0.9543 | Val Loss: 0.1318 Acc: 0.9519                                               \n",
      "Epoch 055 | Train Loss: 0.1169 Acc: 0.9585 | Val Loss: 0.1073 Acc: 0.9636                                               \n",
      "Epoch 056 | Train Loss: 0.1166 Acc: 0.9594 | Val Loss: 0.1111 Acc: 0.9606                                               \n",
      "Epoch 057 | Train Loss: 0.1140 Acc: 0.9598 | Val Loss: 0.0988 Acc: 0.9636                                               \n",
      "Epoch 058 | Train Loss: 0.1043 Acc: 0.9640 | Val Loss: 0.1140 Acc: 0.9588                                               \n",
      "Epoch 059 | Train Loss: 0.1035 Acc: 0.9639 | Val Loss: 0.1004 Acc: 0.9651                                               \n",
      "Epoch 060 | Train Loss: 0.0927 Acc: 0.9683 | Val Loss: 0.0941 Acc: 0.9648                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 32, 'cnn_dropout': 0.15631834827073543, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 16, 'learning_rate': 0.0002048512342776671, 'lstm_dense': 32, 'lstm_hidden_size': 32, 'lstm_layers': 4, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 30.2406 Acc: 0.3707 | Val Loss: 4.9723 Acc: 0.5149                                              \n",
      "Epoch 002 | Train Loss: 9.2922 Acc: 0.4112 | Val Loss: 2.7603 Acc: 0.4764                                               \n",
      "Epoch 003 | Train Loss: 5.1528 Acc: 0.4509 | Val Loss: 2.2182 Acc: 0.5107                                               \n",
      "Epoch 004 | Train Loss: 3.2103 Acc: 0.4863 | Val Loss: 1.7350 Acc: 0.5707                                               \n",
      "Epoch 005 | Train Loss: 2.2165 Acc: 0.5172 | Val Loss: 1.3297 Acc: 0.5475                                               \n",
      "Epoch 006 | Train Loss: 1.7011 Acc: 0.5514 | Val Loss: 0.8879 Acc: 0.6618                                               \n",
      "Epoch 007 | Train Loss: 1.3723 Acc: 0.5788 | Val Loss: 0.6890 Acc: 0.7269                                               \n",
      "Epoch 008 | Train Loss: 1.1354 Acc: 0.6206 | Val Loss: 1.1467 Acc: 0.6809                                               \n",
      "Epoch 009 | Train Loss: 0.9823 Acc: 0.6447 | Val Loss: 0.7169 Acc: 0.6624                                               \n",
      "Epoch 010 | Train Loss: 0.8296 Acc: 0.6852 | Val Loss: 0.7562 Acc: 0.6767                                               \n",
      "Epoch 011 | Train Loss: 0.7251 Acc: 0.7195 | Val Loss: 0.5582 Acc: 0.7710                                               \n",
      "Epoch 012 | Train Loss: 0.6298 Acc: 0.7576 | Val Loss: 0.5498 Acc: 0.7797                                               \n",
      "Epoch 013 | Train Loss: 0.5599 Acc: 0.7872 | Val Loss: 0.4386 Acc: 0.8296                                               \n",
      "Epoch 014 | Train Loss: 0.4975 Acc: 0.8138 | Val Loss: 0.4450 Acc: 0.8322                                               \n",
      "Epoch 015 | Train Loss: 0.4397 Acc: 0.8389 | Val Loss: 0.4352 Acc: 0.8263                                               \n",
      "Epoch 016 | Train Loss: 0.3816 Acc: 0.8629 | Val Loss: 0.2784 Acc: 0.9006                                               \n",
      "Epoch 017 | Train Loss: 0.3506 Acc: 0.8765 | Val Loss: 0.2710 Acc: 0.9048                                               \n",
      "Epoch 018 | Train Loss: 0.3282 Acc: 0.8861 | Val Loss: 0.3009 Acc: 0.8881                                               \n",
      "Epoch 019 | Train Loss: 0.3112 Acc: 0.8923 | Val Loss: 0.2491 Acc: 0.9113                                               \n",
      "Epoch 020 | Train Loss: 0.2869 Acc: 0.8981 | Val Loss: 0.2846 Acc: 0.8887                                               \n",
      "Epoch 021 | Train Loss: 0.2599 Acc: 0.9078 | Val Loss: 0.2100 Acc: 0.9358                                               \n",
      "Epoch 022 | Train Loss: 0.2403 Acc: 0.9162 | Val Loss: 0.2409 Acc: 0.9087                                               \n",
      "Epoch 023 | Train Loss: 0.2288 Acc: 0.9194 | Val Loss: 0.2302 Acc: 0.9164                                               \n",
      "Epoch 024 | Train Loss: 0.2226 Acc: 0.9240 | Val Loss: 0.1632 Acc: 0.9522                                               \n",
      "Epoch 025 | Train Loss: 0.1988 Acc: 0.9321 | Val Loss: 0.1849 Acc: 0.9391                                               \n",
      "Epoch 026 | Train Loss: 0.1958 Acc: 0.9325 | Val Loss: 0.1459 Acc: 0.9519                                               \n",
      "Epoch 027 | Train Loss: 0.1823 Acc: 0.9397 | Val Loss: 0.1499 Acc: 0.9507                                               \n",
      "Epoch 028 | Train Loss: 0.1774 Acc: 0.9417 | Val Loss: 0.1352 Acc: 0.9537                                               \n",
      "Epoch 029 | Train Loss: 0.1652 Acc: 0.9457 | Val Loss: 0.1479 Acc: 0.9507                                               \n",
      "Epoch 030 | Train Loss: 0.1566 Acc: 0.9501 | Val Loss: 0.1400 Acc: 0.9487                                               \n",
      "Epoch 031 | Train Loss: 0.1485 Acc: 0.9509 | Val Loss: 0.1055 Acc: 0.9690                                               \n",
      "Epoch 032 | Train Loss: 0.1384 Acc: 0.9531 | Val Loss: 0.2948 Acc: 0.9012                                               \n",
      "Epoch 033 | Train Loss: 0.1382 Acc: 0.9554 | Val Loss: 0.1171 Acc: 0.9651                                               \n",
      "Epoch 034 | Train Loss: 0.1351 Acc: 0.9565 | Val Loss: 0.1092 Acc: 0.9561                                               \n",
      "Epoch 035 | Train Loss: 0.1267 Acc: 0.9596 | Val Loss: 0.1607 Acc: 0.9394                                               \n",
      "Epoch 036 | Train Loss: 0.1181 Acc: 0.9625 | Val Loss: 0.1269 Acc: 0.9504                                               \n",
      "Epoch 037 | Train Loss: 0.1131 Acc: 0.9638 | Val Loss: 0.1065 Acc: 0.9594                                               \n",
      "Epoch 038 | Train Loss: 0.1161 Acc: 0.9645 | Val Loss: 0.0905 Acc: 0.9693                                               \n",
      "Epoch 039 | Train Loss: 0.1117 Acc: 0.9640 | Val Loss: 0.1329 Acc: 0.9493                                               \n",
      "Epoch 040 | Train Loss: 0.1061 Acc: 0.9675 | Val Loss: 0.0973 Acc: 0.9621                                               \n",
      "Epoch 041 | Train Loss: 0.0963 Acc: 0.9680 | Val Loss: 0.1114 Acc: 0.9699                                               \n",
      "Epoch 042 | Train Loss: 0.0948 Acc: 0.9696 | Val Loss: 0.1127 Acc: 0.9567                                               \n",
      "Epoch 043 | Train Loss: 0.0931 Acc: 0.9703 | Val Loss: 0.1068 Acc: 0.9588                                               \n",
      "Epoch 044 | Train Loss: 0.0938 Acc: 0.9703 | Val Loss: 0.1325 Acc: 0.9484                                               \n",
      "Epoch 045 | Train Loss: 0.0968 Acc: 0.9695 | Val Loss: 0.1273 Acc: 0.9510                                               \n",
      "Epoch 046 | Train Loss: 0.0897 Acc: 0.9714 | Val Loss: 0.1384 Acc: 0.9469                                               \n",
      "Epoch 047 | Train Loss: 0.0815 Acc: 0.9757 | Val Loss: 0.0903 Acc: 0.9642                                               \n",
      "Epoch 048 | Train Loss: 0.0821 Acc: 0.9734 | Val Loss: 0.0827 Acc: 0.9618                                               \n",
      "Epoch 049 | Train Loss: 0.0824 Acc: 0.9737 | Val Loss: 0.0940 Acc: 0.9731                                               \n",
      "Epoch 050 | Train Loss: 0.0792 Acc: 0.9741 | Val Loss: 0.0956 Acc: 0.9624                                               \n",
      "Epoch 051 | Train Loss: 0.0765 Acc: 0.9769 | Val Loss: 0.1153 Acc: 0.9657                                               \n",
      "Epoch 052 | Train Loss: 0.0718 Acc: 0.9766 | Val Loss: 0.0906 Acc: 0.9764                                               \n",
      "Epoch 053 | Train Loss: 0.0735 Acc: 0.9778 | Val Loss: 0.1493 Acc: 0.9466                                               \n",
      "Epoch 054 | Train Loss: 0.0738 Acc: 0.9768 | Val Loss: 0.0912 Acc: 0.9737                                               \n",
      "Epoch 055 | Train Loss: 0.0684 Acc: 0.9801 | Val Loss: 0.0817 Acc: 0.9785                                               \n",
      "Epoch 056 | Train Loss: 0.0711 Acc: 0.9785 | Val Loss: 0.0915 Acc: 0.9767                                               \n",
      "Epoch 057 | Train Loss: 0.0712 Acc: 0.9778 | Val Loss: 0.0854 Acc: 0.9743                                               \n",
      "Epoch 058 | Train Loss: 0.0640 Acc: 0.9801 | Val Loss: 0.0875 Acc: 0.9663                                               \n",
      "Epoch 059 | Train Loss: 0.0556 Acc: 0.9819 | Val Loss: 0.0886 Acc: 0.9707                                               \n",
      "Epoch 060 | Train Loss: 0.0607 Acc: 0.9813 | Val Loss: 0.0829 Acc: 0.9731                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 32, 'cnn_dropout': 0.6896632949387786, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 64, 'cnn_kernels_2': 16, 'learning_rate': 0.00016377501878479746, 'lstm_dense': 32, 'lstm_hidden_size': 96, 'lstm_layers': 5, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 13.2224 Acc: 0.3902 | Val Loss: 2.4202 Acc: 0.4893                                              \n",
      "Epoch 002 | Train Loss: 4.6391 Acc: 0.4444 | Val Loss: 1.6155 Acc: 0.5657                                               \n",
      "Epoch 003 | Train Loss: 2.7246 Acc: 0.5115 | Val Loss: 1.1211 Acc: 0.6299                                               \n",
      "Epoch 004 | Train Loss: 1.9161 Acc: 0.5547 | Val Loss: 0.7805 Acc: 0.7045                                               \n",
      "Epoch 005 | Train Loss: 1.4221 Acc: 0.6013 | Val Loss: 0.9851 Acc: 0.6722                                               \n",
      "Epoch 006 | Train Loss: 1.0864 Acc: 0.6465 | Val Loss: 0.7829 Acc: 0.6403                                               \n",
      "Epoch 007 | Train Loss: 0.9260 Acc: 0.6871 | Val Loss: 0.6199 Acc: 0.7621                                               \n",
      "Epoch 008 | Train Loss: 0.7776 Acc: 0.7189 | Val Loss: 0.6190 Acc: 0.7833                                               \n",
      "Epoch 009 | Train Loss: 0.7076 Acc: 0.7395 | Val Loss: 0.4663 Acc: 0.7964                                               \n",
      "Epoch 010 | Train Loss: 0.6484 Acc: 0.7542 | Val Loss: 0.5059 Acc: 0.8116                                               \n",
      "Epoch 011 | Train Loss: 0.5977 Acc: 0.7805 | Val Loss: 0.4072 Acc: 0.8352                                               \n",
      "Epoch 012 | Train Loss: 0.5398 Acc: 0.7973 | Val Loss: 0.5512 Acc: 0.7910                                               \n",
      "Epoch 013 | Train Loss: 0.5090 Acc: 0.8068 | Val Loss: 0.6831 Acc: 0.7943                                               \n",
      "Epoch 014 | Train Loss: 0.4824 Acc: 0.8223 | Val Loss: 0.6420 Acc: 0.7696                                               \n",
      "Epoch 015 | Train Loss: 0.4472 Acc: 0.8327 | Val Loss: 0.3921 Acc: 0.8466                                               \n",
      "Epoch 016 | Train Loss: 0.4258 Acc: 0.8401 | Val Loss: 0.3398 Acc: 0.8725                                               \n",
      "Epoch 017 | Train Loss: 0.3902 Acc: 0.8554 | Val Loss: 0.3937 Acc: 0.8516                                               \n",
      "Epoch 018 | Train Loss: 0.3818 Acc: 0.8598 | Val Loss: 0.2600 Acc: 0.9027                                               \n",
      "Epoch 019 | Train Loss: 0.3519 Acc: 0.8738 | Val Loss: 0.2715 Acc: 0.9030                                               \n",
      "Epoch 020 | Train Loss: 0.3300 Acc: 0.8812 | Val Loss: 0.2655 Acc: 0.9090                                               \n",
      "Epoch 021 | Train Loss: 0.3007 Acc: 0.8917 | Val Loss: 0.3126 Acc: 0.8836                                               \n",
      "Epoch 022 | Train Loss: 0.2908 Acc: 0.8963 | Val Loss: 0.2309 Acc: 0.9161                                               \n",
      "Epoch 023 | Train Loss: 0.2694 Acc: 0.9006 | Val Loss: 0.2679 Acc: 0.9072                                               \n",
      "Epoch 024 | Train Loss: 0.2550 Acc: 0.9089 | Val Loss: 0.2333 Acc: 0.9143                                               \n",
      "Epoch 025 | Train Loss: 0.2434 Acc: 0.9137 | Val Loss: 0.1707 Acc: 0.9388                                               \n",
      "Epoch 026 | Train Loss: 0.2242 Acc: 0.9194 | Val Loss: 0.1659 Acc: 0.9415                                               \n",
      "Epoch 027 | Train Loss: 0.2195 Acc: 0.9237 | Val Loss: 0.1995 Acc: 0.9257                                               \n",
      "Epoch 028 | Train Loss: 0.1997 Acc: 0.9305 | Val Loss: 0.1795 Acc: 0.9415                                               \n",
      "Epoch 029 | Train Loss: 0.1880 Acc: 0.9354 | Val Loss: 0.1740 Acc: 0.9296                                               \n",
      "Epoch 030 | Train Loss: 0.1890 Acc: 0.9353 | Val Loss: 0.1605 Acc: 0.9409                                               \n",
      "Epoch 031 | Train Loss: 0.1763 Acc: 0.9375 | Val Loss: 0.1650 Acc: 0.9373                                               \n",
      "Epoch 032 | Train Loss: 0.1707 Acc: 0.9408 | Val Loss: 0.1955 Acc: 0.9287                                               \n",
      "Epoch 033 | Train Loss: 0.1619 Acc: 0.9441 | Val Loss: 0.1337 Acc: 0.9525                                               \n",
      "Epoch 034 | Train Loss: 0.1519 Acc: 0.9476 | Val Loss: 0.1334 Acc: 0.9546                                               \n",
      "Epoch 035 | Train Loss: 0.1489 Acc: 0.9496 | Val Loss: 0.1331 Acc: 0.9513                                               \n",
      "Epoch 036 | Train Loss: 0.1440 Acc: 0.9498 | Val Loss: 0.1164 Acc: 0.9660                                               \n",
      "Epoch 037 | Train Loss: 0.1455 Acc: 0.9515 | Val Loss: 0.1295 Acc: 0.9466                                               \n",
      "Epoch 038 | Train Loss: 0.1275 Acc: 0.9560 | Val Loss: 0.1203 Acc: 0.9579                                               \n",
      "Epoch 039 | Train Loss: 0.1313 Acc: 0.9554 | Val Loss: 0.1312 Acc: 0.9543                                               \n",
      "Epoch 040 | Train Loss: 0.1253 Acc: 0.9597 | Val Loss: 0.1346 Acc: 0.9481                                               \n",
      "Epoch 041 | Train Loss: 0.1132 Acc: 0.9627 | Val Loss: 0.1532 Acc: 0.9373                                               \n",
      "Epoch 042 | Train Loss: 0.1171 Acc: 0.9614 | Val Loss: 0.1239 Acc: 0.9579                                               \n",
      "Epoch 043 | Train Loss: 0.1083 Acc: 0.9629 | Val Loss: 0.1188 Acc: 0.9552                                               \n",
      "Epoch 044 | Train Loss: 0.1025 Acc: 0.9651 | Val Loss: 0.1324 Acc: 0.9534                                               \n",
      "Epoch 045 | Train Loss: 0.1155 Acc: 0.9631 | Val Loss: 0.1828 Acc: 0.9296                                               \n",
      "Epoch 046 | Train Loss: 0.0977 Acc: 0.9660 | Val Loss: 0.1247 Acc: 0.9672                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 256, 'cnn_dropout': 0.535094160237936, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 16, 'learning_rate': 0.0005141861405549516, 'lstm_dense': 32, 'lstm_hidden_size': 32, 'lstm_layers': 2, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 20.4080 Acc: 0.3970 | Val Loss: 8.0431 Acc: 0.3481                                              \n",
      "Epoch 002 | Train Loss: 6.3268 Acc: 0.4166 | Val Loss: 2.5831 Acc: 0.4857                                               \n",
      "Epoch 003 | Train Loss: 3.3848 Acc: 0.4597 | Val Loss: 1.5468 Acc: 0.5242                                               \n",
      "Epoch 004 | Train Loss: 2.0536 Acc: 0.4985 | Val Loss: 1.2827 Acc: 0.5815                                               \n",
      "Epoch 005 | Train Loss: 1.3716 Acc: 0.5495 | Val Loss: 1.0200 Acc: 0.5878                                               \n",
      "Epoch 006 | Train Loss: 1.0575 Acc: 0.5900 | Val Loss: 1.0741 Acc: 0.6119                                               \n",
      "Epoch 007 | Train Loss: 0.8985 Acc: 0.6198 | Val Loss: 1.1150 Acc: 0.6104                                               \n",
      "Epoch 008 | Train Loss: 0.8279 Acc: 0.6400 | Val Loss: 0.8572 Acc: 0.6272                                               \n",
      "Epoch 009 | Train Loss: 0.7767 Acc: 0.6618 | Val Loss: 0.7278 Acc: 0.6424                                               \n",
      "Epoch 010 | Train Loss: 0.6904 Acc: 0.7059 | Val Loss: 0.5929 Acc: 0.7540                                               \n",
      "Epoch 011 | Train Loss: 0.6556 Acc: 0.7272 | Val Loss: 0.6644 Acc: 0.7125                                               \n",
      "Epoch 012 | Train Loss: 0.6246 Acc: 0.7457 | Val Loss: 0.5952 Acc: 0.7436                                               \n",
      "Epoch 013 | Train Loss: 0.5603 Acc: 0.7686 | Val Loss: 0.4773 Acc: 0.7937                                               \n",
      "Epoch 014 | Train Loss: 0.5354 Acc: 0.7847 | Val Loss: 0.4097 Acc: 0.8457                                               \n",
      "Epoch 015 | Train Loss: 0.4944 Acc: 0.8079 | Val Loss: 0.4448 Acc: 0.8316                                               \n",
      "Epoch 016 | Train Loss: 0.4627 Acc: 0.8241 | Val Loss: 0.5767 Acc: 0.7549                                               \n",
      "Epoch 017 | Train Loss: 0.4419 Acc: 0.8357 | Val Loss: 0.5663 Acc: 0.7591                                               \n",
      "Epoch 018 | Train Loss: 0.4140 Acc: 0.8436 | Val Loss: 0.5014 Acc: 0.8212                                               \n",
      "Epoch 019 | Train Loss: 0.3763 Acc: 0.8634 | Val Loss: 0.3412 Acc: 0.8710                                               \n",
      "Epoch 020 | Train Loss: 0.3613 Acc: 0.8679 | Val Loss: 0.4155 Acc: 0.8188                                               \n",
      "Epoch 021 | Train Loss: 0.3448 Acc: 0.8778 | Val Loss: 0.3395 Acc: 0.8594                                               \n",
      "Epoch 022 | Train Loss: 0.3090 Acc: 0.8910 | Val Loss: 0.2363 Acc: 0.9245                                               \n",
      "Epoch 023 | Train Loss: 0.3057 Acc: 0.8912 | Val Loss: 0.2140 Acc: 0.9284                                               \n",
      "Epoch 024 | Train Loss: 0.2840 Acc: 0.9004 | Val Loss: 0.3570 Acc: 0.8540                                               \n",
      "Epoch 025 | Train Loss: 0.2695 Acc: 0.9057 | Val Loss: 0.2159 Acc: 0.9257                                               \n",
      "Epoch 026 | Train Loss: 0.2558 Acc: 0.9131 | Val Loss: 0.3806 Acc: 0.8713                                               \n",
      "Epoch 027 | Train Loss: 0.2384 Acc: 0.9175 | Val Loss: 0.2082 Acc: 0.9275                                               \n",
      "Epoch 028 | Train Loss: 0.2224 Acc: 0.9236 | Val Loss: 0.1686 Acc: 0.9457                                               \n",
      "Epoch 029 | Train Loss: 0.2144 Acc: 0.9279 | Val Loss: 0.1555 Acc: 0.9469                                               \n",
      "Epoch 030 | Train Loss: 0.1922 Acc: 0.9323 | Val Loss: 0.1796 Acc: 0.9496                                               \n",
      "Epoch 031 | Train Loss: 0.1966 Acc: 0.9321 | Val Loss: 0.3688 Acc: 0.8716                                               \n",
      "Epoch 032 | Train Loss: 0.1763 Acc: 0.9394 | Val Loss: 0.1426 Acc: 0.9540                                               \n",
      "Epoch 033 | Train Loss: 0.1592 Acc: 0.9475 | Val Loss: 0.4830 Acc: 0.8370                                               \n",
      "Epoch 034 | Train Loss: 0.1646 Acc: 0.9462 | Val Loss: 0.1042 Acc: 0.9693                                               \n",
      "Epoch 035 | Train Loss: 0.1483 Acc: 0.9510 | Val Loss: 0.2267 Acc: 0.9263                                               \n",
      "Epoch 036 | Train Loss: 0.1339 Acc: 0.9560 | Val Loss: 0.2300 Acc: 0.9215                                               \n",
      "Epoch 037 | Train Loss: 0.1268 Acc: 0.9569 | Val Loss: 0.1344 Acc: 0.9531                                               \n",
      "Epoch 038 | Train Loss: 0.1271 Acc: 0.9571 | Val Loss: 0.0936 Acc: 0.9699                                               \n",
      "Epoch 039 | Train Loss: 0.1313 Acc: 0.9596 | Val Loss: 0.1749 Acc: 0.9304                                               \n",
      "Epoch 040 | Train Loss: 0.1139 Acc: 0.9610 | Val Loss: 0.1386 Acc: 0.9630                                               \n",
      "Epoch 041 | Train Loss: 0.1102 Acc: 0.9635 | Val Loss: 0.1105 Acc: 0.9558                                               \n",
      "Epoch 042 | Train Loss: 0.1129 Acc: 0.9624 | Val Loss: 0.1097 Acc: 0.9615                                               \n",
      "Epoch 043 | Train Loss: 0.1124 Acc: 0.9631 | Val Loss: 0.0774 Acc: 0.9767                                               \n",
      "Epoch 044 | Train Loss: 0.1074 Acc: 0.9643 | Val Loss: 0.0885 Acc: 0.9710                                               \n",
      "Epoch 045 | Train Loss: 0.0994 Acc: 0.9668 | Val Loss: 0.0605 Acc: 0.9797                                               \n",
      "Epoch 046 | Train Loss: 0.0902 Acc: 0.9701 | Val Loss: 0.0742 Acc: 0.9749                                               \n",
      "Epoch 047 | Train Loss: 0.0992 Acc: 0.9672 | Val Loss: 0.1035 Acc: 0.9681                                               \n",
      "Epoch 048 | Train Loss: 0.0939 Acc: 0.9685 | Val Loss: 0.0883 Acc: 0.9701                                               \n",
      "Epoch 049 | Train Loss: 0.0977 Acc: 0.9674 | Val Loss: 0.0688 Acc: 0.9812                                               \n",
      "Epoch 050 | Train Loss: 0.0821 Acc: 0.9731 | Val Loss: 0.0679 Acc: 0.9791                                               \n",
      "Epoch 051 | Train Loss: 0.0823 Acc: 0.9728 | Val Loss: 0.0767 Acc: 0.9788                                               \n",
      "Epoch 052 | Train Loss: 0.0957 Acc: 0.9693 | Val Loss: 0.0740 Acc: 0.9740                                               \n",
      "Epoch 053 | Train Loss: 0.0819 Acc: 0.9736 | Val Loss: 0.0670 Acc: 0.9782                                               \n",
      "Epoch 054 | Train Loss: 0.0724 Acc: 0.9765 | Val Loss: 0.0880 Acc: 0.9752                                               \n",
      "Epoch 055 | Train Loss: 0.0734 Acc: 0.9754 | Val Loss: 0.1229 Acc: 0.9591                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 256, 'cnn_dropout': 0.541463090552296, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 64, 'learning_rate': 0.0005379771991255312, 'lstm_dense': 64, 'lstm_hidden_size': 32, 'lstm_layers': 2, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 20.6363 Acc: 0.3982 | Val Loss: 8.5013 Acc: 0.4510                                              \n",
      "Epoch 002 | Train Loss: 7.2995 Acc: 0.4300 | Val Loss: 1.8026 Acc: 0.5764                                               \n",
      "Epoch 003 | Train Loss: 3.9168 Acc: 0.4640 | Val Loss: 1.7769 Acc: 0.5994                                               \n",
      "Epoch 004 | Train Loss: 2.4088 Acc: 0.4971 | Val Loss: 1.6124 Acc: 0.5827                                               \n",
      "Epoch 005 | Train Loss: 1.5378 Acc: 0.5368 | Val Loss: 1.0305 Acc: 0.5475                                               \n",
      "Epoch 006 | Train Loss: 1.0667 Acc: 0.5840 | Val Loss: 0.7206 Acc: 0.6675                                               \n",
      "Epoch 007 | Train Loss: 0.8700 Acc: 0.6211 | Val Loss: 0.9034 Acc: 0.6591                                               \n",
      "Epoch 008 | Train Loss: 0.7538 Acc: 0.6583 | Val Loss: 0.7431 Acc: 0.6839                                               \n",
      "Epoch 009 | Train Loss: 0.6688 Acc: 0.7074 | Val Loss: 0.9227 Acc: 0.6012                                               \n",
      "Epoch 010 | Train Loss: 0.6166 Acc: 0.7415 | Val Loss: 0.6432 Acc: 0.6893                                               \n",
      "Epoch 011 | Train Loss: 0.5454 Acc: 0.7785 | Val Loss: 0.4719 Acc: 0.8313                                               \n",
      "Epoch 012 | Train Loss: 0.5152 Acc: 0.7934 | Val Loss: 0.3690 Acc: 0.8484                                               \n",
      "Epoch 013 | Train Loss: 0.4650 Acc: 0.8175 | Val Loss: 0.3207 Acc: 0.8869                                               \n",
      "Epoch 014 | Train Loss: 0.4190 Acc: 0.8425 | Val Loss: 0.3303 Acc: 0.8704                                               \n",
      "Epoch 015 | Train Loss: 0.3796 Acc: 0.8575 | Val Loss: 0.6379 Acc: 0.7543                                               \n",
      "Epoch 016 | Train Loss: 0.3360 Acc: 0.8801 | Val Loss: 0.3760 Acc: 0.8513                                               \n",
      "Epoch 017 | Train Loss: 0.3156 Acc: 0.8871 | Val Loss: 0.2319 Acc: 0.9101                                               \n",
      "Epoch 018 | Train Loss: 0.2899 Acc: 0.8956 | Val Loss: 0.2881 Acc: 0.8946                                               \n",
      "Epoch 019 | Train Loss: 0.2545 Acc: 0.9104 | Val Loss: 0.1727 Acc: 0.9433                                               \n",
      "Epoch 020 | Train Loss: 0.2377 Acc: 0.9158 | Val Loss: 0.1801 Acc: 0.9352                                               \n",
      "Epoch 021 | Train Loss: 0.2075 Acc: 0.9277 | Val Loss: 0.3383 Acc: 0.8881                                               \n",
      "Epoch 022 | Train Loss: 0.1867 Acc: 0.9342 | Val Loss: 0.1220 Acc: 0.9594                                               \n",
      "Epoch 023 | Train Loss: 0.1905 Acc: 0.9345 | Val Loss: 0.0974 Acc: 0.9660                                               \n",
      "Epoch 024 | Train Loss: 0.1630 Acc: 0.9443 | Val Loss: 0.0819 Acc: 0.9725                                               \n",
      "Epoch 025 | Train Loss: 0.1618 Acc: 0.9432 | Val Loss: 0.0926 Acc: 0.9722                                               \n",
      "Epoch 026 | Train Loss: 0.1310 Acc: 0.9570 | Val Loss: 0.0674 Acc: 0.9758                                               \n",
      "Epoch 027 | Train Loss: 0.1311 Acc: 0.9569 | Val Loss: 0.1289 Acc: 0.9507                                               \n",
      "Epoch 028 | Train Loss: 0.1187 Acc: 0.9582 | Val Loss: 0.1401 Acc: 0.9448                                               \n",
      "Epoch 029 | Train Loss: 0.1358 Acc: 0.9528 | Val Loss: 0.0728 Acc: 0.9749                                               \n",
      "Epoch 030 | Train Loss: 0.1107 Acc: 0.9619 | Val Loss: 0.0664 Acc: 0.9758                                               \n",
      "Epoch 031 | Train Loss: 0.1021 Acc: 0.9651 | Val Loss: 0.0705 Acc: 0.9773                                               \n",
      "Epoch 032 | Train Loss: 0.0929 Acc: 0.9687 | Val Loss: 0.0982 Acc: 0.9690                                               \n",
      "Epoch 033 | Train Loss: 0.1018 Acc: 0.9661 | Val Loss: 0.0494 Acc: 0.9824                                               \n",
      "Epoch 034 | Train Loss: 0.0918 Acc: 0.9696 | Val Loss: 0.0753 Acc: 0.9767                                               \n",
      "Epoch 035 | Train Loss: 0.0956 Acc: 0.9672 | Val Loss: 0.0612 Acc: 0.9782                                               \n",
      "Epoch 036 | Train Loss: 0.0882 Acc: 0.9687 | Val Loss: 0.1118 Acc: 0.9651                                               \n",
      "Epoch 037 | Train Loss: 0.0918 Acc: 0.9686 | Val Loss: 0.0578 Acc: 0.9785                                               \n",
      "Epoch 038 | Train Loss: 0.0937 Acc: 0.9682 | Val Loss: 0.0888 Acc: 0.9704                                               \n",
      "Epoch 039 | Train Loss: 0.0795 Acc: 0.9704 | Val Loss: 0.0500 Acc: 0.9809                                               \n",
      "Epoch 040 | Train Loss: 0.0786 Acc: 0.9719 | Val Loss: 0.0904 Acc: 0.9678                                               \n",
      "Epoch 041 | Train Loss: 0.0793 Acc: 0.9737 | Val Loss: 0.0638 Acc: 0.9806                                               \n",
      "Epoch 042 | Train Loss: 0.0741 Acc: 0.9746 | Val Loss: 0.1008 Acc: 0.9687                                               \n",
      "Epoch 043 | Train Loss: 0.0722 Acc: 0.9759 | Val Loss: 0.0755 Acc: 0.9716                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 256, 'cnn_dropout': 0.2568047875032963, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 64, 'learning_rate': 0.009902138306856817, 'lstm_dense': 64, 'lstm_hidden_size': 32, 'lstm_layers': 4, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 36.8854 Acc: 0.4011 | Val Loss: 1.2440 Acc: 0.4421                                              \n",
      "Epoch 002 | Train Loss: 1.2654 Acc: 0.4465 | Val Loss: 1.2409 Acc: 0.4421                                               \n",
      "Epoch 003 | Train Loss: 1.1812 Acc: 0.4548 | Val Loss: 1.2424 Acc: 0.4421                                               \n",
      "Epoch 004 | Train Loss: 1.1697 Acc: 0.4624 | Val Loss: 1.2145 Acc: 0.4531                                               \n",
      "Epoch 005 | Train Loss: 1.1911 Acc: 0.4480 | Val Loss: 1.2438 Acc: 0.4421                                               \n",
      "Epoch 006 | Train Loss: 1.1885 Acc: 0.4515 | Val Loss: 1.2435 Acc: 0.4421                                               \n",
      "Epoch 007 | Train Loss: 1.2225 Acc: 0.4448 | Val Loss: 1.2438 Acc: 0.4421                                               \n",
      "Epoch 008 | Train Loss: 1.2622 Acc: 0.4477 | Val Loss: 1.2461 Acc: 0.4421                                               \n",
      "Epoch 009 | Train Loss: 1.2211 Acc: 0.4426 | Val Loss: 1.2438 Acc: 0.4421                                               \n",
      "Epoch 010 | Train Loss: 1.2603 Acc: 0.4432 | Val Loss: 1.2442 Acc: 0.4421                                               \n",
      "Epoch 011 | Train Loss: 1.3598 Acc: 0.4388 | Val Loss: 1.2274 Acc: 0.4421                                               \n",
      "Epoch 012 | Train Loss: 1.5219 Acc: 0.4397 | Val Loss: 1.2429 Acc: 0.4421                                               \n",
      "Epoch 013 | Train Loss: 1.3052 Acc: 0.4412 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 014 | Train Loss: 1.3181 Acc: 0.4420 | Val Loss: 1.2420 Acc: 0.4421                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 256, 'cnn_dropout': 0.11320855451544531, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 64, 'cnn_kernels_2': 64, 'learning_rate': 0.00044000702810728664, 'lstm_dense': 64, 'lstm_hidden_size': 32, 'lstm_layers': 3, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 22.6914 Acc: 0.4001 | Val Loss: 20.2064 Acc: 0.4421                                             \n",
      "Epoch 002 | Train Loss: 7.7838 Acc: 0.4336 | Val Loss: 1.6594 Acc: 0.5925                                               \n",
      "Epoch 003 | Train Loss: 4.8317 Acc: 0.4559 | Val Loss: 6.3461 Acc: 0.4322                                               \n",
      "Epoch 004 | Train Loss: 3.4579 Acc: 0.4667 | Val Loss: 1.9491 Acc: 0.6003                                               \n",
      "Epoch 005 | Train Loss: 2.4270 Acc: 0.4938 | Val Loss: 2.3283 Acc: 0.4824                                               \n",
      "Epoch 006 | Train Loss: 1.8012 Acc: 0.5208 | Val Loss: 1.4661 Acc: 0.4773                                               \n",
      "Epoch 007 | Train Loss: 1.5193 Acc: 0.5341 | Val Loss: 0.9052 Acc: 0.6594                                               \n",
      "Epoch 008 | Train Loss: 1.1284 Acc: 0.5747 | Val Loss: 1.0563 Acc: 0.5976                                               \n",
      "Epoch 009 | Train Loss: 0.9666 Acc: 0.5988 | Val Loss: 0.7130 Acc: 0.6687                                               \n",
      "Epoch 010 | Train Loss: 0.8565 Acc: 0.6327 | Val Loss: 0.6915 Acc: 0.6743                                               \n",
      "Epoch 011 | Train Loss: 0.7989 Acc: 0.6548 | Val Loss: 0.8082 Acc: 0.6122                                               \n",
      "Epoch 012 | Train Loss: 0.7490 Acc: 0.6792 | Val Loss: 0.7705 Acc: 0.6672                                               \n",
      "Epoch 013 | Train Loss: 0.7168 Acc: 0.6908 | Val Loss: 0.6351 Acc: 0.7131                                               \n",
      "Epoch 014 | Train Loss: 0.6851 Acc: 0.7053 | Val Loss: 0.5287 Acc: 0.8012                                               \n",
      "Epoch 015 | Train Loss: 0.6506 Acc: 0.7174 | Val Loss: 0.6718 Acc: 0.6800                                               \n",
      "Epoch 016 | Train Loss: 0.6265 Acc: 0.7236 | Val Loss: 0.7692 Acc: 0.6734                                               \n",
      "Epoch 017 | Train Loss: 0.5874 Acc: 0.7454 | Val Loss: 0.4718 Acc: 0.8099                                               \n",
      "Epoch 018 | Train Loss: 0.5556 Acc: 0.7732 | Val Loss: 0.5789 Acc: 0.7594                                               \n",
      "Epoch 019 | Train Loss: 0.5210 Acc: 0.7963 | Val Loss: 0.4058 Acc: 0.8528                                               \n",
      "Epoch 020 | Train Loss: 0.4833 Acc: 0.8166 | Val Loss: 0.4369 Acc: 0.8427                                               \n",
      "Epoch 021 | Train Loss: 0.4482 Acc: 0.8311 | Val Loss: 0.4718 Acc: 0.7907                                               \n",
      "Epoch 022 | Train Loss: 0.4031 Acc: 0.8471 | Val Loss: 0.3834 Acc: 0.8472                                               \n",
      "Epoch 023 | Train Loss: 0.3830 Acc: 0.8550 | Val Loss: 0.2380 Acc: 0.9170                                               \n",
      "Epoch 024 | Train Loss: 0.3493 Acc: 0.8657 | Val Loss: 0.3412 Acc: 0.8681                                               \n",
      "Epoch 025 | Train Loss: 0.3054 Acc: 0.8888 | Val Loss: 0.2326 Acc: 0.9161                                               \n",
      "Epoch 026 | Train Loss: 0.2860 Acc: 0.8960 | Val Loss: 0.2905 Acc: 0.8949                                               \n",
      "Epoch 027 | Train Loss: 0.2370 Acc: 0.9154 | Val Loss: 0.2236 Acc: 0.9227                                               \n",
      "Epoch 028 | Train Loss: 0.2193 Acc: 0.9218 | Val Loss: 0.1783 Acc: 0.9325                                               \n",
      "Epoch 029 | Train Loss: 0.1981 Acc: 0.9307 | Val Loss: 0.1351 Acc: 0.9573                                               \n",
      "Epoch 030 | Train Loss: 0.1594 Acc: 0.9457 | Val Loss: 0.1509 Acc: 0.9481                                               \n",
      "Epoch 031 | Train Loss: 0.1521 Acc: 0.9489 | Val Loss: 0.1553 Acc: 0.9501                                               \n",
      "Epoch 032 | Train Loss: 0.1393 Acc: 0.9524 | Val Loss: 0.1121 Acc: 0.9609                                               \n",
      "Epoch 033 | Train Loss: 0.1408 Acc: 0.9534 | Val Loss: 0.3115 Acc: 0.8973                                               \n",
      "Epoch 034 | Train Loss: 0.1122 Acc: 0.9628 | Val Loss: 0.1051 Acc: 0.9645                                               \n",
      "Epoch 035 | Train Loss: 0.1092 Acc: 0.9662 | Val Loss: 0.0869 Acc: 0.9699                                               \n",
      "Epoch 036 | Train Loss: 0.0967 Acc: 0.9687 | Val Loss: 0.1364 Acc: 0.9522                                               \n",
      "Epoch 037 | Train Loss: 0.0975 Acc: 0.9675 | Val Loss: 0.0745 Acc: 0.9725                                               \n",
      "Epoch 038 | Train Loss: 0.0843 Acc: 0.9732 | Val Loss: 0.2491 Acc: 0.9230                                               \n",
      "Epoch 039 | Train Loss: 0.0788 Acc: 0.9742 | Val Loss: 0.0727 Acc: 0.9734                                               \n",
      "Epoch 040 | Train Loss: 0.0814 Acc: 0.9725 | Val Loss: 0.0795 Acc: 0.9773                                               \n",
      "Epoch 041 | Train Loss: 0.0680 Acc: 0.9781 | Val Loss: 0.1089 Acc: 0.9645                                               \n",
      "Epoch 042 | Train Loss: 0.0611 Acc: 0.9809 | Val Loss: 0.0724 Acc: 0.9764                                               \n",
      "Epoch 043 | Train Loss: 0.0658 Acc: 0.9784 | Val Loss: 0.1214 Acc: 0.9666                                               \n",
      "Epoch 044 | Train Loss: 0.0509 Acc: 0.9837 | Val Loss: 0.1269 Acc: 0.9597                                               \n",
      "Epoch 045 | Train Loss: 0.0596 Acc: 0.9819 | Val Loss: 0.0628 Acc: 0.9788                                               \n",
      "Epoch 046 | Train Loss: 0.0577 Acc: 0.9807 | Val Loss: 0.1069 Acc: 0.9684                                               \n",
      "Epoch 047 | Train Loss: 0.0435 Acc: 0.9867 | Val Loss: 0.0597 Acc: 0.9806                                               \n",
      "Epoch 048 | Train Loss: 0.0625 Acc: 0.9828 | Val Loss: 0.0586 Acc: 0.9836                                               \n",
      "Epoch 049 | Train Loss: 0.0402 Acc: 0.9873 | Val Loss: 0.0737 Acc: 0.9794                                               \n",
      "Epoch 050 | Train Loss: 0.0447 Acc: 0.9870 | Val Loss: 0.0952 Acc: 0.9690                                               \n",
      "Epoch 051 | Train Loss: 0.0359 Acc: 0.9895 | Val Loss: 0.0728 Acc: 0.9761                                               \n",
      "Epoch 052 | Train Loss: 0.0426 Acc: 0.9861 | Val Loss: 0.0601 Acc: 0.9839                                               \n",
      "Epoch 053 | Train Loss: 0.0393 Acc: 0.9869 | Val Loss: 0.0582 Acc: 0.9827                                               \n",
      "Epoch 054 | Train Loss: 0.0392 Acc: 0.9880 | Val Loss: 0.0608 Acc: 0.9818                                               \n",
      "Epoch 055 | Train Loss: 0.0339 Acc: 0.9892 | Val Loss: 0.1044 Acc: 0.9707                                               \n",
      "Epoch 056 | Train Loss: 0.0329 Acc: 0.9904 | Val Loss: 0.0531 Acc: 0.9851                                               \n",
      "Epoch 057 | Train Loss: 0.0372 Acc: 0.9881 | Val Loss: 0.1309 Acc: 0.9603                                               \n",
      "Epoch 058 | Train Loss: 0.0306 Acc: 0.9903 | Val Loss: 0.3336 Acc: 0.9191                                               \n",
      "Epoch 059 | Train Loss: 0.0338 Acc: 0.9894 | Val Loss: 0.0564 Acc: 0.9854                                               \n",
      "Epoch 060 | Train Loss: 0.0263 Acc: 0.9915 | Val Loss: 0.0959 Acc: 0.9773                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 256, 'cnn_dropout': 0.004085020409211548, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 64, 'learning_rate': 0.0006699586365630455, 'lstm_dense': 64, 'lstm_hidden_size': 32, 'lstm_layers': 2, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 11.7073 Acc: 0.4159 | Val Loss: 2.8436 Acc: 0.4863                                              \n",
      "Epoch 002 | Train Loss: 2.7224 Acc: 0.4808 | Val Loss: 1.1488 Acc: 0.5758                                               \n",
      "Epoch 003 | Train Loss: 1.4726 Acc: 0.5332 | Val Loss: 0.9235 Acc: 0.6525                                               \n",
      "Epoch 004 | Train Loss: 1.0321 Acc: 0.5872 | Val Loss: 0.8208 Acc: 0.6445                                               \n",
      "Epoch 005 | Train Loss: 0.8319 Acc: 0.6359 | Val Loss: 0.6028 Acc: 0.7481                                               \n",
      "Epoch 006 | Train Loss: 0.7359 Acc: 0.6714 | Val Loss: 0.6785 Acc: 0.6791                                               \n",
      "Epoch 007 | Train Loss: 0.6777 Acc: 0.6998 | Val Loss: 0.4797 Acc: 0.8481                                               \n",
      "Epoch 008 | Train Loss: 0.6265 Acc: 0.7337 | Val Loss: 0.5898 Acc: 0.7540                                               \n",
      "Epoch 009 | Train Loss: 0.5860 Acc: 0.7515 | Val Loss: 0.4109 Acc: 0.8266                                               \n",
      "Epoch 010 | Train Loss: 0.5196 Acc: 0.7829 | Val Loss: 0.6296 Acc: 0.7188                                               \n",
      "Epoch 011 | Train Loss: 0.4702 Acc: 0.8123 | Val Loss: 0.2921 Acc: 0.9215                                               \n",
      "Epoch 012 | Train Loss: 0.4113 Acc: 0.8452 | Val Loss: 0.3888 Acc: 0.8236                                               \n",
      "Epoch 013 | Train Loss: 0.3578 Acc: 0.8739 | Val Loss: 0.5117 Acc: 0.8155                                               \n",
      "Epoch 014 | Train Loss: 0.3219 Acc: 0.8889 | Val Loss: 0.3326 Acc: 0.8651                                               \n",
      "Epoch 015 | Train Loss: 0.2878 Acc: 0.9045 | Val Loss: 0.2002 Acc: 0.9310                                               \n",
      "Epoch 016 | Train Loss: 0.2550 Acc: 0.9174 | Val Loss: 0.1375 Acc: 0.9630                                               \n",
      "Epoch 017 | Train Loss: 0.2294 Acc: 0.9245 | Val Loss: 0.1709 Acc: 0.9451                                               \n",
      "Epoch 018 | Train Loss: 0.1944 Acc: 0.9351 | Val Loss: 0.1085 Acc: 0.9690                                               \n",
      "Epoch 019 | Train Loss: 0.1805 Acc: 0.9429 | Val Loss: 0.1385 Acc: 0.9528                                               \n",
      "Epoch 020 | Train Loss: 0.1418 Acc: 0.9532 | Val Loss: 0.0829 Acc: 0.9800                                               \n",
      "Epoch 021 | Train Loss: 0.1469 Acc: 0.9501 | Val Loss: 0.1310 Acc: 0.9496                                               \n",
      "Epoch 022 | Train Loss: 0.1422 Acc: 0.9551 | Val Loss: 0.0914 Acc: 0.9731                                               \n",
      "Epoch 023 | Train Loss: 0.1068 Acc: 0.9657 | Val Loss: 0.0917 Acc: 0.9734                                               \n",
      "Epoch 024 | Train Loss: 0.1056 Acc: 0.9658 | Val Loss: 0.0931 Acc: 0.9651                                               \n",
      "Epoch 025 | Train Loss: 0.0855 Acc: 0.9712 | Val Loss: 0.0577 Acc: 0.9842                                               \n",
      "Epoch 026 | Train Loss: 0.0869 Acc: 0.9733 | Val Loss: 0.0614 Acc: 0.9809                                               \n",
      "Epoch 027 | Train Loss: 0.0832 Acc: 0.9737 | Val Loss: 0.0672 Acc: 0.9824                                               \n",
      "Epoch 028 | Train Loss: 0.0742 Acc: 0.9745 | Val Loss: 0.0757 Acc: 0.9761                                               \n",
      "Epoch 029 | Train Loss: 0.0761 Acc: 0.9765 | Val Loss: 0.0637 Acc: 0.9797                                               \n",
      "Epoch 030 | Train Loss: 0.0660 Acc: 0.9797 | Val Loss: 0.1392 Acc: 0.9591                                               \n",
      "Epoch 031 | Train Loss: 0.0630 Acc: 0.9805 | Val Loss: 0.0697 Acc: 0.9749                                               \n",
      "Epoch 032 | Train Loss: 0.0668 Acc: 0.9811 | Val Loss: 0.1110 Acc: 0.9645                                               \n",
      "Epoch 033 | Train Loss: 0.0550 Acc: 0.9816 | Val Loss: 0.0856 Acc: 0.9716                                               \n",
      "Epoch 034 | Train Loss: 0.0688 Acc: 0.9784 | Val Loss: 0.0848 Acc: 0.9704                                               \n",
      "Epoch 035 | Train Loss: 0.0507 Acc: 0.9843 | Val Loss: 0.0545 Acc: 0.9806                                               \n",
      "Epoch 036 | Train Loss: 0.0427 Acc: 0.9866 | Val Loss: 0.0480 Acc: 0.9872                                               \n",
      "Epoch 037 | Train Loss: 0.0515 Acc: 0.9829 | Val Loss: 0.0954 Acc: 0.9722                                               \n",
      "Epoch 038 | Train Loss: 0.0525 Acc: 0.9841 | Val Loss: 0.0559 Acc: 0.9833                                               \n",
      "Epoch 039 | Train Loss: 0.0397 Acc: 0.9881 | Val Loss: 0.0799 Acc: 0.9734                                               \n",
      "Epoch 040 | Train Loss: 0.0371 Acc: 0.9887 | Val Loss: 0.0497 Acc: 0.9857                                               \n",
      "Epoch 041 | Train Loss: 0.0506 Acc: 0.9845 | Val Loss: 0.0529 Acc: 0.9839                                               \n",
      "Epoch 042 | Train Loss: 0.0319 Acc: 0.9899 | Val Loss: 0.0510 Acc: 0.9845                                               \n",
      "Epoch 043 | Train Loss: 0.0360 Acc: 0.9896 | Val Loss: 0.0873 Acc: 0.9746                                               \n",
      "Epoch 044 | Train Loss: 0.0326 Acc: 0.9904 | Val Loss: 0.0534 Acc: 0.9854                                               \n",
      "Epoch 045 | Train Loss: 0.0452 Acc: 0.9865 | Val Loss: 0.0413 Acc: 0.9854                                               \n",
      "Epoch 046 | Train Loss: 0.0335 Acc: 0.9895 | Val Loss: 0.6017 Acc: 0.8472                                               \n",
      "Epoch 047 | Train Loss: 0.0304 Acc: 0.9913 | Val Loss: 0.0514 Acc: 0.9866                                               \n",
      "Epoch 048 | Train Loss: 0.0356 Acc: 0.9902 | Val Loss: 0.0979 Acc: 0.9675                                               \n",
      "Epoch 049 | Train Loss: 0.0342 Acc: 0.9893 | Val Loss: 0.0605 Acc: 0.9827                                               \n",
      "Epoch 050 | Train Loss: 0.0337 Acc: 0.9897 | Val Loss: 0.0557 Acc: 0.9857                                               \n",
      "Epoch 051 | Train Loss: 0.0309 Acc: 0.9905 | Val Loss: 0.0777 Acc: 0.9785                                               \n",
      "Epoch 052 | Train Loss: 0.0287 Acc: 0.9914 | Val Loss: 0.0484 Acc: 0.9851                                               \n",
      "Epoch 053 | Train Loss: 0.0286 Acc: 0.9916 | Val Loss: 0.0499 Acc: 0.9869                                               \n",
      "Epoch 054 | Train Loss: 0.0300 Acc: 0.9898 | Val Loss: 0.0541 Acc: 0.9854                                               \n",
      "Epoch 055 | Train Loss: 0.0278 Acc: 0.9908 | Val Loss: 0.0655 Acc: 0.9827                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 256, 'cnn_dropout': 0.04247755911091164, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 64, 'learning_rate': 0.002675874607660569, 'lstm_dense': 64, 'lstm_hidden_size': 32, 'lstm_layers': 5, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 16.1933 Acc: 0.4290 | Val Loss: 1.0259 Acc: 0.5307                                              \n",
      "Epoch 002 | Train Loss: 1.2565 Acc: 0.5289 | Val Loss: 1.0826 Acc: 0.5499                                               \n",
      "Epoch 003 | Train Loss: 0.8817 Acc: 0.6009 | Val Loss: 0.7184 Acc: 0.6576                                               \n",
      "Epoch 004 | Train Loss: 0.8197 Acc: 0.6265 | Val Loss: 0.8018 Acc: 0.6191                                               \n",
      "Epoch 005 | Train Loss: 0.7572 Acc: 0.6469 | Val Loss: 0.9068 Acc: 0.5755                                               \n",
      "Epoch 006 | Train Loss: 0.7337 Acc: 0.6568 | Val Loss: 0.6348 Acc: 0.6884                                               \n",
      "Epoch 007 | Train Loss: 0.7379 Acc: 0.6592 | Val Loss: 0.6868 Acc: 0.6618                                               \n",
      "Epoch 008 | Train Loss: 0.7196 Acc: 0.6687 | Val Loss: 0.6989 Acc: 0.6976                                               \n",
      "Epoch 009 | Train Loss: 0.7384 Acc: 0.6527 | Val Loss: 0.7014 Acc: 0.7203                                               \n",
      "Epoch 010 | Train Loss: 0.7661 Acc: 0.6504 | Val Loss: 0.6642 Acc: 0.7233                                               \n",
      "Epoch 011 | Train Loss: 0.7345 Acc: 0.6699 | Val Loss: 0.5826 Acc: 0.7958                                               \n",
      "Epoch 012 | Train Loss: 0.6873 Acc: 0.6837 | Val Loss: 0.6915 Acc: 0.6901                                               \n",
      "Epoch 013 | Train Loss: 0.6901 Acc: 0.6862 | Val Loss: 0.7090 Acc: 0.6433                                               \n",
      "Epoch 014 | Train Loss: 0.6742 Acc: 0.6951 | Val Loss: 0.6289 Acc: 0.6794                                               \n",
      "Epoch 015 | Train Loss: 0.6714 Acc: 0.6970 | Val Loss: 0.9435 Acc: 0.5904                                               \n",
      "Epoch 016 | Train Loss: 0.6853 Acc: 0.6890 | Val Loss: 0.6738 Acc: 0.6000                                               \n",
      "Epoch 017 | Train Loss: 0.6691 Acc: 0.6992 | Val Loss: 0.7789 Acc: 0.6433                                               \n",
      "Epoch 018 | Train Loss: 0.6390 Acc: 0.7140 | Val Loss: 0.6591 Acc: 0.7021                                               \n",
      "Epoch 019 | Train Loss: 0.6614 Acc: 0.6996 | Val Loss: 0.7324 Acc: 0.6385                                               \n",
      "Epoch 020 | Train Loss: 0.6856 Acc: 0.6948 | Val Loss: 0.7311 Acc: 0.6725                                               \n",
      "Epoch 021 | Train Loss: 0.6791 Acc: 0.7000 | Val Loss: 0.7534 Acc: 0.6749                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 128, 'cnn_dropout': 0.20055581180819132, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 64, 'learning_rate': 0.0002687185644049404, 'lstm_dense': 128, 'lstm_hidden_size': 32, 'lstm_layers': 5, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 17.6404 Acc: 0.4198 | Val Loss: 4.3567 Acc: 0.4585                                              \n",
      "Epoch 002 | Train Loss: 5.1807 Acc: 0.4572 | Val Loss: 1.5343 Acc: 0.5699                                               \n",
      "Epoch 003 | Train Loss: 3.4217 Acc: 0.4727 | Val Loss: 1.7391 Acc: 0.5675                                               \n",
      "Epoch 004 | Train Loss: 2.4397 Acc: 0.4988 | Val Loss: 1.7155 Acc: 0.5776                                               \n",
      "Epoch 005 | Train Loss: 1.8885 Acc: 0.5229 | Val Loss: 1.3523 Acc: 0.5821                                               \n",
      "Epoch 006 | Train Loss: 1.5617 Acc: 0.5427 | Val Loss: 0.8726 Acc: 0.6301                                               \n",
      "Epoch 007 | Train Loss: 1.3381 Acc: 0.5721 | Val Loss: 0.6856 Acc: 0.7191                                               \n",
      "Epoch 008 | Train Loss: 1.1307 Acc: 0.6133 | Val Loss: 1.0593 Acc: 0.6472                                               \n",
      "Epoch 009 | Train Loss: 1.0338 Acc: 0.6368 | Val Loss: 0.8819 Acc: 0.6633                                               \n",
      "Epoch 010 | Train Loss: 0.8905 Acc: 0.6782 | Val Loss: 0.6186 Acc: 0.7594                                               \n",
      "Epoch 011 | Train Loss: 0.8216 Acc: 0.6948 | Val Loss: 0.8671 Acc: 0.6687                                               \n",
      "Epoch 012 | Train Loss: 0.7295 Acc: 0.7245 | Val Loss: 0.7673 Acc: 0.6869                                               \n",
      "Epoch 013 | Train Loss: 0.6794 Acc: 0.7430 | Val Loss: 0.5294 Acc: 0.7743                                               \n",
      "Epoch 014 | Train Loss: 0.6226 Acc: 0.7657 | Val Loss: 0.4557 Acc: 0.8376                                               \n",
      "Epoch 015 | Train Loss: 0.5632 Acc: 0.7903 | Val Loss: 0.5858 Acc: 0.7618                                               \n",
      "Epoch 016 | Train Loss: 0.5086 Acc: 0.8083 | Val Loss: 0.4421 Acc: 0.8296                                               \n",
      "Epoch 017 | Train Loss: 0.4659 Acc: 0.8334 | Val Loss: 0.5227 Acc: 0.7973                                               \n",
      "Epoch 018 | Train Loss: 0.3956 Acc: 0.8597 | Val Loss: 0.3717 Acc: 0.8722                                               \n",
      "Epoch 019 | Train Loss: 0.3632 Acc: 0.8732 | Val Loss: 0.2679 Acc: 0.9015                                               \n",
      "Epoch 020 | Train Loss: 0.3325 Acc: 0.8830 | Val Loss: 0.2784 Acc: 0.9051                                               \n",
      "Epoch 021 | Train Loss: 0.3018 Acc: 0.8969 | Val Loss: 0.3527 Acc: 0.8740                                               \n",
      "Epoch 022 | Train Loss: 0.2866 Acc: 0.9070 | Val Loss: 0.4837 Acc: 0.8409                                               \n",
      "Epoch 023 | Train Loss: 0.2440 Acc: 0.9173 | Val Loss: 0.2063 Acc: 0.9257                                               \n",
      "Epoch 024 | Train Loss: 0.2297 Acc: 0.9231 | Val Loss: 0.2211 Acc: 0.9197                                               \n",
      "Epoch 025 | Train Loss: 0.2138 Acc: 0.9291 | Val Loss: 0.2276 Acc: 0.9272                                               \n",
      "Epoch 026 | Train Loss: 0.1913 Acc: 0.9356 | Val Loss: 0.1896 Acc: 0.9322                                               \n",
      "Epoch 027 | Train Loss: 0.1818 Acc: 0.9399 | Val Loss: 0.2820 Acc: 0.8919                                               \n",
      "Epoch 028 | Train Loss: 0.1787 Acc: 0.9410 | Val Loss: 0.2254 Acc: 0.9194                                               \n",
      "Epoch 029 | Train Loss: 0.1576 Acc: 0.9499 | Val Loss: 0.1185 Acc: 0.9612                                               \n",
      "Epoch 030 | Train Loss: 0.1395 Acc: 0.9527 | Val Loss: 0.3449 Acc: 0.8875                                               \n",
      "Epoch 031 | Train Loss: 0.1507 Acc: 0.9521 | Val Loss: 0.1202 Acc: 0.9588                                               \n",
      "Epoch 032 | Train Loss: 0.1289 Acc: 0.9571 | Val Loss: 0.1250 Acc: 0.9627                                               \n",
      "Epoch 033 | Train Loss: 0.1247 Acc: 0.9566 | Val Loss: 0.1177 Acc: 0.9561                                               \n",
      "Epoch 034 | Train Loss: 0.1119 Acc: 0.9623 | Val Loss: 0.1275 Acc: 0.9585                                               \n",
      "Epoch 035 | Train Loss: 0.1120 Acc: 0.9631 | Val Loss: 0.1186 Acc: 0.9636                                               \n",
      "Epoch 036 | Train Loss: 0.1025 Acc: 0.9663 | Val Loss: 0.1666 Acc: 0.9448                                               \n",
      "Epoch 037 | Train Loss: 0.0939 Acc: 0.9681 | Val Loss: 0.1074 Acc: 0.9591                                               \n",
      "Epoch 038 | Train Loss: 0.0958 Acc: 0.9693 | Val Loss: 0.1070 Acc: 0.9696                                               \n",
      "Epoch 039 | Train Loss: 0.0944 Acc: 0.9713 | Val Loss: 0.0840 Acc: 0.9725                                               \n",
      "Epoch 040 | Train Loss: 0.0833 Acc: 0.9729 | Val Loss: 0.0773 Acc: 0.9719                                               \n",
      "Epoch 041 | Train Loss: 0.0891 Acc: 0.9710 | Val Loss: 0.1112 Acc: 0.9657                                               \n",
      "Epoch 042 | Train Loss: 0.0682 Acc: 0.9763 | Val Loss: 0.1055 Acc: 0.9663                                               \n",
      "Epoch 043 | Train Loss: 0.0734 Acc: 0.9775 | Val Loss: 0.1186 Acc: 0.9621                                               \n",
      "Epoch 044 | Train Loss: 0.0715 Acc: 0.9770 | Val Loss: 0.0868 Acc: 0.9725                                               \n",
      "Epoch 045 | Train Loss: 0.0616 Acc: 0.9786 | Val Loss: 0.0791 Acc: 0.9767                                               \n",
      "Epoch 046 | Train Loss: 0.0614 Acc: 0.9790 | Val Loss: 0.0761 Acc: 0.9746                                               \n",
      "Epoch 047 | Train Loss: 0.0694 Acc: 0.9778 | Val Loss: 0.0893 Acc: 0.9746                                               \n",
      "Epoch 048 | Train Loss: 0.0582 Acc: 0.9790 | Val Loss: 0.0822 Acc: 0.9764                                               \n",
      "Epoch 049 | Train Loss: 0.0568 Acc: 0.9819 | Val Loss: 0.1063 Acc: 0.9687                                               \n",
      "Epoch 050 | Train Loss: 0.0555 Acc: 0.9798 | Val Loss: 0.1049 Acc: 0.9693                                               \n",
      "Epoch 051 | Train Loss: 0.0540 Acc: 0.9822 | Val Loss: 0.0758 Acc: 0.9767                                               \n",
      "Epoch 052 | Train Loss: 0.0558 Acc: 0.9820 | Val Loss: 0.0997 Acc: 0.9731                                               \n",
      "Epoch 053 | Train Loss: 0.0516 Acc: 0.9817 | Val Loss: 0.0847 Acc: 0.9752                                               \n",
      "Epoch 054 | Train Loss: 0.0419 Acc: 0.9858 | Val Loss: 0.0934 Acc: 0.9773                                               \n",
      "Epoch 055 | Train Loss: 0.0430 Acc: 0.9844 | Val Loss: 0.0624 Acc: 0.9818                                               \n",
      "Epoch 056 | Train Loss: 0.0483 Acc: 0.9846 | Val Loss: 0.0587 Acc: 0.9839                                               \n",
      "Epoch 057 | Train Loss: 0.0454 Acc: 0.9844 | Val Loss: 0.0605 Acc: 0.9830                                               \n",
      "Epoch 058 | Train Loss: 0.0427 Acc: 0.9868 | Val Loss: 0.0644 Acc: 0.9827                                               \n",
      "Epoch 059 | Train Loss: 0.0400 Acc: 0.9872 | Val Loss: 0.1556 Acc: 0.9609                                               \n",
      "Epoch 060 | Train Loss: 0.0358 Acc: 0.9889 | Val Loss: 0.1112 Acc: 0.9743                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 64, 'cnn_dropout': 0.11938455510912643, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 64, 'cnn_kernels_2': 64, 'learning_rate': 0.0009655651004955516, 'lstm_dense': 64, 'lstm_hidden_size': 64, 'lstm_layers': 2, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 14.3695 Acc: 0.4239 | Val Loss: 2.7706 Acc: 0.5758                                              \n",
      "Epoch 002 | Train Loss: 3.0729 Acc: 0.4894 | Val Loss: 1.5161 Acc: 0.6236                                               \n",
      "Epoch 003 | Train Loss: 1.3773 Acc: 0.5465 | Val Loss: 0.7908 Acc: 0.7066                                               \n",
      "Epoch 004 | Train Loss: 0.9160 Acc: 0.6084 | Val Loss: 0.9351 Acc: 0.5973                                               \n",
      "Epoch 005 | Train Loss: 0.8048 Acc: 0.6442 | Val Loss: 0.5921 Acc: 0.7466                                               \n",
      "Epoch 006 | Train Loss: 0.6953 Acc: 0.6861 | Val Loss: 0.5462 Acc: 0.7478                                               \n",
      "Epoch 007 | Train Loss: 0.6603 Acc: 0.7038 | Val Loss: 0.5289 Acc: 0.7746                                               \n",
      "Epoch 008 | Train Loss: 0.6214 Acc: 0.7243 | Val Loss: 0.4933 Acc: 0.8137                                               \n",
      "Epoch 009 | Train Loss: 0.5805 Acc: 0.7379 | Val Loss: 0.4910 Acc: 0.8033                                               \n",
      "Epoch 010 | Train Loss: 0.5826 Acc: 0.7413 | Val Loss: 0.5660 Acc: 0.7669                                               \n",
      "Epoch 011 | Train Loss: 0.5323 Acc: 0.7600 | Val Loss: 0.4790 Acc: 0.7878                                               \n",
      "Epoch 012 | Train Loss: 0.5224 Acc: 0.7673 | Val Loss: 0.5752 Acc: 0.7516                                               \n",
      "Epoch 013 | Train Loss: 0.5240 Acc: 0.7736 | Val Loss: 0.4900 Acc: 0.8012                                               \n",
      "Epoch 014 | Train Loss: 0.4895 Acc: 0.7839 | Val Loss: 0.5271 Acc: 0.7860                                               \n",
      "Epoch 015 | Train Loss: 0.4618 Acc: 0.7979 | Val Loss: 0.4641 Acc: 0.7657                                               \n",
      "Epoch 016 | Train Loss: 0.4796 Acc: 0.7880 | Val Loss: 0.4249 Acc: 0.8293                                               \n",
      "Epoch 017 | Train Loss: 0.4792 Acc: 0.7874 | Val Loss: 0.5017 Acc: 0.8418                                               \n",
      "Epoch 018 | Train Loss: 0.4649 Acc: 0.7960 | Val Loss: 0.4808 Acc: 0.8131                                               \n",
      "Epoch 019 | Train Loss: 0.4724 Acc: 0.7975 | Val Loss: 0.5052 Acc: 0.7254                                               \n",
      "Epoch 020 | Train Loss: 0.4438 Acc: 0.8068 | Val Loss: 0.4523 Acc: 0.8173                                               \n",
      "Epoch 021 | Train Loss: 0.4372 Acc: 0.8104 | Val Loss: 0.5293 Acc: 0.8212                                               \n",
      "Epoch 022 | Train Loss: 0.4383 Acc: 0.8087 | Val Loss: 0.5309 Acc: 0.7131                                               \n",
      "Epoch 023 | Train Loss: 0.4402 Acc: 0.8082 | Val Loss: 0.5092 Acc: 0.8039                                               \n",
      "Epoch 024 | Train Loss: 0.4173 Acc: 0.8171 | Val Loss: 0.4725 Acc: 0.8487                                               \n",
      "Epoch 025 | Train Loss: 0.4222 Acc: 0.8166 | Val Loss: 0.4394 Acc: 0.8176                                               \n",
      "Epoch 026 | Train Loss: 0.4023 Acc: 0.8227 | Val Loss: 0.5751 Acc: 0.7815                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "100%|████████████████████████████████████████████████| 30/30 [28:56<00:00, 57.89s/trial, best loss: 0.04127694398387154]\n",
      "Best hyperparameters: {'batch_size': np.int64(4), 'cnn_dense': np.int64(3), 'cnn_dropout': np.float64(0.004085020409211548), 'cnn_kernel_size_1': np.int64(1), 'cnn_kernel_size_2': np.int64(1), 'cnn_kernels_1': np.int64(3), 'cnn_kernels_2': np.int64(2), 'learning_rate': np.float64(0.0006699586365630455), 'lstm_dense': np.int64(1), 'lstm_hidden_size': np.int64(0), 'lstm_layers': np.int64(1), 'optimizer': np.int64(1)}\n",
      "TPE search finished in 1736.64 seconds\n",
      "Best (raw indices): {'batch_size': np.int64(4), 'cnn_dense': np.int64(3), 'cnn_dropout': np.float64(0.004085020409211548), 'cnn_kernel_size_1': np.int64(1), 'cnn_kernel_size_2': np.int64(1), 'cnn_kernels_1': np.int64(3), 'cnn_kernels_2': np.int64(2), 'learning_rate': np.float64(0.0006699586365630455), 'lstm_dense': np.int64(1), 'lstm_hidden_size': np.int64(0), 'lstm_layers': np.int64(1), 'optimizer': np.int64(1)}\n",
      "Best (interpreted): {'batch_size': 80, 'cnn_dense': 256, 'cnn_dropout': np.float64(0.004085020409211548), 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 64, 'learning_rate': np.float64(0.0006699586365630455), 'lstm_dense': 64, 'lstm_hidden_size': 32, 'lstm_layers': 2, 'optimizer': 'rmsprop'}\n",
      "Starting TPE search...\n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 128, 'cnn_dropout': 0.2954076285629413, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 16, 'cnn_kernels_2': 64, 'learning_rate': 0.00013956012442676997, 'lstm_dense': 256, 'lstm_hidden_size': 32, 'lstm_layers': 2, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 6.0817 Acc: 0.4418 | Val Loss: 1.3076 Acc: 0.4531                                               \n",
      "Epoch 002 | Train Loss: 1.2988 Acc: 0.4499 | Val Loss: 1.2848 Acc: 0.4531                                               \n",
      "Epoch 003 | Train Loss: 1.2781 Acc: 0.4506 | Val Loss: 1.2669 Acc: 0.4531                                               \n",
      "Epoch 004 | Train Loss: 1.2633 Acc: 0.4506 | Val Loss: 1.2541 Acc: 0.4531                                               \n",
      "Epoch 005 | Train Loss: 1.2522 Acc: 0.4509 | Val Loss: 1.2442 Acc: 0.4531                                               \n",
      "Epoch 006 | Train Loss: 1.2449 Acc: 0.4509 | Val Loss: 1.2368 Acc: 0.4531                                               \n",
      "Epoch 007 | Train Loss: 1.2389 Acc: 0.4509 | Val Loss: 1.2315 Acc: 0.4531                                               \n",
      "Epoch 008 | Train Loss: 1.2334 Acc: 0.4515 | Val Loss: 1.2271 Acc: 0.4531                                               \n",
      "Epoch 009 | Train Loss: 1.2303 Acc: 0.4512 | Val Loss: 1.2237 Acc: 0.4531                                               \n",
      "Epoch 010 | Train Loss: 1.2277 Acc: 0.4510 | Val Loss: 1.2209 Acc: 0.4531                                               \n",
      "Epoch 011 | Train Loss: 1.2251 Acc: 0.4512 | Val Loss: 1.2189 Acc: 0.4531                                               \n",
      "Epoch 012 | Train Loss: 1.2236 Acc: 0.4510 | Val Loss: 1.2171 Acc: 0.4531                                               \n",
      "Epoch 013 | Train Loss: 1.2213 Acc: 0.4512 | Val Loss: 1.2157 Acc: 0.4531                                               \n",
      "Epoch 014 | Train Loss: 1.2196 Acc: 0.4515 | Val Loss: 1.2146 Acc: 0.4531                                               \n",
      "Epoch 015 | Train Loss: 1.2180 Acc: 0.4519 | Val Loss: 1.2135 Acc: 0.4531                                               \n",
      "Epoch 016 | Train Loss: 1.2182 Acc: 0.4512 | Val Loss: 1.2126 Acc: 0.4531                                               \n",
      "Epoch 017 | Train Loss: 1.2188 Acc: 0.4508 | Val Loss: 1.2120 Acc: 0.4531                                               \n",
      "Epoch 018 | Train Loss: 1.2167 Acc: 0.4512 | Val Loss: 1.2113 Acc: 0.4531                                               \n",
      "Epoch 019 | Train Loss: 1.2166 Acc: 0.4512 | Val Loss: 1.2109 Acc: 0.4531                                               \n",
      "Epoch 020 | Train Loss: 1.2172 Acc: 0.4508 | Val Loss: 1.2105 Acc: 0.4531                                               \n",
      "Epoch 021 | Train Loss: 1.2148 Acc: 0.4517 | Val Loss: 1.2100 Acc: 0.4531                                               \n",
      "Epoch 022 | Train Loss: 1.2180 Acc: 0.4503 | Val Loss: 1.2098 Acc: 0.4531                                               \n",
      "Epoch 023 | Train Loss: 1.2152 Acc: 0.4512 | Val Loss: 1.2096 Acc: 0.4531                                               \n",
      "Epoch 024 | Train Loss: 1.2161 Acc: 0.4508 | Val Loss: 1.2092 Acc: 0.4531                                               \n",
      "Epoch 025 | Train Loss: 1.2152 Acc: 0.4512 | Val Loss: 1.2091 Acc: 0.4531                                               \n",
      "Epoch 026 | Train Loss: 1.2151 Acc: 0.4510 | Val Loss: 1.2090 Acc: 0.4531                                               \n",
      "Epoch 027 | Train Loss: 1.2138 Acc: 0.4515 | Val Loss: 1.2086 Acc: 0.4531                                               \n",
      "Epoch 028 | Train Loss: 1.2160 Acc: 0.4506 | Val Loss: 1.2085 Acc: 0.4531                                               \n",
      "Epoch 029 | Train Loss: 1.2148 Acc: 0.4509 | Val Loss: 1.2083 Acc: 0.4531                                               \n",
      "Epoch 030 | Train Loss: 1.2121 Acc: 0.4520 | Val Loss: 1.2081 Acc: 0.4531                                               \n",
      "Epoch 031 | Train Loss: 1.2147 Acc: 0.4509 | Val Loss: 1.2083 Acc: 0.4531                                               \n",
      "Epoch 032 | Train Loss: 1.2133 Acc: 0.4514 | Val Loss: 1.2080 Acc: 0.4531                                               \n",
      "Epoch 033 | Train Loss: 1.2161 Acc: 0.4515 | Val Loss: 1.2109 Acc: 0.4531                                               \n",
      "Epoch 034 | Train Loss: 1.2170 Acc: 0.4509 | Val Loss: 1.2106 Acc: 0.4531                                               \n",
      "Epoch 035 | Train Loss: 1.2169 Acc: 0.4509 | Val Loss: 1.2105 Acc: 0.4531                                               \n",
      "Epoch 036 | Train Loss: 1.2159 Acc: 0.4512 | Val Loss: 1.2101 Acc: 0.4531                                               \n",
      "Epoch 037 | Train Loss: 1.2144 Acc: 0.4516 | Val Loss: 1.2100 Acc: 0.4531                                               \n",
      "Epoch 038 | Train Loss: 1.2156 Acc: 0.4512 | Val Loss: 1.2100 Acc: 0.4531                                               \n",
      "Epoch 039 | Train Loss: 1.2160 Acc: 0.4510 | Val Loss: 1.2098 Acc: 0.4531                                               \n",
      "Epoch 040 | Train Loss: 1.2157 Acc: 0.4511 | Val Loss: 1.2094 Acc: 0.4531                                               \n",
      "Epoch 041 | Train Loss: 1.2163 Acc: 0.4507 | Val Loss: 1.2093 Acc: 0.4531                                               \n",
      "Epoch 042 | Train Loss: 1.2157 Acc: 0.4510 | Val Loss: 1.2094 Acc: 0.4531                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 256, 'cnn_dropout': 0.03352853105738163, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 48, 'cnn_kernels_2': 64, 'learning_rate': 0.005623018605247514, 'lstm_dense': 64, 'lstm_hidden_size': 96, 'lstm_layers': 6, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 12.6127 Acc: 0.3946 | Val Loss: 1.0132 Acc: 0.4675                                              \n",
      "Epoch 002 | Train Loss: 1.2195 Acc: 0.4028 | Val Loss: 1.0996 Acc: 0.4528                                               \n",
      "Epoch 003 | Train Loss: 1.0217 Acc: 0.4501 | Val Loss: 0.8867 Acc: 0.5322                                               \n",
      "Epoch 004 | Train Loss: 0.9532 Acc: 0.5047 | Val Loss: 0.8384 Acc: 0.5376                                               \n",
      "Epoch 005 | Train Loss: 0.9627 Acc: 0.5035 | Val Loss: 0.9169 Acc: 0.5573                                               \n",
      "Epoch 006 | Train Loss: 0.9370 Acc: 0.5149 | Val Loss: 1.0535 Acc: 0.4096                                               \n",
      "Epoch 007 | Train Loss: 0.9460 Acc: 0.5125 | Val Loss: 0.8673 Acc: 0.5319                                               \n",
      "Epoch 008 | Train Loss: 0.9092 Acc: 0.5291 | Val Loss: 0.8218 Acc: 0.5699                                               \n",
      "Epoch 009 | Train Loss: 0.9362 Acc: 0.5082 | Val Loss: 0.8125 Acc: 0.5794                                               \n",
      "Epoch 010 | Train Loss: 0.9580 Acc: 0.5053 | Val Loss: 0.9410 Acc: 0.5421                                               \n",
      "Epoch 011 | Train Loss: 0.9308 Acc: 0.4903 | Val Loss: 3.0323 Acc: 0.2528                                               \n",
      "Epoch 012 | Train Loss: 1.0711 Acc: 0.5044 | Val Loss: 0.8051 Acc: 0.6245                                               \n",
      "Epoch 013 | Train Loss: 1.0517 Acc: 0.4927 | Val Loss: 1.0633 Acc: 0.5012                                               \n",
      "Epoch 014 | Train Loss: 1.0472 Acc: 0.4733 | Val Loss: 0.9846 Acc: 0.4406                                               \n",
      "Epoch 015 | Train Loss: 1.0764 Acc: 0.4764 | Val Loss: 1.1175 Acc: 0.4743                                               \n",
      "Epoch 016 | Train Loss: 1.1241 Acc: 0.4718 | Val Loss: 1.1185 Acc: 0.4716                                               \n",
      "Epoch 017 | Train Loss: 1.1297 Acc: 0.4671 | Val Loss: 1.1093 Acc: 0.3845                                               \n",
      "Epoch 018 | Train Loss: 1.1065 Acc: 0.4782 | Val Loss: 1.0153 Acc: 0.5137                                               \n",
      "Epoch 019 | Train Loss: 1.0890 Acc: 0.4879 | Val Loss: 0.9459 Acc: 0.5036                                               \n",
      "Epoch 020 | Train Loss: 1.1382 Acc: 0.4768 | Val Loss: 1.0046 Acc: 0.4857                                               \n",
      "Epoch 021 | Train Loss: 1.1147 Acc: 0.4846 | Val Loss: 0.9699 Acc: 0.5179                                               \n",
      "Epoch 022 | Train Loss: 1.1287 Acc: 0.4626 | Val Loss: 1.0685 Acc: 0.4675                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 32, 'cnn_dropout': 0.12456077818041081, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 32, 'learning_rate': 0.009430991997483318, 'lstm_dense': 64, 'lstm_hidden_size': 128, 'lstm_layers': 4, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 5.3202 Acc: 0.5362 | Val Loss: 0.7736 Acc: 0.6200                                               \n",
      "Epoch 002 | Train Loss: 0.8316 Acc: 0.6221 | Val Loss: 0.8204 Acc: 0.6600                                               \n",
      "Epoch 003 | Train Loss: 0.8761 Acc: 0.6053 | Val Loss: 0.9049 Acc: 0.5337                                               \n",
      "Epoch 004 | Train Loss: 0.8706 Acc: 0.5915 | Val Loss: 0.8653 Acc: 0.6069                                               \n",
      "Epoch 005 | Train Loss: 0.8845 Acc: 0.5809 | Val Loss: 0.7786 Acc: 0.6582                                               \n",
      "Epoch 006 | Train Loss: 0.8706 Acc: 0.5759 | Val Loss: 0.8002 Acc: 0.6107                                               \n",
      "Epoch 007 | Train Loss: 0.8644 Acc: 0.5809 | Val Loss: 0.8221 Acc: 0.5591                                               \n",
      "Epoch 008 | Train Loss: 0.8637 Acc: 0.5843 | Val Loss: 0.8238 Acc: 0.5758                                               \n",
      "Epoch 009 | Train Loss: 0.8599 Acc: 0.5894 | Val Loss: 0.7959 Acc: 0.6430                                               \n",
      "Epoch 010 | Train Loss: 0.8639 Acc: 0.5777 | Val Loss: 0.7930 Acc: 0.6107                                               \n",
      "Epoch 011 | Train Loss: 0.8568 Acc: 0.5773 | Val Loss: 0.7867 Acc: 0.6149                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 32, 'cnn_dropout': 0.22654410859629653, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 48, 'cnn_kernels_2': 16, 'learning_rate': 4.160947371947363e-05, 'lstm_dense': 32, 'lstm_hidden_size': 64, 'lstm_layers': 6, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 51.8093 Acc: 0.3313 | Val Loss: 6.7303 Acc: 0.4561                                              \n",
      "Epoch 002 | Train Loss: 18.3144 Acc: 0.3820 | Val Loss: 3.7622 Acc: 0.4872                                              \n",
      "Epoch 003 | Train Loss: 11.1756 Acc: 0.4050 | Val Loss: 3.3567 Acc: 0.4576                                              \n",
      "Epoch 004 | Train Loss: 8.7290 Acc: 0.4200 | Val Loss: 2.6043 Acc: 0.4119                                               \n",
      "Epoch 005 | Train Loss: 6.8426 Acc: 0.4313 | Val Loss: 2.0497 Acc: 0.4716                                               \n",
      "Epoch 006 | Train Loss: 5.7682 Acc: 0.4423 | Val Loss: 1.4923 Acc: 0.5060                                               \n",
      "Epoch 007 | Train Loss: 4.6940 Acc: 0.4611 | Val Loss: 1.3143 Acc: 0.5487                                               \n",
      "Epoch 008 | Train Loss: 4.1731 Acc: 0.4750 | Val Loss: 1.3402 Acc: 0.5922                                               \n",
      "Epoch 009 | Train Loss: 3.5930 Acc: 0.4876 | Val Loss: 1.1828 Acc: 0.6170                                               \n",
      "Epoch 010 | Train Loss: 3.0820 Acc: 0.5129 | Val Loss: 1.0836 Acc: 0.6669                                               \n",
      "Epoch 011 | Train Loss: 2.7379 Acc: 0.5277 | Val Loss: 0.9215 Acc: 0.6970                                               \n",
      "Epoch 012 | Train Loss: 2.4588 Acc: 0.5428 | Val Loss: 0.9674 Acc: 0.6612                                               \n",
      "Epoch 013 | Train Loss: 2.2046 Acc: 0.5520 | Val Loss: 0.8837 Acc: 0.6522                                               \n",
      "Epoch 014 | Train Loss: 2.0671 Acc: 0.5742 | Val Loss: 0.9736 Acc: 0.6797                                               \n",
      "Epoch 015 | Train Loss: 1.9147 Acc: 0.5849 | Val Loss: 0.7523 Acc: 0.7254                                               \n",
      "Epoch 016 | Train Loss: 1.6973 Acc: 0.6042 | Val Loss: 0.7859 Acc: 0.7230                                               \n",
      "Epoch 017 | Train Loss: 1.5979 Acc: 0.6180 | Val Loss: 0.7249 Acc: 0.7263                                               \n",
      "Epoch 018 | Train Loss: 1.4563 Acc: 0.6317 | Val Loss: 0.7339 Acc: 0.7346                                               \n",
      "Epoch 019 | Train Loss: 1.3677 Acc: 0.6448 | Val Loss: 0.6439 Acc: 0.7537                                               \n",
      "Epoch 020 | Train Loss: 1.1999 Acc: 0.6648 | Val Loss: 0.7878 Acc: 0.7122                                               \n",
      "Epoch 021 | Train Loss: 1.1733 Acc: 0.6714 | Val Loss: 0.7183 Acc: 0.7564                                               \n",
      "Epoch 022 | Train Loss: 1.0947 Acc: 0.6948 | Val Loss: 0.6517 Acc: 0.7648                                               \n",
      "Epoch 023 | Train Loss: 1.0220 Acc: 0.7024 | Val Loss: 0.5752 Acc: 0.7937                                               \n",
      "Epoch 024 | Train Loss: 0.9331 Acc: 0.7137 | Val Loss: 0.5762 Acc: 0.7988                                               \n",
      "Epoch 025 | Train Loss: 0.8731 Acc: 0.7347 | Val Loss: 0.6077 Acc: 0.7767                                               \n",
      "Epoch 026 | Train Loss: 0.8239 Acc: 0.7415 | Val Loss: 0.5662 Acc: 0.7985                                               \n",
      "Epoch 027 | Train Loss: 0.7789 Acc: 0.7560 | Val Loss: 0.5334 Acc: 0.7922                                               \n",
      "Epoch 028 | Train Loss: 0.7440 Acc: 0.7645 | Val Loss: 0.4516 Acc: 0.8206                                               \n",
      "Epoch 029 | Train Loss: 0.6990 Acc: 0.7744 | Val Loss: 0.4914 Acc: 0.8287                                               \n",
      "Epoch 030 | Train Loss: 0.6773 Acc: 0.7844 | Val Loss: 0.4642 Acc: 0.8382                                               \n",
      "Epoch 031 | Train Loss: 0.6500 Acc: 0.7851 | Val Loss: 0.5655 Acc: 0.7916                                               \n",
      "Epoch 032 | Train Loss: 0.6095 Acc: 0.7973 | Val Loss: 0.5220 Acc: 0.8078                                               \n",
      "Epoch 033 | Train Loss: 0.6075 Acc: 0.7967 | Val Loss: 0.4661 Acc: 0.8081                                               \n",
      "Epoch 034 | Train Loss: 0.5863 Acc: 0.8010 | Val Loss: 0.4066 Acc: 0.8415                                               \n",
      "Epoch 035 | Train Loss: 0.5517 Acc: 0.8107 | Val Loss: 0.3949 Acc: 0.8525                                               \n",
      "Epoch 036 | Train Loss: 0.5478 Acc: 0.8101 | Val Loss: 0.3935 Acc: 0.8370                                               \n",
      "Epoch 037 | Train Loss: 0.5142 Acc: 0.8207 | Val Loss: 0.3653 Acc: 0.8576                                               \n",
      "Epoch 038 | Train Loss: 0.5036 Acc: 0.8227 | Val Loss: 0.4631 Acc: 0.8176                                               \n",
      "Epoch 039 | Train Loss: 0.4904 Acc: 0.8292 | Val Loss: 0.4072 Acc: 0.8493                                               \n",
      "Epoch 040 | Train Loss: 0.4728 Acc: 0.8349 | Val Loss: 0.4724 Acc: 0.8197                                               \n",
      "Epoch 041 | Train Loss: 0.4645 Acc: 0.8398 | Val Loss: 0.3886 Acc: 0.8573                                               \n",
      "Epoch 042 | Train Loss: 0.4529 Acc: 0.8407 | Val Loss: 0.3867 Acc: 0.8516                                               \n",
      "Epoch 043 | Train Loss: 0.4227 Acc: 0.8519 | Val Loss: 0.3196 Acc: 0.8740                                               \n",
      "Epoch 044 | Train Loss: 0.4221 Acc: 0.8483 | Val Loss: 0.3840 Acc: 0.8519                                               \n",
      "Epoch 045 | Train Loss: 0.4091 Acc: 0.8579 | Val Loss: 0.3528 Acc: 0.8445                                               \n",
      "Epoch 046 | Train Loss: 0.3950 Acc: 0.8627 | Val Loss: 0.3675 Acc: 0.8460                                               \n",
      "Epoch 047 | Train Loss: 0.3904 Acc: 0.8613 | Val Loss: 0.3914 Acc: 0.8573                                               \n",
      "Epoch 048 | Train Loss: 0.3821 Acc: 0.8668 | Val Loss: 0.3048 Acc: 0.8899                                               \n",
      "Epoch 049 | Train Loss: 0.3676 Acc: 0.8688 | Val Loss: 0.2789 Acc: 0.8925                                               \n",
      "Epoch 050 | Train Loss: 0.3563 Acc: 0.8769 | Val Loss: 0.3241 Acc: 0.8696                                               \n",
      "Epoch 051 | Train Loss: 0.3268 Acc: 0.8841 | Val Loss: 0.2753 Acc: 0.8797                                               \n",
      "Epoch 052 | Train Loss: 0.3515 Acc: 0.8774 | Val Loss: 0.3191 Acc: 0.8743                                               \n",
      "Epoch 053 | Train Loss: 0.3348 Acc: 0.8827 | Val Loss: 0.2501 Acc: 0.9009                                               \n",
      "Epoch 054 | Train Loss: 0.3248 Acc: 0.8879 | Val Loss: 0.4489 Acc: 0.8263                                               \n",
      "Epoch 055 | Train Loss: 0.3183 Acc: 0.8895 | Val Loss: 0.2778 Acc: 0.8896                                               \n",
      "Epoch 056 | Train Loss: 0.2999 Acc: 0.8965 | Val Loss: 0.2640 Acc: 0.9045                                               \n",
      "Epoch 057 | Train Loss: 0.2964 Acc: 0.8966 | Val Loss: 0.2625 Acc: 0.8997                                               \n",
      "Epoch 058 | Train Loss: 0.2929 Acc: 0.8999 | Val Loss: 0.2185 Acc: 0.9227                                               \n",
      "Epoch 059 | Train Loss: 0.2792 Acc: 0.9034 | Val Loss: 0.2745 Acc: 0.8851                                               \n",
      "Epoch 060 | Train Loss: 0.2806 Acc: 0.9049 | Val Loss: 0.3097 Acc: 0.8857                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 128, 'cnn_dropout': 0.541440318963787, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 32, 'cnn_kernels_2': 32, 'learning_rate': 0.0024359237150037303, 'lstm_dense': 128, 'lstm_hidden_size': 96, 'lstm_layers': 2, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 9.4138 Acc: 0.4268 | Val Loss: 2.1263 Acc: 0.5263                                               \n",
      "Epoch 002 | Train Loss: 1.5793 Acc: 0.5318 | Val Loss: 0.8535 Acc: 0.6299                                               \n",
      "Epoch 003 | Train Loss: 0.9788 Acc: 0.5862 | Val Loss: 0.6788 Acc: 0.6991                                               \n",
      "Epoch 004 | Train Loss: 0.9047 Acc: 0.6230 | Val Loss: 0.7077 Acc: 0.6421                                               \n",
      "Epoch 005 | Train Loss: 0.6445 Acc: 0.7161 | Val Loss: 0.5260 Acc: 0.7582                                               \n",
      "Epoch 006 | Train Loss: 0.6211 Acc: 0.7392 | Val Loss: 0.5159 Acc: 0.7800                                               \n",
      "Epoch 007 | Train Loss: 0.5642 Acc: 0.7686 | Val Loss: 0.4519 Acc: 0.8173                                               \n",
      "Epoch 008 | Train Loss: 0.4927 Acc: 0.7977 | Val Loss: 0.3804 Acc: 0.8361                                               \n",
      "Epoch 009 | Train Loss: 0.4753 Acc: 0.8101 | Val Loss: 0.4248 Acc: 0.8215                                               \n",
      "Epoch 010 | Train Loss: 0.4262 Acc: 0.8307 | Val Loss: 0.3620 Acc: 0.8531                                               \n",
      "Epoch 011 | Train Loss: 0.4444 Acc: 0.8212 | Val Loss: 0.3545 Acc: 0.8466                                               \n",
      "Epoch 012 | Train Loss: 0.4014 Acc: 0.8413 | Val Loss: 0.3404 Acc: 0.8558                                               \n",
      "Epoch 013 | Train Loss: 0.4015 Acc: 0.8433 | Val Loss: 0.3945 Acc: 0.8310                                               \n",
      "Epoch 014 | Train Loss: 0.3950 Acc: 0.8473 | Val Loss: 0.2953 Acc: 0.8845                                               \n",
      "Epoch 015 | Train Loss: 0.3649 Acc: 0.8591 | Val Loss: 0.3297 Acc: 0.8621                                               \n",
      "Epoch 016 | Train Loss: 0.3439 Acc: 0.8695 | Val Loss: 0.3104 Acc: 0.8704                                               \n",
      "Epoch 017 | Train Loss: 0.3383 Acc: 0.8707 | Val Loss: 0.2846 Acc: 0.8857                                               \n",
      "Epoch 018 | Train Loss: 0.3424 Acc: 0.8674 | Val Loss: 0.3037 Acc: 0.8746                                               \n",
      "Epoch 019 | Train Loss: 0.2935 Acc: 0.8883 | Val Loss: 0.2797 Acc: 0.8896                                               \n",
      "Epoch 020 | Train Loss: 0.3044 Acc: 0.8836 | Val Loss: 0.2425 Acc: 0.9060                                               \n",
      "Epoch 021 | Train Loss: 0.2965 Acc: 0.8862 | Val Loss: 0.2783 Acc: 0.8934                                               \n",
      "Epoch 022 | Train Loss: 0.3228 Acc: 0.8786 | Val Loss: 0.2635 Acc: 0.8952                                               \n",
      "Epoch 023 | Train Loss: 0.2963 Acc: 0.8892 | Val Loss: 0.2703 Acc: 0.8925                                               \n",
      "Epoch 024 | Train Loss: 0.2854 Acc: 0.8922 | Val Loss: 0.1986 Acc: 0.9242                                               \n",
      "Epoch 025 | Train Loss: 0.2802 Acc: 0.8899 | Val Loss: 0.2570 Acc: 0.9024                                               \n",
      "Epoch 026 | Train Loss: 0.2795 Acc: 0.8948 | Val Loss: 0.3868 Acc: 0.8540                                               \n",
      "Epoch 027 | Train Loss: 0.3185 Acc: 0.8775 | Val Loss: 0.2143 Acc: 0.9161                                               \n",
      "Epoch 028 | Train Loss: 0.2867 Acc: 0.8936 | Val Loss: 0.2086 Acc: 0.9212                                               \n",
      "Epoch 029 | Train Loss: 0.3410 Acc: 0.8686 | Val Loss: 0.3331 Acc: 0.8645                                               \n",
      "Epoch 030 | Train Loss: 0.3118 Acc: 0.8822 | Val Loss: 0.3492 Acc: 0.8510                                               \n",
      "Epoch 031 | Train Loss: 0.2989 Acc: 0.8865 | Val Loss: 0.2787 Acc: 0.8887                                               \n",
      "Epoch 032 | Train Loss: 0.2651 Acc: 0.8982 | Val Loss: 0.3223 Acc: 0.8824                                               \n",
      "Epoch 033 | Train Loss: 0.2840 Acc: 0.8925 | Val Loss: 0.3341 Acc: 0.8776                                               \n",
      "Epoch 034 | Train Loss: 0.2958 Acc: 0.8860 | Val Loss: 0.2152 Acc: 0.9251                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 36, 'cnn_dense': 128, 'cnn_dropout': 0.2866410184727268, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 32, 'cnn_kernels_2': 96, 'learning_rate': 0.003714162163877573, 'lstm_dense': 64, 'lstm_hidden_size': 128, 'lstm_layers': 2, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 8.9497 Acc: 0.4923 | Val Loss: 0.7283 Acc: 0.6107                                               \n",
      "Epoch 002 | Train Loss: 0.8162 Acc: 0.6101 | Val Loss: 1.1714 Acc: 0.5910                                               \n",
      "Epoch 003 | Train Loss: 0.8143 Acc: 0.6027 | Val Loss: 0.9790 Acc: 0.5934                                               \n",
      "Epoch 004 | Train Loss: 0.8110 Acc: 0.6148 | Val Loss: 0.7443 Acc: 0.6704                                               \n",
      "Epoch 005 | Train Loss: 0.8881 Acc: 0.5699 | Val Loss: 1.7183 Acc: 0.4597                                               \n",
      "Epoch 006 | Train Loss: 0.8947 Acc: 0.5627 | Val Loss: 0.8949 Acc: 0.5627                                               \n",
      "Epoch 007 | Train Loss: 0.8805 Acc: 0.5624 | Val Loss: 0.9968 Acc: 0.5030                                               \n",
      "Epoch 008 | Train Loss: 0.9257 Acc: 0.5556 | Val Loss: 0.9036 Acc: 0.5394                                               \n",
      "Epoch 009 | Train Loss: 0.8888 Acc: 0.5518 | Val Loss: 1.0425 Acc: 0.4507                                               \n",
      "Epoch 010 | Train Loss: 0.8954 Acc: 0.5641 | Val Loss: 0.7727 Acc: 0.6099                                               \n",
      "Epoch 011 | Train Loss: 0.9129 Acc: 0.5515 | Val Loss: 1.0065 Acc: 0.4785                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 32, 'cnn_dropout': 0.6579010177397097, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 16, 'cnn_kernels_2': 32, 'learning_rate': 0.0017488218673257728, 'lstm_dense': 128, 'lstm_hidden_size': 128, 'lstm_layers': 2, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 8.5024 Acc: 0.4222 | Val Loss: 0.8580 Acc: 0.6346                                               \n",
      "Epoch 002 | Train Loss: 1.2963 Acc: 0.5048 | Val Loss: 0.8621 Acc: 0.6239                                               \n",
      "Epoch 003 | Train Loss: 0.9880 Acc: 0.5550 | Val Loss: 1.0118 Acc: 0.5764                                               \n",
      "Epoch 004 | Train Loss: 0.8209 Acc: 0.6090 | Val Loss: 0.7056 Acc: 0.6827                                               \n",
      "Epoch 005 | Train Loss: 0.7294 Acc: 0.6521 | Val Loss: 0.8299 Acc: 0.5382                                               \n",
      "Epoch 006 | Train Loss: 0.6691 Acc: 0.6846 | Val Loss: 0.7000 Acc: 0.6761                                               \n",
      "Epoch 007 | Train Loss: 0.6307 Acc: 0.7101 | Val Loss: 0.8298 Acc: 0.6176                                               \n",
      "Epoch 008 | Train Loss: 0.5980 Acc: 0.7259 | Val Loss: 0.6906 Acc: 0.6681                                               \n",
      "Epoch 009 | Train Loss: 0.5777 Acc: 0.7441 | Val Loss: 0.6587 Acc: 0.7319                                               \n",
      "Epoch 010 | Train Loss: 0.5439 Acc: 0.7606 | Val Loss: 0.4921 Acc: 0.8090                                               \n",
      "Epoch 011 | Train Loss: 0.5459 Acc: 0.7552 | Val Loss: 0.6079 Acc: 0.7648                                               \n",
      "Epoch 012 | Train Loss: 0.5393 Acc: 0.7602 | Val Loss: 0.5882 Acc: 0.7621                                               \n",
      "Epoch 013 | Train Loss: 0.5344 Acc: 0.7719 | Val Loss: 0.6033 Acc: 0.7310                                               \n",
      "Epoch 014 | Train Loss: 0.5167 Acc: 0.7757 | Val Loss: 0.5635 Acc: 0.7591                                               \n",
      "Epoch 015 | Train Loss: 0.5125 Acc: 0.7729 | Val Loss: 0.4841 Acc: 0.8075                                               \n",
      "Epoch 016 | Train Loss: 0.5344 Acc: 0.7757 | Val Loss: 0.5396 Acc: 0.7266                                               \n",
      "Epoch 017 | Train Loss: 0.5136 Acc: 0.7786 | Val Loss: 0.5276 Acc: 0.7699                                               \n",
      "Epoch 018 | Train Loss: 0.4988 Acc: 0.7842 | Val Loss: 0.5932 Acc: 0.7609                                               \n",
      "Epoch 019 | Train Loss: 0.4822 Acc: 0.7895 | Val Loss: 0.9186 Acc: 0.6299                                               \n",
      "Epoch 020 | Train Loss: 0.5227 Acc: 0.7786 | Val Loss: 0.6676 Acc: 0.7057                                               \n",
      "Epoch 021 | Train Loss: 0.4919 Acc: 0.7848 | Val Loss: 0.4373 Acc: 0.7958                                               \n",
      "Epoch 022 | Train Loss: 0.4807 Acc: 0.7960 | Val Loss: 0.5373 Acc: 0.7803                                               \n",
      "Epoch 023 | Train Loss: 0.4697 Acc: 0.7963 | Val Loss: 0.5323 Acc: 0.7845                                               \n",
      "Epoch 024 | Train Loss: 0.4658 Acc: 0.8027 | Val Loss: 0.5482 Acc: 0.7785                                               \n",
      "Epoch 025 | Train Loss: 0.4749 Acc: 0.7968 | Val Loss: 0.6699 Acc: 0.7496                                               \n",
      "Epoch 026 | Train Loss: 0.4498 Acc: 0.8001 | Val Loss: 0.5151 Acc: 0.7316                                               \n",
      "Epoch 027 | Train Loss: 0.4460 Acc: 0.8078 | Val Loss: 0.4879 Acc: 0.8030                                               \n",
      "Epoch 028 | Train Loss: 0.4662 Acc: 0.8041 | Val Loss: 0.4479 Acc: 0.8143                                               \n",
      "Epoch 029 | Train Loss: 0.4826 Acc: 0.7908 | Val Loss: 0.6435 Acc: 0.7424                                               \n",
      "Epoch 030 | Train Loss: 0.4678 Acc: 0.8020 | Val Loss: 0.6903 Acc: 0.7090                                               \n",
      "Epoch 031 | Train Loss: 0.4666 Acc: 0.7979 | Val Loss: 0.4247 Acc: 0.8203                                               \n",
      "Epoch 032 | Train Loss: 0.4577 Acc: 0.8039 | Val Loss: 0.8645 Acc: 0.6913                                               \n",
      "Epoch 033 | Train Loss: 0.4498 Acc: 0.8088 | Val Loss: 0.4267 Acc: 0.8316                                               \n",
      "Epoch 034 | Train Loss: 0.4528 Acc: 0.8098 | Val Loss: 0.5968 Acc: 0.7815                                               \n",
      "Epoch 035 | Train Loss: 0.4815 Acc: 0.7986 | Val Loss: 0.5046 Acc: 0.7943                                               \n",
      "Epoch 036 | Train Loss: 0.4568 Acc: 0.8001 | Val Loss: 0.8598 Acc: 0.6597                                               \n",
      "Epoch 037 | Train Loss: 0.4777 Acc: 0.7992 | Val Loss: 0.3922 Acc: 0.8442                                               \n",
      "Epoch 038 | Train Loss: 0.4489 Acc: 0.8083 | Val Loss: 0.5080 Acc: 0.7630                                               \n",
      "Epoch 039 | Train Loss: 0.4671 Acc: 0.7980 | Val Loss: 0.4151 Acc: 0.8191                                               \n",
      "Epoch 040 | Train Loss: 0.4413 Acc: 0.8134 | Val Loss: 1.4101 Acc: 0.5245                                               \n",
      "Epoch 041 | Train Loss: 0.4746 Acc: 0.8048 | Val Loss: 0.3908 Acc: 0.8513                                               \n",
      "Epoch 042 | Train Loss: 0.4440 Acc: 0.8154 | Val Loss: 0.3600 Acc: 0.8439                                               \n",
      "Epoch 043 | Train Loss: 0.4374 Acc: 0.8166 | Val Loss: 0.3856 Acc: 0.8418                                               \n",
      "Epoch 044 | Train Loss: 0.4349 Acc: 0.8151 | Val Loss: 0.4813 Acc: 0.7910                                               \n",
      "Epoch 045 | Train Loss: 0.4617 Acc: 0.8083 | Val Loss: 0.5263 Acc: 0.7576                                               \n",
      "Epoch 046 | Train Loss: 0.4473 Acc: 0.8128 | Val Loss: 0.5646 Acc: 0.7209                                               \n",
      "Epoch 047 | Train Loss: 0.4378 Acc: 0.8211 | Val Loss: 0.5177 Acc: 0.8125                                               \n",
      "Epoch 048 | Train Loss: 0.4397 Acc: 0.8171 | Val Loss: 0.5799 Acc: 0.7385                                               \n",
      "Epoch 049 | Train Loss: 0.4270 Acc: 0.8223 | Val Loss: 0.6460 Acc: 0.6922                                               \n",
      "Epoch 050 | Train Loss: 0.4727 Acc: 0.8010 | Val Loss: 0.4019 Acc: 0.8260                                               \n",
      "Epoch 051 | Train Loss: 0.4687 Acc: 0.7955 | Val Loss: 0.5631 Acc: 0.7946                                               \n",
      "Epoch 052 | Train Loss: 0.4729 Acc: 0.7954 | Val Loss: 0.4990 Acc: 0.7958                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 32, 'cnn_dropout': 0.5499405239424379, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 64, 'learning_rate': 0.004991198148368964, 'lstm_dense': 128, 'lstm_hidden_size': 32, 'lstm_layers': 1, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 8.1045 Acc: 0.4972 | Val Loss: 1.0015 Acc: 0.4734                                               \n",
      "Epoch 002 | Train Loss: 0.9999 Acc: 0.5133 | Val Loss: 1.1742 Acc: 0.4281                                               \n",
      "Epoch 003 | Train Loss: 1.1544 Acc: 0.4648 | Val Loss: 1.8212 Acc: 0.5039                                               \n",
      "Epoch 004 | Train Loss: 1.1896 Acc: 0.4535 | Val Loss: 1.1718 Acc: 0.4600                                               \n",
      "Epoch 005 | Train Loss: 1.1873 Acc: 0.4562 | Val Loss: 1.1722 Acc: 0.4600                                               \n",
      "Epoch 006 | Train Loss: 1.2146 Acc: 0.4464 | Val Loss: 1.2103 Acc: 0.4490                                               \n",
      "Epoch 007 | Train Loss: 1.2165 Acc: 0.4459 | Val Loss: 1.2037 Acc: 0.4490                                               \n",
      "Epoch 008 | Train Loss: 1.2639 Acc: 0.4425 | Val Loss: 1.2555 Acc: 0.4421                                               \n",
      "Epoch 009 | Train Loss: 1.2522 Acc: 0.4412 | Val Loss: 1.2201 Acc: 0.4421                                               \n",
      "Epoch 010 | Train Loss: 1.2328 Acc: 0.4423 | Val Loss: 1.2174 Acc: 0.4421                                               \n",
      "Epoch 011 | Train Loss: 1.2190 Acc: 0.4406 | Val Loss: 1.2212 Acc: 0.4424                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 128, 'cnn_dropout': 0.37472742900597533, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 32, 'cnn_kernels_2': 16, 'learning_rate': 0.0014446627865163913, 'lstm_dense': 64, 'lstm_hidden_size': 32, 'lstm_layers': 6, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 15.3706 Acc: 0.4447 | Val Loss: 2.5035 Acc: 0.5206                                              \n",
      "Epoch 002 | Train Loss: 2.8248 Acc: 0.4968 | Val Loss: 2.1748 Acc: 0.5513                                               \n",
      "Epoch 003 | Train Loss: 1.6325 Acc: 0.5435 | Val Loss: 0.9786 Acc: 0.6334                                               \n",
      "Epoch 004 | Train Loss: 1.1692 Acc: 0.5782 | Val Loss: 0.8287 Acc: 0.6534                                               \n",
      "Epoch 005 | Train Loss: 0.9068 Acc: 0.6206 | Val Loss: 0.6116 Acc: 0.7457                                               \n",
      "Epoch 006 | Train Loss: 0.8263 Acc: 0.6401 | Val Loss: 0.7155 Acc: 0.6597                                               \n",
      "Epoch 007 | Train Loss: 0.7362 Acc: 0.6695 | Val Loss: 0.5438 Acc: 0.7806                                               \n",
      "Epoch 008 | Train Loss: 0.7036 Acc: 0.6872 | Val Loss: 0.6031 Acc: 0.6851                                               \n",
      "Epoch 009 | Train Loss: 0.6834 Acc: 0.6942 | Val Loss: 0.5754 Acc: 0.7785                                               \n",
      "Epoch 010 | Train Loss: 0.6605 Acc: 0.7046 | Val Loss: 0.6917 Acc: 0.6525                                               \n",
      "Epoch 011 | Train Loss: 0.6679 Acc: 0.7090 | Val Loss: 0.5891 Acc: 0.7018                                               \n",
      "Epoch 012 | Train Loss: 0.6508 Acc: 0.7091 | Val Loss: 0.5767 Acc: 0.7612                                               \n",
      "Epoch 013 | Train Loss: 0.6359 Acc: 0.7186 | Val Loss: 0.5242 Acc: 0.7430                                               \n",
      "Epoch 014 | Train Loss: 0.6154 Acc: 0.7196 | Val Loss: 0.5430 Acc: 0.7651                                               \n",
      "Epoch 015 | Train Loss: 0.6056 Acc: 0.7335 | Val Loss: 0.5009 Acc: 0.7848                                               \n",
      "Epoch 016 | Train Loss: 0.5856 Acc: 0.7451 | Val Loss: 0.5178 Acc: 0.7678                                               \n",
      "Epoch 017 | Train Loss: 0.5651 Acc: 0.7479 | Val Loss: 0.4383 Acc: 0.8287                                               \n",
      "Epoch 018 | Train Loss: 0.5496 Acc: 0.7531 | Val Loss: 0.4128 Acc: 0.8349                                               \n",
      "Epoch 019 | Train Loss: 0.5641 Acc: 0.7496 | Val Loss: 0.4723 Acc: 0.8122                                               \n",
      "Epoch 020 | Train Loss: 0.5504 Acc: 0.7526 | Val Loss: 0.4300 Acc: 0.8224                                               \n",
      "Epoch 021 | Train Loss: 0.5761 Acc: 0.7508 | Val Loss: 0.5247 Acc: 0.7794                                               \n",
      "Epoch 022 | Train Loss: 0.5635 Acc: 0.7560 | Val Loss: 0.5124 Acc: 0.7764                                               \n",
      "Epoch 023 | Train Loss: 0.5439 Acc: 0.7638 | Val Loss: 0.4463 Acc: 0.8412                                               \n",
      "Epoch 024 | Train Loss: 0.5162 Acc: 0.7755 | Val Loss: 0.4686 Acc: 0.8272                                               \n",
      "Epoch 025 | Train Loss: 0.5213 Acc: 0.7756 | Val Loss: 0.4985 Acc: 0.7707                                               \n",
      "Epoch 026 | Train Loss: 0.5339 Acc: 0.7651 | Val Loss: 0.4231 Acc: 0.8304                                               \n",
      "Epoch 027 | Train Loss: 0.5324 Acc: 0.7720 | Val Loss: 0.4598 Acc: 0.7800                                               \n",
      "Epoch 028 | Train Loss: 0.5249 Acc: 0.7742 | Val Loss: 0.5649 Acc: 0.7087                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 32, 'cnn_dropout': 0.4556016618282694, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 32, 'cnn_kernels_2': 16, 'learning_rate': 0.00017332625192565407, 'lstm_dense': 64, 'lstm_hidden_size': 128, 'lstm_layers': 3, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 8.3254 Acc: 0.3861 | Val Loss: 1.0965 Acc: 0.4185                                               \n",
      "Epoch 002 | Train Loss: 1.1100 Acc: 0.4319 | Val Loss: 0.9450 Acc: 0.5331                                               \n",
      "Epoch 003 | Train Loss: 1.0754 Acc: 0.4432 | Val Loss: 0.9338 Acc: 0.5334                                               \n",
      "Epoch 004 | Train Loss: 1.0693 Acc: 0.4662 | Val Loss: 0.9332 Acc: 0.5254                                               \n",
      "Epoch 005 | Train Loss: 1.0504 Acc: 0.4774 | Val Loss: 0.9472 Acc: 0.5337                                               \n",
      "Epoch 006 | Train Loss: 1.0318 Acc: 0.4823 | Val Loss: 0.9047 Acc: 0.5576                                               \n",
      "Epoch 007 | Train Loss: 1.0277 Acc: 0.4894 | Val Loss: 0.9153 Acc: 0.5418                                               \n",
      "Epoch 008 | Train Loss: 1.0320 Acc: 0.4754 | Val Loss: 0.8925 Acc: 0.5087                                               \n",
      "Epoch 009 | Train Loss: 1.0320 Acc: 0.4867 | Val Loss: 0.8710 Acc: 0.5606                                               \n",
      "Epoch 010 | Train Loss: 1.0325 Acc: 0.4929 | Val Loss: 0.8558 Acc: 0.5203                                               \n",
      "Epoch 011 | Train Loss: 1.0063 Acc: 0.5020 | Val Loss: 0.8608 Acc: 0.5388                                               \n",
      "Epoch 012 | Train Loss: 1.0023 Acc: 0.5036 | Val Loss: 0.8461 Acc: 0.5346                                               \n",
      "Epoch 013 | Train Loss: 1.0139 Acc: 0.4956 | Val Loss: 0.8594 Acc: 0.5540                                               \n",
      "Epoch 014 | Train Loss: 1.0084 Acc: 0.5038 | Val Loss: 1.0010 Acc: 0.4475                                               \n",
      "Epoch 015 | Train Loss: 0.9936 Acc: 0.5011 | Val Loss: 0.9468 Acc: 0.5200                                               \n",
      "Epoch 016 | Train Loss: 0.9965 Acc: 0.4969 | Val Loss: 0.8627 Acc: 0.6033                                               \n",
      "Epoch 017 | Train Loss: 1.0088 Acc: 0.5033 | Val Loss: 0.8403 Acc: 0.5669                                               \n",
      "Epoch 018 | Train Loss: 0.9960 Acc: 0.5042 | Val Loss: 0.8510 Acc: 0.5600                                               \n",
      "Epoch 019 | Train Loss: 0.9818 Acc: 0.5059 | Val Loss: 0.8240 Acc: 0.5448                                               \n",
      "Epoch 020 | Train Loss: 0.9872 Acc: 0.5017 | Val Loss: 0.9806 Acc: 0.4672                                               \n",
      "Epoch 021 | Train Loss: 1.0148 Acc: 0.4959 | Val Loss: 0.8611 Acc: 0.5690                                               \n",
      "Epoch 022 | Train Loss: 0.9951 Acc: 0.4997 | Val Loss: 0.8565 Acc: 0.5552                                               \n",
      "Epoch 023 | Train Loss: 0.9730 Acc: 0.5100 | Val Loss: 0.8122 Acc: 0.5460                                               \n",
      "Epoch 024 | Train Loss: 0.9780 Acc: 0.5068 | Val Loss: 0.8353 Acc: 0.5749                                               \n",
      "Epoch 025 | Train Loss: 0.9729 Acc: 0.5065 | Val Loss: 0.8036 Acc: 0.5773                                               \n",
      "Epoch 026 | Train Loss: 0.9629 Acc: 0.5155 | Val Loss: 0.8195 Acc: 0.5925                                               \n",
      "Epoch 027 | Train Loss: 0.9818 Acc: 0.5093 | Val Loss: 0.8038 Acc: 0.5857                                               \n",
      "Epoch 028 | Train Loss: 0.9663 Acc: 0.5080 | Val Loss: 0.8522 Acc: 0.5287                                               \n",
      "Epoch 029 | Train Loss: 0.9673 Acc: 0.5102 | Val Loss: 0.8143 Acc: 0.5710                                               \n",
      "Epoch 030 | Train Loss: 0.9882 Acc: 0.4997 | Val Loss: 0.7968 Acc: 0.5946                                               \n",
      "Epoch 031 | Train Loss: 0.9480 Acc: 0.5163 | Val Loss: 0.8049 Acc: 0.5737                                               \n",
      "Epoch 032 | Train Loss: 1.0652 Acc: 0.4830 | Val Loss: 1.1776 Acc: 0.4857                                               \n",
      "Epoch 033 | Train Loss: 1.2741 Acc: 0.4487 | Val Loss: 1.2398 Acc: 0.4490                                               \n",
      "Epoch 034 | Train Loss: 1.2434 Acc: 0.4456 | Val Loss: 1.2361 Acc: 0.4490                                               \n",
      "Epoch 035 | Train Loss: 1.2420 Acc: 0.4460 | Val Loss: 1.2344 Acc: 0.4490                                               \n",
      "Epoch 036 | Train Loss: 1.2404 Acc: 0.4457 | Val Loss: 1.2332 Acc: 0.4490                                               \n",
      "Epoch 037 | Train Loss: 1.2389 Acc: 0.4459 | Val Loss: 1.2310 Acc: 0.4490                                               \n",
      "Epoch 038 | Train Loss: 1.2381 Acc: 0.4457 | Val Loss: 1.2309 Acc: 0.4490                                               \n",
      "Epoch 039 | Train Loss: 1.2370 Acc: 0.4456 | Val Loss: 1.2284 Acc: 0.4490                                               \n",
      "Epoch 040 | Train Loss: 1.2360 Acc: 0.4456 | Val Loss: 1.2287 Acc: 0.4490                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 256, 'cnn_dropout': 0.365313335377801, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 96, 'learning_rate': 0.00016293923333189617, 'lstm_dense': 32, 'lstm_hidden_size': 96, 'lstm_layers': 2, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 29.8681 Acc: 0.3497 | Val Loss: 7.4205 Acc: 0.4212                                              \n",
      "Epoch 002 | Train Loss: 11.0842 Acc: 0.3991 | Val Loss: 2.7503 Acc: 0.4048                                              \n",
      "Epoch 003 | Train Loss: 7.2153 Acc: 0.4139 | Val Loss: 2.1932 Acc: 0.5173                                               \n",
      "Epoch 004 | Train Loss: 5.1759 Acc: 0.4368 | Val Loss: 1.5482 Acc: 0.5525                                               \n",
      "Epoch 005 | Train Loss: 4.1015 Acc: 0.4467 | Val Loss: 1.4563 Acc: 0.5782                                               \n",
      "Epoch 006 | Train Loss: 3.3838 Acc: 0.4646 | Val Loss: 1.3303 Acc: 0.5710                                               \n",
      "Epoch 007 | Train Loss: 2.9892 Acc: 0.4656 | Val Loss: 1.0303 Acc: 0.6134                                               \n",
      "Epoch 008 | Train Loss: 2.5964 Acc: 0.4892 | Val Loss: 1.2501 Acc: 0.5896                                               \n",
      "Epoch 009 | Train Loss: 2.2434 Acc: 0.5165 | Val Loss: 1.0519 Acc: 0.5696                                               \n",
      "Epoch 010 | Train Loss: 2.0294 Acc: 0.5422 | Val Loss: 1.3958 Acc: 0.5985                                               \n",
      "Epoch 011 | Train Loss: 1.8125 Acc: 0.5680 | Val Loss: 0.9621 Acc: 0.6833                                               \n",
      "Epoch 012 | Train Loss: 1.5820 Acc: 0.6039 | Val Loss: 0.9468 Acc: 0.6884                                               \n",
      "Epoch 013 | Train Loss: 1.4190 Acc: 0.6304 | Val Loss: 0.8896 Acc: 0.7364                                               \n",
      "Epoch 014 | Train Loss: 1.2667 Acc: 0.6550 | Val Loss: 0.7555 Acc: 0.7119                                               \n",
      "Epoch 015 | Train Loss: 1.1326 Acc: 0.6912 | Val Loss: 0.6996 Acc: 0.7821                                               \n",
      "Epoch 016 | Train Loss: 1.0052 Acc: 0.7139 | Val Loss: 0.5670 Acc: 0.8263                                               \n",
      "Epoch 017 | Train Loss: 0.8681 Acc: 0.7479 | Val Loss: 0.5307 Acc: 0.8275                                               \n",
      "Epoch 018 | Train Loss: 0.8011 Acc: 0.7635 | Val Loss: 0.4943 Acc: 0.8376                                               \n",
      "Epoch 019 | Train Loss: 0.6899 Acc: 0.7874 | Val Loss: 0.6130 Acc: 0.8113                                               \n",
      "Epoch 020 | Train Loss: 0.6356 Acc: 0.8101 | Val Loss: 0.3709 Acc: 0.8842                                               \n",
      "Epoch 021 | Train Loss: 0.5481 Acc: 0.8328 | Val Loss: 0.5082 Acc: 0.8313                                               \n",
      "Epoch 022 | Train Loss: 0.4779 Acc: 0.8557 | Val Loss: 0.3755 Acc: 0.8848                                               \n",
      "Epoch 023 | Train Loss: 0.4266 Acc: 0.8695 | Val Loss: 0.3154 Acc: 0.8878                                               \n",
      "Epoch 024 | Train Loss: 0.3994 Acc: 0.8790 | Val Loss: 0.2517 Acc: 0.9266                                               \n",
      "Epoch 025 | Train Loss: 0.3551 Acc: 0.8913 | Val Loss: 0.2467 Acc: 0.9251                                               \n",
      "Epoch 026 | Train Loss: 0.3151 Acc: 0.9028 | Val Loss: 0.2767 Acc: 0.9200                                               \n",
      "Epoch 027 | Train Loss: 0.2931 Acc: 0.9089 | Val Loss: 0.3395 Acc: 0.9128                                               \n",
      "Epoch 028 | Train Loss: 0.2773 Acc: 0.9178 | Val Loss: 0.2739 Acc: 0.9227                                               \n",
      "Epoch 029 | Train Loss: 0.2447 Acc: 0.9247 | Val Loss: 0.2089 Acc: 0.9382                                               \n",
      "Epoch 030 | Train Loss: 0.2171 Acc: 0.9325 | Val Loss: 0.2036 Acc: 0.9409                                               \n",
      "Epoch 031 | Train Loss: 0.1966 Acc: 0.9383 | Val Loss: 0.3053 Acc: 0.9119                                               \n",
      "Epoch 032 | Train Loss: 0.1780 Acc: 0.9440 | Val Loss: 0.1631 Acc: 0.9573                                               \n",
      "Epoch 033 | Train Loss: 0.1781 Acc: 0.9441 | Val Loss: 0.1681 Acc: 0.9531                                               \n",
      "Epoch 034 | Train Loss: 0.1614 Acc: 0.9501 | Val Loss: 0.1865 Acc: 0.9460                                               \n",
      "Epoch 035 | Train Loss: 0.1540 Acc: 0.9515 | Val Loss: 0.1696 Acc: 0.9478                                               \n",
      "Epoch 036 | Train Loss: 0.1516 Acc: 0.9514 | Val Loss: 0.1366 Acc: 0.9636                                               \n",
      "Epoch 037 | Train Loss: 0.1263 Acc: 0.9596 | Val Loss: 0.1419 Acc: 0.9579                                               \n",
      "Epoch 038 | Train Loss: 0.1209 Acc: 0.9625 | Val Loss: 0.1512 Acc: 0.9567                                               \n",
      "Epoch 039 | Train Loss: 0.1339 Acc: 0.9593 | Val Loss: 0.1259 Acc: 0.9660                                               \n",
      "Epoch 040 | Train Loss: 0.1026 Acc: 0.9677 | Val Loss: 0.1491 Acc: 0.9573                                               \n",
      "Epoch 041 | Train Loss: 0.1022 Acc: 0.9687 | Val Loss: 0.1201 Acc: 0.9678                                               \n",
      "Epoch 042 | Train Loss: 0.0866 Acc: 0.9723 | Val Loss: 0.1098 Acc: 0.9633                                               \n",
      "Epoch 043 | Train Loss: 0.0839 Acc: 0.9738 | Val Loss: 0.1143 Acc: 0.9663                                               \n",
      "Epoch 044 | Train Loss: 0.0820 Acc: 0.9741 | Val Loss: 0.1271 Acc: 0.9633                                               \n",
      "Epoch 045 | Train Loss: 0.1017 Acc: 0.9690 | Val Loss: 0.1356 Acc: 0.9636                                               \n",
      "Epoch 046 | Train Loss: 0.0680 Acc: 0.9785 | Val Loss: 0.1450 Acc: 0.9597                                               \n",
      "Epoch 047 | Train Loss: 0.0566 Acc: 0.9817 | Val Loss: 0.1002 Acc: 0.9719                                               \n",
      "Epoch 048 | Train Loss: 0.0629 Acc: 0.9806 | Val Loss: 0.1335 Acc: 0.9654                                               \n",
      "Epoch 049 | Train Loss: 0.0834 Acc: 0.9766 | Val Loss: 0.1538 Acc: 0.9597                                               \n",
      "Epoch 050 | Train Loss: 0.0746 Acc: 0.9789 | Val Loss: 0.1369 Acc: 0.9636                                               \n",
      "Epoch 051 | Train Loss: 0.0677 Acc: 0.9789 | Val Loss: 0.1339 Acc: 0.9663                                               \n",
      "Epoch 052 | Train Loss: 0.0753 Acc: 0.9772 | Val Loss: 0.1235 Acc: 0.9707                                               \n",
      "Epoch 053 | Train Loss: 0.0530 Acc: 0.9835 | Val Loss: 0.1668 Acc: 0.9606                                               \n",
      "Epoch 054 | Train Loss: 0.0438 Acc: 0.9865 | Val Loss: 0.0921 Acc: 0.9746                                               \n",
      "Epoch 055 | Train Loss: 0.0470 Acc: 0.9852 | Val Loss: 0.1293 Acc: 0.9696                                               \n",
      "Epoch 056 | Train Loss: 0.0661 Acc: 0.9790 | Val Loss: 0.1131 Acc: 0.9710                                               \n",
      "Epoch 057 | Train Loss: 0.0641 Acc: 0.9814 | Val Loss: 0.1332 Acc: 0.9666                                               \n",
      "Epoch 058 | Train Loss: 0.0441 Acc: 0.9855 | Val Loss: 0.1190 Acc: 0.9681                                               \n",
      "Epoch 059 | Train Loss: 0.0554 Acc: 0.9837 | Val Loss: 0.0990 Acc: 0.9716                                               \n",
      "Epoch 060 | Train Loss: 0.0384 Acc: 0.9876 | Val Loss: 0.0882 Acc: 0.9788                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 64, 'cnn_dropout': 0.05034497909722454, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 32, 'cnn_kernels_2': 32, 'learning_rate': 1.267138629699817e-05, 'lstm_dense': 64, 'lstm_hidden_size': 128, 'lstm_layers': 2, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 71.8898 Acc: 0.3098 | Val Loss: 27.6775 Acc: 0.4260                                             \n",
      "Epoch 002 | Train Loss: 57.4502 Acc: 0.3332 | Val Loss: 23.0599 Acc: 0.4257                                             \n",
      "Epoch 003 | Train Loss: 50.0307 Acc: 0.3317 | Val Loss: 20.8525 Acc: 0.4460                                             \n",
      "Epoch 004 | Train Loss: 42.1169 Acc: 0.3327 | Val Loss: 19.2558 Acc: 0.4513                                             \n",
      "Epoch 005 | Train Loss: 34.7174 Acc: 0.3407 | Val Loss: 17.3098 Acc: 0.4233                                             \n",
      "Epoch 006 | Train Loss: 29.9633 Acc: 0.3379 | Val Loss: 15.4770 Acc: 0.3699                                             \n",
      "Epoch 007 | Train Loss: 25.3325 Acc: 0.3380 | Val Loss: 13.6214 Acc: 0.3161                                             \n",
      "Epoch 008 | Train Loss: 22.2674 Acc: 0.3463 | Val Loss: 11.8598 Acc: 0.3087                                             \n",
      "Epoch 009 | Train Loss: 19.0528 Acc: 0.3511 | Val Loss: 12.0401 Acc: 0.2910                                             \n",
      "Epoch 010 | Train Loss: 16.7793 Acc: 0.3602 | Val Loss: 10.2377 Acc: 0.3018                                             \n",
      "Epoch 011 | Train Loss: 14.9872 Acc: 0.3719 | Val Loss: 10.1096 Acc: 0.2994                                             \n",
      "Epoch 012 | Train Loss: 13.5722 Acc: 0.3775 | Val Loss: 8.3402 Acc: 0.3382                                              \n",
      "Epoch 013 | Train Loss: 12.2998 Acc: 0.3825 | Val Loss: 7.5857 Acc: 0.3818                                              \n",
      "Epoch 014 | Train Loss: 11.2545 Acc: 0.4013 | Val Loss: 7.4109 Acc: 0.3934                                              \n",
      "Epoch 015 | Train Loss: 10.2925 Acc: 0.4077 | Val Loss: 6.7804 Acc: 0.4340                                              \n",
      "Epoch 016 | Train Loss: 9.4114 Acc: 0.4225 | Val Loss: 5.0476 Acc: 0.4946                                               \n",
      "Epoch 017 | Train Loss: 9.1172 Acc: 0.4262 | Val Loss: 5.5269 Acc: 0.4525                                               \n",
      "Epoch 018 | Train Loss: 8.4368 Acc: 0.4348 | Val Loss: 5.5118 Acc: 0.4301                                               \n",
      "Epoch 019 | Train Loss: 8.0749 Acc: 0.4457 | Val Loss: 4.9304 Acc: 0.4549                                               \n",
      "Epoch 020 | Train Loss: 7.7063 Acc: 0.4426 | Val Loss: 4.5293 Acc: 0.4818                                               \n",
      "Epoch 021 | Train Loss: 7.3485 Acc: 0.4549 | Val Loss: 3.9236 Acc: 0.5224                                               \n",
      "Epoch 022 | Train Loss: 6.9990 Acc: 0.4585 | Val Loss: 4.0103 Acc: 0.4985                                               \n",
      "Epoch 023 | Train Loss: 6.5315 Acc: 0.4702 | Val Loss: 3.5466 Acc: 0.5430                                               \n",
      "Epoch 024 | Train Loss: 6.3331 Acc: 0.4771 | Val Loss: 3.8285 Acc: 0.5215                                               \n",
      "Epoch 025 | Train Loss: 5.9415 Acc: 0.4791 | Val Loss: 3.2918 Acc: 0.5352                                               \n",
      "Epoch 026 | Train Loss: 5.6664 Acc: 0.4971 | Val Loss: 2.9597 Acc: 0.5633                                               \n",
      "Epoch 027 | Train Loss: 5.6121 Acc: 0.4912 | Val Loss: 3.1913 Acc: 0.5203                                               \n",
      "Epoch 028 | Train Loss: 5.3840 Acc: 0.4963 | Val Loss: 2.8098 Acc: 0.5794                                               \n",
      "Epoch 029 | Train Loss: 5.2770 Acc: 0.4964 | Val Loss: 2.6880 Acc: 0.5779                                               \n",
      "Epoch 030 | Train Loss: 4.9480 Acc: 0.5141 | Val Loss: 2.5323 Acc: 0.5842                                               \n",
      "Epoch 031 | Train Loss: 4.8034 Acc: 0.5129 | Val Loss: 2.5065 Acc: 0.5785                                               \n",
      "Epoch 032 | Train Loss: 4.7628 Acc: 0.5157 | Val Loss: 2.6001 Acc: 0.5925                                               \n",
      "Epoch 033 | Train Loss: 4.5378 Acc: 0.5249 | Val Loss: 2.6548 Acc: 0.5976                                               \n",
      "Epoch 034 | Train Loss: 4.4580 Acc: 0.5212 | Val Loss: 2.4800 Acc: 0.6006                                               \n",
      "Epoch 035 | Train Loss: 4.2299 Acc: 0.5243 | Val Loss: 2.3494 Acc: 0.6200                                               \n",
      "Epoch 036 | Train Loss: 4.1060 Acc: 0.5335 | Val Loss: 2.1899 Acc: 0.6215                                               \n",
      "Epoch 037 | Train Loss: 4.0502 Acc: 0.5360 | Val Loss: 2.7428 Acc: 0.5666                                               \n",
      "Epoch 038 | Train Loss: 3.8558 Acc: 0.5503 | Val Loss: 2.3310 Acc: 0.6290                                               \n",
      "Epoch 039 | Train Loss: 3.8905 Acc: 0.5392 | Val Loss: 1.8934 Acc: 0.6558                                               \n",
      "Epoch 040 | Train Loss: 3.5763 Acc: 0.5583 | Val Loss: 2.3143 Acc: 0.6176                                               \n",
      "Epoch 041 | Train Loss: 3.5363 Acc: 0.5566 | Val Loss: 2.1135 Acc: 0.6361                                               \n",
      "Epoch 042 | Train Loss: 3.3914 Acc: 0.5647 | Val Loss: 2.2249 Acc: 0.6481                                               \n",
      "Epoch 043 | Train Loss: 3.4246 Acc: 0.5603 | Val Loss: 2.2080 Acc: 0.6367                                               \n",
      "Epoch 044 | Train Loss: 3.2842 Acc: 0.5668 | Val Loss: 1.8420 Acc: 0.6749                                               \n",
      "Epoch 045 | Train Loss: 3.1267 Acc: 0.5754 | Val Loss: 2.0815 Acc: 0.6549                                               \n",
      "Epoch 046 | Train Loss: 3.0838 Acc: 0.5834 | Val Loss: 1.9877 Acc: 0.6513                                               \n",
      "Epoch 047 | Train Loss: 2.9976 Acc: 0.5765 | Val Loss: 2.0897 Acc: 0.6385                                               \n",
      "Epoch 048 | Train Loss: 2.9214 Acc: 0.5891 | Val Loss: 1.8260 Acc: 0.6704                                               \n",
      "Epoch 049 | Train Loss: 2.8970 Acc: 0.5878 | Val Loss: 1.8322 Acc: 0.6609                                               \n",
      "Epoch 050 | Train Loss: 2.8502 Acc: 0.5892 | Val Loss: 1.8669 Acc: 0.6603                                               \n",
      "Epoch 051 | Train Loss: 2.8314 Acc: 0.5844 | Val Loss: 1.7620 Acc: 0.6570                                               \n",
      "Epoch 052 | Train Loss: 2.6670 Acc: 0.6020 | Val Loss: 1.9364 Acc: 0.6621                                               \n",
      "Epoch 053 | Train Loss: 2.7143 Acc: 0.5908 | Val Loss: 1.5804 Acc: 0.6818                                               \n",
      "Epoch 054 | Train Loss: 2.6424 Acc: 0.5973 | Val Loss: 1.6816 Acc: 0.6812                                               \n",
      "Epoch 055 | Train Loss: 2.5199 Acc: 0.6062 | Val Loss: 1.7729 Acc: 0.6630                                               \n",
      "Epoch 056 | Train Loss: 2.5131 Acc: 0.6068 | Val Loss: 1.5698 Acc: 0.6767                                               \n",
      "Epoch 057 | Train Loss: 2.5359 Acc: 0.6057 | Val Loss: 1.7257 Acc: 0.6421                                               \n",
      "Epoch 058 | Train Loss: 2.3974 Acc: 0.6149 | Val Loss: 1.4612 Acc: 0.6893                                               \n",
      "Epoch 059 | Train Loss: 2.3986 Acc: 0.6145 | Val Loss: 1.7343 Acc: 0.6693                                               \n",
      "Epoch 060 | Train Loss: 2.3212 Acc: 0.6200 | Val Loss: 1.5784 Acc: 0.6609                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 128, 'cnn_dropout': 0.04210815889545831, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 32, 'cnn_kernels_2': 96, 'learning_rate': 0.0012183244944537492, 'lstm_dense': 128, 'lstm_hidden_size': 32, 'lstm_layers': 1, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 30.4903 Acc: 0.4079 | Val Loss: 3.9943 Acc: 0.5409                                              \n",
      "Epoch 002 | Train Loss: 6.2023 Acc: 0.4531 | Val Loss: 1.5091 Acc: 0.5997                                               \n",
      "Epoch 003 | Train Loss: 3.0793 Acc: 0.4870 | Val Loss: 1.0261 Acc: 0.6642                                               \n",
      "Epoch 004 | Train Loss: 1.9182 Acc: 0.5215 | Val Loss: 1.0381 Acc: 0.5767                                               \n",
      "Epoch 005 | Train Loss: 1.4702 Acc: 0.5496 | Val Loss: 1.3992 Acc: 0.5188                                               \n",
      "Epoch 006 | Train Loss: 1.3846 Acc: 0.5586 | Val Loss: 0.8146 Acc: 0.6451                                               \n",
      "Epoch 007 | Train Loss: 1.2312 Acc: 0.5771 | Val Loss: 0.7975 Acc: 0.6364                                               \n",
      "Epoch 008 | Train Loss: 0.9884 Acc: 0.6108 | Val Loss: 0.7042 Acc: 0.6931                                               \n",
      "Epoch 009 | Train Loss: 0.9357 Acc: 0.6136 | Val Loss: 0.6534 Acc: 0.7266                                               \n",
      "Epoch 010 | Train Loss: 0.8084 Acc: 0.6534 | Val Loss: 0.7088 Acc: 0.6949                                               \n",
      "Epoch 011 | Train Loss: 0.7821 Acc: 0.6592 | Val Loss: 0.6406 Acc: 0.7352                                               \n",
      "Epoch 012 | Train Loss: 0.7558 Acc: 0.6722 | Val Loss: 0.7566 Acc: 0.6263                                               \n",
      "Epoch 013 | Train Loss: 0.7452 Acc: 0.6728 | Val Loss: 0.7911 Acc: 0.6773                                               \n",
      "Epoch 014 | Train Loss: 0.7149 Acc: 0.6845 | Val Loss: 0.6780 Acc: 0.7075                                               \n",
      "Epoch 015 | Train Loss: 0.6792 Acc: 0.6974 | Val Loss: 0.6559 Acc: 0.7060                                               \n",
      "Epoch 016 | Train Loss: 0.6637 Acc: 0.7039 | Val Loss: 0.5669 Acc: 0.7669                                               \n",
      "Epoch 017 | Train Loss: 0.6603 Acc: 0.7082 | Val Loss: 0.5373 Acc: 0.7731                                               \n",
      "Epoch 018 | Train Loss: 0.6402 Acc: 0.7107 | Val Loss: 0.5464 Acc: 0.7582                                               \n",
      "Epoch 019 | Train Loss: 0.6533 Acc: 0.7098 | Val Loss: 0.5708 Acc: 0.7337                                               \n",
      "Epoch 020 | Train Loss: 0.6253 Acc: 0.7183 | Val Loss: 0.5306 Acc: 0.7612                                               \n",
      "Epoch 021 | Train Loss: 0.6350 Acc: 0.7181 | Val Loss: 0.7181 Acc: 0.6937                                               \n",
      "Epoch 022 | Train Loss: 0.6090 Acc: 0.7269 | Val Loss: 0.4648 Acc: 0.8099                                               \n",
      "Epoch 023 | Train Loss: 0.6054 Acc: 0.7328 | Val Loss: 0.5547 Acc: 0.7681                                               \n",
      "Epoch 024 | Train Loss: 0.6234 Acc: 0.7283 | Val Loss: 0.4862 Acc: 0.7836                                               \n",
      "Epoch 025 | Train Loss: 0.5989 Acc: 0.7342 | Val Loss: 0.5079 Acc: 0.7749                                               \n",
      "Epoch 026 | Train Loss: 0.5750 Acc: 0.7446 | Val Loss: 0.5971 Acc: 0.7200                                               \n",
      "Epoch 027 | Train Loss: 0.5637 Acc: 0.7457 | Val Loss: 0.4816 Acc: 0.7782                                               \n",
      "Epoch 028 | Train Loss: 0.5662 Acc: 0.7552 | Val Loss: 0.4685 Acc: 0.7779                                               \n",
      "Epoch 029 | Train Loss: 0.5727 Acc: 0.7498 | Val Loss: 0.5119 Acc: 0.7704                                               \n",
      "Epoch 030 | Train Loss: 0.5672 Acc: 0.7558 | Val Loss: 0.4524 Acc: 0.8072                                               \n",
      "Epoch 031 | Train Loss: 0.5534 Acc: 0.7552 | Val Loss: 0.5205 Acc: 0.7552                                               \n",
      "Epoch 032 | Train Loss: 0.5597 Acc: 0.7568 | Val Loss: 0.5216 Acc: 0.7499                                               \n",
      "Epoch 033 | Train Loss: 0.5206 Acc: 0.7737 | Val Loss: 0.4228 Acc: 0.7994                                               \n",
      "Epoch 034 | Train Loss: 0.5142 Acc: 0.7811 | Val Loss: 0.5342 Acc: 0.7752                                               \n",
      "Epoch 035 | Train Loss: 0.5386 Acc: 0.7614 | Val Loss: 0.4227 Acc: 0.8137                                               \n",
      "Epoch 036 | Train Loss: 0.5146 Acc: 0.7794 | Val Loss: 0.4611 Acc: 0.7815                                               \n",
      "Epoch 037 | Train Loss: 0.4944 Acc: 0.7927 | Val Loss: 0.4711 Acc: 0.7919                                               \n",
      "Epoch 038 | Train Loss: 0.5129 Acc: 0.7764 | Val Loss: 0.4652 Acc: 0.8125                                               \n",
      "Epoch 039 | Train Loss: 0.5085 Acc: 0.7821 | Val Loss: 0.4259 Acc: 0.8284                                               \n",
      "Epoch 040 | Train Loss: 0.4973 Acc: 0.7851 | Val Loss: 0.4986 Acc: 0.7534                                               \n",
      "Epoch 041 | Train Loss: 0.4970 Acc: 0.7882 | Val Loss: 0.4961 Acc: 0.8015                                               \n",
      "Epoch 042 | Train Loss: 0.5358 Acc: 0.7786 | Val Loss: 0.5693 Acc: 0.7651                                               \n",
      "Epoch 043 | Train Loss: 0.5076 Acc: 0.7883 | Val Loss: 0.5123 Acc: 0.7973                                               \n",
      "Epoch 044 | Train Loss: 0.5004 Acc: 0.7924 | Val Loss: 0.6369 Acc: 0.7379                                               \n",
      "Epoch 045 | Train Loss: 0.5013 Acc: 0.7877 | Val Loss: 0.4406 Acc: 0.8290                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 64, 'cnn_dropout': 0.6816311593018681, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 64, 'learning_rate': 5.20570699365634e-05, 'lstm_dense': 256, 'lstm_hidden_size': 128, 'lstm_layers': 6, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 54.6582 Acc: 0.3241 | Val Loss: 15.9496 Acc: 0.4531                                             \n",
      "Epoch 002 | Train Loss: 14.7242 Acc: 0.3802 | Val Loss: 2.9768 Acc: 0.5018                                              \n",
      "Epoch 003 | Train Loss: 9.3009 Acc: 0.4009 | Val Loss: 3.1096 Acc: 0.4794                                               \n",
      "Epoch 004 | Train Loss: 6.6277 Acc: 0.4200 | Val Loss: 2.5543 Acc: 0.4872                                               \n",
      "Epoch 005 | Train Loss: 5.3787 Acc: 0.4303 | Val Loss: 2.2443 Acc: 0.4749                                               \n",
      "Epoch 006 | Train Loss: 4.4094 Acc: 0.4427 | Val Loss: 1.8699 Acc: 0.5009                                               \n",
      "Epoch 007 | Train Loss: 3.8596 Acc: 0.4388 | Val Loss: 2.6344 Acc: 0.4630                                               \n",
      "Epoch 008 | Train Loss: 3.1634 Acc: 0.4545 | Val Loss: 2.7857 Acc: 0.3373                                               \n",
      "Epoch 009 | Train Loss: 2.7742 Acc: 0.4652 | Val Loss: 1.9335 Acc: 0.5197                                               \n",
      "Epoch 010 | Train Loss: 2.4789 Acc: 0.4765 | Val Loss: 1.3621 Acc: 0.5266                                               \n",
      "Epoch 011 | Train Loss: 2.2011 Acc: 0.4980 | Val Loss: 1.3622 Acc: 0.5994                                               \n",
      "Epoch 012 | Train Loss: 1.9969 Acc: 0.5132 | Val Loss: 0.9657 Acc: 0.6516                                               \n",
      "Epoch 013 | Train Loss: 1.7252 Acc: 0.5453 | Val Loss: 0.8670 Acc: 0.6081                                               \n",
      "Epoch 014 | Train Loss: 1.6090 Acc: 0.5601 | Val Loss: 0.8763 Acc: 0.6481                                               \n",
      "Epoch 015 | Train Loss: 1.4837 Acc: 0.5672 | Val Loss: 0.8896 Acc: 0.6504                                               \n",
      "Epoch 016 | Train Loss: 1.3352 Acc: 0.5921 | Val Loss: 0.7954 Acc: 0.6994                                               \n",
      "Epoch 017 | Train Loss: 1.1933 Acc: 0.6095 | Val Loss: 0.7850 Acc: 0.6645                                               \n",
      "Epoch 018 | Train Loss: 1.1291 Acc: 0.6343 | Val Loss: 1.4100 Acc: 0.5534                                               \n",
      "Epoch 019 | Train Loss: 1.0462 Acc: 0.6442 | Val Loss: 0.6132 Acc: 0.7316                                               \n",
      "Epoch 020 | Train Loss: 0.9832 Acc: 0.6577 | Val Loss: 0.8149 Acc: 0.6809                                               \n",
      "Epoch 021 | Train Loss: 0.9135 Acc: 0.6725 | Val Loss: 0.7311 Acc: 0.7012                                               \n",
      "Epoch 022 | Train Loss: 0.8731 Acc: 0.6911 | Val Loss: 0.9682 Acc: 0.6684                                               \n",
      "Epoch 023 | Train Loss: 0.8083 Acc: 0.7077 | Val Loss: 0.7490 Acc: 0.7269                                               \n",
      "Epoch 024 | Train Loss: 0.7637 Acc: 0.7200 | Val Loss: 0.7071 Acc: 0.7501                                               \n",
      "Epoch 025 | Train Loss: 0.7158 Acc: 0.7337 | Val Loss: 0.6139 Acc: 0.7806                                               \n",
      "Epoch 026 | Train Loss: 0.6753 Acc: 0.7522 | Val Loss: 0.6125 Acc: 0.7878                                               \n",
      "Epoch 027 | Train Loss: 0.6646 Acc: 0.7559 | Val Loss: 0.5989 Acc: 0.7875                                               \n",
      "Epoch 028 | Train Loss: 0.6186 Acc: 0.7751 | Val Loss: 0.6805 Acc: 0.7570                                               \n",
      "Epoch 029 | Train Loss: 0.5930 Acc: 0.7762 | Val Loss: 0.6591 Acc: 0.7731                                               \n",
      "Epoch 030 | Train Loss: 0.5580 Acc: 0.7926 | Val Loss: 1.3218 Acc: 0.6687                                               \n",
      "Epoch 031 | Train Loss: 0.5461 Acc: 0.7970 | Val Loss: 1.2274 Acc: 0.6896                                               \n",
      "Epoch 032 | Train Loss: 0.5159 Acc: 0.8071 | Val Loss: 0.5220 Acc: 0.8233                                               \n",
      "Epoch 033 | Train Loss: 0.5067 Acc: 0.8127 | Val Loss: 0.4581 Acc: 0.8248                                               \n",
      "Epoch 034 | Train Loss: 0.4821 Acc: 0.8198 | Val Loss: 0.5711 Acc: 0.7722                                               \n",
      "Epoch 035 | Train Loss: 0.4505 Acc: 0.8314 | Val Loss: 0.3884 Acc: 0.8687                                               \n",
      "Epoch 036 | Train Loss: 0.4363 Acc: 0.8383 | Val Loss: 0.4273 Acc: 0.8555                                               \n",
      "Epoch 037 | Train Loss: 0.4217 Acc: 0.8414 | Val Loss: 0.4424 Acc: 0.8370                                               \n",
      "Epoch 038 | Train Loss: 0.4042 Acc: 0.8477 | Val Loss: 0.3522 Acc: 0.8791                                               \n",
      "Epoch 039 | Train Loss: 0.3898 Acc: 0.8582 | Val Loss: 0.4213 Acc: 0.8427                                               \n",
      "Epoch 040 | Train Loss: 0.3784 Acc: 0.8635 | Val Loss: 0.4584 Acc: 0.8248                                               \n",
      "Epoch 041 | Train Loss: 0.3583 Acc: 0.8689 | Val Loss: 0.3595 Acc: 0.8618                                               \n",
      "Epoch 042 | Train Loss: 0.3448 Acc: 0.8726 | Val Loss: 0.3994 Acc: 0.8654                                               \n",
      "Epoch 043 | Train Loss: 0.3388 Acc: 0.8775 | Val Loss: 0.3631 Acc: 0.8669                                               \n",
      "Epoch 044 | Train Loss: 0.3277 Acc: 0.8841 | Val Loss: 0.3154 Acc: 0.8872                                               \n",
      "Epoch 045 | Train Loss: 0.3181 Acc: 0.8865 | Val Loss: 0.3009 Acc: 0.8985                                               \n",
      "Epoch 046 | Train Loss: 0.2952 Acc: 0.8937 | Val Loss: 1.1729 Acc: 0.6636                                               \n",
      "Epoch 047 | Train Loss: 0.2991 Acc: 0.8930 | Val Loss: 0.3200 Acc: 0.8797                                               \n",
      "Epoch 048 | Train Loss: 0.2825 Acc: 0.8987 | Val Loss: 0.3725 Acc: 0.8749                                               \n",
      "Epoch 049 | Train Loss: 0.2675 Acc: 0.9043 | Val Loss: 0.3047 Acc: 0.8964                                               \n",
      "Epoch 050 | Train Loss: 0.2689 Acc: 0.9048 | Val Loss: 0.6160 Acc: 0.7890                                               \n",
      "Epoch 051 | Train Loss: 0.2553 Acc: 0.9104 | Val Loss: 0.2370 Acc: 0.9251                                               \n",
      "Epoch 052 | Train Loss: 0.2425 Acc: 0.9159 | Val Loss: 0.2977 Acc: 0.8994                                               \n",
      "Epoch 053 | Train Loss: 0.2389 Acc: 0.9166 | Val Loss: 0.2112 Acc: 0.9263                                               \n",
      "Epoch 054 | Train Loss: 0.2220 Acc: 0.9216 | Val Loss: 0.2427 Acc: 0.9161                                               \n",
      "Epoch 055 | Train Loss: 0.2178 Acc: 0.9231 | Val Loss: 0.1817 Acc: 0.9367                                               \n",
      "Epoch 056 | Train Loss: 0.2085 Acc: 0.9276 | Val Loss: 0.7736 Acc: 0.7627                                               \n",
      "Epoch 057 | Train Loss: 0.2093 Acc: 0.9296 | Val Loss: 0.2812 Acc: 0.9057                                               \n",
      "Epoch 058 | Train Loss: 0.1987 Acc: 0.9304 | Val Loss: 0.2711 Acc: 0.9063                                               \n",
      "Epoch 059 | Train Loss: 0.1909 Acc: 0.9340 | Val Loss: 0.3841 Acc: 0.8600                                               \n",
      "Epoch 060 | Train Loss: 0.1814 Acc: 0.9384 | Val Loss: 0.2142 Acc: 0.9293                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 32, 'cnn_dropout': 0.5312281431133524, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 16, 'learning_rate': 2.8385097496837277e-05, 'lstm_dense': 64, 'lstm_hidden_size': 64, 'lstm_layers': 4, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 230.1049 Acc: 0.2393 | Val Loss: 112.7259 Acc: 0.2549                                           \n",
      "Epoch 002 | Train Loss: 73.0102 Acc: 0.2650 | Val Loss: 29.7798 Acc: 0.3582                                             \n",
      "Epoch 003 | Train Loss: 46.0494 Acc: 0.3152 | Val Loss: 20.9929 Acc: 0.3615                                             \n",
      "Epoch 004 | Train Loss: 34.6887 Acc: 0.3242 | Val Loss: 13.2797 Acc: 0.4299                                             \n",
      "Epoch 005 | Train Loss: 27.3611 Acc: 0.3387 | Val Loss: 10.6830 Acc: 0.4678                                             \n",
      "Epoch 006 | Train Loss: 21.1036 Acc: 0.3524 | Val Loss: 8.1185 Acc: 0.4779                                              \n",
      "Epoch 007 | Train Loss: 17.6584 Acc: 0.3628 | Val Loss: 7.3597 Acc: 0.5006                                              \n",
      "Epoch 008 | Train Loss: 15.1950 Acc: 0.3667 | Val Loss: 5.7808 Acc: 0.5110                                              \n",
      "Epoch 009 | Train Loss: 12.8367 Acc: 0.3841 | Val Loss: 4.6450 Acc: 0.4725                                              \n",
      "Epoch 010 | Train Loss: 10.7420 Acc: 0.3996 | Val Loss: 4.1389 Acc: 0.4773                                              \n",
      "Epoch 011 | Train Loss: 9.8021 Acc: 0.3928 | Val Loss: 3.7634 Acc: 0.4946                                               \n",
      "Epoch 012 | Train Loss: 8.9517 Acc: 0.3948 | Val Loss: 3.1429 Acc: 0.4919                                               \n",
      "Epoch 013 | Train Loss: 8.3175 Acc: 0.4029 | Val Loss: 2.6921 Acc: 0.4821                                               \n",
      "Epoch 014 | Train Loss: 7.6449 Acc: 0.4039 | Val Loss: 2.5377 Acc: 0.4066                                               \n",
      "Epoch 015 | Train Loss: 6.8466 Acc: 0.4197 | Val Loss: 2.0945 Acc: 0.4701                                               \n",
      "Epoch 016 | Train Loss: 6.2813 Acc: 0.4136 | Val Loss: 2.9534 Acc: 0.4200                                               \n",
      "Epoch 017 | Train Loss: 5.9453 Acc: 0.4220 | Val Loss: 1.9132 Acc: 0.4328                                               \n",
      "Epoch 018 | Train Loss: 5.4441 Acc: 0.4268 | Val Loss: 1.6370 Acc: 0.5158                                               \n",
      "Epoch 019 | Train Loss: 5.1751 Acc: 0.4268 | Val Loss: 1.5350 Acc: 0.5304                                               \n",
      "Epoch 020 | Train Loss: 4.7391 Acc: 0.4358 | Val Loss: 1.4999 Acc: 0.5579                                               \n",
      "Epoch 021 | Train Loss: 4.5928 Acc: 0.4371 | Val Loss: 1.4253 Acc: 0.5200                                               \n",
      "Epoch 022 | Train Loss: 4.0855 Acc: 0.4479 | Val Loss: 1.3357 Acc: 0.5203                                               \n",
      "Epoch 023 | Train Loss: 3.9581 Acc: 0.4502 | Val Loss: 1.2476 Acc: 0.5734                                               \n",
      "Epoch 024 | Train Loss: 3.8282 Acc: 0.4526 | Val Loss: 1.2564 Acc: 0.5639                                               \n",
      "Epoch 025 | Train Loss: 3.5069 Acc: 0.4676 | Val Loss: 1.2499 Acc: 0.5185                                               \n",
      "Epoch 026 | Train Loss: 3.3508 Acc: 0.4713 | Val Loss: 1.2135 Acc: 0.5651                                               \n",
      "Epoch 027 | Train Loss: 3.0970 Acc: 0.4877 | Val Loss: 1.1367 Acc: 0.5893                                               \n",
      "Epoch 028 | Train Loss: 2.9371 Acc: 0.4917 | Val Loss: 1.1234 Acc: 0.6075                                               \n",
      "Epoch 029 | Train Loss: 2.9192 Acc: 0.4854 | Val Loss: 1.4567 Acc: 0.5507                                               \n",
      "Epoch 030 | Train Loss: 2.7968 Acc: 0.4997 | Val Loss: 1.1587 Acc: 0.5973                                               \n",
      "Epoch 031 | Train Loss: 2.6828 Acc: 0.4985 | Val Loss: 1.1852 Acc: 0.5666                                               \n",
      "Epoch 032 | Train Loss: 2.5368 Acc: 0.5115 | Val Loss: 0.9575 Acc: 0.5893                                               \n",
      "Epoch 033 | Train Loss: 2.5007 Acc: 0.5100 | Val Loss: 1.0650 Acc: 0.6012                                               \n",
      "Epoch 034 | Train Loss: 2.3215 Acc: 0.5147 | Val Loss: 1.0615 Acc: 0.6119                                               \n",
      "Epoch 035 | Train Loss: 2.3169 Acc: 0.5195 | Val Loss: 1.1465 Acc: 0.5782                                               \n",
      "Epoch 036 | Train Loss: 2.1956 Acc: 0.5214 | Val Loss: 1.0724 Acc: 0.6146                                               \n",
      "Epoch 037 | Train Loss: 2.1494 Acc: 0.5250 | Val Loss: 1.0067 Acc: 0.5985                                               \n",
      "Epoch 038 | Train Loss: 2.0312 Acc: 0.5341 | Val Loss: 1.2319 Acc: 0.5773                                               \n",
      "Epoch 039 | Train Loss: 1.9843 Acc: 0.5378 | Val Loss: 1.2994 Acc: 0.5519                                               \n",
      "Epoch 040 | Train Loss: 1.9626 Acc: 0.5307 | Val Loss: 1.0578 Acc: 0.5928                                               \n",
      "Epoch 041 | Train Loss: 1.8812 Acc: 0.5442 | Val Loss: 1.0405 Acc: 0.6063                                               \n",
      "Epoch 042 | Train Loss: 1.8370 Acc: 0.5402 | Val Loss: 0.9094 Acc: 0.6522                                               \n",
      "Epoch 043 | Train Loss: 1.7653 Acc: 0.5507 | Val Loss: 0.9171 Acc: 0.5866                                               \n",
      "Epoch 044 | Train Loss: 1.7008 Acc: 0.5575 | Val Loss: 0.9178 Acc: 0.6627                                               \n",
      "Epoch 045 | Train Loss: 1.6600 Acc: 0.5612 | Val Loss: 1.0518 Acc: 0.6137                                               \n",
      "Epoch 046 | Train Loss: 1.6163 Acc: 0.5691 | Val Loss: 1.1145 Acc: 0.6146                                               \n",
      "Epoch 047 | Train Loss: 1.5220 Acc: 0.5711 | Val Loss: 0.9066 Acc: 0.6075                                               \n",
      "Epoch 048 | Train Loss: 1.4842 Acc: 0.5753 | Val Loss: 0.9165 Acc: 0.6075                                               \n",
      "Epoch 049 | Train Loss: 1.4844 Acc: 0.5759 | Val Loss: 0.9209 Acc: 0.6537                                               \n",
      "Epoch 050 | Train Loss: 1.4057 Acc: 0.5876 | Val Loss: 1.1096 Acc: 0.6143                                               \n",
      "Epoch 051 | Train Loss: 1.4008 Acc: 0.5845 | Val Loss: 0.8473 Acc: 0.5982                                               \n",
      "Epoch 052 | Train Loss: 1.3224 Acc: 0.5907 | Val Loss: 0.9196 Acc: 0.6528                                               \n",
      "Epoch 053 | Train Loss: 1.3208 Acc: 0.5933 | Val Loss: 0.7865 Acc: 0.6669                                               \n",
      "Epoch 054 | Train Loss: 1.2411 Acc: 0.6047 | Val Loss: 0.9238 Acc: 0.6349                                               \n",
      "Epoch 055 | Train Loss: 1.2322 Acc: 0.6079 | Val Loss: 1.0974 Acc: 0.5827                                               \n",
      "Epoch 056 | Train Loss: 1.2380 Acc: 0.6089 | Val Loss: 0.9186 Acc: 0.5919                                               \n",
      "Epoch 057 | Train Loss: 1.2145 Acc: 0.6186 | Val Loss: 0.8104 Acc: 0.6409                                               \n",
      "Epoch 058 | Train Loss: 1.1970 Acc: 0.6134 | Val Loss: 0.7809 Acc: 0.6636                                               \n",
      "Epoch 059 | Train Loss: 1.1507 Acc: 0.6260 | Val Loss: 0.8333 Acc: 0.6690                                               \n",
      "Epoch 060 | Train Loss: 1.1340 Acc: 0.6268 | Val Loss: 0.8563 Acc: 0.6809                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 32, 'cnn_dropout': 0.5987874933430917, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 96, 'learning_rate': 0.0012295937684863188, 'lstm_dense': 256, 'lstm_hidden_size': 32, 'lstm_layers': 6, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 4.8247 Acc: 0.4942 | Val Loss: 0.9480 Acc: 0.6281                                               \n",
      "Epoch 002 | Train Loss: 0.8837 Acc: 0.6188 | Val Loss: 0.6859 Acc: 0.7003                                               \n",
      "Epoch 003 | Train Loss: 0.7948 Acc: 0.6353 | Val Loss: 0.6071 Acc: 0.7696                                               \n",
      "Epoch 004 | Train Loss: 0.7269 Acc: 0.6663 | Val Loss: 0.8238 Acc: 0.6116                                               \n",
      "Epoch 005 | Train Loss: 0.6759 Acc: 0.6945 | Val Loss: 0.6924 Acc: 0.6042                                               \n",
      "Epoch 006 | Train Loss: 0.6583 Acc: 0.7063 | Val Loss: 0.6281 Acc: 0.7161                                               \n",
      "Epoch 007 | Train Loss: 0.6491 Acc: 0.7132 | Val Loss: 0.5421 Acc: 0.7737                                               \n",
      "Epoch 008 | Train Loss: 0.6112 Acc: 0.7330 | Val Loss: 0.4869 Acc: 0.8182                                               \n",
      "Epoch 009 | Train Loss: 0.6034 Acc: 0.7352 | Val Loss: 0.5468 Acc: 0.7487                                               \n",
      "Epoch 010 | Train Loss: 0.5946 Acc: 0.7442 | Val Loss: 0.5987 Acc: 0.7782                                               \n",
      "Epoch 011 | Train Loss: 0.5821 Acc: 0.7416 | Val Loss: 0.5143 Acc: 0.7639                                               \n",
      "Epoch 012 | Train Loss: 0.5638 Acc: 0.7530 | Val Loss: 0.4899 Acc: 0.8140                                               \n",
      "Epoch 013 | Train Loss: 0.5733 Acc: 0.7524 | Val Loss: 0.5893 Acc: 0.7618                                               \n",
      "Epoch 014 | Train Loss: 0.5695 Acc: 0.7538 | Val Loss: 0.5630 Acc: 0.7570                                               \n",
      "Epoch 015 | Train Loss: 0.5661 Acc: 0.7530 | Val Loss: 0.5272 Acc: 0.7806                                               \n",
      "Epoch 016 | Train Loss: 0.5709 Acc: 0.7577 | Val Loss: 0.5487 Acc: 0.7585                                               \n",
      "Epoch 017 | Train Loss: 0.6412 Acc: 0.7168 | Val Loss: 0.5809 Acc: 0.7681                                               \n",
      "Epoch 018 | Train Loss: 0.6129 Acc: 0.7274 | Val Loss: 0.6698 Acc: 0.7182                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 128, 'cnn_dropout': 0.6811145882704882, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 16, 'cnn_kernels_2': 64, 'learning_rate': 0.0011069262252264192, 'lstm_dense': 32, 'lstm_hidden_size': 96, 'lstm_layers': 3, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 8.8643 Acc: 0.4208 | Val Loss: 1.2513 Acc: 0.4421                                               \n",
      "Epoch 002 | Train Loss: 1.2422 Acc: 0.4422 | Val Loss: 1.2309 Acc: 0.4421                                               \n",
      "Epoch 003 | Train Loss: 1.2296 Acc: 0.4417 | Val Loss: 1.2129 Acc: 0.4466                                               \n",
      "Epoch 004 | Train Loss: 1.2040 Acc: 0.4562 | Val Loss: 1.1800 Acc: 0.4779                                               \n",
      "Epoch 005 | Train Loss: 1.1785 Acc: 0.4734 | Val Loss: 1.1518 Acc: 0.4881                                               \n",
      "Epoch 006 | Train Loss: 1.1517 Acc: 0.4940 | Val Loss: 1.1363 Acc: 0.5099                                               \n",
      "Epoch 007 | Train Loss: 1.1249 Acc: 0.5111 | Val Loss: 1.1215 Acc: 0.5287                                               \n",
      "Epoch 008 | Train Loss: 1.1257 Acc: 0.5100 | Val Loss: 1.0811 Acc: 0.5430                                               \n",
      "Epoch 009 | Train Loss: 1.0937 Acc: 0.5265 | Val Loss: 1.0724 Acc: 0.5403                                               \n",
      "Epoch 010 | Train Loss: 1.0634 Acc: 0.5427 | Val Loss: 1.0516 Acc: 0.5561                                               \n",
      "Epoch 011 | Train Loss: 1.0437 Acc: 0.5483 | Val Loss: 0.9968 Acc: 0.5719                                               \n",
      "Epoch 012 | Train Loss: 1.0119 Acc: 0.5645 | Val Loss: 0.9773 Acc: 0.5704                                               \n",
      "Epoch 013 | Train Loss: 0.9787 Acc: 0.5759 | Val Loss: 0.9696 Acc: 0.5633                                               \n",
      "Epoch 014 | Train Loss: 0.9459 Acc: 0.5844 | Val Loss: 0.9371 Acc: 0.5991                                               \n",
      "Epoch 015 | Train Loss: 0.9251 Acc: 0.5922 | Val Loss: 0.8928 Acc: 0.5884                                               \n",
      "Epoch 016 | Train Loss: 0.8904 Acc: 0.6041 | Val Loss: 0.8199 Acc: 0.6564                                               \n",
      "Epoch 017 | Train Loss: 0.8657 Acc: 0.6220 | Val Loss: 0.8155 Acc: 0.6609                                               \n",
      "Epoch 018 | Train Loss: 0.8399 Acc: 0.6368 | Val Loss: 0.8541 Acc: 0.6493                                               \n",
      "Epoch 019 | Train Loss: 0.8085 Acc: 0.6531 | Val Loss: 0.8317 Acc: 0.6603                                               \n",
      "Epoch 020 | Train Loss: 0.8029 Acc: 0.6582 | Val Loss: 0.7886 Acc: 0.6907                                               \n",
      "Epoch 021 | Train Loss: 0.7782 Acc: 0.6764 | Val Loss: 0.8815 Acc: 0.6800                                               \n",
      "Epoch 022 | Train Loss: 0.7689 Acc: 0.6745 | Val Loss: 0.7315 Acc: 0.7104                                               \n",
      "Epoch 023 | Train Loss: 0.7501 Acc: 0.6881 | Val Loss: 0.9406 Acc: 0.6603                                               \n",
      "Epoch 024 | Train Loss: 0.7134 Acc: 0.7030 | Val Loss: 0.8074 Acc: 0.7084                                               \n",
      "Epoch 025 | Train Loss: 0.7049 Acc: 0.7135 | Val Loss: 0.6927 Acc: 0.7310                                               \n",
      "Epoch 026 | Train Loss: 0.6737 Acc: 0.7328 | Val Loss: 0.6097 Acc: 0.7696                                               \n",
      "Epoch 027 | Train Loss: 0.6456 Acc: 0.7510 | Val Loss: 0.6511 Acc: 0.7618                                               \n",
      "Epoch 028 | Train Loss: 0.6136 Acc: 0.7654 | Val Loss: 0.6689 Acc: 0.7621                                               \n",
      "Epoch 029 | Train Loss: 0.5607 Acc: 0.7909 | Val Loss: 0.6386 Acc: 0.7833                                               \n",
      "Epoch 030 | Train Loss: 0.5287 Acc: 0.8026 | Val Loss: 0.6011 Acc: 0.7884                                               \n",
      "Epoch 031 | Train Loss: 0.4695 Acc: 0.8244 | Val Loss: 0.5260 Acc: 0.8116                                               \n",
      "Epoch 032 | Train Loss: 0.4554 Acc: 0.8289 | Val Loss: 0.4223 Acc: 0.8388                                               \n",
      "Epoch 033 | Train Loss: 0.4483 Acc: 0.8313 | Val Loss: 0.3915 Acc: 0.8528                                               \n",
      "Epoch 034 | Train Loss: 0.4149 Acc: 0.8451 | Val Loss: 0.4487 Acc: 0.8460                                               \n",
      "Epoch 035 | Train Loss: 0.3863 Acc: 0.8577 | Val Loss: 0.4794 Acc: 0.8391                                               \n",
      "Epoch 036 | Train Loss: 0.3792 Acc: 0.8607 | Val Loss: 0.3550 Acc: 0.8725                                               \n",
      "Epoch 037 | Train Loss: 0.3604 Acc: 0.8691 | Val Loss: 0.3362 Acc: 0.8770                                               \n",
      "Epoch 038 | Train Loss: 0.3503 Acc: 0.8763 | Val Loss: 0.3692 Acc: 0.8704                                               \n",
      "Epoch 039 | Train Loss: 0.3356 Acc: 0.8808 | Val Loss: 0.3203 Acc: 0.8922                                               \n",
      "Epoch 040 | Train Loss: 0.3077 Acc: 0.8955 | Val Loss: 0.3219 Acc: 0.8913                                               \n",
      "Epoch 041 | Train Loss: 0.3045 Acc: 0.8971 | Val Loss: 0.3156 Acc: 0.9000                                               \n",
      "Epoch 042 | Train Loss: 0.2924 Acc: 0.8991 | Val Loss: 0.3814 Acc: 0.8809                                               \n",
      "Epoch 043 | Train Loss: 0.2761 Acc: 0.9065 | Val Loss: 0.3848 Acc: 0.8728                                               \n",
      "Epoch 044 | Train Loss: 0.2865 Acc: 0.9028 | Val Loss: 0.2910 Acc: 0.9113                                               \n",
      "Epoch 045 | Train Loss: 0.2899 Acc: 0.9037 | Val Loss: 0.4219 Acc: 0.8639                                               \n",
      "Epoch 046 | Train Loss: 0.2592 Acc: 0.9142 | Val Loss: 0.3034 Acc: 0.9090                                               \n",
      "Epoch 047 | Train Loss: 0.2496 Acc: 0.9193 | Val Loss: 0.3365 Acc: 0.8937                                               \n",
      "Epoch 048 | Train Loss: 0.2636 Acc: 0.9114 | Val Loss: 0.2768 Acc: 0.9149                                               \n",
      "Epoch 049 | Train Loss: 0.2226 Acc: 0.9246 | Val Loss: 0.3619 Acc: 0.8878                                               \n",
      "Epoch 050 | Train Loss: 0.2307 Acc: 0.9242 | Val Loss: 0.2620 Acc: 0.9221                                               \n",
      "Epoch 051 | Train Loss: 0.2082 Acc: 0.9308 | Val Loss: 0.3120 Acc: 0.9039                                               \n",
      "Epoch 052 | Train Loss: 0.2110 Acc: 0.9315 | Val Loss: 0.2546 Acc: 0.9185                                               \n",
      "Epoch 053 | Train Loss: 0.1925 Acc: 0.9383 | Val Loss: 0.2001 Acc: 0.9322                                               \n",
      "Epoch 054 | Train Loss: 0.2009 Acc: 0.9313 | Val Loss: 0.2298 Acc: 0.9239                                               \n",
      "Epoch 055 | Train Loss: 0.1795 Acc: 0.9395 | Val Loss: 0.2519 Acc: 0.9212                                               \n",
      "Epoch 056 | Train Loss: 0.1701 Acc: 0.9411 | Val Loss: 0.2480 Acc: 0.9170                                               \n",
      "Epoch 057 | Train Loss: 0.1736 Acc: 0.9412 | Val Loss: 0.2160 Acc: 0.9331                                               \n",
      "Epoch 058 | Train Loss: 0.1644 Acc: 0.9435 | Val Loss: 0.2368 Acc: 0.9257                                               \n",
      "Epoch 059 | Train Loss: 0.1509 Acc: 0.9490 | Val Loss: 0.2948 Acc: 0.9099                                               \n",
      "Epoch 060 | Train Loss: 0.1532 Acc: 0.9484 | Val Loss: 0.2751 Acc: 0.9167                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 64, 'cnn_dropout': 0.523390567528538, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 48, 'cnn_kernels_2': 16, 'learning_rate': 1.267744698979078e-05, 'lstm_dense': 128, 'lstm_hidden_size': 64, 'lstm_layers': 5, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 11.1943 Acc: 0.3932 | Val Loss: 1.2088 Acc: 0.4078                                              \n",
      "Epoch 002 | Train Loss: 1.4744 Acc: 0.4262 | Val Loss: 1.2280 Acc: 0.3830                                               \n",
      "Epoch 003 | Train Loss: 1.2647 Acc: 0.4341 | Val Loss: 1.1783 Acc: 0.4233                                               \n",
      "Epoch 004 | Train Loss: 1.2079 Acc: 0.4317 | Val Loss: 1.1192 Acc: 0.4534                                               \n",
      "Epoch 005 | Train Loss: 1.1642 Acc: 0.4424 | Val Loss: 1.0831 Acc: 0.4322                                               \n",
      "Epoch 006 | Train Loss: 1.1342 Acc: 0.4512 | Val Loss: 1.0712 Acc: 0.4349                                               \n",
      "Epoch 007 | Train Loss: 1.1014 Acc: 0.4535 | Val Loss: 1.0206 Acc: 0.5313                                               \n",
      "Epoch 008 | Train Loss: 1.0810 Acc: 0.4626 | Val Loss: 1.0466 Acc: 0.4696                                               \n",
      "Epoch 009 | Train Loss: 1.0806 Acc: 0.4618 | Val Loss: 1.0088 Acc: 0.4797                                               \n",
      "Epoch 010 | Train Loss: 1.0622 Acc: 0.4663 | Val Loss: 0.9515 Acc: 0.5504                                               \n",
      "Epoch 011 | Train Loss: 1.0392 Acc: 0.4719 | Val Loss: 0.9995 Acc: 0.5221                                               \n",
      "Epoch 012 | Train Loss: 1.0379 Acc: 0.4713 | Val Loss: 0.9269 Acc: 0.5642                                               \n",
      "Epoch 013 | Train Loss: 1.0193 Acc: 0.4685 | Val Loss: 0.9378 Acc: 0.4579                                               \n",
      "Epoch 014 | Train Loss: 1.0146 Acc: 0.4797 | Val Loss: 0.9251 Acc: 0.4579                                               \n",
      "Epoch 015 | Train Loss: 0.9872 Acc: 0.4865 | Val Loss: 0.9393 Acc: 0.4484                                               \n",
      "Epoch 016 | Train Loss: 0.9975 Acc: 0.4841 | Val Loss: 0.9676 Acc: 0.4731                                               \n",
      "Epoch 017 | Train Loss: 0.9862 Acc: 0.4865 | Val Loss: 0.9000 Acc: 0.4901                                               \n",
      "Epoch 018 | Train Loss: 0.9826 Acc: 0.4900 | Val Loss: 0.8998 Acc: 0.4660                                               \n",
      "Epoch 019 | Train Loss: 0.9798 Acc: 0.4889 | Val Loss: 0.9312 Acc: 0.4534                                               \n",
      "Epoch 020 | Train Loss: 0.9794 Acc: 0.4985 | Val Loss: 0.9097 Acc: 0.5301                                               \n",
      "Epoch 021 | Train Loss: 0.9568 Acc: 0.5041 | Val Loss: 0.8913 Acc: 0.5767                                               \n",
      "Epoch 022 | Train Loss: 0.9670 Acc: 0.4968 | Val Loss: 0.9297 Acc: 0.5370                                               \n",
      "Epoch 023 | Train Loss: 0.9632 Acc: 0.5018 | Val Loss: 0.9001 Acc: 0.5639                                               \n",
      "Epoch 024 | Train Loss: 0.9543 Acc: 0.5023 | Val Loss: 0.8929 Acc: 0.5242                                               \n",
      "Epoch 025 | Train Loss: 0.9492 Acc: 0.5102 | Val Loss: 0.9002 Acc: 0.4340                                               \n",
      "Epoch 026 | Train Loss: 0.9540 Acc: 0.5012 | Val Loss: 0.8771 Acc: 0.5466                                               \n",
      "Epoch 027 | Train Loss: 0.9551 Acc: 0.5068 | Val Loss: 0.8735 Acc: 0.5451                                               \n",
      "Epoch 028 | Train Loss: 0.9400 Acc: 0.5107 | Val Loss: 0.8813 Acc: 0.5215                                               \n",
      "Epoch 029 | Train Loss: 0.9319 Acc: 0.5198 | Val Loss: 0.8650 Acc: 0.5543                                               \n",
      "Epoch 030 | Train Loss: 0.9279 Acc: 0.5210 | Val Loss: 0.8743 Acc: 0.4675                                               \n",
      "Epoch 031 | Train Loss: 0.9345 Acc: 0.5165 | Val Loss: 0.8738 Acc: 0.5627                                               \n",
      "Epoch 032 | Train Loss: 0.9251 Acc: 0.5179 | Val Loss: 0.8760 Acc: 0.5110                                               \n",
      "Epoch 033 | Train Loss: 0.9285 Acc: 0.5206 | Val Loss: 0.8808 Acc: 0.5466                                               \n",
      "Epoch 034 | Train Loss: 0.9191 Acc: 0.5264 | Val Loss: 0.8590 Acc: 0.5627                                               \n",
      "Epoch 035 | Train Loss: 0.9217 Acc: 0.5209 | Val Loss: 0.8684 Acc: 0.4612                                               \n",
      "Epoch 036 | Train Loss: 0.9172 Acc: 0.5246 | Val Loss: 0.8738 Acc: 0.5116                                               \n",
      "Epoch 037 | Train Loss: 0.9072 Acc: 0.5371 | Val Loss: 0.8555 Acc: 0.5287                                               \n",
      "Epoch 038 | Train Loss: 0.9097 Acc: 0.5317 | Val Loss: 0.8652 Acc: 0.5594                                               \n",
      "Epoch 039 | Train Loss: 0.8992 Acc: 0.5421 | Val Loss: 0.8742 Acc: 0.4322                                               \n",
      "Epoch 040 | Train Loss: 0.9084 Acc: 0.5409 | Val Loss: 0.8539 Acc: 0.4940                                               \n",
      "Epoch 041 | Train Loss: 0.8997 Acc: 0.5368 | Val Loss: 0.8625 Acc: 0.4824                                               \n",
      "Epoch 042 | Train Loss: 0.8988 Acc: 0.5364 | Val Loss: 0.9254 Acc: 0.4940                                               \n",
      "Epoch 043 | Train Loss: 0.9043 Acc: 0.5335 | Val Loss: 0.8553 Acc: 0.5585                                               \n",
      "Epoch 044 | Train Loss: 0.9041 Acc: 0.5350 | Val Loss: 0.8584 Acc: 0.5054                                               \n",
      "Epoch 045 | Train Loss: 0.8903 Acc: 0.5507 | Val Loss: 0.8671 Acc: 0.5236                                               \n",
      "Epoch 046 | Train Loss: 0.8942 Acc: 0.5432 | Val Loss: 0.8288 Acc: 0.5884                                               \n",
      "Epoch 047 | Train Loss: 0.8913 Acc: 0.5459 | Val Loss: 0.8240 Acc: 0.6182                                               \n",
      "Epoch 048 | Train Loss: 0.8840 Acc: 0.5460 | Val Loss: 0.8303 Acc: 0.6104                                               \n",
      "Epoch 049 | Train Loss: 0.8805 Acc: 0.5530 | Val Loss: 0.8348 Acc: 0.5878                                               \n",
      "Epoch 050 | Train Loss: 0.8698 Acc: 0.5610 | Val Loss: 0.8127 Acc: 0.6158                                               \n",
      "Epoch 051 | Train Loss: 0.8764 Acc: 0.5561 | Val Loss: 0.8121 Acc: 0.6218                                               \n",
      "Epoch 052 | Train Loss: 0.8715 Acc: 0.5547 | Val Loss: 0.8207 Acc: 0.5573                                               \n",
      "Epoch 053 | Train Loss: 0.8714 Acc: 0.5667 | Val Loss: 0.8285 Acc: 0.6060                                               \n",
      "Epoch 054 | Train Loss: 0.8733 Acc: 0.5560 | Val Loss: 0.7970 Acc: 0.6525                                               \n",
      "Epoch 055 | Train Loss: 0.8679 Acc: 0.5662 | Val Loss: 0.7964 Acc: 0.6140                                               \n",
      "Epoch 056 | Train Loss: 0.8683 Acc: 0.5623 | Val Loss: 0.8109 Acc: 0.6057                                               \n",
      "Epoch 057 | Train Loss: 0.8615 Acc: 0.5648 | Val Loss: 0.8004 Acc: 0.6281                                               \n",
      "Epoch 058 | Train Loss: 0.8544 Acc: 0.5742 | Val Loss: 0.8057 Acc: 0.6027                                               \n",
      "Epoch 059 | Train Loss: 0.8600 Acc: 0.5715 | Val Loss: 0.8004 Acc: 0.6406                                               \n",
      "Epoch 060 | Train Loss: 0.8558 Acc: 0.5716 | Val Loss: 0.7932 Acc: 0.6579                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 36, 'cnn_dense': 32, 'cnn_dropout': 0.6345207971864095, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 64, 'cnn_kernels_2': 16, 'learning_rate': 0.003092806863621353, 'lstm_dense': 64, 'lstm_hidden_size': 128, 'lstm_layers': 6, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 3.3128 Acc: 0.5385 | Val Loss: 0.7133 Acc: 0.6904                                               \n",
      "Epoch 002 | Train Loss: 0.7847 Acc: 0.6374 | Val Loss: 0.6012 Acc: 0.7164                                               \n",
      "Epoch 003 | Train Loss: 0.6905 Acc: 0.6853 | Val Loss: 0.6681 Acc: 0.6836                                               \n",
      "Epoch 004 | Train Loss: 0.6502 Acc: 0.6934 | Val Loss: 0.5674 Acc: 0.7203                                               \n",
      "Epoch 005 | Train Loss: 0.6490 Acc: 0.7048 | Val Loss: 0.5772 Acc: 0.7719                                               \n",
      "Epoch 006 | Train Loss: 0.6452 Acc: 0.6966 | Val Loss: 0.6908 Acc: 0.6848                                               \n",
      "Epoch 007 | Train Loss: 0.6425 Acc: 0.6953 | Val Loss: 0.7082 Acc: 0.6922                                               \n",
      "Epoch 008 | Train Loss: 0.6462 Acc: 0.6924 | Val Loss: 0.6108 Acc: 0.7451                                               \n",
      "Epoch 009 | Train Loss: 0.6701 Acc: 0.6829 | Val Loss: 0.7605 Acc: 0.6119                                               \n",
      "Epoch 010 | Train Loss: 0.6698 Acc: 0.6830 | Val Loss: 0.7245 Acc: 0.6487                                               \n",
      "Epoch 011 | Train Loss: 0.6600 Acc: 0.6882 | Val Loss: 0.7725 Acc: 0.6340                                               \n",
      "Epoch 012 | Train Loss: 0.6793 Acc: 0.6814 | Val Loss: 0.6569 Acc: 0.6899                                               \n",
      "Epoch 013 | Train Loss: 0.6852 Acc: 0.6817 | Val Loss: 0.6979 Acc: 0.6367                                               \n",
      "Epoch 014 | Train Loss: 0.6718 Acc: 0.6812 | Val Loss: 0.7351 Acc: 0.6412                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 32, 'cnn_dropout': 0.2674482225117832, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 16, 'cnn_kernels_2': 32, 'learning_rate': 0.0006588979022090218, 'lstm_dense': 32, 'lstm_hidden_size': 128, 'lstm_layers': 4, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 23.1705 Acc: 0.3764 | Val Loss: 3.1054 Acc: 0.5191                                              \n",
      "Epoch 002 | Train Loss: 4.8427 Acc: 0.4523 | Val Loss: 1.3022 Acc: 0.6134                                               \n",
      "Epoch 003 | Train Loss: 2.9707 Acc: 0.5206 | Val Loss: 1.0542 Acc: 0.6728                                               \n",
      "Epoch 004 | Train Loss: 2.0297 Acc: 0.5700 | Val Loss: 0.8507 Acc: 0.6504                                               \n",
      "Epoch 005 | Train Loss: 1.4578 Acc: 0.6314 | Val Loss: 0.8426 Acc: 0.7409                                               \n",
      "Epoch 006 | Train Loss: 1.1810 Acc: 0.6814 | Val Loss: 0.8631 Acc: 0.6573                                               \n",
      "Epoch 007 | Train Loss: 1.0925 Acc: 0.6930 | Val Loss: 0.6685 Acc: 0.7543                                               \n",
      "Epoch 008 | Train Loss: 0.9872 Acc: 0.7258 | Val Loss: 0.5088 Acc: 0.8173                                               \n",
      "Epoch 009 | Train Loss: 0.8753 Acc: 0.7454 | Val Loss: 0.6962 Acc: 0.7710                                               \n",
      "Epoch 010 | Train Loss: 0.6704 Acc: 0.7891 | Val Loss: 0.4895 Acc: 0.8343                                               \n",
      "Epoch 011 | Train Loss: 0.5580 Acc: 0.8212 | Val Loss: 0.4555 Acc: 0.8355                                               \n",
      "Epoch 012 | Train Loss: 0.4527 Acc: 0.8530 | Val Loss: 0.3284 Acc: 0.8857                                               \n",
      "Epoch 013 | Train Loss: 0.3933 Acc: 0.8685 | Val Loss: 0.2667 Acc: 0.8979                                               \n",
      "Epoch 014 | Train Loss: 0.3617 Acc: 0.8789 | Val Loss: 0.3791 Acc: 0.8678                                               \n",
      "Epoch 015 | Train Loss: 0.2971 Acc: 0.9013 | Val Loss: 0.3210 Acc: 0.8833                                               \n",
      "Epoch 016 | Train Loss: 0.2904 Acc: 0.9052 | Val Loss: 0.2532 Acc: 0.9101                                               \n",
      "Epoch 017 | Train Loss: 0.2543 Acc: 0.9182 | Val Loss: 0.2951 Acc: 0.9063                                               \n",
      "Epoch 018 | Train Loss: 0.2303 Acc: 0.9266 | Val Loss: 0.2535 Acc: 0.9081                                               \n",
      "Epoch 019 | Train Loss: 0.2150 Acc: 0.9305 | Val Loss: 0.2004 Acc: 0.9287                                               \n",
      "Epoch 020 | Train Loss: 0.1985 Acc: 0.9348 | Val Loss: 0.1885 Acc: 0.9394                                               \n",
      "Epoch 021 | Train Loss: 0.1797 Acc: 0.9437 | Val Loss: 0.1746 Acc: 0.9322                                               \n",
      "Epoch 022 | Train Loss: 0.1713 Acc: 0.9452 | Val Loss: 0.3260 Acc: 0.9051                                               \n",
      "Epoch 023 | Train Loss: 0.1542 Acc: 0.9503 | Val Loss: 0.2478 Acc: 0.9304                                               \n",
      "Epoch 024 | Train Loss: 0.1340 Acc: 0.9572 | Val Loss: 0.1702 Acc: 0.9466                                               \n",
      "Epoch 025 | Train Loss: 0.1328 Acc: 0.9569 | Val Loss: 0.1519 Acc: 0.9501                                               \n",
      "Epoch 026 | Train Loss: 0.1113 Acc: 0.9630 | Val Loss: 0.1150 Acc: 0.9651                                               \n",
      "Epoch 027 | Train Loss: 0.1206 Acc: 0.9619 | Val Loss: 0.1715 Acc: 0.9463                                               \n",
      "Epoch 028 | Train Loss: 0.1129 Acc: 0.9631 | Val Loss: 0.1856 Acc: 0.9439                                               \n",
      "Epoch 029 | Train Loss: 0.1009 Acc: 0.9684 | Val Loss: 0.1183 Acc: 0.9618                                               \n",
      "Epoch 030 | Train Loss: 0.0999 Acc: 0.9718 | Val Loss: 0.1992 Acc: 0.9534                                               \n",
      "Epoch 031 | Train Loss: 0.0810 Acc: 0.9746 | Val Loss: 0.0959 Acc: 0.9716                                               \n",
      "Epoch 032 | Train Loss: 0.1184 Acc: 0.9628 | Val Loss: 0.1762 Acc: 0.9537                                               \n",
      "Epoch 033 | Train Loss: 0.0847 Acc: 0.9740 | Val Loss: 0.1220 Acc: 0.9654                                               \n",
      "Epoch 034 | Train Loss: 0.0830 Acc: 0.9749 | Val Loss: 0.2054 Acc: 0.9451                                               \n",
      "Epoch 035 | Train Loss: 0.0642 Acc: 0.9793 | Val Loss: 0.0961 Acc: 0.9722                                               \n",
      "Epoch 036 | Train Loss: 0.0532 Acc: 0.9842 | Val Loss: 0.1139 Acc: 0.9684                                               \n",
      "Epoch 037 | Train Loss: 0.0629 Acc: 0.9803 | Val Loss: 0.1299 Acc: 0.9639                                               \n",
      "Epoch 038 | Train Loss: 0.0704 Acc: 0.9775 | Val Loss: 0.1469 Acc: 0.9612                                               \n",
      "Epoch 039 | Train Loss: 0.0598 Acc: 0.9826 | Val Loss: 0.1540 Acc: 0.9612                                               \n",
      "Epoch 040 | Train Loss: 0.0633 Acc: 0.9800 | Val Loss: 0.0880 Acc: 0.9746                                               \n",
      "Epoch 041 | Train Loss: 0.0651 Acc: 0.9796 | Val Loss: 0.1314 Acc: 0.9657                                               \n",
      "Epoch 042 | Train Loss: 0.0686 Acc: 0.9784 | Val Loss: 0.1484 Acc: 0.9588                                               \n",
      "Epoch 043 | Train Loss: 0.0418 Acc: 0.9869 | Val Loss: 0.1072 Acc: 0.9684                                               \n",
      "Epoch 044 | Train Loss: 0.0508 Acc: 0.9851 | Val Loss: 0.1515 Acc: 0.9567                                               \n",
      "Epoch 045 | Train Loss: 0.0443 Acc: 0.9866 | Val Loss: 0.1426 Acc: 0.9639                                               \n",
      "Epoch 046 | Train Loss: 0.0405 Acc: 0.9868 | Val Loss: 0.1223 Acc: 0.9657                                               \n",
      "Epoch 047 | Train Loss: 0.0692 Acc: 0.9778 | Val Loss: 0.1368 Acc: 0.9612                                               \n",
      "Epoch 048 | Train Loss: 0.0415 Acc: 0.9875 | Val Loss: 0.1273 Acc: 0.9651                                               \n",
      "Epoch 049 | Train Loss: 0.0415 Acc: 0.9878 | Val Loss: 0.1794 Acc: 0.9564                                               \n",
      "Epoch 050 | Train Loss: 0.0440 Acc: 0.9854 | Val Loss: 0.1150 Acc: 0.9704                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 256, 'cnn_dropout': 0.1566943996861603, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 96, 'learning_rate': 0.0004927447983189168, 'lstm_dense': 32, 'lstm_hidden_size': 96, 'lstm_layers': 4, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 12.1345 Acc: 0.3882 | Val Loss: 1.5874 Acc: 0.5806                                              \n",
      "Epoch 002 | Train Loss: 2.0076 Acc: 0.5068 | Val Loss: 1.3089 Acc: 0.5740                                               \n",
      "Epoch 003 | Train Loss: 1.3664 Acc: 0.5519 | Val Loss: 0.7061 Acc: 0.6994                                               \n",
      "Epoch 004 | Train Loss: 1.0842 Acc: 0.5891 | Val Loss: 0.7399 Acc: 0.6716                                               \n",
      "Epoch 005 | Train Loss: 0.9762 Acc: 0.5976 | Val Loss: 0.6503 Acc: 0.7063                                               \n",
      "Epoch 006 | Train Loss: 0.9222 Acc: 0.6065 | Val Loss: 0.6098 Acc: 0.7349                                               \n",
      "Epoch 007 | Train Loss: 0.8590 Acc: 0.6256 | Val Loss: 0.6488 Acc: 0.7137                                               \n",
      "Epoch 008 | Train Loss: 0.7957 Acc: 0.6405 | Val Loss: 0.7324 Acc: 0.6537                                               \n",
      "Epoch 009 | Train Loss: 0.7934 Acc: 0.6425 | Val Loss: 0.6404 Acc: 0.7242                                               \n",
      "Epoch 010 | Train Loss: 0.7580 Acc: 0.6509 | Val Loss: 0.6453 Acc: 0.7030                                               \n",
      "Epoch 011 | Train Loss: 0.7604 Acc: 0.6558 | Val Loss: 0.6414 Acc: 0.7328                                               \n",
      "Epoch 012 | Train Loss: 0.7266 Acc: 0.6665 | Val Loss: 0.5995 Acc: 0.7254                                               \n",
      "Epoch 013 | Train Loss: 0.7031 Acc: 0.6782 | Val Loss: 0.6382 Acc: 0.7069                                               \n",
      "Epoch 014 | Train Loss: 0.7004 Acc: 0.6768 | Val Loss: 0.5803 Acc: 0.7666                                               \n",
      "Epoch 015 | Train Loss: 0.6924 Acc: 0.6801 | Val Loss: 0.6339 Acc: 0.7248                                               \n",
      "Epoch 016 | Train Loss: 0.6772 Acc: 0.6895 | Val Loss: 0.5331 Acc: 0.7528                                               \n",
      "Epoch 017 | Train Loss: 0.6930 Acc: 0.6815 | Val Loss: 0.5901 Acc: 0.7433                                               \n",
      "Epoch 018 | Train Loss: 0.6581 Acc: 0.6945 | Val Loss: 0.5457 Acc: 0.7564                                               \n",
      "Epoch 019 | Train Loss: 0.6235 Acc: 0.7138 | Val Loss: 0.4862 Acc: 0.8116                                               \n",
      "Epoch 020 | Train Loss: 0.6426 Acc: 0.7041 | Val Loss: 0.4564 Acc: 0.7803                                               \n",
      "Epoch 021 | Train Loss: 0.5927 Acc: 0.7368 | Val Loss: 0.5214 Acc: 0.7866                                               \n",
      "Epoch 022 | Train Loss: 0.5781 Acc: 0.7460 | Val Loss: 0.5797 Acc: 0.7212                                               \n",
      "Epoch 023 | Train Loss: 0.5772 Acc: 0.7546 | Val Loss: 0.5052 Acc: 0.7600                                               \n",
      "Epoch 024 | Train Loss: 0.5519 Acc: 0.7666 | Val Loss: 0.5017 Acc: 0.7973                                               \n",
      "Epoch 025 | Train Loss: 0.5303 Acc: 0.7792 | Val Loss: 0.5006 Acc: 0.7848                                               \n",
      "Epoch 026 | Train Loss: 0.5072 Acc: 0.7873 | Val Loss: 0.4323 Acc: 0.8272                                               \n",
      "Epoch 027 | Train Loss: 0.4796 Acc: 0.8070 | Val Loss: 0.4050 Acc: 0.8328                                               \n",
      "Epoch 028 | Train Loss: 0.4727 Acc: 0.8173 | Val Loss: 0.4160 Acc: 0.8099                                               \n",
      "Epoch 029 | Train Loss: 0.4338 Acc: 0.8304 | Val Loss: 0.4080 Acc: 0.8364                                               \n",
      "Epoch 030 | Train Loss: 0.4124 Acc: 0.8420 | Val Loss: 0.3216 Acc: 0.8728                                               \n",
      "Epoch 031 | Train Loss: 0.3667 Acc: 0.8632 | Val Loss: 0.3548 Acc: 0.8564                                               \n",
      "Epoch 032 | Train Loss: 0.3357 Acc: 0.8722 | Val Loss: 0.3136 Acc: 0.8806                                               \n",
      "Epoch 033 | Train Loss: 0.3091 Acc: 0.8880 | Val Loss: 0.2419 Acc: 0.9069                                               \n",
      "Epoch 034 | Train Loss: 0.2580 Acc: 0.9072 | Val Loss: 0.1965 Acc: 0.9278                                               \n",
      "Epoch 035 | Train Loss: 0.2333 Acc: 0.9158 | Val Loss: 0.3354 Acc: 0.8764                                               \n",
      "Epoch 036 | Train Loss: 0.2143 Acc: 0.9235 | Val Loss: 0.1712 Acc: 0.9346                                               \n",
      "Epoch 037 | Train Loss: 0.1879 Acc: 0.9368 | Val Loss: 0.2686 Acc: 0.9084                                               \n",
      "Epoch 038 | Train Loss: 0.1814 Acc: 0.9388 | Val Loss: 0.1726 Acc: 0.9448                                               \n",
      "Epoch 039 | Train Loss: 0.1448 Acc: 0.9522 | Val Loss: 0.1959 Acc: 0.9346                                               \n",
      "Epoch 040 | Train Loss: 0.1370 Acc: 0.9543 | Val Loss: 0.1675 Acc: 0.9484                                               \n",
      "Epoch 041 | Train Loss: 0.1379 Acc: 0.9524 | Val Loss: 0.1873 Acc: 0.9367                                               \n",
      "Epoch 042 | Train Loss: 0.1240 Acc: 0.9575 | Val Loss: 0.1709 Acc: 0.9519                                               \n",
      "Epoch 043 | Train Loss: 0.1116 Acc: 0.9630 | Val Loss: 0.1392 Acc: 0.9570                                               \n",
      "Epoch 044 | Train Loss: 0.1184 Acc: 0.9613 | Val Loss: 0.1303 Acc: 0.9507                                               \n",
      "Epoch 045 | Train Loss: 0.0989 Acc: 0.9663 | Val Loss: 0.1441 Acc: 0.9570                                               \n",
      "Epoch 046 | Train Loss: 0.0943 Acc: 0.9695 | Val Loss: 0.1950 Acc: 0.9412                                               \n",
      "Epoch 047 | Train Loss: 0.0900 Acc: 0.9702 | Val Loss: 0.1731 Acc: 0.9484                                               \n",
      "Epoch 048 | Train Loss: 0.0888 Acc: 0.9727 | Val Loss: 0.2335 Acc: 0.9370                                               \n",
      "Epoch 049 | Train Loss: 0.0877 Acc: 0.9721 | Val Loss: 0.1490 Acc: 0.9546                                               \n",
      "Epoch 050 | Train Loss: 0.0786 Acc: 0.9744 | Val Loss: 0.1882 Acc: 0.9427                                               \n",
      "Epoch 051 | Train Loss: 0.0794 Acc: 0.9740 | Val Loss: 0.1763 Acc: 0.9501                                               \n",
      "Epoch 052 | Train Loss: 0.0847 Acc: 0.9726 | Val Loss: 0.1462 Acc: 0.9558                                               \n",
      "Epoch 053 | Train Loss: 0.0899 Acc: 0.9716 | Val Loss: 0.1400 Acc: 0.9579                                               \n",
      "Epoch 054 | Train Loss: 0.0744 Acc: 0.9767 | Val Loss: 0.1612 Acc: 0.9516                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 256, 'cnn_dropout': 0.39314599511748666, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 32, 'learning_rate': 0.00046176088490724603, 'lstm_dense': 32, 'lstm_hidden_size': 96, 'lstm_layers': 5, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 21.0314 Acc: 0.3753 | Val Loss: 2.3077 Acc: 0.4555                                              \n",
      "Epoch 002 | Train Loss: 3.3255 Acc: 0.4653 | Val Loss: 0.9160 Acc: 0.6269                                               \n",
      "Epoch 003 | Train Loss: 2.1200 Acc: 0.4979 | Val Loss: 1.2064 Acc: 0.5830                                               \n",
      "Epoch 004 | Train Loss: 1.6206 Acc: 0.5161 | Val Loss: 0.8963 Acc: 0.6236                                               \n",
      "Epoch 005 | Train Loss: 1.2969 Acc: 0.5434 | Val Loss: 0.7602 Acc: 0.5749                                               \n",
      "Epoch 006 | Train Loss: 1.0475 Acc: 0.5917 | Val Loss: 0.7299 Acc: 0.6493                                               \n",
      "Epoch 007 | Train Loss: 0.9051 Acc: 0.6403 | Val Loss: 0.6005 Acc: 0.7340                                               \n",
      "Epoch 008 | Train Loss: 0.7976 Acc: 0.6829 | Val Loss: 0.6982 Acc: 0.6830                                               \n",
      "Epoch 009 | Train Loss: 0.7519 Acc: 0.7044 | Val Loss: 0.5767 Acc: 0.7630                                               \n",
      "Epoch 010 | Train Loss: 0.6665 Acc: 0.7430 | Val Loss: 0.5406 Acc: 0.7761                                               \n",
      "Epoch 011 | Train Loss: 0.5821 Acc: 0.7754 | Val Loss: 0.4505 Acc: 0.8409                                               \n",
      "Epoch 012 | Train Loss: 0.5072 Acc: 0.8080 | Val Loss: 0.3393 Acc: 0.8758                                               \n",
      "Epoch 013 | Train Loss: 0.4069 Acc: 0.8525 | Val Loss: 0.3188 Acc: 0.8922                                               \n",
      "Epoch 014 | Train Loss: 0.3479 Acc: 0.8754 | Val Loss: 0.2840 Acc: 0.8922                                               \n",
      "Epoch 015 | Train Loss: 0.3121 Acc: 0.8902 | Val Loss: 0.2311 Acc: 0.9155                                               \n",
      "Epoch 016 | Train Loss: 0.2716 Acc: 0.9071 | Val Loss: 0.2079 Acc: 0.9245                                               \n",
      "Epoch 017 | Train Loss: 0.2454 Acc: 0.9177 | Val Loss: 0.1849 Acc: 0.9340                                               \n",
      "Epoch 018 | Train Loss: 0.2135 Acc: 0.9266 | Val Loss: 0.2032 Acc: 0.9212                                               \n",
      "Epoch 019 | Train Loss: 0.1805 Acc: 0.9416 | Val Loss: 0.1595 Acc: 0.9346                                               \n",
      "Epoch 020 | Train Loss: 0.1689 Acc: 0.9430 | Val Loss: 0.1500 Acc: 0.9373                                               \n",
      "Epoch 021 | Train Loss: 0.1547 Acc: 0.9479 | Val Loss: 0.1579 Acc: 0.9406                                               \n",
      "Epoch 022 | Train Loss: 0.1307 Acc: 0.9574 | Val Loss: 0.1371 Acc: 0.9430                                               \n",
      "Epoch 023 | Train Loss: 0.1311 Acc: 0.9578 | Val Loss: 0.1709 Acc: 0.9355                                               \n",
      "Epoch 024 | Train Loss: 0.1195 Acc: 0.9610 | Val Loss: 0.1666 Acc: 0.9370                                               \n",
      "Epoch 025 | Train Loss: 0.1037 Acc: 0.9661 | Val Loss: 0.1815 Acc: 0.9212                                               \n",
      "Epoch 026 | Train Loss: 0.1057 Acc: 0.9631 | Val Loss: 0.1309 Acc: 0.9582                                               \n",
      "Epoch 027 | Train Loss: 0.0925 Acc: 0.9692 | Val Loss: 0.1238 Acc: 0.9567                                               \n",
      "Epoch 028 | Train Loss: 0.0930 Acc: 0.9694 | Val Loss: 0.1354 Acc: 0.9573                                               \n",
      "Epoch 029 | Train Loss: 0.0912 Acc: 0.9672 | Val Loss: 0.1079 Acc: 0.9725                                               \n",
      "Epoch 030 | Train Loss: 0.0785 Acc: 0.9740 | Val Loss: 0.1180 Acc: 0.9651                                               \n",
      "Epoch 031 | Train Loss: 0.0845 Acc: 0.9713 | Val Loss: 0.1299 Acc: 0.9615                                               \n",
      "Epoch 032 | Train Loss: 0.0724 Acc: 0.9761 | Val Loss: 0.1125 Acc: 0.9654                                               \n",
      "Epoch 033 | Train Loss: 0.0696 Acc: 0.9774 | Val Loss: 0.1968 Acc: 0.9421                                               \n",
      "Epoch 034 | Train Loss: 0.0861 Acc: 0.9722 | Val Loss: 0.1026 Acc: 0.9669                                               \n",
      "Epoch 035 | Train Loss: 0.0791 Acc: 0.9731 | Val Loss: 0.1090 Acc: 0.9630                                               \n",
      "Epoch 036 | Train Loss: 0.0746 Acc: 0.9758 | Val Loss: 0.0941 Acc: 0.9734                                               \n",
      "Epoch 037 | Train Loss: 0.0633 Acc: 0.9793 | Val Loss: 0.1399 Acc: 0.9567                                               \n",
      "Epoch 038 | Train Loss: 0.0541 Acc: 0.9802 | Val Loss: 0.0739 Acc: 0.9749                                               \n",
      "Epoch 039 | Train Loss: 0.0476 Acc: 0.9826 | Val Loss: 0.0871 Acc: 0.9755                                               \n",
      "Epoch 040 | Train Loss: 0.0508 Acc: 0.9822 | Val Loss: 0.1548 Acc: 0.9555                                               \n",
      "Epoch 041 | Train Loss: 0.0514 Acc: 0.9831 | Val Loss: 0.1720 Acc: 0.9522                                               \n",
      "Epoch 042 | Train Loss: 0.0731 Acc: 0.9773 | Val Loss: 0.0977 Acc: 0.9725                                               \n",
      "Epoch 043 | Train Loss: 0.0471 Acc: 0.9839 | Val Loss: 0.0955 Acc: 0.9746                                               \n",
      "Epoch 044 | Train Loss: 0.0768 Acc: 0.9776 | Val Loss: 0.1284 Acc: 0.9678                                               \n",
      "Epoch 045 | Train Loss: 0.0578 Acc: 0.9813 | Val Loss: 0.1044 Acc: 0.9704                                               \n",
      "Epoch 046 | Train Loss: 0.0405 Acc: 0.9860 | Val Loss: 0.1393 Acc: 0.9648                                               \n",
      "Epoch 047 | Train Loss: 0.0429 Acc: 0.9866 | Val Loss: 0.0800 Acc: 0.9776                                               \n",
      "Epoch 048 | Train Loss: 0.0740 Acc: 0.9778 | Val Loss: 0.0967 Acc: 0.9737                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 256, 'cnn_dropout': 0.4349841601571182, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 32, 'learning_rate': 0.0005349458253510081, 'lstm_dense': 32, 'lstm_hidden_size': 96, 'lstm_layers': 5, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 13.5089 Acc: 0.3929 | Val Loss: 1.7586 Acc: 0.5290                                              \n",
      "Epoch 002 | Train Loss: 2.2271 Acc: 0.5068 | Val Loss: 1.0526 Acc: 0.5907                                               \n",
      "Epoch 003 | Train Loss: 1.4635 Acc: 0.5494 | Val Loss: 0.7137 Acc: 0.6639                                               \n",
      "Epoch 004 | Train Loss: 1.2071 Acc: 0.5635 | Val Loss: 0.8342 Acc: 0.6704                                               \n",
      "Epoch 005 | Train Loss: 1.1031 Acc: 0.5814 | Val Loss: 0.6414 Acc: 0.7257                                               \n",
      "Epoch 006 | Train Loss: 1.0168 Acc: 0.5918 | Val Loss: 0.7979 Acc: 0.6788                                               \n",
      "Epoch 007 | Train Loss: 0.9948 Acc: 0.6014 | Val Loss: 0.6817 Acc: 0.6501                                               \n",
      "Epoch 008 | Train Loss: 0.9198 Acc: 0.6206 | Val Loss: 0.6856 Acc: 0.6710                                               \n",
      "Epoch 009 | Train Loss: 0.8713 Acc: 0.6515 | Val Loss: 0.5167 Acc: 0.8033                                               \n",
      "Epoch 010 | Train Loss: 0.8010 Acc: 0.6730 | Val Loss: 0.6454 Acc: 0.7301                                               \n",
      "Epoch 011 | Train Loss: 0.7117 Acc: 0.7174 | Val Loss: 0.6253 Acc: 0.7218                                               \n",
      "Epoch 012 | Train Loss: 0.6339 Acc: 0.7505 | Val Loss: 0.4713 Acc: 0.8087                                               \n",
      "Epoch 013 | Train Loss: 0.5426 Acc: 0.7910 | Val Loss: 0.3360 Acc: 0.8764                                               \n",
      "Epoch 014 | Train Loss: 0.5101 Acc: 0.8116 | Val Loss: 0.5061 Acc: 0.8078                                               \n",
      "Epoch 015 | Train Loss: 0.4240 Acc: 0.8492 | Val Loss: 0.3851 Acc: 0.8624                                               \n",
      "Epoch 016 | Train Loss: 0.3644 Acc: 0.8742 | Val Loss: 0.3067 Acc: 0.8952                                               \n",
      "Epoch 017 | Train Loss: 0.3212 Acc: 0.8915 | Val Loss: 0.2923 Acc: 0.8931                                               \n",
      "Epoch 018 | Train Loss: 0.2927 Acc: 0.9033 | Val Loss: 0.4484 Acc: 0.8546                                               \n",
      "Epoch 019 | Train Loss: 0.2652 Acc: 0.9092 | Val Loss: 0.2198 Acc: 0.9164                                               \n",
      "Epoch 020 | Train Loss: 0.2296 Acc: 0.9239 | Val Loss: 0.1676 Acc: 0.9418                                               \n",
      "Epoch 021 | Train Loss: 0.1926 Acc: 0.9340 | Val Loss: 0.1276 Acc: 0.9582                                               \n",
      "Epoch 022 | Train Loss: 0.1902 Acc: 0.9345 | Val Loss: 0.1416 Acc: 0.9496                                               \n",
      "Epoch 023 | Train Loss: 0.1666 Acc: 0.9425 | Val Loss: 0.1794 Acc: 0.9382                                               \n",
      "Epoch 024 | Train Loss: 0.1590 Acc: 0.9466 | Val Loss: 0.1081 Acc: 0.9630                                               \n",
      "Epoch 025 | Train Loss: 0.1334 Acc: 0.9545 | Val Loss: 0.1645 Acc: 0.9460                                               \n",
      "Epoch 026 | Train Loss: 0.1238 Acc: 0.9583 | Val Loss: 0.1133 Acc: 0.9615                                               \n",
      "Epoch 027 | Train Loss: 0.1175 Acc: 0.9598 | Val Loss: 0.1446 Acc: 0.9493                                               \n",
      "Epoch 028 | Train Loss: 0.1092 Acc: 0.9633 | Val Loss: 0.1200 Acc: 0.9591                                               \n",
      "Epoch 029 | Train Loss: 0.0944 Acc: 0.9693 | Val Loss: 0.1196 Acc: 0.9642                                               \n",
      "Epoch 030 | Train Loss: 0.1071 Acc: 0.9666 | Val Loss: 0.1327 Acc: 0.9588                                               \n",
      "Epoch 031 | Train Loss: 0.0817 Acc: 0.9720 | Val Loss: 0.1031 Acc: 0.9716                                               \n",
      "Epoch 032 | Train Loss: 0.0847 Acc: 0.9738 | Val Loss: 0.1342 Acc: 0.9573                                               \n",
      "Epoch 033 | Train Loss: 0.0878 Acc: 0.9730 | Val Loss: 0.1164 Acc: 0.9663                                               \n",
      "Epoch 034 | Train Loss: 0.0753 Acc: 0.9757 | Val Loss: 0.0767 Acc: 0.9731                                               \n",
      "Epoch 035 | Train Loss: 0.0561 Acc: 0.9818 | Val Loss: 0.1032 Acc: 0.9713                                               \n",
      "Epoch 036 | Train Loss: 0.0733 Acc: 0.9764 | Val Loss: 0.1010 Acc: 0.9666                                               \n",
      "Epoch 037 | Train Loss: 0.0628 Acc: 0.9788 | Val Loss: 0.1025 Acc: 0.9675                                               \n",
      "Epoch 038 | Train Loss: 0.0600 Acc: 0.9805 | Val Loss: 0.0646 Acc: 0.9779                                               \n",
      "Epoch 039 | Train Loss: 0.0627 Acc: 0.9795 | Val Loss: 0.0727 Acc: 0.9761                                               \n",
      "Epoch 040 | Train Loss: 0.0583 Acc: 0.9816 | Val Loss: 0.0691 Acc: 0.9764                                               \n",
      "Epoch 041 | Train Loss: 0.0509 Acc: 0.9828 | Val Loss: 0.0796 Acc: 0.9758                                               \n",
      "Epoch 042 | Train Loss: 0.0559 Acc: 0.9820 | Val Loss: 0.1089 Acc: 0.9713                                               \n",
      "Epoch 043 | Train Loss: 0.0533 Acc: 0.9830 | Val Loss: 0.0798 Acc: 0.9782                                               \n",
      "Epoch 044 | Train Loss: 0.0451 Acc: 0.9881 | Val Loss: 0.0783 Acc: 0.9791                                               \n",
      "Epoch 045 | Train Loss: 0.0528 Acc: 0.9825 | Val Loss: 0.0695 Acc: 0.9773                                               \n",
      "Epoch 046 | Train Loss: 0.0480 Acc: 0.9857 | Val Loss: 0.0979 Acc: 0.9719                                               \n",
      "Epoch 047 | Train Loss: 0.0505 Acc: 0.9842 | Val Loss: 0.0626 Acc: 0.9791                                               \n",
      "Epoch 048 | Train Loss: 0.0446 Acc: 0.9853 | Val Loss: 0.0827 Acc: 0.9755                                               \n",
      "Epoch 049 | Train Loss: 0.0528 Acc: 0.9845 | Val Loss: 0.0793 Acc: 0.9794                                               \n",
      "Epoch 050 | Train Loss: 0.0537 Acc: 0.9843 | Val Loss: 0.0907 Acc: 0.9749                                               \n",
      "Epoch 051 | Train Loss: 0.0530 Acc: 0.9831 | Val Loss: 0.0677 Acc: 0.9800                                               \n",
      "Epoch 052 | Train Loss: 0.0382 Acc: 0.9885 | Val Loss: 0.0760 Acc: 0.9773                                               \n",
      "Epoch 053 | Train Loss: 0.0537 Acc: 0.9843 | Val Loss: 0.1077 Acc: 0.9719                                               \n",
      "Epoch 054 | Train Loss: 0.0346 Acc: 0.9893 | Val Loss: 0.0755 Acc: 0.9758                                               \n",
      "Epoch 055 | Train Loss: 0.0410 Acc: 0.9872 | Val Loss: 0.0652 Acc: 0.9797                                               \n",
      "Epoch 056 | Train Loss: 0.0353 Acc: 0.9891 | Val Loss: 0.0664 Acc: 0.9812                                               \n",
      "Epoch 057 | Train Loss: 0.0319 Acc: 0.9895 | Val Loss: 0.0560 Acc: 0.9830                                               \n",
      "Epoch 058 | Train Loss: 0.0371 Acc: 0.9878 | Val Loss: 0.0682 Acc: 0.9833                                               \n",
      "Epoch 059 | Train Loss: 0.0326 Acc: 0.9901 | Val Loss: 0.0708 Acc: 0.9803                                               \n",
      "Epoch 060 | Train Loss: 0.0250 Acc: 0.9923 | Val Loss: 0.0607 Acc: 0.9812                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 256, 'cnn_dropout': 0.4536759247534833, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 32, 'learning_rate': 0.0003337193403691384, 'lstm_dense': 32, 'lstm_hidden_size': 96, 'lstm_layers': 5, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 28.7154 Acc: 0.3791 | Val Loss: 4.8574 Acc: 0.5051                                              \n",
      "Epoch 002 | Train Loss: 8.5934 Acc: 0.4476 | Val Loss: 2.1248 Acc: 0.5358                                               \n",
      "Epoch 003 | Train Loss: 5.1707 Acc: 0.4742 | Val Loss: 2.0067 Acc: 0.5979                                               \n",
      "Epoch 004 | Train Loss: 3.6688 Acc: 0.5012 | Val Loss: 1.8506 Acc: 0.5113                                               \n",
      "Epoch 005 | Train Loss: 2.8344 Acc: 0.5447 | Val Loss: 1.1217 Acc: 0.6069                                               \n",
      "Epoch 006 | Train Loss: 2.1967 Acc: 0.5835 | Val Loss: 1.2731 Acc: 0.6931                                               \n",
      "Epoch 007 | Train Loss: 1.7812 Acc: 0.6252 | Val Loss: 0.7755 Acc: 0.6943                                               \n",
      "Epoch 008 | Train Loss: 1.5996 Acc: 0.6541 | Val Loss: 0.8691 Acc: 0.7370                                               \n",
      "Epoch 009 | Train Loss: 1.3082 Acc: 0.6843 | Val Loss: 0.5381 Acc: 0.7919                                               \n",
      "Epoch 010 | Train Loss: 1.1823 Acc: 0.6985 | Val Loss: 0.6482 Acc: 0.7418                                               \n",
      "Epoch 011 | Train Loss: 1.0238 Acc: 0.7314 | Val Loss: 0.4891 Acc: 0.8200                                               \n",
      "Epoch 012 | Train Loss: 0.8722 Acc: 0.7620 | Val Loss: 0.4023 Acc: 0.8540                                               \n",
      "Epoch 013 | Train Loss: 0.7152 Acc: 0.7924 | Val Loss: 0.3954 Acc: 0.8510                                               \n",
      "Epoch 014 | Train Loss: 0.6203 Acc: 0.8161 | Val Loss: 0.3184 Acc: 0.8845                                               \n",
      "Epoch 015 | Train Loss: 0.5703 Acc: 0.8322 | Val Loss: 0.3945 Acc: 0.8719                                               \n",
      "Epoch 016 | Train Loss: 0.4574 Acc: 0.8603 | Val Loss: 0.3579 Acc: 0.8788                                               \n",
      "Epoch 017 | Train Loss: 0.4229 Acc: 0.8732 | Val Loss: 0.2111 Acc: 0.9185                                               \n",
      "Epoch 018 | Train Loss: 0.3813 Acc: 0.8854 | Val Loss: 0.2074 Acc: 0.9269                                               \n",
      "Epoch 019 | Train Loss: 0.3072 Acc: 0.9028 | Val Loss: 0.1824 Acc: 0.9382                                               \n",
      "Epoch 020 | Train Loss: 0.2721 Acc: 0.9137 | Val Loss: 0.1696 Acc: 0.9436                                               \n",
      "Epoch 021 | Train Loss: 0.2454 Acc: 0.9219 | Val Loss: 0.2223 Acc: 0.9218                                               \n",
      "Epoch 022 | Train Loss: 0.2319 Acc: 0.9260 | Val Loss: 0.1614 Acc: 0.9490                                               \n",
      "Epoch 023 | Train Loss: 0.1855 Acc: 0.9366 | Val Loss: 0.1506 Acc: 0.9531                                               \n",
      "Epoch 024 | Train Loss: 0.1773 Acc: 0.9449 | Val Loss: 0.1186 Acc: 0.9645                                               \n",
      "Epoch 025 | Train Loss: 0.1620 Acc: 0.9491 | Val Loss: 0.1201 Acc: 0.9663                                               \n",
      "Epoch 026 | Train Loss: 0.1599 Acc: 0.9475 | Val Loss: 0.1481 Acc: 0.9507                                               \n",
      "Epoch 027 | Train Loss: 0.1277 Acc: 0.9605 | Val Loss: 0.1017 Acc: 0.9713                                               \n",
      "Epoch 028 | Train Loss: 0.1346 Acc: 0.9584 | Val Loss: 0.1124 Acc: 0.9684                                               \n",
      "Epoch 029 | Train Loss: 0.1078 Acc: 0.9646 | Val Loss: 0.0809 Acc: 0.9770                                               \n",
      "Epoch 030 | Train Loss: 0.0959 Acc: 0.9683 | Val Loss: 0.0830 Acc: 0.9737                                               \n",
      "Epoch 031 | Train Loss: 0.0906 Acc: 0.9716 | Val Loss: 0.1352 Acc: 0.9654                                               \n",
      "Epoch 032 | Train Loss: 0.0906 Acc: 0.9720 | Val Loss: 0.1041 Acc: 0.9678                                               \n",
      "Epoch 033 | Train Loss: 0.0923 Acc: 0.9721 | Val Loss: 0.1021 Acc: 0.9725                                               \n",
      "Epoch 034 | Train Loss: 0.0840 Acc: 0.9735 | Val Loss: 0.0803 Acc: 0.9788                                               \n",
      "Epoch 035 | Train Loss: 0.0713 Acc: 0.9760 | Val Loss: 0.1058 Acc: 0.9764                                               \n",
      "Epoch 036 | Train Loss: 0.0697 Acc: 0.9788 | Val Loss: 0.0909 Acc: 0.9776                                               \n",
      "Epoch 037 | Train Loss: 0.0619 Acc: 0.9787 | Val Loss: 0.0833 Acc: 0.9782                                               \n",
      "Epoch 038 | Train Loss: 0.0717 Acc: 0.9769 | Val Loss: 0.0913 Acc: 0.9716                                               \n",
      "Epoch 039 | Train Loss: 0.0726 Acc: 0.9763 | Val Loss: 0.0621 Acc: 0.9818                                               \n",
      "Epoch 040 | Train Loss: 0.0559 Acc: 0.9832 | Val Loss: 0.0696 Acc: 0.9806                                               \n",
      "Epoch 041 | Train Loss: 0.0502 Acc: 0.9826 | Val Loss: 0.0684 Acc: 0.9806                                               \n",
      "Epoch 042 | Train Loss: 0.0492 Acc: 0.9838 | Val Loss: 0.0912 Acc: 0.9815                                               \n",
      "Epoch 043 | Train Loss: 0.0555 Acc: 0.9831 | Val Loss: 0.0974 Acc: 0.9791                                               \n",
      "Epoch 044 | Train Loss: 0.0519 Acc: 0.9832 | Val Loss: 0.0677 Acc: 0.9818                                               \n",
      "Epoch 045 | Train Loss: 0.0679 Acc: 0.9783 | Val Loss: 0.0761 Acc: 0.9824                                               \n",
      "Epoch 046 | Train Loss: 0.0489 Acc: 0.9834 | Val Loss: 0.0636 Acc: 0.9842                                               \n",
      "Epoch 047 | Train Loss: 0.0519 Acc: 0.9835 | Val Loss: 0.0745 Acc: 0.9791                                               \n",
      "Epoch 048 | Train Loss: 0.0462 Acc: 0.9851 | Val Loss: 0.0692 Acc: 0.9803                                               \n",
      "Epoch 049 | Train Loss: 0.0596 Acc: 0.9828 | Val Loss: 0.1283 Acc: 0.9642                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 256, 'cnn_dropout': 0.4507544438471445, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 32, 'learning_rate': 8.615635239588021e-05, 'lstm_dense': 32, 'lstm_hidden_size': 96, 'lstm_layers': 5, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 40.7814 Acc: 0.3529 | Val Loss: 9.1511 Acc: 0.4430                                              \n",
      "Epoch 002 | Train Loss: 13.3885 Acc: 0.4009 | Val Loss: 3.0356 Acc: 0.4594                                              \n",
      "Epoch 003 | Train Loss: 8.1830 Acc: 0.4452 | Val Loss: 2.0018 Acc: 0.5913                                               \n",
      "Epoch 004 | Train Loss: 6.0300 Acc: 0.4692 | Val Loss: 1.5487 Acc: 0.6185                                               \n",
      "Epoch 005 | Train Loss: 4.6065 Acc: 0.4937 | Val Loss: 1.3639 Acc: 0.6221                                               \n",
      "Epoch 006 | Train Loss: 3.5946 Acc: 0.5246 | Val Loss: 1.5090 Acc: 0.6570                                               \n",
      "Epoch 007 | Train Loss: 3.0277 Acc: 0.5441 | Val Loss: 1.3008 Acc: 0.6531                                               \n",
      "Epoch 008 | Train Loss: 2.6076 Acc: 0.5547 | Val Loss: 1.3037 Acc: 0.6737                                               \n",
      "Epoch 009 | Train Loss: 2.3053 Acc: 0.5730 | Val Loss: 1.1081 Acc: 0.6916                                               \n",
      "Epoch 010 | Train Loss: 1.9367 Acc: 0.5945 | Val Loss: 1.1761 Acc: 0.6588                                               \n",
      "Epoch 011 | Train Loss: 1.7366 Acc: 0.6071 | Val Loss: 1.2298 Acc: 0.6561                                               \n",
      "Epoch 012 | Train Loss: 1.5407 Acc: 0.6203 | Val Loss: 0.9258 Acc: 0.6985                                               \n",
      "Epoch 013 | Train Loss: 1.3897 Acc: 0.6345 | Val Loss: 1.0806 Acc: 0.6997                                               \n",
      "Epoch 014 | Train Loss: 1.2495 Acc: 0.6527 | Val Loss: 0.9517 Acc: 0.7230                                               \n",
      "Epoch 015 | Train Loss: 1.1041 Acc: 0.6739 | Val Loss: 0.9540 Acc: 0.7015                                               \n",
      "Epoch 016 | Train Loss: 1.0485 Acc: 0.6850 | Val Loss: 0.8236 Acc: 0.7266                                               \n",
      "Epoch 017 | Train Loss: 0.9768 Acc: 0.7039 | Val Loss: 0.6559 Acc: 0.7701                                               \n",
      "Epoch 018 | Train Loss: 0.9133 Acc: 0.7172 | Val Loss: 0.7262 Acc: 0.7337                                               \n",
      "Epoch 019 | Train Loss: 0.8064 Acc: 0.7345 | Val Loss: 0.6338 Acc: 0.7743                                               \n",
      "Epoch 020 | Train Loss: 0.7687 Acc: 0.7448 | Val Loss: 0.5555 Acc: 0.7761                                               \n",
      "Epoch 021 | Train Loss: 0.7034 Acc: 0.7640 | Val Loss: 0.5165 Acc: 0.7910                                               \n",
      "Epoch 022 | Train Loss: 0.6787 Acc: 0.7766 | Val Loss: 0.5958 Acc: 0.7755                                               \n",
      "Epoch 023 | Train Loss: 0.5916 Acc: 0.7979 | Val Loss: 0.3894 Acc: 0.8528                                               \n",
      "Epoch 024 | Train Loss: 0.5295 Acc: 0.8194 | Val Loss: 0.4434 Acc: 0.8418                                               \n",
      "Epoch 025 | Train Loss: 0.4958 Acc: 0.8289 | Val Loss: 0.4022 Acc: 0.8537                                               \n",
      "Epoch 026 | Train Loss: 0.4615 Acc: 0.8440 | Val Loss: 0.3981 Acc: 0.8522                                               \n",
      "Epoch 027 | Train Loss: 0.4142 Acc: 0.8563 | Val Loss: 0.3900 Acc: 0.8687                                               \n",
      "Epoch 028 | Train Loss: 0.3821 Acc: 0.8738 | Val Loss: 0.2721 Acc: 0.8991                                               \n",
      "Epoch 029 | Train Loss: 0.3396 Acc: 0.8854 | Val Loss: 0.2856 Acc: 0.9018                                               \n",
      "Epoch 030 | Train Loss: 0.3092 Acc: 0.8947 | Val Loss: 0.2681 Acc: 0.9027                                               \n",
      "Epoch 031 | Train Loss: 0.2920 Acc: 0.9024 | Val Loss: 0.2895 Acc: 0.8916                                               \n",
      "Epoch 032 | Train Loss: 0.2720 Acc: 0.9096 | Val Loss: 0.1880 Acc: 0.9301                                               \n",
      "Epoch 033 | Train Loss: 0.2648 Acc: 0.9093 | Val Loss: 0.2175 Acc: 0.9185                                               \n",
      "Epoch 034 | Train Loss: 0.2348 Acc: 0.9232 | Val Loss: 0.1915 Acc: 0.9284                                               \n",
      "Epoch 035 | Train Loss: 0.2378 Acc: 0.9218 | Val Loss: 0.2284 Acc: 0.9143                                               \n",
      "Epoch 036 | Train Loss: 0.2168 Acc: 0.9275 | Val Loss: 0.2209 Acc: 0.9152                                               \n",
      "Epoch 037 | Train Loss: 0.1908 Acc: 0.9375 | Val Loss: 0.1773 Acc: 0.9367                                               \n",
      "Epoch 038 | Train Loss: 0.1809 Acc: 0.9384 | Val Loss: 0.1530 Acc: 0.9448                                               \n",
      "Epoch 039 | Train Loss: 0.1740 Acc: 0.9414 | Val Loss: 0.2314 Acc: 0.9194                                               \n",
      "Epoch 040 | Train Loss: 0.1554 Acc: 0.9477 | Val Loss: 0.1597 Acc: 0.9400                                               \n",
      "Epoch 041 | Train Loss: 0.1480 Acc: 0.9515 | Val Loss: 0.1508 Acc: 0.9496                                               \n",
      "Epoch 042 | Train Loss: 0.1279 Acc: 0.9575 | Val Loss: 0.1506 Acc: 0.9436                                               \n",
      "Epoch 043 | Train Loss: 0.1322 Acc: 0.9560 | Val Loss: 0.1384 Acc: 0.9519                                               \n",
      "Epoch 044 | Train Loss: 0.1204 Acc: 0.9599 | Val Loss: 0.1377 Acc: 0.9561                                               \n",
      "Epoch 045 | Train Loss: 0.1184 Acc: 0.9598 | Val Loss: 0.1490 Acc: 0.9522                                               \n",
      "Epoch 046 | Train Loss: 0.1090 Acc: 0.9642 | Val Loss: 0.1065 Acc: 0.9588                                               \n",
      "Epoch 047 | Train Loss: 0.1018 Acc: 0.9688 | Val Loss: 0.1402 Acc: 0.9567                                               \n",
      "Epoch 048 | Train Loss: 0.0965 Acc: 0.9686 | Val Loss: 0.0982 Acc: 0.9630                                               \n",
      "Epoch 049 | Train Loss: 0.0960 Acc: 0.9713 | Val Loss: 0.1273 Acc: 0.9546                                               \n",
      "Epoch 050 | Train Loss: 0.0891 Acc: 0.9702 | Val Loss: 0.1427 Acc: 0.9481                                               \n",
      "Epoch 051 | Train Loss: 0.0802 Acc: 0.9737 | Val Loss: 0.1071 Acc: 0.9633                                               \n",
      "Epoch 052 | Train Loss: 0.0843 Acc: 0.9737 | Val Loss: 0.0994 Acc: 0.9681                                               \n",
      "Epoch 053 | Train Loss: 0.0843 Acc: 0.9722 | Val Loss: 0.1755 Acc: 0.9481                                               \n",
      "Epoch 054 | Train Loss: 0.0714 Acc: 0.9767 | Val Loss: 0.1999 Acc: 0.9349                                               \n",
      "Epoch 055 | Train Loss: 0.0718 Acc: 0.9758 | Val Loss: 0.0736 Acc: 0.9701                                               \n",
      "Epoch 056 | Train Loss: 0.0725 Acc: 0.9764 | Val Loss: 0.0733 Acc: 0.9710                                               \n",
      "Epoch 057 | Train Loss: 0.0733 Acc: 0.9768 | Val Loss: 0.0966 Acc: 0.9630                                               \n",
      "Epoch 058 | Train Loss: 0.0532 Acc: 0.9825 | Val Loss: 0.0663 Acc: 0.9779                                               \n",
      "Epoch 059 | Train Loss: 0.0520 Acc: 0.9836 | Val Loss: 0.1583 Acc: 0.9493                                               \n",
      "Epoch 060 | Train Loss: 0.0514 Acc: 0.9831 | Val Loss: 0.1013 Acc: 0.9687                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 256, 'cnn_dropout': 0.4522590964280721, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 32, 'learning_rate': 0.00026022835765929393, 'lstm_dense': 32, 'lstm_hidden_size': 96, 'lstm_layers': 5, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 23.0869 Acc: 0.3832 | Val Loss: 2.9423 Acc: 0.4800                                              \n",
      "Epoch 002 | Train Loss: 6.6791 Acc: 0.4385 | Val Loss: 1.7605 Acc: 0.5310                                               \n",
      "Epoch 003 | Train Loss: 4.3713 Acc: 0.4523 | Val Loss: 1.3626 Acc: 0.6087                                               \n",
      "Epoch 004 | Train Loss: 3.2289 Acc: 0.4763 | Val Loss: 1.4427 Acc: 0.5955                                               \n",
      "Epoch 005 | Train Loss: 2.5413 Acc: 0.4936 | Val Loss: 1.3493 Acc: 0.5496                                               \n",
      "Epoch 006 | Train Loss: 2.2037 Acc: 0.5085 | Val Loss: 1.1353 Acc: 0.6227                                               \n",
      "Epoch 007 | Train Loss: 1.8241 Acc: 0.5292 | Val Loss: 1.0585 Acc: 0.5719                                               \n",
      "Epoch 008 | Train Loss: 1.5385 Acc: 0.5438 | Val Loss: 0.8498 Acc: 0.5722                                               \n",
      "Epoch 009 | Train Loss: 1.4979 Acc: 0.5472 | Val Loss: 0.9620 Acc: 0.6227                                               \n",
      "Epoch 010 | Train Loss: 1.2442 Acc: 0.5708 | Val Loss: 0.6539 Acc: 0.7021                                               \n",
      "Epoch 011 | Train Loss: 1.1404 Acc: 0.5988 | Val Loss: 0.7091 Acc: 0.6725                                               \n",
      "Epoch 012 | Train Loss: 1.0591 Acc: 0.6224 | Val Loss: 0.6808 Acc: 0.7119                                               \n",
      "Epoch 013 | Train Loss: 0.9150 Acc: 0.6541 | Val Loss: 0.6842 Acc: 0.7301                                               \n",
      "Epoch 014 | Train Loss: 0.8450 Acc: 0.6815 | Val Loss: 0.5766 Acc: 0.7558                                               \n",
      "Epoch 015 | Train Loss: 0.7770 Acc: 0.7013 | Val Loss: 0.6692 Acc: 0.6734                                               \n",
      "Epoch 016 | Train Loss: 0.7439 Acc: 0.7248 | Val Loss: 0.4729 Acc: 0.7982                                               \n",
      "Epoch 017 | Train Loss: 0.6596 Acc: 0.7513 | Val Loss: 0.4516 Acc: 0.8364                                               \n",
      "Epoch 018 | Train Loss: 0.5626 Acc: 0.7924 | Val Loss: 0.4288 Acc: 0.8400                                               \n",
      "Epoch 019 | Train Loss: 0.5028 Acc: 0.8174 | Val Loss: 0.3622 Acc: 0.8755                                               \n",
      "Epoch 020 | Train Loss: 0.4647 Acc: 0.8354 | Val Loss: 0.3482 Acc: 0.8696                                               \n",
      "Epoch 021 | Train Loss: 0.3945 Acc: 0.8578 | Val Loss: 0.3205 Acc: 0.8836                                               \n",
      "Epoch 022 | Train Loss: 0.3350 Acc: 0.8795 | Val Loss: 0.2839 Acc: 0.8967                                               \n",
      "Epoch 023 | Train Loss: 0.2982 Acc: 0.8951 | Val Loss: 0.2611 Acc: 0.9072                                               \n",
      "Epoch 024 | Train Loss: 0.2622 Acc: 0.9085 | Val Loss: 0.2090 Acc: 0.9278                                               \n",
      "Epoch 025 | Train Loss: 0.2257 Acc: 0.9227 | Val Loss: 0.1954 Acc: 0.9373                                               \n",
      "Epoch 026 | Train Loss: 0.2253 Acc: 0.9244 | Val Loss: 0.1831 Acc: 0.9391                                               \n",
      "Epoch 027 | Train Loss: 0.2016 Acc: 0.9316 | Val Loss: 0.1537 Acc: 0.9504                                               \n",
      "Epoch 028 | Train Loss: 0.1753 Acc: 0.9442 | Val Loss: 0.1447 Acc: 0.9537                                               \n",
      "Epoch 029 | Train Loss: 0.1786 Acc: 0.9390 | Val Loss: 0.1618 Acc: 0.9457                                               \n",
      "Epoch 030 | Train Loss: 0.1468 Acc: 0.9525 | Val Loss: 0.1369 Acc: 0.9552                                               \n",
      "Epoch 031 | Train Loss: 0.1399 Acc: 0.9528 | Val Loss: 0.1453 Acc: 0.9501                                               \n",
      "Epoch 032 | Train Loss: 0.1273 Acc: 0.9587 | Val Loss: 0.1589 Acc: 0.9469                                               \n",
      "Epoch 033 | Train Loss: 0.1235 Acc: 0.9603 | Val Loss: 0.1405 Acc: 0.9546                                               \n",
      "Epoch 034 | Train Loss: 0.1136 Acc: 0.9610 | Val Loss: 0.1409 Acc: 0.9546                                               \n",
      "Epoch 035 | Train Loss: 0.1107 Acc: 0.9660 | Val Loss: 0.1029 Acc: 0.9693                                               \n",
      "Epoch 036 | Train Loss: 0.0957 Acc: 0.9694 | Val Loss: 0.1206 Acc: 0.9606                                               \n",
      "Epoch 037 | Train Loss: 0.1028 Acc: 0.9666 | Val Loss: 0.0917 Acc: 0.9687                                               \n",
      "Epoch 038 | Train Loss: 0.0878 Acc: 0.9718 | Val Loss: 0.1006 Acc: 0.9681                                               \n",
      "Epoch 039 | Train Loss: 0.0842 Acc: 0.9717 | Val Loss: 0.1581 Acc: 0.9522                                               \n",
      "Epoch 040 | Train Loss: 0.0874 Acc: 0.9716 | Val Loss: 0.0904 Acc: 0.9734                                               \n",
      "Epoch 041 | Train Loss: 0.0777 Acc: 0.9748 | Val Loss: 0.0959 Acc: 0.9722                                               \n",
      "Epoch 042 | Train Loss: 0.0742 Acc: 0.9742 | Val Loss: 0.1005 Acc: 0.9713                                               \n",
      "Epoch 043 | Train Loss: 0.0716 Acc: 0.9763 | Val Loss: 0.0794 Acc: 0.9773                                               \n",
      "Epoch 044 | Train Loss: 0.0678 Acc: 0.9762 | Val Loss: 0.0789 Acc: 0.9755                                               \n",
      "Epoch 045 | Train Loss: 0.0618 Acc: 0.9797 | Val Loss: 0.0837 Acc: 0.9749                                               \n",
      "Epoch 046 | Train Loss: 0.0606 Acc: 0.9796 | Val Loss: 0.1172 Acc: 0.9660                                               \n",
      "Epoch 047 | Train Loss: 0.0703 Acc: 0.9775 | Val Loss: 0.0873 Acc: 0.9725                                               \n",
      "Epoch 048 | Train Loss: 0.0608 Acc: 0.9807 | Val Loss: 0.0792 Acc: 0.9752                                               \n",
      "Epoch 049 | Train Loss: 0.0523 Acc: 0.9837 | Val Loss: 0.0810 Acc: 0.9782                                               \n",
      "Epoch 050 | Train Loss: 0.0632 Acc: 0.9801 | Val Loss: 0.0735 Acc: 0.9782                                               \n",
      "Epoch 051 | Train Loss: 0.0517 Acc: 0.9835 | Val Loss: 0.0850 Acc: 0.9743                                               \n",
      "Epoch 052 | Train Loss: 0.0521 Acc: 0.9826 | Val Loss: 0.0847 Acc: 0.9755                                               \n",
      "Epoch 053 | Train Loss: 0.0464 Acc: 0.9848 | Val Loss: 0.0659 Acc: 0.9785                                               \n",
      "Epoch 054 | Train Loss: 0.0405 Acc: 0.9867 | Val Loss: 0.0803 Acc: 0.9773                                               \n",
      "Epoch 055 | Train Loss: 0.0400 Acc: 0.9870 | Val Loss: 0.0708 Acc: 0.9773                                               \n",
      "Epoch 056 | Train Loss: 0.0439 Acc: 0.9871 | Val Loss: 0.1087 Acc: 0.9678                                               \n",
      "Epoch 057 | Train Loss: 0.0380 Acc: 0.9875 | Val Loss: 0.1012 Acc: 0.9699                                               \n",
      "Epoch 058 | Train Loss: 0.0505 Acc: 0.9832 | Val Loss: 0.0661 Acc: 0.9800                                               \n",
      "Epoch 059 | Train Loss: 0.0397 Acc: 0.9866 | Val Loss: 0.0780 Acc: 0.9782                                               \n",
      "Epoch 060 | Train Loss: 0.0388 Acc: 0.9889 | Val Loss: 0.0755 Acc: 0.9818                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 36, 'cnn_dense': 256, 'cnn_dropout': 0.4233091646778922, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 32, 'learning_rate': 0.0006309309463391703, 'lstm_dense': 32, 'lstm_hidden_size': 96, 'lstm_layers': 5, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 12.2936 Acc: 0.4187 | Val Loss: 1.7737 Acc: 0.5415                                              \n",
      "Epoch 002 | Train Loss: 3.2462 Acc: 0.4756 | Val Loss: 1.2094 Acc: 0.5660                                               \n",
      "Epoch 003 | Train Loss: 1.8689 Acc: 0.5088 | Val Loss: 0.9202 Acc: 0.5555                                               \n",
      "Epoch 004 | Train Loss: 1.3864 Acc: 0.5330 | Val Loss: 0.7821 Acc: 0.6878                                               \n",
      "Epoch 005 | Train Loss: 1.1310 Acc: 0.5656 | Val Loss: 1.0631 Acc: 0.6099                                               \n",
      "Epoch 006 | Train Loss: 0.9480 Acc: 0.5831 | Val Loss: 0.8026 Acc: 0.6045                                               \n",
      "Epoch 007 | Train Loss: 0.8628 Acc: 0.5980 | Val Loss: 0.8468 Acc: 0.6278                                               \n",
      "Epoch 008 | Train Loss: 0.7671 Acc: 0.6355 | Val Loss: 0.6548 Acc: 0.6594                                               \n",
      "Epoch 009 | Train Loss: 0.7147 Acc: 0.6589 | Val Loss: 0.5929 Acc: 0.7299                                               \n",
      "Epoch 010 | Train Loss: 0.6673 Acc: 0.6846 | Val Loss: 0.5178 Acc: 0.7433                                               \n",
      "Epoch 011 | Train Loss: 0.6431 Acc: 0.6987 | Val Loss: 0.5656 Acc: 0.7472                                               \n",
      "Epoch 012 | Train Loss: 0.6087 Acc: 0.7160 | Val Loss: 0.5059 Acc: 0.7687                                               \n",
      "Epoch 013 | Train Loss: 0.5749 Acc: 0.7391 | Val Loss: 0.5723 Acc: 0.7155                                               \n",
      "Epoch 014 | Train Loss: 0.5684 Acc: 0.7453 | Val Loss: 0.5024 Acc: 0.7755                                               \n",
      "Epoch 015 | Train Loss: 0.5449 Acc: 0.7558 | Val Loss: 0.4989 Acc: 0.7561                                               \n",
      "Epoch 016 | Train Loss: 0.5069 Acc: 0.7749 | Val Loss: 0.5020 Acc: 0.7893                                               \n",
      "Epoch 017 | Train Loss: 0.5218 Acc: 0.7730 | Val Loss: 0.4816 Acc: 0.8060                                               \n",
      "Epoch 018 | Train Loss: 0.4817 Acc: 0.7820 | Val Loss: 0.5582 Acc: 0.7242                                               \n",
      "Epoch 019 | Train Loss: 0.4729 Acc: 0.7934 | Val Loss: 0.5371 Acc: 0.7236                                               \n",
      "Epoch 020 | Train Loss: 0.4595 Acc: 0.8001 | Val Loss: 0.5204 Acc: 0.7809                                               \n",
      "Epoch 021 | Train Loss: 0.4447 Acc: 0.8034 | Val Loss: 0.5541 Acc: 0.7293                                               \n",
      "Epoch 022 | Train Loss: 0.4177 Acc: 0.8138 | Val Loss: 0.5566 Acc: 0.6964                                               \n",
      "Epoch 023 | Train Loss: 0.4405 Acc: 0.8080 | Val Loss: 0.6198 Acc: 0.7027                                               \n",
      "Epoch 024 | Train Loss: 0.4202 Acc: 0.8172 | Val Loss: 0.3558 Acc: 0.8316                                               \n",
      "Epoch 025 | Train Loss: 0.4139 Acc: 0.8230 | Val Loss: 0.5196 Acc: 0.8221                                               \n",
      "Epoch 026 | Train Loss: 0.3885 Acc: 0.8345 | Val Loss: 0.5787 Acc: 0.7552                                               \n",
      "Epoch 027 | Train Loss: 0.3885 Acc: 0.8329 | Val Loss: 0.7405 Acc: 0.7122                                               \n",
      "Epoch 028 | Train Loss: 0.3569 Acc: 0.8486 | Val Loss: 0.5462 Acc: 0.7857                                               \n",
      "Epoch 029 | Train Loss: 0.3586 Acc: 0.8502 | Val Loss: 0.5765 Acc: 0.7615                                               \n",
      "Epoch 030 | Train Loss: 0.3501 Acc: 0.8534 | Val Loss: 0.4375 Acc: 0.8057                                               \n",
      "Epoch 031 | Train Loss: 0.3313 Acc: 0.8602 | Val Loss: 0.4262 Acc: 0.8245                                               \n",
      "Epoch 032 | Train Loss: 0.3636 Acc: 0.8500 | Val Loss: 0.6578 Acc: 0.7469                                               \n",
      "Epoch 033 | Train Loss: 0.3409 Acc: 0.8612 | Val Loss: 0.5975 Acc: 0.7722                                               \n",
      "Epoch 034 | Train Loss: 0.3377 Acc: 0.8625 | Val Loss: 0.4994 Acc: 0.7803                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 256, 'cnn_dropout': 0.32608119113789513, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 32, 'learning_rate': 0.0002874094516956967, 'lstm_dense': 32, 'lstm_hidden_size': 96, 'lstm_layers': 5, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 12.8508 Acc: 0.4341 | Val Loss: 1.2458 Acc: 0.4421                                              \n",
      "Epoch 002 | Train Loss: 1.2438 Acc: 0.4422 | Val Loss: 1.2424 Acc: 0.4421                                               \n",
      "Epoch 003 | Train Loss: 1.2424 Acc: 0.4422 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 004 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 005 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 006 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 007 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 008 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 009 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 010 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 011 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 012 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 013 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 014 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 015 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 016 | Train Loss: 1.2418 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 017 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 018 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 019 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 020 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 021 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 022 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 023 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 024 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 025 | Train Loss: 1.2418 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 026 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 027 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 028 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 029 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 030 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 031 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 032 | Train Loss: 1.2418 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 033 | Train Loss: 1.2418 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 034 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 035 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 036 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 037 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 038 | Train Loss: 1.2418 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 039 | Train Loss: 1.2418 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 040 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 041 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 042 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 043 | Train Loss: 1.2418 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 044 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 045 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 046 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 047 | Train Loss: 1.2418 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 048 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 049 | Train Loss: 1.2418 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 050 | Train Loss: 1.2418 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 051 | Train Loss: 1.2418 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 052 | Train Loss: 1.2418 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 053 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 054 | Train Loss: 1.2418 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 055 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 056 | Train Loss: 1.2418 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 057 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 058 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 059 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 060 | Train Loss: 1.2418 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 256, 'cnn_dropout': 0.48831012156431375, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 32, 'learning_rate': 7.413771701290013e-05, 'lstm_dense': 256, 'lstm_hidden_size': 96, 'lstm_layers': 5, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 58.1610 Acc: 0.3114 | Val Loss: 16.1695 Acc: 0.4531                                             \n",
      "Epoch 002 | Train Loss: 25.0214 Acc: 0.3553 | Val Loss: 5.8674 Acc: 0.4104                                              \n",
      "Epoch 003 | Train Loss: 14.8036 Acc: 0.3906 | Val Loss: 3.6849 Acc: 0.3078                                              \n",
      "Epoch 004 | Train Loss: 10.7832 Acc: 0.4164 | Val Loss: 2.5888 Acc: 0.4791                                              \n",
      "Epoch 005 | Train Loss: 8.3478 Acc: 0.4350 | Val Loss: 2.5039 Acc: 0.5000                                               \n",
      "Epoch 006 | Train Loss: 7.0825 Acc: 0.4386 | Val Loss: 2.2120 Acc: 0.5349                                               \n",
      "Epoch 007 | Train Loss: 5.8743 Acc: 0.4549 | Val Loss: 2.5734 Acc: 0.5299                                               \n",
      "Epoch 008 | Train Loss: 5.0130 Acc: 0.4634 | Val Loss: 2.0439 Acc: 0.5370                                               \n",
      "Epoch 009 | Train Loss: 4.5833 Acc: 0.4677 | Val Loss: 1.8483 Acc: 0.5696                                               \n",
      "Epoch 010 | Train Loss: 4.0619 Acc: 0.4828 | Val Loss: 1.3053 Acc: 0.5824                                               \n",
      "Epoch 011 | Train Loss: 3.6106 Acc: 0.5092 | Val Loss: 1.4025 Acc: 0.6030                                               \n",
      "Epoch 012 | Train Loss: 3.1871 Acc: 0.5285 | Val Loss: 1.2460 Acc: 0.6087                                               \n",
      "Epoch 013 | Train Loss: 2.9583 Acc: 0.5372 | Val Loss: 1.3403 Acc: 0.6018                                               \n",
      "Epoch 014 | Train Loss: 2.6035 Acc: 0.5550 | Val Loss: 1.4232 Acc: 0.5624                                               \n",
      "Epoch 015 | Train Loss: 2.4017 Acc: 0.5624 | Val Loss: 1.3787 Acc: 0.6140                                               \n",
      "Epoch 016 | Train Loss: 2.1696 Acc: 0.5817 | Val Loss: 1.0624 Acc: 0.6707                                               \n",
      "Epoch 017 | Train Loss: 2.0395 Acc: 0.5919 | Val Loss: 1.1386 Acc: 0.5890                                               \n",
      "Epoch 018 | Train Loss: 1.8495 Acc: 0.6095 | Val Loss: 0.9544 Acc: 0.7054                                               \n",
      "Epoch 019 | Train Loss: 1.7545 Acc: 0.6222 | Val Loss: 0.8453 Acc: 0.7203                                               \n",
      "Epoch 020 | Train Loss: 1.6676 Acc: 0.6220 | Val Loss: 0.8524 Acc: 0.6869                                               \n",
      "Epoch 021 | Train Loss: 1.5540 Acc: 0.6299 | Val Loss: 0.8296 Acc: 0.7030                                               \n",
      "Epoch 022 | Train Loss: 1.5430 Acc: 0.6380 | Val Loss: 0.8383 Acc: 0.6931                                               \n",
      "Epoch 023 | Train Loss: 1.3727 Acc: 0.6550 | Val Loss: 0.9259 Acc: 0.6794                                               \n",
      "Epoch 024 | Train Loss: 1.3229 Acc: 0.6599 | Val Loss: 0.7338 Acc: 0.6976                                               \n",
      "Epoch 025 | Train Loss: 1.2520 Acc: 0.6683 | Val Loss: 0.6955 Acc: 0.7707                                               \n",
      "Epoch 026 | Train Loss: 1.1247 Acc: 0.6820 | Val Loss: 0.6031 Acc: 0.7890                                               \n",
      "Epoch 027 | Train Loss: 1.0718 Acc: 0.6904 | Val Loss: 0.6002 Acc: 0.7504                                               \n",
      "Epoch 028 | Train Loss: 1.0261 Acc: 0.6986 | Val Loss: 0.6891 Acc: 0.7352                                               \n",
      "Epoch 029 | Train Loss: 0.9631 Acc: 0.7087 | Val Loss: 0.6452 Acc: 0.7406                                               \n",
      "Epoch 030 | Train Loss: 0.9447 Acc: 0.7212 | Val Loss: 0.6342 Acc: 0.7782                                               \n",
      "Epoch 031 | Train Loss: 0.9109 Acc: 0.7291 | Val Loss: 0.6364 Acc: 0.7770                                               \n",
      "Epoch 032 | Train Loss: 0.8120 Acc: 0.7491 | Val Loss: 0.8382 Acc: 0.7054                                               \n",
      "Epoch 033 | Train Loss: 0.7814 Acc: 0.7560 | Val Loss: 0.5894 Acc: 0.7901                                               \n",
      "Epoch 034 | Train Loss: 0.7247 Acc: 0.7773 | Val Loss: 0.5427 Acc: 0.8185                                               \n",
      "Epoch 035 | Train Loss: 0.6887 Acc: 0.7913 | Val Loss: 0.6441 Acc: 0.7857                                               \n",
      "Epoch 036 | Train Loss: 0.6646 Acc: 0.8000 | Val Loss: 0.4086 Acc: 0.8657                                               \n",
      "Epoch 037 | Train Loss: 0.5833 Acc: 0.8192 | Val Loss: 0.5679 Acc: 0.8290                                               \n",
      "Epoch 038 | Train Loss: 0.5491 Acc: 0.8296 | Val Loss: 0.3225 Acc: 0.8931                                               \n",
      "Epoch 039 | Train Loss: 0.5027 Acc: 0.8425 | Val Loss: 0.3368 Acc: 0.8919                                               \n",
      "Epoch 040 | Train Loss: 0.4600 Acc: 0.8564 | Val Loss: 0.4247 Acc: 0.8654                                               \n",
      "Epoch 041 | Train Loss: 0.4184 Acc: 0.8655 | Val Loss: 0.2999 Acc: 0.9033                                               \n",
      "Epoch 042 | Train Loss: 0.3804 Acc: 0.8807 | Val Loss: 0.2861 Acc: 0.9149                                               \n",
      "Epoch 043 | Train Loss: 0.3754 Acc: 0.8800 | Val Loss: 0.3081 Acc: 0.9072                                               \n",
      "Epoch 044 | Train Loss: 0.3342 Acc: 0.8969 | Val Loss: 0.3293 Acc: 0.9021                                               \n",
      "Epoch 045 | Train Loss: 0.3241 Acc: 0.9007 | Val Loss: 0.3075 Acc: 0.8991                                               \n",
      "Epoch 046 | Train Loss: 0.2980 Acc: 0.9060 | Val Loss: 0.2230 Acc: 0.9278                                               \n",
      "Epoch 047 | Train Loss: 0.2834 Acc: 0.9159 | Val Loss: 0.1939 Acc: 0.9343                                               \n",
      "Epoch 048 | Train Loss: 0.2589 Acc: 0.9194 | Val Loss: 0.1705 Acc: 0.9448                                               \n",
      "Epoch 049 | Train Loss: 0.2513 Acc: 0.9212 | Val Loss: 0.2019 Acc: 0.9340                                               \n",
      "Epoch 050 | Train Loss: 0.2337 Acc: 0.9266 | Val Loss: 0.2181 Acc: 0.9293                                               \n",
      "Epoch 051 | Train Loss: 0.1983 Acc: 0.9375 | Val Loss: 0.1897 Acc: 0.9376                                               \n",
      "Epoch 052 | Train Loss: 0.1933 Acc: 0.9391 | Val Loss: 0.1999 Acc: 0.9343                                               \n",
      "Epoch 053 | Train Loss: 0.1699 Acc: 0.9451 | Val Loss: 0.1687 Acc: 0.9507                                               \n",
      "Epoch 054 | Train Loss: 0.1756 Acc: 0.9459 | Val Loss: 0.1473 Acc: 0.9531                                               \n",
      "Epoch 055 | Train Loss: 0.1605 Acc: 0.9501 | Val Loss: 0.1826 Acc: 0.9412                                               \n",
      "Epoch 056 | Train Loss: 0.1686 Acc: 0.9459 | Val Loss: 0.1359 Acc: 0.9636                                               \n",
      "Epoch 057 | Train Loss: 0.1521 Acc: 0.9535 | Val Loss: 0.1405 Acc: 0.9555                                               \n",
      "Epoch 058 | Train Loss: 0.1598 Acc: 0.9511 | Val Loss: 0.1290 Acc: 0.9621                                               \n",
      "Epoch 059 | Train Loss: 0.1395 Acc: 0.9576 | Val Loss: 0.2002 Acc: 0.9421                                               \n",
      "Epoch 060 | Train Loss: 0.1363 Acc: 0.9582 | Val Loss: 0.1404 Acc: 0.9588                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 256, 'cnn_dropout': 0.1975099417046841, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 32, 'learning_rate': 9.361596704045016e-05, 'lstm_dense': 32, 'lstm_hidden_size': 96, 'lstm_layers': 5, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 5.1869 Acc: 0.4379 | Val Loss: 0.9474 Acc: 0.5570                                               \n",
      "Epoch 002 | Train Loss: 1.0019 Acc: 0.4865 | Val Loss: 0.8578 Acc: 0.5806                                               \n",
      "Epoch 003 | Train Loss: 0.9301 Acc: 0.5189 | Val Loss: 0.7909 Acc: 0.5657                                               \n",
      "Epoch 004 | Train Loss: 0.8784 Acc: 0.5359 | Val Loss: 0.7492 Acc: 0.6313                                               \n",
      "Epoch 005 | Train Loss: 0.8504 Acc: 0.5526 | Val Loss: 0.7487 Acc: 0.6603                                               \n",
      "Epoch 006 | Train Loss: 0.8456 Acc: 0.5574 | Val Loss: 0.7392 Acc: 0.6645                                               \n",
      "Epoch 007 | Train Loss: 0.8313 Acc: 0.5597 | Val Loss: 0.6986 Acc: 0.6764                                               \n",
      "Epoch 008 | Train Loss: 0.8101 Acc: 0.5715 | Val Loss: 0.6765 Acc: 0.6591                                               \n",
      "Epoch 009 | Train Loss: 0.8051 Acc: 0.5737 | Val Loss: 0.6796 Acc: 0.6681                                               \n",
      "Epoch 010 | Train Loss: 0.7915 Acc: 0.5882 | Val Loss: 0.6734 Acc: 0.6699                                               \n",
      "Epoch 011 | Train Loss: 0.7948 Acc: 0.5758 | Val Loss: 0.6646 Acc: 0.6985                                               \n",
      "Epoch 012 | Train Loss: 0.7852 Acc: 0.5962 | Val Loss: 0.6726 Acc: 0.6099                                               \n",
      "Epoch 013 | Train Loss: 0.7674 Acc: 0.6080 | Val Loss: 0.6384 Acc: 0.6684                                               \n",
      "Epoch 014 | Train Loss: 0.7729 Acc: 0.6027 | Val Loss: 0.6469 Acc: 0.7287                                               \n",
      "Epoch 015 | Train Loss: 0.7770 Acc: 0.6105 | Val Loss: 0.6324 Acc: 0.6606                                               \n",
      "Epoch 016 | Train Loss: 0.7760 Acc: 0.6145 | Val Loss: 0.6521 Acc: 0.6773                                               \n",
      "Epoch 017 | Train Loss: 0.7641 Acc: 0.6159 | Val Loss: 0.6517 Acc: 0.6325                                               \n",
      "Epoch 018 | Train Loss: 0.7662 Acc: 0.6169 | Val Loss: 0.6442 Acc: 0.7242                                               \n",
      "Epoch 019 | Train Loss: 0.7477 Acc: 0.6224 | Val Loss: 0.6623 Acc: 0.7075                                               \n",
      "Epoch 020 | Train Loss: 0.7705 Acc: 0.6207 | Val Loss: 0.6435 Acc: 0.7227                                               \n",
      "Epoch 021 | Train Loss: 0.7542 Acc: 0.6229 | Val Loss: 0.6187 Acc: 0.7009                                               \n",
      "Epoch 022 | Train Loss: 0.7527 Acc: 0.6235 | Val Loss: 0.6212 Acc: 0.7045                                               \n",
      "Epoch 023 | Train Loss: 0.7389 Acc: 0.6333 | Val Loss: 0.6205 Acc: 0.7349                                               \n",
      "Epoch 024 | Train Loss: 0.7585 Acc: 0.6187 | Val Loss: 0.6111 Acc: 0.7051                                               \n",
      "Epoch 025 | Train Loss: 0.7472 Acc: 0.6247 | Val Loss: 0.6140 Acc: 0.7463                                               \n",
      "Epoch 026 | Train Loss: 0.7294 Acc: 0.6327 | Val Loss: 0.5959 Acc: 0.7496                                               \n",
      "Epoch 027 | Train Loss: 0.7364 Acc: 0.6363 | Val Loss: 0.5829 Acc: 0.7152                                               \n",
      "Epoch 028 | Train Loss: 0.7373 Acc: 0.6327 | Val Loss: 0.5906 Acc: 0.6890                                               \n",
      "Epoch 029 | Train Loss: 0.7267 Acc: 0.6347 | Val Loss: 0.6088 Acc: 0.7406                                               \n",
      "Epoch 030 | Train Loss: 0.7245 Acc: 0.6402 | Val Loss: 0.6150 Acc: 0.7018                                               \n",
      "Epoch 031 | Train Loss: 0.7261 Acc: 0.6407 | Val Loss: 0.5984 Acc: 0.6907                                               \n",
      "Epoch 032 | Train Loss: 0.7256 Acc: 0.6447 | Val Loss: 0.5890 Acc: 0.7570                                               \n",
      "Epoch 033 | Train Loss: 0.7172 Acc: 0.6406 | Val Loss: 0.5659 Acc: 0.7293                                               \n",
      "Epoch 034 | Train Loss: 0.7222 Acc: 0.6454 | Val Loss: 0.5604 Acc: 0.7394                                               \n",
      "Epoch 035 | Train Loss: 0.7223 Acc: 0.6460 | Val Loss: 0.5841 Acc: 0.7609                                               \n",
      "Epoch 036 | Train Loss: 0.7349 Acc: 0.6424 | Val Loss: 0.5870 Acc: 0.7451                                               \n",
      "Epoch 037 | Train Loss: 0.7123 Acc: 0.6619 | Val Loss: 0.6049 Acc: 0.7290                                               \n",
      "Epoch 038 | Train Loss: 0.7043 Acc: 0.6625 | Val Loss: 0.5481 Acc: 0.7800                                               \n",
      "Epoch 039 | Train Loss: 0.7045 Acc: 0.6584 | Val Loss: 0.5411 Acc: 0.7809                                               \n",
      "Epoch 040 | Train Loss: 0.6943 Acc: 0.6711 | Val Loss: 0.5648 Acc: 0.7254                                               \n",
      "Epoch 041 | Train Loss: 0.7049 Acc: 0.6667 | Val Loss: 0.5848 Acc: 0.7260                                               \n",
      "Epoch 042 | Train Loss: 0.7032 Acc: 0.6635 | Val Loss: 0.5380 Acc: 0.7549                                               \n",
      "Epoch 043 | Train Loss: 0.7040 Acc: 0.6672 | Val Loss: 0.5328 Acc: 0.7254                                               \n",
      "Epoch 044 | Train Loss: 0.7009 Acc: 0.6690 | Val Loss: 0.5315 Acc: 0.7815                                               \n",
      "Epoch 045 | Train Loss: 0.6942 Acc: 0.6725 | Val Loss: 0.5414 Acc: 0.7478                                               \n",
      "Epoch 046 | Train Loss: 0.6835 Acc: 0.6758 | Val Loss: 0.5386 Acc: 0.7809                                               \n",
      "Epoch 047 | Train Loss: 0.6877 Acc: 0.6803 | Val Loss: 0.5473 Acc: 0.7851                                               \n",
      "Epoch 048 | Train Loss: 0.6841 Acc: 0.6794 | Val Loss: 0.5727 Acc: 0.7764                                               \n",
      "Epoch 049 | Train Loss: 0.6865 Acc: 0.6819 | Val Loss: 0.5553 Acc: 0.7988                                               \n",
      "Epoch 050 | Train Loss: 0.6850 Acc: 0.6778 | Val Loss: 0.5123 Acc: 0.7770                                               \n",
      "Epoch 051 | Train Loss: 0.6835 Acc: 0.6810 | Val Loss: 0.5336 Acc: 0.7782                                               \n",
      "Epoch 052 | Train Loss: 0.6892 Acc: 0.6779 | Val Loss: 0.5525 Acc: 0.7606                                               \n",
      "Epoch 053 | Train Loss: 0.6787 Acc: 0.6817 | Val Loss: 0.5281 Acc: 0.7612                                               \n",
      "Epoch 054 | Train Loss: 0.6748 Acc: 0.6841 | Val Loss: 0.5154 Acc: 0.7970                                               \n",
      "Epoch 055 | Train Loss: 0.6716 Acc: 0.6948 | Val Loss: 0.5168 Acc: 0.8012                                               \n",
      "Epoch 056 | Train Loss: 0.6666 Acc: 0.6889 | Val Loss: 0.5614 Acc: 0.7531                                               \n",
      "Epoch 057 | Train Loss: 0.6880 Acc: 0.6791 | Val Loss: 0.5138 Acc: 0.7854                                               \n",
      "Epoch 058 | Train Loss: 0.6701 Acc: 0.6907 | Val Loss: 0.5263 Acc: 0.7809                                               \n",
      "Epoch 059 | Train Loss: 0.6702 Acc: 0.6951 | Val Loss: 0.5094 Acc: 0.7812                                               \n",
      "Epoch 060 | Train Loss: 0.6687 Acc: 0.6927 | Val Loss: 0.5308 Acc: 0.8057                                               \n",
      "100%|████████████████████████████████████████████████| 30/30 [22:36<00:00, 45.20s/trial, best loss: 0.05597059470726483]\n",
      "Best hyperparameters: {'batch_size': np.int64(5), 'cnn_dense': np.int64(3), 'cnn_dropout': np.float64(0.4349841601571182), 'cnn_kernel_size_1': np.int64(0), 'cnn_kernel_size_2': np.int64(1), 'cnn_kernels_1': np.int64(0), 'cnn_kernels_2': np.int64(1), 'learning_rate': np.float64(0.0005349458253510081), 'lstm_dense': np.int64(0), 'lstm_hidden_size': np.int64(2), 'lstm_layers': np.int64(4), 'optimizer': np.int64(0)}\n",
      "TPE search finished in 1356.09 seconds\n",
      "Best (raw indices): {'batch_size': np.int64(5), 'cnn_dense': np.int64(3), 'cnn_dropout': np.float64(0.4349841601571182), 'cnn_kernel_size_1': np.int64(0), 'cnn_kernel_size_2': np.int64(1), 'cnn_kernels_1': np.int64(0), 'cnn_kernels_2': np.int64(1), 'learning_rate': np.float64(0.0005349458253510081), 'lstm_dense': np.int64(0), 'lstm_hidden_size': np.int64(2), 'lstm_layers': np.int64(4), 'optimizer': np.int64(0)}\n",
      "Best (interpreted): {'batch_size': 96, 'cnn_dense': 256, 'cnn_dropout': np.float64(0.4349841601571182), 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 32, 'learning_rate': np.float64(0.0005349458253510081), 'lstm_dense': 32, 'lstm_hidden_size': 96, 'lstm_layers': 5, 'optimizer': 'adam'}\n",
      "Starting TPE search...\n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 256, 'cnn_dropout': 0.28397030120562156, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 64, 'cnn_kernels_2': 32, 'learning_rate': 0.00012230707680006866, 'lstm_dense': 256, 'lstm_hidden_size': 128, 'lstm_layers': 4, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 3.6175 Acc: 0.3835 | Val Loss: 0.9505 Acc: 0.5251                                               \n",
      "Epoch 002 | Train Loss: 1.0523 Acc: 0.4131 | Val Loss: 0.9199 Acc: 0.5078                                               \n",
      "Epoch 003 | Train Loss: 1.0290 Acc: 0.4135 | Val Loss: 0.9067 Acc: 0.5173                                               \n",
      "Epoch 004 | Train Loss: 1.0284 Acc: 0.4256 | Val Loss: 0.9134 Acc: 0.4818                                               \n",
      "Epoch 005 | Train Loss: 0.9888 Acc: 0.4540 | Val Loss: 0.9603 Acc: 0.5379                                               \n",
      "Epoch 006 | Train Loss: 0.9932 Acc: 0.4897 | Val Loss: 0.9006 Acc: 0.5687                                               \n",
      "Epoch 007 | Train Loss: 0.9452 Acc: 0.5121 | Val Loss: 0.7971 Acc: 0.5973                                               \n",
      "Epoch 008 | Train Loss: 0.9034 Acc: 0.5276 | Val Loss: 0.7250 Acc: 0.6484                                               \n",
      "Epoch 009 | Train Loss: 0.9133 Acc: 0.5264 | Val Loss: 0.8079 Acc: 0.6042                                               \n",
      "Epoch 010 | Train Loss: 0.8950 Acc: 0.5223 | Val Loss: 0.7189 Acc: 0.5884                                               \n",
      "Epoch 011 | Train Loss: 0.8868 Acc: 0.5283 | Val Loss: 0.7123 Acc: 0.6797                                               \n",
      "Epoch 012 | Train Loss: 0.8812 Acc: 0.5362 | Val Loss: 0.7349 Acc: 0.6101                                               \n",
      "Epoch 013 | Train Loss: 0.8795 Acc: 0.5349 | Val Loss: 0.7345 Acc: 0.6725                                               \n",
      "Epoch 014 | Train Loss: 0.8844 Acc: 0.5282 | Val Loss: 0.7052 Acc: 0.6710                                               \n",
      "Epoch 015 | Train Loss: 0.8744 Acc: 0.5406 | Val Loss: 0.7896 Acc: 0.6579                                               \n",
      "Epoch 016 | Train Loss: 0.8739 Acc: 0.5429 | Val Loss: 0.7086 Acc: 0.6848                                               \n",
      "Epoch 017 | Train Loss: 0.8637 Acc: 0.5531 | Val Loss: 0.6877 Acc: 0.6863                                               \n",
      "Epoch 018 | Train Loss: 0.8621 Acc: 0.5471 | Val Loss: 0.6777 Acc: 0.6576                                               \n",
      "Epoch 019 | Train Loss: 0.8535 Acc: 0.5546 | Val Loss: 0.7178 Acc: 0.6552                                               \n",
      "Epoch 020 | Train Loss: 0.8405 Acc: 0.5687 | Val Loss: 0.6674 Acc: 0.6713                                               \n",
      "Epoch 021 | Train Loss: 0.8296 Acc: 0.5757 | Val Loss: 0.7001 Acc: 0.6585                                               \n",
      "Epoch 022 | Train Loss: 0.8256 Acc: 0.5739 | Val Loss: 0.6674 Acc: 0.6713                                               \n",
      "Epoch 023 | Train Loss: 0.8256 Acc: 0.5795 | Val Loss: 0.6815 Acc: 0.6830                                               \n",
      "Epoch 024 | Train Loss: 0.8188 Acc: 0.5791 | Val Loss: 0.6065 Acc: 0.7248                                               \n",
      "Epoch 025 | Train Loss: 0.8316 Acc: 0.5718 | Val Loss: 0.6339 Acc: 0.7096                                               \n",
      "Epoch 026 | Train Loss: 0.8328 Acc: 0.5800 | Val Loss: 0.7653 Acc: 0.6519                                               \n",
      "Epoch 027 | Train Loss: 0.8621 Acc: 0.5620 | Val Loss: 0.7089 Acc: 0.6710                                               \n",
      "Epoch 028 | Train Loss: 0.8486 Acc: 0.5668 | Val Loss: 0.7158 Acc: 0.6863                                               \n",
      "Epoch 029 | Train Loss: 0.8572 Acc: 0.5650 | Val Loss: 0.7129 Acc: 0.6546                                               \n",
      "Epoch 030 | Train Loss: 0.8644 Acc: 0.5560 | Val Loss: 0.7414 Acc: 0.5776                                               \n",
      "Epoch 031 | Train Loss: 0.8536 Acc: 0.5594 | Val Loss: 0.7070 Acc: 0.6501                                               \n",
      "Epoch 032 | Train Loss: 0.8397 Acc: 0.5756 | Val Loss: 0.6645 Acc: 0.6851                                               \n",
      "Epoch 033 | Train Loss: 0.8480 Acc: 0.5716 | Val Loss: 0.6730 Acc: 0.7024                                               \n",
      "Epoch 034 | Train Loss: 0.8342 Acc: 0.5779 | Val Loss: 0.6467 Acc: 0.6973                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 64, 'cnn_dropout': 0.03321169581052617, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 32, 'cnn_kernels_2': 96, 'learning_rate': 3.210577076681023e-05, 'lstm_dense': 256, 'lstm_hidden_size': 128, 'lstm_layers': 6, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 8.5105 Acc: 0.2811 | Val Loss: 1.3985 Acc: 0.2036                                               \n",
      "Epoch 002 | Train Loss: 1.3315 Acc: 0.3044 | Val Loss: 1.1752 Acc: 0.2290                                               \n",
      "Epoch 003 | Train Loss: 1.2075 Acc: 0.3581 | Val Loss: 1.0902 Acc: 0.3710                                               \n",
      "Epoch 004 | Train Loss: 1.1512 Acc: 0.3823 | Val Loss: 1.0650 Acc: 0.4179                                               \n",
      "Epoch 005 | Train Loss: 1.1154 Acc: 0.3959 | Val Loss: 1.0369 Acc: 0.3997                                               \n",
      "Epoch 006 | Train Loss: 1.0765 Acc: 0.4075 | Val Loss: 1.0315 Acc: 0.4421                                               \n",
      "Epoch 007 | Train Loss: 1.0615 Acc: 0.4213 | Val Loss: 1.0006 Acc: 0.4307                                               \n",
      "Epoch 008 | Train Loss: 1.0421 Acc: 0.4118 | Val Loss: 0.9944 Acc: 0.4391                                               \n",
      "Epoch 009 | Train Loss: 1.0400 Acc: 0.4151 | Val Loss: 0.9781 Acc: 0.4000                                               \n",
      "Epoch 010 | Train Loss: 1.0210 Acc: 0.4303 | Val Loss: 0.9660 Acc: 0.4352                                               \n",
      "Epoch 011 | Train Loss: 1.0143 Acc: 0.4181 | Val Loss: 0.9594 Acc: 0.4501                                               \n",
      "Epoch 012 | Train Loss: 1.0081 Acc: 0.4232 | Val Loss: 0.9663 Acc: 0.4531                                               \n",
      "Epoch 013 | Train Loss: 0.9965 Acc: 0.4233 | Val Loss: 0.9473 Acc: 0.4406                                               \n",
      "Epoch 014 | Train Loss: 1.0001 Acc: 0.4205 | Val Loss: 0.9654 Acc: 0.3904                                               \n",
      "Epoch 015 | Train Loss: 0.9971 Acc: 0.4282 | Val Loss: 0.9441 Acc: 0.4501                                               \n",
      "Epoch 016 | Train Loss: 0.9967 Acc: 0.4300 | Val Loss: 0.9505 Acc: 0.4540                                               \n",
      "Epoch 017 | Train Loss: 0.9900 Acc: 0.4306 | Val Loss: 0.9552 Acc: 0.3701                                               \n",
      "Epoch 018 | Train Loss: 0.9842 Acc: 0.4317 | Val Loss: 0.9534 Acc: 0.4534                                               \n",
      "Epoch 019 | Train Loss: 0.9878 Acc: 0.4429 | Val Loss: 0.9322 Acc: 0.4645                                               \n",
      "Epoch 020 | Train Loss: 0.9752 Acc: 0.4385 | Val Loss: 0.9472 Acc: 0.4307                                               \n",
      "Epoch 021 | Train Loss: 0.9769 Acc: 0.4380 | Val Loss: 0.9424 Acc: 0.4713                                               \n",
      "Epoch 022 | Train Loss: 0.9798 Acc: 0.4341 | Val Loss: 0.9354 Acc: 0.4501                                               \n",
      "Epoch 023 | Train Loss: 0.9701 Acc: 0.4394 | Val Loss: 0.9355 Acc: 0.4540                                               \n",
      "Epoch 024 | Train Loss: 0.9640 Acc: 0.4538 | Val Loss: 0.9241 Acc: 0.3940                                               \n",
      "Epoch 025 | Train Loss: 0.9625 Acc: 0.4409 | Val Loss: 0.9286 Acc: 0.4343                                               \n",
      "Epoch 026 | Train Loss: 0.9596 Acc: 0.4502 | Val Loss: 0.9104 Acc: 0.4946                                               \n",
      "Epoch 027 | Train Loss: 0.9502 Acc: 0.4541 | Val Loss: 0.9090 Acc: 0.4507                                               \n",
      "Epoch 028 | Train Loss: 0.9547 Acc: 0.4512 | Val Loss: 0.9063 Acc: 0.4901                                               \n",
      "Epoch 029 | Train Loss: 0.9477 Acc: 0.4612 | Val Loss: 0.9034 Acc: 0.4442                                               \n",
      "Epoch 030 | Train Loss: 0.9486 Acc: 0.4662 | Val Loss: 0.9038 Acc: 0.5042                                               \n",
      "Epoch 031 | Train Loss: 0.9398 Acc: 0.4713 | Val Loss: 0.9348 Acc: 0.4293                                               \n",
      "Epoch 032 | Train Loss: 0.9394 Acc: 0.4748 | Val Loss: 0.9031 Acc: 0.5125                                               \n",
      "Epoch 033 | Train Loss: 0.9445 Acc: 0.4702 | Val Loss: 0.9107 Acc: 0.5466                                               \n",
      "Epoch 034 | Train Loss: 0.9442 Acc: 0.4766 | Val Loss: 0.8857 Acc: 0.5528                                               \n",
      "Epoch 035 | Train Loss: 0.9328 Acc: 0.4841 | Val Loss: 0.8767 Acc: 0.4806                                               \n",
      "Epoch 036 | Train Loss: 0.9230 Acc: 0.4930 | Val Loss: 0.8705 Acc: 0.5316                                               \n",
      "Epoch 037 | Train Loss: 0.9269 Acc: 0.4856 | Val Loss: 0.8625 Acc: 0.5125                                               \n",
      "Epoch 038 | Train Loss: 0.9226 Acc: 0.4955 | Val Loss: 0.9028 Acc: 0.4890                                               \n",
      "Epoch 039 | Train Loss: 0.9287 Acc: 0.4862 | Val Loss: 0.8701 Acc: 0.4934                                               \n",
      "Epoch 040 | Train Loss: 0.9212 Acc: 0.4849 | Val Loss: 0.8481 Acc: 0.4994                                               \n",
      "Epoch 041 | Train Loss: 0.9090 Acc: 0.4959 | Val Loss: 0.8607 Acc: 0.5000                                               \n",
      "Epoch 042 | Train Loss: 0.9177 Acc: 0.4867 | Val Loss: 0.8466 Acc: 0.5531                                               \n",
      "Epoch 043 | Train Loss: 0.9097 Acc: 0.4883 | Val Loss: 0.8413 Acc: 0.5343                                               \n",
      "Epoch 044 | Train Loss: 0.9041 Acc: 0.5002 | Val Loss: 0.8617 Acc: 0.5045                                               \n",
      "Epoch 045 | Train Loss: 0.9067 Acc: 0.4975 | Val Loss: 0.8444 Acc: 0.5275                                               \n",
      "Epoch 046 | Train Loss: 0.9049 Acc: 0.5015 | Val Loss: 0.8565 Acc: 0.5719                                               \n",
      "Epoch 047 | Train Loss: 0.9015 Acc: 0.5036 | Val Loss: 0.8620 Acc: 0.5985                                               \n",
      "Epoch 048 | Train Loss: 0.9078 Acc: 0.4968 | Val Loss: 0.8430 Acc: 0.5221                                               \n",
      "Epoch 049 | Train Loss: 0.8858 Acc: 0.5084 | Val Loss: 0.8337 Acc: 0.5313                                               \n",
      "Epoch 050 | Train Loss: 0.8972 Acc: 0.5003 | Val Loss: 0.8954 Acc: 0.4824                                               \n",
      "Epoch 051 | Train Loss: 0.8978 Acc: 0.4988 | Val Loss: 0.8227 Acc: 0.5481                                               \n",
      "Epoch 052 | Train Loss: 0.8854 Acc: 0.5200 | Val Loss: 0.8220 Acc: 0.5713                                               \n",
      "Epoch 053 | Train Loss: 0.8847 Acc: 0.5101 | Val Loss: 0.8220 Acc: 0.4955                                               \n",
      "Epoch 054 | Train Loss: 0.8829 Acc: 0.5166 | Val Loss: 0.8309 Acc: 0.5078                                               \n",
      "Epoch 055 | Train Loss: 0.8752 Acc: 0.5255 | Val Loss: 0.8056 Acc: 0.5272                                               \n",
      "Epoch 056 | Train Loss: 0.8737 Acc: 0.5177 | Val Loss: 0.8078 Acc: 0.5522                                               \n",
      "Epoch 057 | Train Loss: 0.8759 Acc: 0.5181 | Val Loss: 0.7945 Acc: 0.5728                                               \n",
      "Epoch 058 | Train Loss: 0.8692 Acc: 0.5241 | Val Loss: 0.8134 Acc: 0.6304                                               \n",
      "Epoch 059 | Train Loss: 0.8785 Acc: 0.5171 | Val Loss: 0.8661 Acc: 0.4627                                               \n",
      "Epoch 060 | Train Loss: 0.8686 Acc: 0.5196 | Val Loss: 0.7980 Acc: 0.5618                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 256, 'cnn_dropout': 0.19359038572627102, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 16, 'cnn_kernels_2': 32, 'learning_rate': 0.0006763786731248906, 'lstm_dense': 32, 'lstm_hidden_size': 64, 'lstm_layers': 6, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 5.0567 Acc: 0.4403 | Val Loss: 1.2618 Acc: 0.4421                                               \n",
      "Epoch 002 | Train Loss: 1.2555 Acc: 0.4422 | Val Loss: 1.2509 Acc: 0.4421                                               \n",
      "Epoch 003 | Train Loss: 1.2486 Acc: 0.4422 | Val Loss: 1.2463 Acc: 0.4421                                               \n",
      "Epoch 004 | Train Loss: 1.2453 Acc: 0.4422 | Val Loss: 1.2441 Acc: 0.4421                                               \n",
      "Epoch 005 | Train Loss: 1.2437 Acc: 0.4422 | Val Loss: 1.2429 Acc: 0.4421                                               \n",
      "Epoch 006 | Train Loss: 1.2429 Acc: 0.4422 | Val Loss: 1.2424 Acc: 0.4421                                               \n",
      "Epoch 007 | Train Loss: 1.2425 Acc: 0.4422 | Val Loss: 1.2421 Acc: 0.4421                                               \n",
      "Epoch 008 | Train Loss: 1.2423 Acc: 0.4422 | Val Loss: 1.2420 Acc: 0.4421                                               \n",
      "Epoch 009 | Train Loss: 1.2422 Acc: 0.4422 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 010 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 011 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 012 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 013 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 014 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 015 | Train Loss: 1.2422 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 016 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 017 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 018 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 019 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 020 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 021 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 022 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 023 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2420 Acc: 0.4421                                               \n",
      "Epoch 024 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 025 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2420 Acc: 0.4421                                               \n",
      "Epoch 026 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 027 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 028 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 029 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 030 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 031 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 032 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 033 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 034 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 035 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 036 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 037 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 038 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 039 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 040 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 041 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 042 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 043 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 044 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 045 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 046 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 047 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 048 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 049 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 050 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 051 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 052 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 053 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 054 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 055 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 056 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 057 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 058 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 059 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 060 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 256, 'cnn_dropout': 0.6970329471061838, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 16, 'learning_rate': 0.005268662245953721, 'lstm_dense': 32, 'lstm_hidden_size': 96, 'lstm_layers': 6, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 155.4182 Acc: 0.4394 | Val Loss: 1.2445 Acc: 0.4421                                             \n",
      "Epoch 002 | Train Loss: 1.2444 Acc: 0.4422 | Val Loss: 1.2421 Acc: 0.4421                                               \n",
      "Epoch 003 | Train Loss: 1.2431 Acc: 0.4422 | Val Loss: 1.2447 Acc: 0.4421                                               \n",
      "Epoch 004 | Train Loss: 1.2431 Acc: 0.4422 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 005 | Train Loss: 1.2432 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 006 | Train Loss: 1.2428 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 007 | Train Loss: 1.2432 Acc: 0.4422 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 008 | Train Loss: 1.2430 Acc: 0.4422 | Val Loss: 1.2420 Acc: 0.4421                                               \n",
      "Epoch 009 | Train Loss: 1.2428 Acc: 0.4422 | Val Loss: 1.2436 Acc: 0.4421                                               \n",
      "Epoch 010 | Train Loss: 1.2430 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 011 | Train Loss: 1.2436 Acc: 0.4422 | Val Loss: 1.2427 Acc: 0.4421                                               \n",
      "Epoch 012 | Train Loss: 1.2434 Acc: 0.4422 | Val Loss: 1.2420 Acc: 0.4421                                               \n",
      "Epoch 013 | Train Loss: 1.2431 Acc: 0.4422 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 014 | Train Loss: 1.2429 Acc: 0.4422 | Val Loss: 1.2425 Acc: 0.4421                                               \n",
      "Epoch 015 | Train Loss: 1.2429 Acc: 0.4422 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 128, 'cnn_dropout': 0.5431800334215964, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 32, 'cnn_kernels_2': 16, 'learning_rate': 0.00015414275433853832, 'lstm_dense': 128, 'lstm_hidden_size': 128, 'lstm_layers': 5, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 16.6168 Acc: 0.3833 | Val Loss: 2.1939 Acc: 0.5281                                              \n",
      "Epoch 002 | Train Loss: 5.4116 Acc: 0.4327 | Val Loss: 1.4315 Acc: 0.5797                                               \n",
      "Epoch 003 | Train Loss: 3.4208 Acc: 0.4871 | Val Loss: 1.9529 Acc: 0.5394                                               \n",
      "Epoch 004 | Train Loss: 2.4748 Acc: 0.5447 | Val Loss: 1.1197 Acc: 0.6528                                               \n",
      "Epoch 005 | Train Loss: 1.9108 Acc: 0.5743 | Val Loss: 1.0815 Acc: 0.6033                                               \n",
      "Epoch 006 | Train Loss: 1.4395 Acc: 0.6356 | Val Loss: 1.3393 Acc: 0.6027                                               \n",
      "Epoch 007 | Train Loss: 1.1739 Acc: 0.6680 | Val Loss: 0.6060 Acc: 0.8000                                               \n",
      "Epoch 008 | Train Loss: 0.9521 Acc: 0.7060 | Val Loss: 0.8648 Acc: 0.7576                                               \n",
      "Epoch 009 | Train Loss: 0.8027 Acc: 0.7381 | Val Loss: 0.6010 Acc: 0.8191                                               \n",
      "Epoch 010 | Train Loss: 0.7065 Acc: 0.7608 | Val Loss: 0.5304 Acc: 0.7991                                               \n",
      "Epoch 011 | Train Loss: 0.5918 Acc: 0.7853 | Val Loss: 0.5939 Acc: 0.8146                                               \n",
      "Epoch 012 | Train Loss: 0.5350 Acc: 0.8073 | Val Loss: 0.3810 Acc: 0.8701                                               \n",
      "Epoch 013 | Train Loss: 0.4692 Acc: 0.8333 | Val Loss: 0.4538 Acc: 0.8442                                               \n",
      "Epoch 014 | Train Loss: 0.4078 Acc: 0.8550 | Val Loss: 0.4234 Acc: 0.8499                                               \n",
      "Epoch 015 | Train Loss: 0.3737 Acc: 0.8719 | Val Loss: 0.3594 Acc: 0.8872                                               \n",
      "Epoch 016 | Train Loss: 0.3386 Acc: 0.8839 | Val Loss: 0.2784 Acc: 0.9036                                               \n",
      "Epoch 017 | Train Loss: 0.2879 Acc: 0.8993 | Val Loss: 0.2226 Acc: 0.9328                                               \n",
      "Epoch 018 | Train Loss: 0.2494 Acc: 0.9160 | Val Loss: 0.3106 Acc: 0.9024                                               \n",
      "Epoch 019 | Train Loss: 0.2175 Acc: 0.9260 | Val Loss: 0.3759 Acc: 0.8639                                               \n",
      "Epoch 020 | Train Loss: 0.1947 Acc: 0.9338 | Val Loss: 0.2124 Acc: 0.9236                                               \n",
      "Epoch 021 | Train Loss: 0.1732 Acc: 0.9431 | Val Loss: 0.1928 Acc: 0.9358                                               \n",
      "Epoch 022 | Train Loss: 0.1642 Acc: 0.9460 | Val Loss: 0.1775 Acc: 0.9379                                               \n",
      "Epoch 023 | Train Loss: 0.1488 Acc: 0.9503 | Val Loss: 0.1638 Acc: 0.9370                                               \n",
      "Epoch 024 | Train Loss: 0.1445 Acc: 0.9497 | Val Loss: 0.1331 Acc: 0.9516                                               \n",
      "Epoch 025 | Train Loss: 0.1233 Acc: 0.9579 | Val Loss: 0.1370 Acc: 0.9475                                               \n",
      "Epoch 026 | Train Loss: 0.1189 Acc: 0.9611 | Val Loss: 0.1780 Acc: 0.9358                                               \n",
      "Epoch 027 | Train Loss: 0.1120 Acc: 0.9642 | Val Loss: 0.0940 Acc: 0.9627                                               \n",
      "Epoch 028 | Train Loss: 0.1021 Acc: 0.9657 | Val Loss: 0.1900 Acc: 0.9370                                               \n",
      "Epoch 029 | Train Loss: 0.0922 Acc: 0.9707 | Val Loss: 0.1201 Acc: 0.9660                                               \n",
      "Epoch 030 | Train Loss: 0.0940 Acc: 0.9684 | Val Loss: 0.1314 Acc: 0.9576                                               \n",
      "Epoch 031 | Train Loss: 0.0824 Acc: 0.9744 | Val Loss: 0.0861 Acc: 0.9716                                               \n",
      "Epoch 032 | Train Loss: 0.0809 Acc: 0.9747 | Val Loss: 0.1032 Acc: 0.9684                                               \n",
      "Epoch 033 | Train Loss: 0.0748 Acc: 0.9756 | Val Loss: 0.0908 Acc: 0.9699                                               \n",
      "Epoch 034 | Train Loss: 0.0728 Acc: 0.9766 | Val Loss: 0.0842 Acc: 0.9734                                               \n",
      "Epoch 035 | Train Loss: 0.0736 Acc: 0.9780 | Val Loss: 0.0858 Acc: 0.9666                                               \n",
      "Epoch 036 | Train Loss: 0.0632 Acc: 0.9797 | Val Loss: 0.0681 Acc: 0.9788                                               \n",
      "Epoch 037 | Train Loss: 0.0654 Acc: 0.9781 | Val Loss: 0.0922 Acc: 0.9681                                               \n",
      "Epoch 038 | Train Loss: 0.0592 Acc: 0.9812 | Val Loss: 0.0893 Acc: 0.9749                                               \n",
      "Epoch 039 | Train Loss: 0.0536 Acc: 0.9838 | Val Loss: 0.0785 Acc: 0.9761                                               \n",
      "Epoch 040 | Train Loss: 0.0557 Acc: 0.9817 | Val Loss: 0.0701 Acc: 0.9764                                               \n",
      "Epoch 041 | Train Loss: 0.0463 Acc: 0.9868 | Val Loss: 0.0809 Acc: 0.9716                                               \n",
      "Epoch 042 | Train Loss: 0.0480 Acc: 0.9856 | Val Loss: 0.0920 Acc: 0.9707                                               \n",
      "Epoch 043 | Train Loss: 0.0463 Acc: 0.9858 | Val Loss: 0.0574 Acc: 0.9806                                               \n",
      "Epoch 044 | Train Loss: 0.0418 Acc: 0.9866 | Val Loss: 0.0852 Acc: 0.9746                                               \n",
      "Epoch 045 | Train Loss: 0.0478 Acc: 0.9849 | Val Loss: 0.0497 Acc: 0.9863                                               \n",
      "Epoch 046 | Train Loss: 0.0433 Acc: 0.9861 | Val Loss: 0.0728 Acc: 0.9743                                               \n",
      "Epoch 047 | Train Loss: 0.0442 Acc: 0.9864 | Val Loss: 0.0683 Acc: 0.9773                                               \n",
      "Epoch 048 | Train Loss: 0.0373 Acc: 0.9889 | Val Loss: 0.0703 Acc: 0.9782                                               \n",
      "Epoch 049 | Train Loss: 0.0415 Acc: 0.9872 | Val Loss: 0.0736 Acc: 0.9809                                               \n",
      "Epoch 050 | Train Loss: 0.0383 Acc: 0.9887 | Val Loss: 0.0559 Acc: 0.9824                                               \n",
      "Epoch 051 | Train Loss: 0.0398 Acc: 0.9877 | Val Loss: 0.0657 Acc: 0.9800                                               \n",
      "Epoch 052 | Train Loss: 0.0305 Acc: 0.9907 | Val Loss: 0.0776 Acc: 0.9761                                               \n",
      "Epoch 053 | Train Loss: 0.0375 Acc: 0.9898 | Val Loss: 0.0801 Acc: 0.9728                                               \n",
      "Epoch 054 | Train Loss: 0.0313 Acc: 0.9898 | Val Loss: 0.1013 Acc: 0.9740                                               \n",
      "Epoch 055 | Train Loss: 0.0334 Acc: 0.9901 | Val Loss: 0.0458 Acc: 0.9857                                               \n",
      "Epoch 056 | Train Loss: 0.0302 Acc: 0.9902 | Val Loss: 0.0707 Acc: 0.9824                                               \n",
      "Epoch 057 | Train Loss: 0.0294 Acc: 0.9907 | Val Loss: 0.1038 Acc: 0.9719                                               \n",
      "Epoch 058 | Train Loss: 0.0354 Acc: 0.9890 | Val Loss: 0.0567 Acc: 0.9797                                               \n",
      "Epoch 059 | Train Loss: 0.0312 Acc: 0.9898 | Val Loss: 0.0633 Acc: 0.9812                                               \n",
      "Epoch 060 | Train Loss: 0.0294 Acc: 0.9907 | Val Loss: 0.1012 Acc: 0.9719                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 36, 'cnn_dense': 128, 'cnn_dropout': 0.05380537278637338, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 16, 'cnn_kernels_2': 32, 'learning_rate': 1.168168236460308e-05, 'lstm_dense': 256, 'lstm_hidden_size': 96, 'lstm_layers': 2, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 57.2854 Acc: 0.3175 | Val Loss: 24.0911 Acc: 0.3934                                             \n",
      "Epoch 002 | Train Loss: 44.6345 Acc: 0.3380 | Val Loss: 20.2342 Acc: 0.3588                                             \n",
      "Epoch 003 | Train Loss: 35.8729 Acc: 0.3415 | Val Loss: 16.7206 Acc: 0.3537                                             \n",
      "Epoch 004 | Train Loss: 28.8977 Acc: 0.3516 | Val Loss: 12.6220 Acc: 0.3472                                             \n",
      "Epoch 005 | Train Loss: 23.3684 Acc: 0.3563 | Val Loss: 9.9169 Acc: 0.3821                                              \n",
      "Epoch 006 | Train Loss: 19.4163 Acc: 0.3688 | Val Loss: 7.5705 Acc: 0.4370                                              \n",
      "Epoch 007 | Train Loss: 16.4617 Acc: 0.3752 | Val Loss: 6.4261 Acc: 0.4949                                              \n",
      "Epoch 008 | Train Loss: 13.9804 Acc: 0.3897 | Val Loss: 5.4862 Acc: 0.4934                                              \n",
      "Epoch 009 | Train Loss: 12.2012 Acc: 0.3935 | Val Loss: 4.6804 Acc: 0.4988                                              \n",
      "Epoch 010 | Train Loss: 10.5395 Acc: 0.4149 | Val Loss: 4.6855 Acc: 0.4890                                              \n",
      "Epoch 011 | Train Loss: 9.5895 Acc: 0.4200 | Val Loss: 4.7702 Acc: 0.5182                                               \n",
      "Epoch 012 | Train Loss: 8.5154 Acc: 0.4317 | Val Loss: 3.4917 Acc: 0.5239                                               \n",
      "Epoch 013 | Train Loss: 7.6648 Acc: 0.4535 | Val Loss: 3.5216 Acc: 0.5379                                               \n",
      "Epoch 014 | Train Loss: 7.2520 Acc: 0.4572 | Val Loss: 3.2516 Acc: 0.5457                                               \n",
      "Epoch 015 | Train Loss: 6.7687 Acc: 0.4628 | Val Loss: 3.3353 Acc: 0.5606                                               \n",
      "Epoch 016 | Train Loss: 6.2705 Acc: 0.4815 | Val Loss: 2.7965 Acc: 0.5764                                               \n",
      "Epoch 017 | Train Loss: 5.8699 Acc: 0.4864 | Val Loss: 2.5293 Acc: 0.5684                                               \n",
      "Epoch 018 | Train Loss: 5.5995 Acc: 0.4897 | Val Loss: 2.0047 Acc: 0.5916                                               \n",
      "Epoch 019 | Train Loss: 5.0522 Acc: 0.5148 | Val Loss: 2.0817 Acc: 0.5654                                               \n",
      "Epoch 020 | Train Loss: 4.7881 Acc: 0.5175 | Val Loss: 1.8146 Acc: 0.6316                                               \n",
      "Epoch 021 | Train Loss: 4.5381 Acc: 0.5279 | Val Loss: 1.9370 Acc: 0.6167                                               \n",
      "Epoch 022 | Train Loss: 4.3108 Acc: 0.5327 | Val Loss: 1.7313 Acc: 0.6493                                               \n",
      "Epoch 023 | Train Loss: 4.1326 Acc: 0.5359 | Val Loss: 1.7484 Acc: 0.6307                                               \n",
      "Epoch 024 | Train Loss: 3.8287 Acc: 0.5385 | Val Loss: 1.6467 Acc: 0.6603                                               \n",
      "Epoch 025 | Train Loss: 3.6821 Acc: 0.5536 | Val Loss: 1.7244 Acc: 0.6525                                               \n",
      "Epoch 026 | Train Loss: 3.5393 Acc: 0.5651 | Val Loss: 1.4811 Acc: 0.6773                                               \n",
      "Epoch 027 | Train Loss: 3.3474 Acc: 0.5641 | Val Loss: 1.5577 Acc: 0.6707                                               \n",
      "Epoch 028 | Train Loss: 3.2155 Acc: 0.5694 | Val Loss: 1.2798 Acc: 0.6788                                               \n",
      "Epoch 029 | Train Loss: 3.0829 Acc: 0.5812 | Val Loss: 1.4135 Acc: 0.6890                                               \n",
      "Epoch 030 | Train Loss: 2.9413 Acc: 0.5826 | Val Loss: 1.4704 Acc: 0.6681                                               \n",
      "Epoch 031 | Train Loss: 2.8176 Acc: 0.5878 | Val Loss: 1.3191 Acc: 0.6958                                               \n",
      "Epoch 032 | Train Loss: 2.6839 Acc: 0.5894 | Val Loss: 1.5010 Acc: 0.6764                                               \n",
      "Epoch 033 | Train Loss: 2.5493 Acc: 0.6030 | Val Loss: 1.3642 Acc: 0.6946                                               \n",
      "Epoch 034 | Train Loss: 2.4586 Acc: 0.6041 | Val Loss: 1.2469 Acc: 0.7048                                               \n",
      "Epoch 035 | Train Loss: 2.3443 Acc: 0.6116 | Val Loss: 1.2063 Acc: 0.6988                                               \n",
      "Epoch 036 | Train Loss: 2.3138 Acc: 0.6130 | Val Loss: 1.0874 Acc: 0.7194                                               \n",
      "Epoch 037 | Train Loss: 2.1542 Acc: 0.6285 | Val Loss: 1.1877 Acc: 0.7176                                               \n",
      "Epoch 038 | Train Loss: 2.0619 Acc: 0.6406 | Val Loss: 1.3252 Acc: 0.6988                                               \n",
      "Epoch 039 | Train Loss: 1.9916 Acc: 0.6380 | Val Loss: 0.9548 Acc: 0.7388                                               \n",
      "Epoch 040 | Train Loss: 1.9442 Acc: 0.6389 | Val Loss: 1.2118 Acc: 0.7012                                               \n",
      "Epoch 041 | Train Loss: 1.8748 Acc: 0.6448 | Val Loss: 1.0368 Acc: 0.7376                                               \n",
      "Epoch 042 | Train Loss: 1.7269 Acc: 0.6559 | Val Loss: 0.8946 Acc: 0.7573                                               \n",
      "Epoch 043 | Train Loss: 1.7209 Acc: 0.6582 | Val Loss: 0.8110 Acc: 0.7612                                               \n",
      "Epoch 044 | Train Loss: 1.7057 Acc: 0.6634 | Val Loss: 0.9718 Acc: 0.7340                                               \n",
      "Epoch 045 | Train Loss: 1.6220 Acc: 0.6649 | Val Loss: 1.2159 Acc: 0.6997                                               \n",
      "Epoch 046 | Train Loss: 1.5113 Acc: 0.6836 | Val Loss: 1.0229 Acc: 0.7361                                               \n",
      "Epoch 047 | Train Loss: 1.5098 Acc: 0.6794 | Val Loss: 0.7535 Acc: 0.7761                                               \n",
      "Epoch 048 | Train Loss: 1.4304 Acc: 0.6853 | Val Loss: 0.7069 Acc: 0.7770                                               \n",
      "Epoch 049 | Train Loss: 1.3754 Acc: 0.6943 | Val Loss: 0.7105 Acc: 0.7934                                               \n",
      "Epoch 050 | Train Loss: 1.3518 Acc: 0.6938 | Val Loss: 0.6729 Acc: 0.7991                                               \n",
      "Epoch 051 | Train Loss: 1.3164 Acc: 0.7056 | Val Loss: 0.6262 Acc: 0.7904                                               \n",
      "Epoch 052 | Train Loss: 1.2895 Acc: 0.7039 | Val Loss: 0.7820 Acc: 0.7791                                               \n",
      "Epoch 053 | Train Loss: 1.2253 Acc: 0.7126 | Val Loss: 0.7648 Acc: 0.7899                                               \n",
      "Epoch 054 | Train Loss: 1.1774 Acc: 0.7159 | Val Loss: 0.8539 Acc: 0.7555                                               \n",
      "Epoch 055 | Train Loss: 1.1528 Acc: 0.7184 | Val Loss: 0.6345 Acc: 0.8054                                               \n",
      "Epoch 056 | Train Loss: 1.1425 Acc: 0.7289 | Val Loss: 0.5852 Acc: 0.8063                                               \n",
      "Epoch 057 | Train Loss: 1.0627 Acc: 0.7345 | Val Loss: 0.5650 Acc: 0.8137                                               \n",
      "Epoch 058 | Train Loss: 1.0220 Acc: 0.7404 | Val Loss: 0.6020 Acc: 0.8003                                               \n",
      "Epoch 059 | Train Loss: 1.0230 Acc: 0.7437 | Val Loss: 0.5618 Acc: 0.8248                                               \n",
      "Epoch 060 | Train Loss: 0.9917 Acc: 0.7515 | Val Loss: 0.5584 Acc: 0.8131                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 256, 'cnn_dropout': 0.22706407345067528, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 32, 'cnn_kernels_2': 64, 'learning_rate': 0.00014499669837636674, 'lstm_dense': 128, 'lstm_hidden_size': 32, 'lstm_layers': 2, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 22.1027 Acc: 0.3531 | Val Loss: 8.4441 Acc: 0.4481                                              \n",
      "Epoch 002 | Train Loss: 9.6127 Acc: 0.3902 | Val Loss: 3.4038 Acc: 0.4490                                               \n",
      "Epoch 003 | Train Loss: 6.4765 Acc: 0.4112 | Val Loss: 3.6226 Acc: 0.4928                                               \n",
      "Epoch 004 | Train Loss: 4.8826 Acc: 0.4171 | Val Loss: 9.4421 Acc: 0.4421                                               \n",
      "Epoch 005 | Train Loss: 3.8435 Acc: 0.4372 | Val Loss: 2.3205 Acc: 0.5140                                               \n",
      "Epoch 006 | Train Loss: 3.0696 Acc: 0.4521 | Val Loss: 3.3821 Acc: 0.4690                                               \n",
      "Epoch 007 | Train Loss: 2.4729 Acc: 0.4565 | Val Loss: 4.3086 Acc: 0.4313                                               \n",
      "Epoch 008 | Train Loss: 2.0819 Acc: 0.4779 | Val Loss: 3.5237 Acc: 0.5218                                               \n",
      "Epoch 009 | Train Loss: 1.7845 Acc: 0.5046 | Val Loss: 1.3407 Acc: 0.5430                                               \n",
      "Epoch 010 | Train Loss: 1.5537 Acc: 0.5243 | Val Loss: 4.3218 Acc: 0.4215                                               \n",
      "Epoch 011 | Train Loss: 1.3948 Acc: 0.5451 | Val Loss: 1.3615 Acc: 0.6048                                               \n",
      "Epoch 012 | Train Loss: 1.1714 Acc: 0.5891 | Val Loss: 1.0114 Acc: 0.6027                                               \n",
      "Epoch 013 | Train Loss: 1.0919 Acc: 0.6021 | Val Loss: 1.2488 Acc: 0.5687                                               \n",
      "Epoch 014 | Train Loss: 0.9634 Acc: 0.6296 | Val Loss: 0.8252 Acc: 0.6075                                               \n",
      "Epoch 015 | Train Loss: 0.8901 Acc: 0.6560 | Val Loss: 1.7866 Acc: 0.6239                                               \n",
      "Epoch 016 | Train Loss: 0.8048 Acc: 0.6928 | Val Loss: 0.7459 Acc: 0.6397                                               \n",
      "Epoch 017 | Train Loss: 0.7572 Acc: 0.7109 | Val Loss: 0.7469 Acc: 0.6469                                               \n",
      "Epoch 018 | Train Loss: 0.6748 Acc: 0.7433 | Val Loss: 0.8477 Acc: 0.6654                                               \n",
      "Epoch 019 | Train Loss: 0.6365 Acc: 0.7598 | Val Loss: 0.6366 Acc: 0.7254                                               \n",
      "Epoch 020 | Train Loss: 0.5859 Acc: 0.7732 | Val Loss: 1.3081 Acc: 0.6122                                               \n",
      "Epoch 021 | Train Loss: 0.5551 Acc: 0.7882 | Val Loss: 0.7291 Acc: 0.6884                                               \n",
      "Epoch 022 | Train Loss: 0.5193 Acc: 0.8017 | Val Loss: 0.5650 Acc: 0.7943                                               \n",
      "Epoch 023 | Train Loss: 0.5011 Acc: 0.8089 | Val Loss: 0.3808 Acc: 0.8597                                               \n",
      "Epoch 024 | Train Loss: 0.4466 Acc: 0.8329 | Val Loss: 0.4000 Acc: 0.8316                                               \n",
      "Epoch 025 | Train Loss: 0.4167 Acc: 0.8481 | Val Loss: 0.2670 Acc: 0.9143                                               \n",
      "Epoch 026 | Train Loss: 0.3852 Acc: 0.8609 | Val Loss: 0.3420 Acc: 0.8531                                               \n",
      "Epoch 027 | Train Loss: 0.3658 Acc: 0.8676 | Val Loss: 0.2387 Acc: 0.9173                                               \n",
      "Epoch 028 | Train Loss: 0.3359 Acc: 0.8788 | Val Loss: 0.2562 Acc: 0.8970                                               \n",
      "Epoch 029 | Train Loss: 0.3123 Acc: 0.8894 | Val Loss: 0.2943 Acc: 0.8663                                               \n",
      "Epoch 030 | Train Loss: 0.2915 Acc: 0.8948 | Val Loss: 0.3922 Acc: 0.8382                                               \n",
      "Epoch 031 | Train Loss: 0.2617 Acc: 0.9133 | Val Loss: 0.3434 Acc: 0.8749                                               \n",
      "Epoch 032 | Train Loss: 0.2574 Acc: 0.9108 | Val Loss: 0.2259 Acc: 0.9239                                               \n",
      "Epoch 033 | Train Loss: 0.2428 Acc: 0.9169 | Val Loss: 0.1755 Acc: 0.9421                                               \n",
      "Epoch 034 | Train Loss: 0.2262 Acc: 0.9204 | Val Loss: 0.2242 Acc: 0.9173                                               \n",
      "Epoch 035 | Train Loss: 0.2123 Acc: 0.9257 | Val Loss: 0.1815 Acc: 0.9343                                               \n",
      "Epoch 036 | Train Loss: 0.2004 Acc: 0.9310 | Val Loss: 0.2642 Acc: 0.9084                                               \n",
      "Epoch 037 | Train Loss: 0.1855 Acc: 0.9375 | Val Loss: 0.2803 Acc: 0.8946                                               \n",
      "Epoch 038 | Train Loss: 0.1756 Acc: 0.9430 | Val Loss: 0.1588 Acc: 0.9418                                               \n",
      "Epoch 039 | Train Loss: 0.1701 Acc: 0.9423 | Val Loss: 0.1695 Acc: 0.9388                                               \n",
      "Epoch 040 | Train Loss: 0.1589 Acc: 0.9479 | Val Loss: 0.2725 Acc: 0.8997                                               \n",
      "Epoch 041 | Train Loss: 0.1459 Acc: 0.9504 | Val Loss: 0.1826 Acc: 0.9355                                               \n",
      "Epoch 042 | Train Loss: 0.1356 Acc: 0.9533 | Val Loss: 0.1198 Acc: 0.9537                                               \n",
      "Epoch 043 | Train Loss: 0.1317 Acc: 0.9550 | Val Loss: 0.1867 Acc: 0.9349                                               \n",
      "Epoch 044 | Train Loss: 0.1262 Acc: 0.9573 | Val Loss: 0.2540 Acc: 0.9158                                               \n",
      "Epoch 045 | Train Loss: 0.1172 Acc: 0.9620 | Val Loss: 0.2512 Acc: 0.9140                                               \n",
      "Epoch 046 | Train Loss: 0.1145 Acc: 0.9623 | Val Loss: 0.3896 Acc: 0.8651                                               \n",
      "Epoch 047 | Train Loss: 0.1051 Acc: 0.9658 | Val Loss: 0.1123 Acc: 0.9582                                               \n",
      "Epoch 048 | Train Loss: 0.0983 Acc: 0.9654 | Val Loss: 0.0988 Acc: 0.9678                                               \n",
      "Epoch 049 | Train Loss: 0.0920 Acc: 0.9690 | Val Loss: 0.0928 Acc: 0.9690                                               \n",
      "Epoch 050 | Train Loss: 0.0886 Acc: 0.9697 | Val Loss: 0.2141 Acc: 0.9275                                               \n",
      "Epoch 051 | Train Loss: 0.0837 Acc: 0.9719 | Val Loss: 0.1182 Acc: 0.9591                                               \n",
      "Epoch 052 | Train Loss: 0.0812 Acc: 0.9733 | Val Loss: 0.0684 Acc: 0.9794                                               \n",
      "Epoch 053 | Train Loss: 0.0785 Acc: 0.9738 | Val Loss: 0.1216 Acc: 0.9618                                               \n",
      "Epoch 054 | Train Loss: 0.0810 Acc: 0.9739 | Val Loss: 0.0877 Acc: 0.9710                                               \n",
      "Epoch 055 | Train Loss: 0.0682 Acc: 0.9769 | Val Loss: 0.0746 Acc: 0.9743                                               \n",
      "Epoch 056 | Train Loss: 0.0643 Acc: 0.9793 | Val Loss: 0.0616 Acc: 0.9797                                               \n",
      "Epoch 057 | Train Loss: 0.0671 Acc: 0.9792 | Val Loss: 0.1029 Acc: 0.9642                                               \n",
      "Epoch 058 | Train Loss: 0.0588 Acc: 0.9801 | Val Loss: 0.0600 Acc: 0.9782                                               \n",
      "Epoch 059 | Train Loss: 0.0684 Acc: 0.9791 | Val Loss: 0.2191 Acc: 0.9221                                               \n",
      "Epoch 060 | Train Loss: 0.0524 Acc: 0.9829 | Val Loss: 0.0789 Acc: 0.9755                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 256, 'cnn_dropout': 0.0025084501015309123, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 32, 'cnn_kernels_2': 16, 'learning_rate': 8.969576059050104e-05, 'lstm_dense': 128, 'lstm_hidden_size': 64, 'lstm_layers': 1, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 46.9541 Acc: 0.3104 | Val Loss: 6.9503 Acc: 0.4481                                              \n",
      "Epoch 002 | Train Loss: 21.1757 Acc: 0.3646 | Val Loss: 7.0198 Acc: 0.4170                                              \n",
      "Epoch 003 | Train Loss: 13.9035 Acc: 0.3860 | Val Loss: 3.7251 Acc: 0.4597                                              \n",
      "Epoch 004 | Train Loss: 10.7017 Acc: 0.3962 | Val Loss: 2.9917 Acc: 0.4887                                              \n",
      "Epoch 005 | Train Loss: 9.1937 Acc: 0.3997 | Val Loss: 2.2717 Acc: 0.4582                                               \n",
      "Epoch 006 | Train Loss: 7.6637 Acc: 0.4120 | Val Loss: 3.9479 Acc: 0.4696                                               \n",
      "Epoch 007 | Train Loss: 6.6922 Acc: 0.4139 | Val Loss: 1.8022 Acc: 0.5000                                               \n",
      "Epoch 008 | Train Loss: 5.9747 Acc: 0.4191 | Val Loss: 1.5855 Acc: 0.5487                                               \n",
      "Epoch 009 | Train Loss: 5.1031 Acc: 0.4308 | Val Loss: 1.5701 Acc: 0.4761                                               \n",
      "Epoch 010 | Train Loss: 4.5555 Acc: 0.4365 | Val Loss: 1.8696 Acc: 0.5418                                               \n",
      "Epoch 011 | Train Loss: 4.1061 Acc: 0.4463 | Val Loss: 1.4576 Acc: 0.5478                                               \n",
      "Epoch 012 | Train Loss: 3.5492 Acc: 0.4588 | Val Loss: 1.2476 Acc: 0.5937                                               \n",
      "Epoch 013 | Train Loss: 3.1757 Acc: 0.4707 | Val Loss: 1.3086 Acc: 0.5866                                               \n",
      "Epoch 014 | Train Loss: 2.8602 Acc: 0.4903 | Val Loss: 1.4650 Acc: 0.5872                                               \n",
      "Epoch 015 | Train Loss: 2.6126 Acc: 0.4947 | Val Loss: 0.9334 Acc: 0.6693                                               \n",
      "Epoch 016 | Train Loss: 2.3430 Acc: 0.5109 | Val Loss: 1.1550 Acc: 0.6078                                               \n",
      "Epoch 017 | Train Loss: 2.1526 Acc: 0.5291 | Val Loss: 0.9877 Acc: 0.6090                                               \n",
      "Epoch 018 | Train Loss: 1.9488 Acc: 0.5493 | Val Loss: 0.9544 Acc: 0.6519                                               \n",
      "Epoch 019 | Train Loss: 1.7458 Acc: 0.5746 | Val Loss: 0.9304 Acc: 0.6836                                               \n",
      "Epoch 020 | Train Loss: 1.5545 Acc: 0.5983 | Val Loss: 0.9595 Acc: 0.6627                                               \n",
      "Epoch 021 | Train Loss: 1.4424 Acc: 0.6211 | Val Loss: 0.7194 Acc: 0.7310                                               \n",
      "Epoch 022 | Train Loss: 1.2801 Acc: 0.6439 | Val Loss: 0.7068 Acc: 0.7499                                               \n",
      "Epoch 023 | Train Loss: 1.1841 Acc: 0.6833 | Val Loss: 0.5607 Acc: 0.7887                                               \n",
      "Epoch 024 | Train Loss: 1.0366 Acc: 0.7110 | Val Loss: 0.7614 Acc: 0.7609                                               \n",
      "Epoch 025 | Train Loss: 0.9198 Acc: 0.7307 | Val Loss: 0.5388 Acc: 0.8045                                               \n",
      "Epoch 026 | Train Loss: 0.8294 Acc: 0.7614 | Val Loss: 0.4970 Acc: 0.8197                                               \n",
      "Epoch 027 | Train Loss: 0.7641 Acc: 0.7726 | Val Loss: 0.4869 Acc: 0.8310                                               \n",
      "Epoch 028 | Train Loss: 0.6916 Acc: 0.7908 | Val Loss: 0.5352 Acc: 0.8149                                               \n",
      "Epoch 029 | Train Loss: 0.6450 Acc: 0.8083 | Val Loss: 0.4694 Acc: 0.8364                                               \n",
      "Epoch 030 | Train Loss: 0.5858 Acc: 0.8204 | Val Loss: 0.4232 Acc: 0.8475                                               \n",
      "Epoch 031 | Train Loss: 0.5178 Acc: 0.8366 | Val Loss: 0.3725 Acc: 0.8603                                               \n",
      "Epoch 032 | Train Loss: 0.4729 Acc: 0.8469 | Val Loss: 0.4607 Acc: 0.8460                                               \n",
      "Epoch 033 | Train Loss: 0.4331 Acc: 0.8633 | Val Loss: 0.3798 Acc: 0.8651                                               \n",
      "Epoch 034 | Train Loss: 0.3912 Acc: 0.8714 | Val Loss: 0.3894 Acc: 0.8570                                               \n",
      "Epoch 035 | Train Loss: 0.3675 Acc: 0.8812 | Val Loss: 0.3723 Acc: 0.8791                                               \n",
      "Epoch 036 | Train Loss: 0.3569 Acc: 0.8851 | Val Loss: 0.3486 Acc: 0.8860                                               \n",
      "Epoch 037 | Train Loss: 0.3088 Acc: 0.8990 | Val Loss: 0.3007 Acc: 0.9018                                               \n",
      "Epoch 038 | Train Loss: 0.3060 Acc: 0.9028 | Val Loss: 0.3559 Acc: 0.8875                                               \n",
      "Epoch 039 | Train Loss: 0.2790 Acc: 0.9089 | Val Loss: 0.3114 Acc: 0.9015                                               \n",
      "Epoch 040 | Train Loss: 0.2664 Acc: 0.9151 | Val Loss: 0.2780 Acc: 0.9110                                               \n",
      "Epoch 041 | Train Loss: 0.2525 Acc: 0.9205 | Val Loss: 0.3287 Acc: 0.8943                                               \n",
      "Epoch 042 | Train Loss: 0.2268 Acc: 0.9269 | Val Loss: 0.2783 Acc: 0.9101                                               \n",
      "Epoch 043 | Train Loss: 0.2225 Acc: 0.9275 | Val Loss: 0.2694 Acc: 0.9179                                               \n",
      "Epoch 044 | Train Loss: 0.2201 Acc: 0.9280 | Val Loss: 0.3137 Acc: 0.9078                                               \n",
      "Epoch 045 | Train Loss: 0.2213 Acc: 0.9271 | Val Loss: 0.3013 Acc: 0.9030                                               \n",
      "Epoch 046 | Train Loss: 0.1962 Acc: 0.9357 | Val Loss: 0.2511 Acc: 0.9239                                               \n",
      "Epoch 047 | Train Loss: 0.1641 Acc: 0.9474 | Val Loss: 0.2377 Acc: 0.9272                                               \n",
      "Epoch 048 | Train Loss: 0.1909 Acc: 0.9389 | Val Loss: 0.2858 Acc: 0.9107                                               \n",
      "Epoch 049 | Train Loss: 0.1461 Acc: 0.9522 | Val Loss: 0.2519 Acc: 0.9218                                               \n",
      "Epoch 050 | Train Loss: 0.1536 Acc: 0.9509 | Val Loss: 0.2336 Acc: 0.9278                                               \n",
      "Epoch 051 | Train Loss: 0.1430 Acc: 0.9529 | Val Loss: 0.2679 Acc: 0.9209                                               \n",
      "Epoch 052 | Train Loss: 0.1311 Acc: 0.9559 | Val Loss: 0.2604 Acc: 0.9179                                               \n",
      "Epoch 053 | Train Loss: 0.1357 Acc: 0.9558 | Val Loss: 0.2422 Acc: 0.9269                                               \n",
      "Epoch 054 | Train Loss: 0.1208 Acc: 0.9619 | Val Loss: 0.2714 Acc: 0.9215                                               \n",
      "Epoch 055 | Train Loss: 0.1230 Acc: 0.9612 | Val Loss: 0.2763 Acc: 0.9164                                               \n",
      "Epoch 056 | Train Loss: 0.1212 Acc: 0.9594 | Val Loss: 0.2200 Acc: 0.9293                                               \n",
      "Epoch 057 | Train Loss: 0.1427 Acc: 0.9541 | Val Loss: 0.2089 Acc: 0.9367                                               \n",
      "Epoch 058 | Train Loss: 0.1123 Acc: 0.9630 | Val Loss: 0.2151 Acc: 0.9352                                               \n",
      "Epoch 059 | Train Loss: 0.0818 Acc: 0.9733 | Val Loss: 0.2256 Acc: 0.9382                                               \n",
      "Epoch 060 | Train Loss: 0.0808 Acc: 0.9737 | Val Loss: 0.2041 Acc: 0.9376                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 128, 'cnn_dropout': 0.23950359691848755, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 32, 'cnn_kernels_2': 16, 'learning_rate': 0.0001180936523263642, 'lstm_dense': 64, 'lstm_hidden_size': 96, 'lstm_layers': 5, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 3.9205 Acc: 0.4223 | Val Loss: 1.1058 Acc: 0.4113                                               \n",
      "Epoch 002 | Train Loss: 1.0765 Acc: 0.4615 | Val Loss: 0.9357 Acc: 0.5513                                               \n",
      "Epoch 003 | Train Loss: 0.9880 Acc: 0.4849 | Val Loss: 0.8782 Acc: 0.4684                                               \n",
      "Epoch 004 | Train Loss: 0.9433 Acc: 0.5020 | Val Loss: 0.8593 Acc: 0.5284                                               \n",
      "Epoch 005 | Train Loss: 0.8991 Acc: 0.5310 | Val Loss: 0.7971 Acc: 0.6728                                               \n",
      "Epoch 006 | Train Loss: 0.8718 Acc: 0.5497 | Val Loss: 0.7999 Acc: 0.6334                                               \n",
      "Epoch 007 | Train Loss: 0.8816 Acc: 0.5501 | Val Loss: 0.7526 Acc: 0.6128                                               \n",
      "Epoch 008 | Train Loss: 0.8332 Acc: 0.5847 | Val Loss: 0.7561 Acc: 0.6848                                               \n",
      "Epoch 009 | Train Loss: 0.8266 Acc: 0.5888 | Val Loss: 0.7209 Acc: 0.6543                                               \n",
      "Epoch 010 | Train Loss: 0.8133 Acc: 0.5894 | Val Loss: 0.7123 Acc: 0.6585                                               \n",
      "Epoch 011 | Train Loss: 0.7868 Acc: 0.6081 | Val Loss: 0.6524 Acc: 0.7349                                               \n",
      "Epoch 012 | Train Loss: 0.7889 Acc: 0.6000 | Val Loss: 0.6959 Acc: 0.6188                                               \n",
      "Epoch 013 | Train Loss: 0.7647 Acc: 0.6236 | Val Loss: 0.6258 Acc: 0.7579                                               \n",
      "Epoch 014 | Train Loss: 0.7693 Acc: 0.6211 | Val Loss: 0.6564 Acc: 0.7173                                               \n",
      "Epoch 015 | Train Loss: 0.7525 Acc: 0.6326 | Val Loss: 0.6446 Acc: 0.7442                                               \n",
      "Epoch 016 | Train Loss: 0.7593 Acc: 0.6309 | Val Loss: 0.6577 Acc: 0.6985                                               \n",
      "Epoch 017 | Train Loss: 0.7572 Acc: 0.6303 | Val Loss: 0.6521 Acc: 0.7564                                               \n",
      "Epoch 018 | Train Loss: 0.7424 Acc: 0.6381 | Val Loss: 0.6248 Acc: 0.7269                                               \n",
      "Epoch 019 | Train Loss: 0.7465 Acc: 0.6365 | Val Loss: 0.6362 Acc: 0.7499                                               \n",
      "Epoch 020 | Train Loss: 0.7467 Acc: 0.6453 | Val Loss: 0.6509 Acc: 0.6833                                               \n",
      "Epoch 021 | Train Loss: 0.7301 Acc: 0.6462 | Val Loss: 0.6756 Acc: 0.6809                                               \n",
      "Epoch 022 | Train Loss: 0.7296 Acc: 0.6521 | Val Loss: 0.6045 Acc: 0.7582                                               \n",
      "Epoch 023 | Train Loss: 0.7207 Acc: 0.6472 | Val Loss: 0.6319 Acc: 0.7266                                               \n",
      "Epoch 024 | Train Loss: 0.7120 Acc: 0.6556 | Val Loss: 0.5866 Acc: 0.7606                                               \n",
      "Epoch 025 | Train Loss: 0.7050 Acc: 0.6556 | Val Loss: 0.5915 Acc: 0.7391                                               \n",
      "Epoch 026 | Train Loss: 0.7292 Acc: 0.6511 | Val Loss: 0.6137 Acc: 0.7573                                               \n",
      "Epoch 027 | Train Loss: 0.7556 Acc: 0.6446 | Val Loss: 0.6199 Acc: 0.7113                                               \n",
      "Epoch 028 | Train Loss: 0.7476 Acc: 0.6407 | Val Loss: 0.6779 Acc: 0.7370                                               \n",
      "Epoch 029 | Train Loss: 0.7633 Acc: 0.6367 | Val Loss: 0.7280 Acc: 0.6269                                               \n",
      "Epoch 030 | Train Loss: 0.7772 Acc: 0.6285 | Val Loss: 0.6313 Acc: 0.7319                                               \n",
      "Epoch 031 | Train Loss: 0.7413 Acc: 0.6480 | Val Loss: 0.5876 Acc: 0.7504                                               \n",
      "Epoch 032 | Train Loss: 0.7203 Acc: 0.6608 | Val Loss: 0.5779 Acc: 0.7334                                               \n",
      "Epoch 033 | Train Loss: 0.6947 Acc: 0.6684 | Val Loss: 0.5554 Acc: 0.7609                                               \n",
      "Epoch 034 | Train Loss: 0.6922 Acc: 0.6748 | Val Loss: 0.6080 Acc: 0.7149                                               \n",
      "Epoch 035 | Train Loss: 0.6926 Acc: 0.6668 | Val Loss: 0.5309 Acc: 0.7761                                               \n",
      "Epoch 036 | Train Loss: 0.6788 Acc: 0.6776 | Val Loss: 0.5812 Acc: 0.7346                                               \n",
      "Epoch 037 | Train Loss: 0.6664 Acc: 0.6782 | Val Loss: 0.5560 Acc: 0.7293                                               \n",
      "Epoch 038 | Train Loss: 0.6698 Acc: 0.6765 | Val Loss: 0.5619 Acc: 0.7516                                               \n",
      "Epoch 039 | Train Loss: 0.6774 Acc: 0.6733 | Val Loss: 0.5549 Acc: 0.7528                                               \n",
      "Epoch 040 | Train Loss: 0.6612 Acc: 0.6822 | Val Loss: 0.5667 Acc: 0.7588                                               \n",
      "Epoch 041 | Train Loss: 0.6784 Acc: 0.6675 | Val Loss: 0.5667 Acc: 0.7424                                               \n",
      "Epoch 042 | Train Loss: 0.6661 Acc: 0.6727 | Val Loss: 0.5257 Acc: 0.7713                                               \n",
      "Epoch 043 | Train Loss: 0.6718 Acc: 0.6762 | Val Loss: 0.5757 Acc: 0.7567                                               \n",
      "Epoch 044 | Train Loss: 0.6697 Acc: 0.6824 | Val Loss: 0.5477 Acc: 0.7307                                               \n",
      "Epoch 045 | Train Loss: 0.6626 Acc: 0.6848 | Val Loss: 0.5351 Acc: 0.7722                                               \n",
      "Epoch 046 | Train Loss: 0.6678 Acc: 0.6839 | Val Loss: 0.6550 Acc: 0.6770                                               \n",
      "Epoch 047 | Train Loss: 0.7058 Acc: 0.6586 | Val Loss: 0.6398 Acc: 0.7546                                               \n",
      "Epoch 048 | Train Loss: 0.6809 Acc: 0.6703 | Val Loss: 0.6059 Acc: 0.7245                                               \n",
      "Epoch 049 | Train Loss: 0.6904 Acc: 0.6659 | Val Loss: 0.5619 Acc: 0.7875                                               \n",
      "Epoch 050 | Train Loss: 0.6998 Acc: 0.6662 | Val Loss: 0.6093 Acc: 0.7200                                               \n",
      "Epoch 051 | Train Loss: 0.7004 Acc: 0.6711 | Val Loss: 0.6607 Acc: 0.6785                                               \n",
      "Epoch 052 | Train Loss: 0.6763 Acc: 0.6803 | Val Loss: 0.5564 Acc: 0.7803                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 128, 'cnn_dropout': 0.3351667532372843, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 64, 'cnn_kernels_2': 16, 'learning_rate': 0.0010995627872369964, 'lstm_dense': 128, 'lstm_hidden_size': 96, 'lstm_layers': 3, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 8.4735 Acc: 0.4405 | Val Loss: 2.0035 Acc: 0.5269                                               \n",
      "Epoch 002 | Train Loss: 1.5365 Acc: 0.5311 | Val Loss: 0.8730 Acc: 0.6119                                               \n",
      "Epoch 003 | Train Loss: 0.9769 Acc: 0.5910 | Val Loss: 0.6857 Acc: 0.6910                                               \n",
      "Epoch 004 | Train Loss: 0.8076 Acc: 0.6434 | Val Loss: 0.6880 Acc: 0.6821                                               \n",
      "Epoch 005 | Train Loss: 0.7174 Acc: 0.6824 | Val Loss: 0.6089 Acc: 0.7531                                               \n",
      "Epoch 006 | Train Loss: 0.6711 Acc: 0.7003 | Val Loss: 0.5236 Acc: 0.7504                                               \n",
      "Epoch 007 | Train Loss: 0.6298 Acc: 0.7201 | Val Loss: 0.5289 Acc: 0.7645                                               \n",
      "Epoch 008 | Train Loss: 0.6118 Acc: 0.7307 | Val Loss: 0.5111 Acc: 0.7943                                               \n",
      "Epoch 009 | Train Loss: 0.5939 Acc: 0.7375 | Val Loss: 0.5562 Acc: 0.7430                                               \n",
      "Epoch 010 | Train Loss: 0.5571 Acc: 0.7564 | Val Loss: 0.5381 Acc: 0.7633                                               \n",
      "Epoch 011 | Train Loss: 0.5456 Acc: 0.7646 | Val Loss: 0.5481 Acc: 0.7266                                               \n",
      "Epoch 012 | Train Loss: 0.5199 Acc: 0.7702 | Val Loss: 0.4947 Acc: 0.7982                                               \n",
      "Epoch 013 | Train Loss: 0.4947 Acc: 0.7917 | Val Loss: 0.3762 Acc: 0.8615                                               \n",
      "Epoch 014 | Train Loss: 0.5068 Acc: 0.7844 | Val Loss: 0.5474 Acc: 0.7310                                               \n",
      "Epoch 015 | Train Loss: 0.4997 Acc: 0.7958 | Val Loss: 0.4549 Acc: 0.8101                                               \n",
      "Epoch 016 | Train Loss: 0.4750 Acc: 0.8036 | Val Loss: 0.4972 Acc: 0.7773                                               \n",
      "Epoch 017 | Train Loss: 0.4896 Acc: 0.7944 | Val Loss: 0.5533 Acc: 0.7687                                               \n",
      "Epoch 018 | Train Loss: 0.4716 Acc: 0.8025 | Val Loss: 0.4517 Acc: 0.8024                                               \n",
      "Epoch 019 | Train Loss: 0.4777 Acc: 0.8052 | Val Loss: 0.4802 Acc: 0.8182                                               \n",
      "Epoch 020 | Train Loss: 0.4363 Acc: 0.8177 | Val Loss: 0.4716 Acc: 0.7884                                               \n",
      "Epoch 021 | Train Loss: 0.4284 Acc: 0.8231 | Val Loss: 0.4701 Acc: 0.8212                                               \n",
      "Epoch 022 | Train Loss: 0.4198 Acc: 0.8302 | Val Loss: 0.5077 Acc: 0.7925                                               \n",
      "Epoch 023 | Train Loss: 0.4125 Acc: 0.8319 | Val Loss: 0.5629 Acc: 0.7737                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 32, 'cnn_dropout': 0.13131357974105626, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 32, 'learning_rate': 7.354756697750488e-05, 'lstm_dense': 64, 'lstm_hidden_size': 32, 'lstm_layers': 1, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 6.8459 Acc: 0.3680 | Val Loss: 1.0767 Acc: 0.3651                                               \n",
      "Epoch 002 | Train Loss: 1.0752 Acc: 0.4556 | Val Loss: 1.0540 Acc: 0.4925                                               \n",
      "Epoch 003 | Train Loss: 1.0538 Acc: 0.4830 | Val Loss: 1.0450 Acc: 0.5827                                               \n",
      "Epoch 004 | Train Loss: 1.0129 Acc: 0.4981 | Val Loss: 0.9774 Acc: 0.4910                                               \n",
      "Epoch 005 | Train Loss: 0.9799 Acc: 0.4877 | Val Loss: 0.9302 Acc: 0.5248                                               \n",
      "Epoch 006 | Train Loss: 0.9563 Acc: 0.4958 | Val Loss: 0.9325 Acc: 0.5600                                               \n",
      "Epoch 007 | Train Loss: 0.9587 Acc: 0.4980 | Val Loss: 0.9101 Acc: 0.4955                                               \n",
      "Epoch 008 | Train Loss: 0.9429 Acc: 0.4930 | Val Loss: 0.9340 Acc: 0.4863                                               \n",
      "Epoch 009 | Train Loss: 0.9273 Acc: 0.5049 | Val Loss: 0.8801 Acc: 0.5349                                               \n",
      "Epoch 010 | Train Loss: 0.9274 Acc: 0.5100 | Val Loss: 0.8794 Acc: 0.5018                                               \n",
      "Epoch 011 | Train Loss: 0.9169 Acc: 0.5169 | Val Loss: 0.8752 Acc: 0.5612                                               \n",
      "Epoch 012 | Train Loss: 0.9113 Acc: 0.5133 | Val Loss: 0.8621 Acc: 0.6245                                               \n",
      "Epoch 013 | Train Loss: 0.9052 Acc: 0.5181 | Val Loss: 0.8617 Acc: 0.5719                                               \n",
      "Epoch 014 | Train Loss: 0.8972 Acc: 0.5239 | Val Loss: 0.8431 Acc: 0.5496                                               \n",
      "Epoch 015 | Train Loss: 0.8953 Acc: 0.5259 | Val Loss: 0.8850 Acc: 0.6027                                               \n",
      "Epoch 016 | Train Loss: 0.8915 Acc: 0.5343 | Val Loss: 0.8347 Acc: 0.6060                                               \n",
      "Epoch 017 | Train Loss: 0.8919 Acc: 0.5319 | Val Loss: 0.8516 Acc: 0.5719                                               \n",
      "Epoch 018 | Train Loss: 0.8791 Acc: 0.5407 | Val Loss: 0.8210 Acc: 0.5800                                               \n",
      "Epoch 019 | Train Loss: 0.8836 Acc: 0.5371 | Val Loss: 0.8240 Acc: 0.5710                                               \n",
      "Epoch 020 | Train Loss: 0.8718 Acc: 0.5474 | Val Loss: 0.8377 Acc: 0.6093                                               \n",
      "Epoch 021 | Train Loss: 0.8611 Acc: 0.5384 | Val Loss: 0.7991 Acc: 0.5800                                               \n",
      "Epoch 022 | Train Loss: 0.8673 Acc: 0.5395 | Val Loss: 0.8391 Acc: 0.5904                                               \n",
      "Epoch 023 | Train Loss: 0.8675 Acc: 0.5343 | Val Loss: 0.8219 Acc: 0.5946                                               \n",
      "Epoch 024 | Train Loss: 0.8641 Acc: 0.5395 | Val Loss: 0.8245 Acc: 0.5684                                               \n",
      "Epoch 025 | Train Loss: 0.8471 Acc: 0.5506 | Val Loss: 0.8118 Acc: 0.6024                                               \n",
      "Epoch 026 | Train Loss: 0.8590 Acc: 0.5436 | Val Loss: 0.8088 Acc: 0.5376                                               \n",
      "Epoch 027 | Train Loss: 0.8550 Acc: 0.5464 | Val Loss: 0.8164 Acc: 0.5955                                               \n",
      "Epoch 028 | Train Loss: 0.8421 Acc: 0.5459 | Val Loss: 0.8068 Acc: 0.5863                                               \n",
      "Epoch 029 | Train Loss: 0.8471 Acc: 0.5535 | Val Loss: 0.7744 Acc: 0.5564                                               \n",
      "Epoch 030 | Train Loss: 0.8481 Acc: 0.5597 | Val Loss: 0.8049 Acc: 0.5499                                               \n",
      "Epoch 031 | Train Loss: 0.8339 Acc: 0.5594 | Val Loss: 0.7815 Acc: 0.5197                                               \n",
      "Epoch 032 | Train Loss: 0.8365 Acc: 0.5606 | Val Loss: 0.7871 Acc: 0.6101                                               \n",
      "Epoch 033 | Train Loss: 0.8404 Acc: 0.5613 | Val Loss: 0.7731 Acc: 0.6203                                               \n",
      "Epoch 034 | Train Loss: 0.8265 Acc: 0.5734 | Val Loss: 0.7946 Acc: 0.5299                                               \n",
      "Epoch 035 | Train Loss: 0.8338 Acc: 0.5646 | Val Loss: 0.8154 Acc: 0.6349                                               \n",
      "Epoch 036 | Train Loss: 0.8301 Acc: 0.5751 | Val Loss: 0.7937 Acc: 0.6409                                               \n",
      "Epoch 037 | Train Loss: 0.8223 Acc: 0.5756 | Val Loss: 0.8291 Acc: 0.6158                                               \n",
      "Epoch 038 | Train Loss: 0.8091 Acc: 0.5847 | Val Loss: 0.7894 Acc: 0.6555                                               \n",
      "Epoch 039 | Train Loss: 0.8118 Acc: 0.5854 | Val Loss: 0.7934 Acc: 0.6421                                               \n",
      "Epoch 040 | Train Loss: 0.8161 Acc: 0.5918 | Val Loss: 0.7836 Acc: 0.6218                                               \n",
      "Epoch 041 | Train Loss: 0.8046 Acc: 0.5994 | Val Loss: 0.7918 Acc: 0.5364                                               \n",
      "Epoch 042 | Train Loss: 0.8025 Acc: 0.5964 | Val Loss: 0.7688 Acc: 0.6358                                               \n",
      "Epoch 043 | Train Loss: 0.7917 Acc: 0.6000 | Val Loss: 0.7313 Acc: 0.6379                                               \n",
      "Epoch 044 | Train Loss: 0.8071 Acc: 0.5969 | Val Loss: 0.7191 Acc: 0.6284                                               \n",
      "Epoch 045 | Train Loss: 0.7925 Acc: 0.6065 | Val Loss: 0.7089 Acc: 0.6997                                               \n",
      "Epoch 046 | Train Loss: 0.7977 Acc: 0.6004 | Val Loss: 0.7178 Acc: 0.6466                                               \n",
      "Epoch 047 | Train Loss: 0.7840 Acc: 0.6083 | Val Loss: 0.7092 Acc: 0.6618                                               \n",
      "Epoch 048 | Train Loss: 0.7875 Acc: 0.6126 | Val Loss: 0.7076 Acc: 0.6803                                               \n",
      "Epoch 049 | Train Loss: 0.7757 Acc: 0.6112 | Val Loss: 0.7288 Acc: 0.6119                                               \n",
      "Epoch 050 | Train Loss: 0.7821 Acc: 0.6140 | Val Loss: 0.7629 Acc: 0.6669                                               \n",
      "Epoch 051 | Train Loss: 0.7811 Acc: 0.6156 | Val Loss: 0.6710 Acc: 0.7081                                               \n",
      "Epoch 052 | Train Loss: 0.7687 Acc: 0.6270 | Val Loss: 0.7092 Acc: 0.6797                                               \n",
      "Epoch 053 | Train Loss: 0.7593 Acc: 0.6339 | Val Loss: 0.6654 Acc: 0.6666                                               \n",
      "Epoch 054 | Train Loss: 0.7652 Acc: 0.6332 | Val Loss: 0.6834 Acc: 0.7084                                               \n",
      "Epoch 055 | Train Loss: 0.7556 Acc: 0.6377 | Val Loss: 0.6647 Acc: 0.7248                                               \n",
      "Epoch 056 | Train Loss: 0.7546 Acc: 0.6389 | Val Loss: 0.8005 Acc: 0.6084                                               \n",
      "Epoch 057 | Train Loss: 0.7539 Acc: 0.6314 | Val Loss: 0.6290 Acc: 0.7472                                               \n",
      "Epoch 058 | Train Loss: 0.7363 Acc: 0.6469 | Val Loss: 0.6280 Acc: 0.6958                                               \n",
      "Epoch 059 | Train Loss: 0.7409 Acc: 0.6427 | Val Loss: 0.6528 Acc: 0.7319                                               \n",
      "Epoch 060 | Train Loss: 0.7385 Acc: 0.6504 | Val Loss: 0.6563 Acc: 0.7099                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 256, 'cnn_dropout': 0.1828839881975063, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 16, 'learning_rate': 0.006165283404261356, 'lstm_dense': 128, 'lstm_hidden_size': 64, 'lstm_layers': 1, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 13.9578 Acc: 0.4385 | Val Loss: 0.9261 Acc: 0.5301                                              \n",
      "Epoch 002 | Train Loss: 1.0779 Acc: 0.5274 | Val Loss: 0.8273 Acc: 0.5952                                               \n",
      "Epoch 003 | Train Loss: 0.8377 Acc: 0.5892 | Val Loss: 0.7140 Acc: 0.6890                                               \n",
      "Epoch 004 | Train Loss: 0.7678 Acc: 0.6236 | Val Loss: 0.6608 Acc: 0.7152                                               \n",
      "Epoch 005 | Train Loss: 0.7278 Acc: 0.6445 | Val Loss: 0.6156 Acc: 0.7367                                               \n",
      "Epoch 006 | Train Loss: 0.6983 Acc: 0.6628 | Val Loss: 0.6366 Acc: 0.6806                                               \n",
      "Epoch 007 | Train Loss: 0.6651 Acc: 0.6794 | Val Loss: 0.5955 Acc: 0.7206                                               \n",
      "Epoch 008 | Train Loss: 0.6577 Acc: 0.6838 | Val Loss: 0.5701 Acc: 0.7549                                               \n",
      "Epoch 009 | Train Loss: 0.6330 Acc: 0.7070 | Val Loss: 0.5719 Acc: 0.7499                                               \n",
      "Epoch 010 | Train Loss: 0.6589 Acc: 0.6853 | Val Loss: 0.6413 Acc: 0.7090                                               \n",
      "Epoch 011 | Train Loss: 0.6643 Acc: 0.6848 | Val Loss: 0.6230 Acc: 0.7125                                               \n",
      "Epoch 012 | Train Loss: 0.6557 Acc: 0.6857 | Val Loss: 0.6151 Acc: 0.7021                                               \n",
      "Epoch 013 | Train Loss: 0.6273 Acc: 0.7118 | Val Loss: 0.5240 Acc: 0.7827                                               \n",
      "Epoch 014 | Train Loss: 0.6127 Acc: 0.7174 | Val Loss: 0.6727 Acc: 0.6857                                               \n",
      "Epoch 015 | Train Loss: 0.6236 Acc: 0.7168 | Val Loss: 0.5537 Acc: 0.7522                                               \n",
      "Epoch 016 | Train Loss: 0.6405 Acc: 0.7154 | Val Loss: 0.6324 Acc: 0.7293                                               \n",
      "Epoch 017 | Train Loss: 0.6517 Acc: 0.7024 | Val Loss: 0.7431 Acc: 0.6379                                               \n",
      "Epoch 018 | Train Loss: 0.6795 Acc: 0.6819 | Val Loss: 0.7352 Acc: 0.6278                                               \n",
      "Epoch 019 | Train Loss: 0.6583 Acc: 0.6901 | Val Loss: 0.7583 Acc: 0.6537                                               \n",
      "Epoch 020 | Train Loss: 0.6462 Acc: 0.6913 | Val Loss: 0.8076 Acc: 0.6466                                               \n",
      "Epoch 021 | Train Loss: 0.6371 Acc: 0.7006 | Val Loss: 0.7221 Acc: 0.6842                                               \n",
      "Epoch 022 | Train Loss: 0.6355 Acc: 0.7032 | Val Loss: 0.7789 Acc: 0.5964                                               \n",
      "Epoch 023 | Train Loss: 0.6516 Acc: 0.6945 | Val Loss: 0.7691 Acc: 0.6794                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 32, 'cnn_dropout': 0.6530144452518749, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 64, 'cnn_kernels_2': 32, 'learning_rate': 1.4897797508942035e-05, 'lstm_dense': 32, 'lstm_hidden_size': 32, 'lstm_layers': 2, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 143.0979 Acc: 0.3414 | Val Loss: 101.7221 Acc: 0.3752                                           \n",
      "Epoch 002 | Train Loss: 97.6772 Acc: 0.3407 | Val Loss: 61.9242 Acc: 0.4245                                             \n",
      "Epoch 003 | Train Loss: 69.0015 Acc: 0.3114 | Val Loss: 39.2036 Acc: 0.3928                                             \n",
      "Epoch 004 | Train Loss: 54.4314 Acc: 0.2976 | Val Loss: 27.5175 Acc: 0.3788                                             \n",
      "Epoch 005 | Train Loss: 44.1104 Acc: 0.3044 | Val Loss: 20.0567 Acc: 0.3976                                             \n",
      "Epoch 006 | Train Loss: 36.2721 Acc: 0.3171 | Val Loss: 12.8245 Acc: 0.3887                                             \n",
      "Epoch 007 | Train Loss: 30.5100 Acc: 0.3315 | Val Loss: 8.5497 Acc: 0.3893                                              \n",
      "Epoch 008 | Train Loss: 26.1994 Acc: 0.3521 | Val Loss: 6.6044 Acc: 0.4245                                              \n",
      "Epoch 009 | Train Loss: 22.5399 Acc: 0.3540 | Val Loss: 5.1092 Acc: 0.4496                                              \n",
      "Epoch 010 | Train Loss: 20.2333 Acc: 0.3602 | Val Loss: 4.1177 Acc: 0.3806                                              \n",
      "Epoch 011 | Train Loss: 17.6120 Acc: 0.3677 | Val Loss: 3.7752 Acc: 0.3693                                              \n",
      "Epoch 012 | Train Loss: 16.2017 Acc: 0.3787 | Val Loss: 3.5923 Acc: 0.3827                                              \n",
      "Epoch 013 | Train Loss: 14.7875 Acc: 0.3783 | Val Loss: 3.4617 Acc: 0.4436                                              \n",
      "Epoch 014 | Train Loss: 13.5504 Acc: 0.3884 | Val Loss: 3.2596 Acc: 0.4782                                              \n",
      "Epoch 015 | Train Loss: 12.6870 Acc: 0.3983 | Val Loss: 3.2843 Acc: 0.4457                                              \n",
      "Epoch 016 | Train Loss: 11.7091 Acc: 0.4001 | Val Loss: 3.0679 Acc: 0.4048                                              \n",
      "Epoch 017 | Train Loss: 10.9507 Acc: 0.4009 | Val Loss: 3.0601 Acc: 0.4316                                              \n",
      "Epoch 018 | Train Loss: 10.4791 Acc: 0.3962 | Val Loss: 3.2765 Acc: 0.4627                                              \n",
      "Epoch 019 | Train Loss: 9.7473 Acc: 0.4085 | Val Loss: 3.1120 Acc: 0.4654                                               \n",
      "Epoch 020 | Train Loss: 9.1335 Acc: 0.4135 | Val Loss: 2.8353 Acc: 0.4248                                               \n",
      "Epoch 021 | Train Loss: 8.8169 Acc: 0.4169 | Val Loss: 2.6697 Acc: 0.4609                                               \n",
      "Epoch 022 | Train Loss: 8.1891 Acc: 0.4213 | Val Loss: 2.7874 Acc: 0.4731                                               \n",
      "Epoch 023 | Train Loss: 7.7921 Acc: 0.4265 | Val Loss: 2.5824 Acc: 0.4719                                               \n",
      "Epoch 024 | Train Loss: 7.1898 Acc: 0.4342 | Val Loss: 2.6286 Acc: 0.5051                                               \n",
      "Epoch 025 | Train Loss: 6.9456 Acc: 0.4312 | Val Loss: 2.4627 Acc: 0.5036                                               \n",
      "Epoch 026 | Train Loss: 6.8827 Acc: 0.4283 | Val Loss: 2.3375 Acc: 0.4919                                               \n",
      "Epoch 027 | Train Loss: 6.3611 Acc: 0.4409 | Val Loss: 2.1876 Acc: 0.5239                                               \n",
      "Epoch 028 | Train Loss: 6.0753 Acc: 0.4397 | Val Loss: 2.2221 Acc: 0.4534                                               \n",
      "Epoch 029 | Train Loss: 5.9559 Acc: 0.4332 | Val Loss: 2.0158 Acc: 0.4755                                               \n",
      "Epoch 030 | Train Loss: 5.6973 Acc: 0.4495 | Val Loss: 2.1983 Acc: 0.5107                                               \n",
      "Epoch 031 | Train Loss: 5.2308 Acc: 0.4540 | Val Loss: 1.9541 Acc: 0.5236                                               \n",
      "Epoch 032 | Train Loss: 5.3354 Acc: 0.4413 | Val Loss: 1.9240 Acc: 0.4836                                               \n",
      "Epoch 033 | Train Loss: 4.9086 Acc: 0.4471 | Val Loss: 1.8299 Acc: 0.5072                                               \n",
      "Epoch 034 | Train Loss: 4.7951 Acc: 0.4494 | Val Loss: 1.7011 Acc: 0.5158                                               \n",
      "Epoch 035 | Train Loss: 4.6375 Acc: 0.4465 | Val Loss: 1.6561 Acc: 0.5015                                               \n",
      "Epoch 036 | Train Loss: 4.4578 Acc: 0.4538 | Val Loss: 1.7264 Acc: 0.5275                                               \n",
      "Epoch 037 | Train Loss: 4.3883 Acc: 0.4615 | Val Loss: 1.5180 Acc: 0.5400                                               \n",
      "Epoch 038 | Train Loss: 4.1547 Acc: 0.4550 | Val Loss: 1.5460 Acc: 0.5239                                               \n",
      "Epoch 039 | Train Loss: 3.9455 Acc: 0.4671 | Val Loss: 1.4608 Acc: 0.5430                                               \n",
      "Epoch 040 | Train Loss: 3.7747 Acc: 0.4654 | Val Loss: 1.4644 Acc: 0.5370                                               \n",
      "Epoch 041 | Train Loss: 3.6598 Acc: 0.4603 | Val Loss: 1.4718 Acc: 0.5546                                               \n",
      "Epoch 042 | Train Loss: 3.4989 Acc: 0.4725 | Val Loss: 1.4143 Acc: 0.5731                                               \n",
      "Epoch 043 | Train Loss: 3.5167 Acc: 0.4717 | Val Loss: 1.4794 Acc: 0.5475                                               \n",
      "Epoch 044 | Train Loss: 3.2758 Acc: 0.4713 | Val Loss: 1.3587 Acc: 0.5818                                               \n",
      "Epoch 045 | Train Loss: 3.1300 Acc: 0.4760 | Val Loss: 1.3461 Acc: 0.5630                                               \n",
      "Epoch 046 | Train Loss: 3.0221 Acc: 0.4778 | Val Loss: 1.4502 Acc: 0.6137                                               \n",
      "Epoch 047 | Train Loss: 2.9738 Acc: 0.4768 | Val Loss: 1.2060 Acc: 0.5684                                               \n",
      "Epoch 048 | Train Loss: 2.8862 Acc: 0.4759 | Val Loss: 1.1784 Acc: 0.5875                                               \n",
      "Epoch 049 | Train Loss: 2.7494 Acc: 0.4855 | Val Loss: 1.1266 Acc: 0.6301                                               \n",
      "Epoch 050 | Train Loss: 2.6140 Acc: 0.4846 | Val Loss: 1.2940 Acc: 0.5716                                               \n",
      "Epoch 051 | Train Loss: 2.5468 Acc: 0.4884 | Val Loss: 1.2179 Acc: 0.5806                                               \n",
      "Epoch 052 | Train Loss: 2.3859 Acc: 0.4934 | Val Loss: 1.2033 Acc: 0.6122                                               \n",
      "Epoch 053 | Train Loss: 2.4129 Acc: 0.4941 | Val Loss: 1.1298 Acc: 0.6090                                               \n",
      "Epoch 054 | Train Loss: 2.3209 Acc: 0.5003 | Val Loss: 1.1319 Acc: 0.5976                                               \n",
      "Epoch 055 | Train Loss: 2.2460 Acc: 0.5045 | Val Loss: 1.1669 Acc: 0.6003                                               \n",
      "Epoch 056 | Train Loss: 2.1849 Acc: 0.5018 | Val Loss: 1.0912 Acc: 0.6239                                               \n",
      "Epoch 057 | Train Loss: 2.1212 Acc: 0.5028 | Val Loss: 1.0538 Acc: 0.6125                                               \n",
      "Epoch 058 | Train Loss: 2.0183 Acc: 0.5120 | Val Loss: 1.0333 Acc: 0.6194                                               \n",
      "Epoch 059 | Train Loss: 1.9051 Acc: 0.5164 | Val Loss: 1.0721 Acc: 0.6188                                               \n",
      "Epoch 060 | Train Loss: 1.9288 Acc: 0.5149 | Val Loss: 1.0352 Acc: 0.6063                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 256, 'cnn_dropout': 0.50769083480027, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 64, 'learning_rate': 0.0004192814516156497, 'lstm_dense': 256, 'lstm_hidden_size': 96, 'lstm_layers': 6, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 11.0521 Acc: 0.4349 | Val Loss: 1.3367 Acc: 0.4421                                              \n",
      "Epoch 002 | Train Loss: 1.3199 Acc: 0.4422 | Val Loss: 1.3055 Acc: 0.4421                                               \n",
      "Epoch 003 | Train Loss: 1.2953 Acc: 0.4422 | Val Loss: 1.2861 Acc: 0.4421                                               \n",
      "Epoch 004 | Train Loss: 1.2795 Acc: 0.4422 | Val Loss: 1.2733 Acc: 0.4421                                               \n",
      "Epoch 005 | Train Loss: 1.2688 Acc: 0.4422 | Val Loss: 1.2644 Acc: 0.4421                                               \n",
      "Epoch 006 | Train Loss: 1.2614 Acc: 0.4422 | Val Loss: 1.2584 Acc: 0.4421                                               \n",
      "Epoch 007 | Train Loss: 1.2564 Acc: 0.4422 | Val Loss: 1.2540 Acc: 0.4421                                               \n",
      "Epoch 008 | Train Loss: 1.2525 Acc: 0.4422 | Val Loss: 1.2510 Acc: 0.4421                                               \n",
      "Epoch 009 | Train Loss: 1.2499 Acc: 0.4422 | Val Loss: 1.2487 Acc: 0.4421                                               \n",
      "Epoch 010 | Train Loss: 1.2479 Acc: 0.4422 | Val Loss: 1.2469 Acc: 0.4421                                               \n",
      "Epoch 011 | Train Loss: 1.2465 Acc: 0.4422 | Val Loss: 1.2457 Acc: 0.4421                                               \n",
      "Epoch 012 | Train Loss: 1.2454 Acc: 0.4422 | Val Loss: 1.2447 Acc: 0.4421                                               \n",
      "Epoch 013 | Train Loss: 1.2446 Acc: 0.4422 | Val Loss: 1.2440 Acc: 0.4421                                               \n",
      "Epoch 014 | Train Loss: 1.2439 Acc: 0.4422 | Val Loss: 1.2435 Acc: 0.4421                                               \n",
      "Epoch 015 | Train Loss: 1.2435 Acc: 0.4422 | Val Loss: 1.2431 Acc: 0.4421                                               \n",
      "Epoch 016 | Train Loss: 1.2431 Acc: 0.4422 | Val Loss: 1.2427 Acc: 0.4421                                               \n",
      "Epoch 017 | Train Loss: 1.2429 Acc: 0.4422 | Val Loss: 1.2425 Acc: 0.4421                                               \n",
      "Epoch 018 | Train Loss: 1.2427 Acc: 0.4422 | Val Loss: 1.2423 Acc: 0.4421                                               \n",
      "Epoch 019 | Train Loss: 1.2424 Acc: 0.4422 | Val Loss: 1.2422 Acc: 0.4421                                               \n",
      "Epoch 020 | Train Loss: 1.2424 Acc: 0.4422 | Val Loss: 1.2421 Acc: 0.4421                                               \n",
      "Epoch 021 | Train Loss: 1.2423 Acc: 0.4422 | Val Loss: 1.2422 Acc: 0.4421                                               \n",
      "Epoch 022 | Train Loss: 1.2424 Acc: 0.4422 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 023 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 024 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 025 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 026 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 027 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 028 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 029 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 030 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 031 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 032 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 033 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 034 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 035 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 036 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 037 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 038 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 039 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 040 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 041 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 042 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 043 | Train Loss: 1.2418 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 044 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 045 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 046 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 047 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 048 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 049 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 050 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 051 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 052 | Train Loss: 1.2418 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 053 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 054 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 055 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 056 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 36, 'cnn_dense': 64, 'cnn_dropout': 0.5643902318895031, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 96, 'learning_rate': 1.098262055331392e-05, 'lstm_dense': 128, 'lstm_hidden_size': 32, 'lstm_layers': 4, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 134.9940 Acc: 0.3117 | Val Loss: 21.4013 Acc: 0.3603                                            \n",
      "Epoch 002 | Train Loss: 98.9444 Acc: 0.3280 | Val Loss: 16.2834 Acc: 0.3215                                             \n",
      "Epoch 003 | Train Loss: 79.7602 Acc: 0.3366 | Val Loss: 14.0345 Acc: 0.3696                                             \n",
      "Epoch 004 | Train Loss: 66.5863 Acc: 0.3454 | Val Loss: 15.2201 Acc: 0.3672                                             \n",
      "Epoch 005 | Train Loss: 55.5061 Acc: 0.3544 | Val Loss: 16.1349 Acc: 0.3624                                             \n",
      "Epoch 006 | Train Loss: 47.4573 Acc: 0.3620 | Val Loss: 17.3602 Acc: 0.3490                                             \n",
      "Epoch 007 | Train Loss: 40.6292 Acc: 0.3653 | Val Loss: 16.8510 Acc: 0.3579                                             \n",
      "Epoch 008 | Train Loss: 35.8591 Acc: 0.3707 | Val Loss: 13.6440 Acc: 0.3606                                             \n",
      "Epoch 009 | Train Loss: 32.2101 Acc: 0.3711 | Val Loss: 15.1124 Acc: 0.3728                                             \n",
      "Epoch 010 | Train Loss: 27.8638 Acc: 0.3909 | Val Loss: 13.9514 Acc: 0.3716                                             \n",
      "Epoch 011 | Train Loss: 25.6527 Acc: 0.3838 | Val Loss: 11.9035 Acc: 0.3785                                             \n",
      "Epoch 012 | Train Loss: 23.7288 Acc: 0.3855 | Val Loss: 9.8810 Acc: 0.3970                                              \n",
      "Epoch 013 | Train Loss: 21.4800 Acc: 0.3960 | Val Loss: 9.5241 Acc: 0.3890                                              \n",
      "Epoch 014 | Train Loss: 19.9777 Acc: 0.3959 | Val Loss: 8.8020 Acc: 0.3824                                              \n",
      "Epoch 015 | Train Loss: 18.7958 Acc: 0.4026 | Val Loss: 6.4980 Acc: 0.3907                                              \n",
      "Epoch 016 | Train Loss: 17.6945 Acc: 0.4056 | Val Loss: 5.7916 Acc: 0.4146                                              \n",
      "Epoch 017 | Train Loss: 16.7020 Acc: 0.4002 | Val Loss: 5.3549 Acc: 0.4116                                              \n",
      "Epoch 018 | Train Loss: 15.8458 Acc: 0.4033 | Val Loss: 5.1730 Acc: 0.4090                                              \n",
      "Epoch 019 | Train Loss: 15.2195 Acc: 0.4010 | Val Loss: 4.5427 Acc: 0.4516                                              \n",
      "Epoch 020 | Train Loss: 14.4798 Acc: 0.4062 | Val Loss: 4.4147 Acc: 0.4606                                              \n",
      "Epoch 021 | Train Loss: 13.8514 Acc: 0.4063 | Val Loss: 4.2308 Acc: 0.4767                                              \n",
      "Epoch 022 | Train Loss: 13.1234 Acc: 0.4135 | Val Loss: 4.3398 Acc: 0.5039                                              \n",
      "Epoch 023 | Train Loss: 12.8456 Acc: 0.4126 | Val Loss: 4.3580 Acc: 0.4857                                              \n",
      "Epoch 024 | Train Loss: 12.5367 Acc: 0.4129 | Val Loss: 4.1030 Acc: 0.4693                                              \n",
      "Epoch 025 | Train Loss: 11.6247 Acc: 0.4188 | Val Loss: 3.7675 Acc: 0.4719                                              \n",
      "Epoch 026 | Train Loss: 11.2023 Acc: 0.4123 | Val Loss: 3.8581 Acc: 0.4964                                              \n",
      "Epoch 027 | Train Loss: 10.5202 Acc: 0.4179 | Val Loss: 3.6375 Acc: 0.5009                                              \n",
      "Epoch 028 | Train Loss: 10.4619 Acc: 0.4197 | Val Loss: 3.6656 Acc: 0.4878                                              \n",
      "Epoch 029 | Train Loss: 9.8863 Acc: 0.4197 | Val Loss: 4.0816 Acc: 0.5185                                               \n",
      "Epoch 030 | Train Loss: 9.6151 Acc: 0.4204 | Val Loss: 3.3763 Acc: 0.5024                                               \n",
      "Epoch 031 | Train Loss: 9.2649 Acc: 0.4184 | Val Loss: 3.2514 Acc: 0.5024                                               \n",
      "Epoch 032 | Train Loss: 9.0252 Acc: 0.4206 | Val Loss: 3.0583 Acc: 0.5024                                               \n",
      "Epoch 033 | Train Loss: 8.7661 Acc: 0.4201 | Val Loss: 3.1319 Acc: 0.5155                                               \n",
      "Epoch 034 | Train Loss: 8.3794 Acc: 0.4268 | Val Loss: 2.7529 Acc: 0.4955                                               \n",
      "Epoch 035 | Train Loss: 7.9503 Acc: 0.4222 | Val Loss: 2.7153 Acc: 0.5015                                               \n",
      "Epoch 036 | Train Loss: 7.8824 Acc: 0.4335 | Val Loss: 2.5802 Acc: 0.5233                                               \n",
      "Epoch 037 | Train Loss: 7.6980 Acc: 0.4281 | Val Loss: 2.8793 Acc: 0.5155                                               \n",
      "Epoch 038 | Train Loss: 7.5509 Acc: 0.4240 | Val Loss: 2.3950 Acc: 0.5051                                               \n",
      "Epoch 039 | Train Loss: 7.3033 Acc: 0.4321 | Val Loss: 2.2887 Acc: 0.4893                                               \n",
      "Epoch 040 | Train Loss: 6.9828 Acc: 0.4309 | Val Loss: 2.2623 Acc: 0.5099                                               \n",
      "Epoch 041 | Train Loss: 6.7542 Acc: 0.4371 | Val Loss: 2.2420 Acc: 0.5337                                               \n",
      "Epoch 042 | Train Loss: 6.5930 Acc: 0.4357 | Val Loss: 2.1671 Acc: 0.5627                                               \n",
      "Epoch 043 | Train Loss: 6.2338 Acc: 0.4441 | Val Loss: 2.0631 Acc: 0.5063                                               \n",
      "Epoch 044 | Train Loss: 6.0096 Acc: 0.4509 | Val Loss: 1.9764 Acc: 0.5275                                               \n",
      "Epoch 045 | Train Loss: 5.7282 Acc: 0.4523 | Val Loss: 1.9330 Acc: 0.5122                                               \n",
      "Epoch 046 | Train Loss: 5.6687 Acc: 0.4520 | Val Loss: 2.1890 Acc: 0.4991                                               \n",
      "Epoch 047 | Train Loss: 5.4938 Acc: 0.4620 | Val Loss: 1.9091 Acc: 0.5221                                               \n",
      "Epoch 048 | Train Loss: 5.2900 Acc: 0.4603 | Val Loss: 1.7643 Acc: 0.5155                                               \n",
      "Epoch 049 | Train Loss: 5.2360 Acc: 0.4665 | Val Loss: 1.7448 Acc: 0.5567                                               \n",
      "Epoch 050 | Train Loss: 5.0247 Acc: 0.4695 | Val Loss: 1.6334 Acc: 0.5857                                               \n",
      "Epoch 051 | Train Loss: 4.9803 Acc: 0.4691 | Val Loss: 1.5662 Acc: 0.5448                                               \n",
      "Epoch 052 | Train Loss: 4.6963 Acc: 0.4657 | Val Loss: 1.6262 Acc: 0.5361                                               \n",
      "Epoch 053 | Train Loss: 4.6342 Acc: 0.4827 | Val Loss: 1.4884 Acc: 0.5540                                               \n",
      "Epoch 054 | Train Loss: 4.4536 Acc: 0.4763 | Val Loss: 1.4409 Acc: 0.5845                                               \n",
      "Epoch 055 | Train Loss: 4.3506 Acc: 0.4875 | Val Loss: 1.6220 Acc: 0.5000                                               \n",
      "Epoch 056 | Train Loss: 4.2546 Acc: 0.4918 | Val Loss: 1.5311 Acc: 0.5728                                               \n",
      "Epoch 057 | Train Loss: 4.1968 Acc: 0.4856 | Val Loss: 1.4950 Acc: 0.5690                                               \n",
      "Epoch 058 | Train Loss: 3.8923 Acc: 0.4958 | Val Loss: 1.4204 Acc: 0.5970                                               \n",
      "Epoch 059 | Train Loss: 3.9170 Acc: 0.5037 | Val Loss: 1.6585 Acc: 0.5501                                               \n",
      "Epoch 060 | Train Loss: 3.8136 Acc: 0.5023 | Val Loss: 1.3793 Acc: 0.5696                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 64, 'cnn_dropout': 0.12459008069679106, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 16, 'cnn_kernels_2': 96, 'learning_rate': 3.4999370049428195e-05, 'lstm_dense': 128, 'lstm_hidden_size': 128, 'lstm_layers': 1, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 7.4768 Acc: 0.4162 | Val Loss: 1.1049 Acc: 0.4469                                               \n",
      "Epoch 002 | Train Loss: 1.2505 Acc: 0.4301 | Val Loss: 1.1086 Acc: 0.4531                                               \n",
      "Epoch 003 | Train Loss: 1.1884 Acc: 0.4365 | Val Loss: 1.0953 Acc: 0.3890                                               \n",
      "Epoch 004 | Train Loss: 1.1512 Acc: 0.4391 | Val Loss: 1.0983 Acc: 0.4391                                               \n",
      "Epoch 005 | Train Loss: 1.1239 Acc: 0.4387 | Val Loss: 1.0633 Acc: 0.4469                                               \n",
      "Epoch 006 | Train Loss: 1.1151 Acc: 0.4397 | Val Loss: 1.0549 Acc: 0.4469                                               \n",
      "Epoch 007 | Train Loss: 1.1043 Acc: 0.4386 | Val Loss: 1.0330 Acc: 0.4469                                               \n",
      "Epoch 008 | Train Loss: 1.0953 Acc: 0.4430 | Val Loss: 1.0465 Acc: 0.4469                                               \n",
      "Epoch 009 | Train Loss: 1.0800 Acc: 0.4385 | Val Loss: 1.0416 Acc: 0.4469                                               \n",
      "Epoch 010 | Train Loss: 1.0691 Acc: 0.4433 | Val Loss: 0.9984 Acc: 0.4469                                               \n",
      "Epoch 011 | Train Loss: 1.0513 Acc: 0.4403 | Val Loss: 0.9882 Acc: 0.4469                                               \n",
      "Epoch 012 | Train Loss: 1.0401 Acc: 0.4444 | Val Loss: 1.0014 Acc: 0.4021                                               \n",
      "Epoch 013 | Train Loss: 1.0340 Acc: 0.4415 | Val Loss: 0.9947 Acc: 0.4296                                               \n",
      "Epoch 014 | Train Loss: 1.0227 Acc: 0.4414 | Val Loss: 0.9611 Acc: 0.3922                                               \n",
      "Epoch 015 | Train Loss: 1.0095 Acc: 0.4414 | Val Loss: 0.9940 Acc: 0.4245                                               \n",
      "Epoch 016 | Train Loss: 1.0102 Acc: 0.4441 | Val Loss: 0.9435 Acc: 0.4469                                               \n",
      "Epoch 017 | Train Loss: 1.0011 Acc: 0.4438 | Val Loss: 0.9580 Acc: 0.3737                                               \n",
      "Epoch 018 | Train Loss: 0.9991 Acc: 0.4498 | Val Loss: 0.9727 Acc: 0.4388                                               \n",
      "Epoch 019 | Train Loss: 0.9929 Acc: 0.4481 | Val Loss: 0.9466 Acc: 0.4119                                               \n",
      "Epoch 020 | Train Loss: 0.9910 Acc: 0.4465 | Val Loss: 0.9326 Acc: 0.4531                                               \n",
      "Epoch 021 | Train Loss: 0.9820 Acc: 0.4470 | Val Loss: 0.9415 Acc: 0.4469                                               \n",
      "Epoch 022 | Train Loss: 0.9874 Acc: 0.4515 | Val Loss: 0.9322 Acc: 0.4388                                               \n",
      "Epoch 023 | Train Loss: 0.9854 Acc: 0.4545 | Val Loss: 0.9465 Acc: 0.4519                                               \n",
      "Epoch 024 | Train Loss: 0.9728 Acc: 0.4579 | Val Loss: 0.9513 Acc: 0.4615                                               \n",
      "Epoch 025 | Train Loss: 0.9701 Acc: 0.4556 | Val Loss: 0.9304 Acc: 0.4531                                               \n",
      "Epoch 026 | Train Loss: 0.9736 Acc: 0.4553 | Val Loss: 0.9232 Acc: 0.4531                                               \n",
      "Epoch 027 | Train Loss: 0.9705 Acc: 0.4485 | Val Loss: 0.9384 Acc: 0.4531                                               \n",
      "Epoch 028 | Train Loss: 0.9663 Acc: 0.4623 | Val Loss: 0.9319 Acc: 0.5173                                               \n",
      "Epoch 029 | Train Loss: 0.9652 Acc: 0.4565 | Val Loss: 0.9316 Acc: 0.4263                                               \n",
      "Epoch 030 | Train Loss: 0.9612 Acc: 0.4568 | Val Loss: 0.9155 Acc: 0.4522                                               \n",
      "Epoch 031 | Train Loss: 0.9644 Acc: 0.4561 | Val Loss: 0.9527 Acc: 0.4612                                               \n",
      "Epoch 032 | Train Loss: 0.9653 Acc: 0.4607 | Val Loss: 0.9265 Acc: 0.4531                                               \n",
      "Epoch 033 | Train Loss: 0.9621 Acc: 0.4613 | Val Loss: 0.9771 Acc: 0.4842                                               \n",
      "Epoch 034 | Train Loss: 0.9562 Acc: 0.4676 | Val Loss: 0.9066 Acc: 0.4525                                               \n",
      "Epoch 035 | Train Loss: 0.9522 Acc: 0.4622 | Val Loss: 0.9133 Acc: 0.3904                                               \n",
      "Epoch 036 | Train Loss: 0.9620 Acc: 0.4686 | Val Loss: 0.9031 Acc: 0.4672                                               \n",
      "Epoch 037 | Train Loss: 0.9503 Acc: 0.4633 | Val Loss: 1.0477 Acc: 0.4122                                               \n",
      "Epoch 038 | Train Loss: 0.9574 Acc: 0.4627 | Val Loss: 0.9045 Acc: 0.4740                                               \n",
      "Epoch 039 | Train Loss: 0.9548 Acc: 0.4687 | Val Loss: 0.9190 Acc: 0.5084                                               \n",
      "Epoch 040 | Train Loss: 0.9538 Acc: 0.4749 | Val Loss: 0.9190 Acc: 0.4699                                               \n",
      "Epoch 041 | Train Loss: 0.9479 Acc: 0.4771 | Val Loss: 0.8983 Acc: 0.4615                                               \n",
      "Epoch 042 | Train Loss: 0.9406 Acc: 0.4783 | Val Loss: 0.9503 Acc: 0.4507                                               \n",
      "Epoch 043 | Train Loss: 0.9434 Acc: 0.4695 | Val Loss: 0.9744 Acc: 0.4230                                               \n",
      "Epoch 044 | Train Loss: 0.9448 Acc: 0.4784 | Val Loss: 0.8913 Acc: 0.5003                                               \n",
      "Epoch 045 | Train Loss: 0.9372 Acc: 0.4760 | Val Loss: 0.8930 Acc: 0.4919                                               \n",
      "Epoch 046 | Train Loss: 0.9337 Acc: 0.4748 | Val Loss: 0.8923 Acc: 0.5090                                               \n",
      "Epoch 047 | Train Loss: 0.9328 Acc: 0.4803 | Val Loss: 0.9062 Acc: 0.4558                                               \n",
      "Epoch 048 | Train Loss: 0.9397 Acc: 0.4800 | Val Loss: 0.8985 Acc: 0.4588                                               \n",
      "Epoch 049 | Train Loss: 0.9282 Acc: 0.4842 | Val Loss: 0.8815 Acc: 0.5200                                               \n",
      "Epoch 050 | Train Loss: 0.9315 Acc: 0.4898 | Val Loss: 0.8931 Acc: 0.5122                                               \n",
      "Epoch 051 | Train Loss: 0.9303 Acc: 0.4873 | Val Loss: 0.8932 Acc: 0.4316                                               \n",
      "Epoch 052 | Train Loss: 0.9221 Acc: 0.4961 | Val Loss: 0.8684 Acc: 0.4973                                               \n",
      "Epoch 053 | Train Loss: 0.9172 Acc: 0.4955 | Val Loss: 0.8662 Acc: 0.5361                                               \n",
      "Epoch 054 | Train Loss: 0.9206 Acc: 0.5020 | Val Loss: 0.8625 Acc: 0.5179                                               \n",
      "Epoch 055 | Train Loss: 0.9093 Acc: 0.5066 | Val Loss: 0.8418 Acc: 0.5009                                               \n",
      "Epoch 056 | Train Loss: 0.9132 Acc: 0.5130 | Val Loss: 0.8768 Acc: 0.5325                                               \n",
      "Epoch 057 | Train Loss: 0.9092 Acc: 0.5030 | Val Loss: 0.8467 Acc: 0.5493                                               \n",
      "Epoch 058 | Train Loss: 0.8998 Acc: 0.5069 | Val Loss: 0.8503 Acc: 0.5427                                               \n",
      "Epoch 059 | Train Loss: 0.9091 Acc: 0.5129 | Val Loss: 0.8403 Acc: 0.4761                                               \n",
      "Epoch 060 | Train Loss: 0.9001 Acc: 0.5150 | Val Loss: 0.8235 Acc: 0.5391                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 36, 'cnn_dense': 64, 'cnn_dropout': 0.44717060057624014, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 64, 'cnn_kernels_2': 64, 'learning_rate': 0.00026147346411639323, 'lstm_dense': 128, 'lstm_hidden_size': 64, 'lstm_layers': 5, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 19.9380 Acc: 0.3960 | Val Loss: 3.6986 Acc: 0.4597                                              \n",
      "Epoch 002 | Train Loss: 5.5860 Acc: 0.4379 | Val Loss: 1.6529 Acc: 0.5412                                               \n",
      "Epoch 003 | Train Loss: 3.3801 Acc: 0.4757 | Val Loss: 1.3582 Acc: 0.5919                                               \n",
      "Epoch 004 | Train Loss: 2.2598 Acc: 0.5294 | Val Loss: 0.9062 Acc: 0.5955                                               \n",
      "Epoch 005 | Train Loss: 1.6334 Acc: 0.5669 | Val Loss: 1.0563 Acc: 0.6552                                               \n",
      "Epoch 006 | Train Loss: 1.3589 Acc: 0.5991 | Val Loss: 0.6199 Acc: 0.7875                                               \n",
      "Epoch 007 | Train Loss: 1.1715 Acc: 0.6451 | Val Loss: 0.9381 Acc: 0.6272                                               \n",
      "Epoch 008 | Train Loss: 0.9234 Acc: 0.7048 | Val Loss: 0.9352 Acc: 0.7293                                               \n",
      "Epoch 009 | Train Loss: 0.8129 Acc: 0.7381 | Val Loss: 0.6345 Acc: 0.7949                                               \n",
      "Epoch 010 | Train Loss: 0.6462 Acc: 0.7813 | Val Loss: 0.5782 Acc: 0.7331                                               \n",
      "Epoch 011 | Train Loss: 0.5782 Acc: 0.8123 | Val Loss: 0.3589 Acc: 0.8669                                               \n",
      "Epoch 012 | Train Loss: 0.5388 Acc: 0.8319 | Val Loss: 0.4196 Acc: 0.8576                                               \n",
      "Epoch 013 | Train Loss: 0.4205 Acc: 0.8687 | Val Loss: 0.2371 Acc: 0.9293                                               \n",
      "Epoch 014 | Train Loss: 0.3672 Acc: 0.8845 | Val Loss: 0.1960 Acc: 0.9272                                               \n",
      "Epoch 015 | Train Loss: 0.3273 Acc: 0.8961 | Val Loss: 0.1794 Acc: 0.9406                                               \n",
      "Epoch 016 | Train Loss: 0.2976 Acc: 0.9069 | Val Loss: 0.1609 Acc: 0.9507                                               \n",
      "Epoch 017 | Train Loss: 0.2787 Acc: 0.9155 | Val Loss: 0.2417 Acc: 0.9388                                               \n",
      "Epoch 018 | Train Loss: 0.2477 Acc: 0.9254 | Val Loss: 0.1603 Acc: 0.9472                                               \n",
      "Epoch 019 | Train Loss: 0.2045 Acc: 0.9312 | Val Loss: 0.2089 Acc: 0.9269                                               \n",
      "Epoch 020 | Train Loss: 0.1835 Acc: 0.9392 | Val Loss: 0.1430 Acc: 0.9531                                               \n",
      "Epoch 021 | Train Loss: 0.1745 Acc: 0.9449 | Val Loss: 0.1433 Acc: 0.9561                                               \n",
      "Epoch 022 | Train Loss: 0.1805 Acc: 0.9439 | Val Loss: 0.1454 Acc: 0.9573                                               \n",
      "Epoch 023 | Train Loss: 0.1392 Acc: 0.9567 | Val Loss: 0.1399 Acc: 0.9567                                               \n",
      "Epoch 024 | Train Loss: 0.1441 Acc: 0.9531 | Val Loss: 0.1984 Acc: 0.9370                                               \n",
      "Epoch 025 | Train Loss: 0.1146 Acc: 0.9630 | Val Loss: 0.1099 Acc: 0.9675                                               \n",
      "Epoch 026 | Train Loss: 0.1379 Acc: 0.9581 | Val Loss: 0.2267 Acc: 0.9361                                               \n",
      "Epoch 027 | Train Loss: 0.1250 Acc: 0.9605 | Val Loss: 0.1614 Acc: 0.9567                                               \n",
      "Epoch 028 | Train Loss: 0.1039 Acc: 0.9655 | Val Loss: 0.1254 Acc: 0.9594                                               \n",
      "Epoch 029 | Train Loss: 0.1074 Acc: 0.9669 | Val Loss: 0.0990 Acc: 0.9699                                               \n",
      "Epoch 030 | Train Loss: 0.0948 Acc: 0.9711 | Val Loss: 0.1273 Acc: 0.9567                                               \n",
      "Epoch 031 | Train Loss: 0.0841 Acc: 0.9728 | Val Loss: 0.0967 Acc: 0.9737                                               \n",
      "Epoch 032 | Train Loss: 0.0986 Acc: 0.9702 | Val Loss: 0.0870 Acc: 0.9687                                               \n",
      "Epoch 033 | Train Loss: 0.1054 Acc: 0.9682 | Val Loss: 0.1413 Acc: 0.9504                                               \n",
      "Epoch 034 | Train Loss: 0.0793 Acc: 0.9757 | Val Loss: 0.1311 Acc: 0.9546                                               \n",
      "Epoch 035 | Train Loss: 0.0746 Acc: 0.9762 | Val Loss: 0.1261 Acc: 0.9591                                               \n",
      "Epoch 036 | Train Loss: 0.0772 Acc: 0.9782 | Val Loss: 0.0973 Acc: 0.9648                                               \n",
      "Epoch 037 | Train Loss: 0.0698 Acc: 0.9789 | Val Loss: 0.1518 Acc: 0.9573                                               \n",
      "Epoch 038 | Train Loss: 0.0745 Acc: 0.9764 | Val Loss: 0.1240 Acc: 0.9666                                               \n",
      "Epoch 039 | Train Loss: 0.0681 Acc: 0.9777 | Val Loss: 0.0824 Acc: 0.9669                                               \n",
      "Epoch 040 | Train Loss: 0.0756 Acc: 0.9777 | Val Loss: 0.1813 Acc: 0.9534                                               \n",
      "Epoch 041 | Train Loss: 0.0675 Acc: 0.9806 | Val Loss: 0.0983 Acc: 0.9764                                               \n",
      "Epoch 042 | Train Loss: 0.0580 Acc: 0.9828 | Val Loss: 0.0750 Acc: 0.9797                                               \n",
      "Epoch 043 | Train Loss: 0.0515 Acc: 0.9838 | Val Loss: 0.0790 Acc: 0.9743                                               \n",
      "Epoch 044 | Train Loss: 0.0476 Acc: 0.9849 | Val Loss: 0.1043 Acc: 0.9669                                               \n",
      "Epoch 045 | Train Loss: 0.0572 Acc: 0.9818 | Val Loss: 0.0678 Acc: 0.9797                                               \n",
      "Epoch 046 | Train Loss: 0.0491 Acc: 0.9856 | Val Loss: 0.1587 Acc: 0.9612                                               \n",
      "Epoch 047 | Train Loss: 0.0526 Acc: 0.9837 | Val Loss: 0.0821 Acc: 0.9797                                               \n",
      "Epoch 048 | Train Loss: 0.0441 Acc: 0.9869 | Val Loss: 0.0882 Acc: 0.9764                                               \n",
      "Epoch 049 | Train Loss: 0.0604 Acc: 0.9827 | Val Loss: 0.0927 Acc: 0.9755                                               \n",
      "Epoch 050 | Train Loss: 0.0492 Acc: 0.9852 | Val Loss: 0.0858 Acc: 0.9785                                               \n",
      "Epoch 051 | Train Loss: 0.0539 Acc: 0.9839 | Val Loss: 0.0722 Acc: 0.9818                                               \n",
      "Epoch 052 | Train Loss: 0.0417 Acc: 0.9856 | Val Loss: 0.0619 Acc: 0.9836                                               \n",
      "Epoch 053 | Train Loss: 0.0362 Acc: 0.9898 | Val Loss: 0.1106 Acc: 0.9696                                               \n",
      "Epoch 054 | Train Loss: 0.0353 Acc: 0.9887 | Val Loss: 0.0884 Acc: 0.9755                                               \n",
      "Epoch 055 | Train Loss: 0.0394 Acc: 0.9897 | Val Loss: 0.0808 Acc: 0.9764                                               \n",
      "Epoch 056 | Train Loss: 0.0414 Acc: 0.9880 | Val Loss: 0.0774 Acc: 0.9749                                               \n",
      "Epoch 057 | Train Loss: 0.0369 Acc: 0.9891 | Val Loss: 0.0695 Acc: 0.9809                                               \n",
      "Epoch 058 | Train Loss: 0.0296 Acc: 0.9917 | Val Loss: 0.0497 Acc: 0.9872                                               \n",
      "Epoch 059 | Train Loss: 0.0407 Acc: 0.9874 | Val Loss: 0.0850 Acc: 0.9770                                               \n",
      "Epoch 060 | Train Loss: 0.0357 Acc: 0.9895 | Val Loss: 0.0684 Acc: 0.9833                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 128, 'cnn_dropout': 0.6690924366139088, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 96, 'learning_rate': 0.0001735299564652289, 'lstm_dense': 128, 'lstm_hidden_size': 128, 'lstm_layers': 5, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 7.5313 Acc: 0.4088 | Val Loss: 2.0801 Acc: 0.5284                                               \n",
      "Epoch 002 | Train Loss: 2.7518 Acc: 0.4789 | Val Loss: 1.7674 Acc: 0.5119                                               \n",
      "Epoch 003 | Train Loss: 1.8895 Acc: 0.5346 | Val Loss: 1.2420 Acc: 0.5543                                               \n",
      "Epoch 004 | Train Loss: 1.3192 Acc: 0.6009 | Val Loss: 0.8291 Acc: 0.7081                                               \n",
      "Epoch 005 | Train Loss: 1.1186 Acc: 0.6595 | Val Loss: 0.6833 Acc: 0.7534                                               \n",
      "Epoch 006 | Train Loss: 0.9349 Acc: 0.7072 | Val Loss: 0.6369 Acc: 0.7800                                               \n",
      "Epoch 007 | Train Loss: 0.7672 Acc: 0.7435 | Val Loss: 0.4184 Acc: 0.8516                                               \n",
      "Epoch 008 | Train Loss: 0.6849 Acc: 0.7701 | Val Loss: 0.4402 Acc: 0.8570                                               \n",
      "Epoch 009 | Train Loss: 0.6090 Acc: 0.7981 | Val Loss: 0.3055 Acc: 0.8857                                               \n",
      "Epoch 010 | Train Loss: 0.5220 Acc: 0.8236 | Val Loss: 0.3494 Acc: 0.8740                                               \n",
      "Epoch 011 | Train Loss: 0.4913 Acc: 0.8369 | Val Loss: 0.3818 Acc: 0.8719                                               \n",
      "Epoch 012 | Train Loss: 0.4316 Acc: 0.8541 | Val Loss: 0.3817 Acc: 0.8531                                               \n",
      "Epoch 013 | Train Loss: 0.3559 Acc: 0.8787 | Val Loss: 0.2990 Acc: 0.8979                                               \n",
      "Epoch 014 | Train Loss: 0.3164 Acc: 0.8974 | Val Loss: 0.3875 Acc: 0.8681                                               \n",
      "Epoch 015 | Train Loss: 0.2854 Acc: 0.9066 | Val Loss: 0.2278 Acc: 0.9125                                               \n",
      "Epoch 016 | Train Loss: 0.2441 Acc: 0.9210 | Val Loss: 0.2147 Acc: 0.9334                                               \n",
      "Epoch 017 | Train Loss: 0.2127 Acc: 0.9298 | Val Loss: 0.2374 Acc: 0.9218                                               \n",
      "Epoch 018 | Train Loss: 0.2062 Acc: 0.9353 | Val Loss: 0.1684 Acc: 0.9454                                               \n",
      "Epoch 019 | Train Loss: 0.1883 Acc: 0.9401 | Val Loss: 0.1724 Acc: 0.9412                                               \n",
      "Epoch 020 | Train Loss: 0.1581 Acc: 0.9492 | Val Loss: 0.1427 Acc: 0.9478                                               \n",
      "Epoch 021 | Train Loss: 0.1520 Acc: 0.9548 | Val Loss: 0.1587 Acc: 0.9415                                               \n",
      "Epoch 022 | Train Loss: 0.1369 Acc: 0.9581 | Val Loss: 0.2050 Acc: 0.9340                                               \n",
      "Epoch 023 | Train Loss: 0.1263 Acc: 0.9621 | Val Loss: 0.1723 Acc: 0.9424                                               \n",
      "Epoch 024 | Train Loss: 0.1097 Acc: 0.9654 | Val Loss: 0.1150 Acc: 0.9606                                               \n",
      "Epoch 025 | Train Loss: 0.1048 Acc: 0.9669 | Val Loss: 0.1110 Acc: 0.9663                                               \n",
      "Epoch 026 | Train Loss: 0.0927 Acc: 0.9713 | Val Loss: 0.0843 Acc: 0.9761                                               \n",
      "Epoch 027 | Train Loss: 0.0969 Acc: 0.9717 | Val Loss: 0.1646 Acc: 0.9457                                               \n",
      "Epoch 028 | Train Loss: 0.0857 Acc: 0.9737 | Val Loss: 0.1255 Acc: 0.9603                                               \n",
      "Epoch 029 | Train Loss: 0.0738 Acc: 0.9777 | Val Loss: 0.1076 Acc: 0.9630                                               \n",
      "Epoch 030 | Train Loss: 0.0773 Acc: 0.9756 | Val Loss: 0.1142 Acc: 0.9633                                               \n",
      "Epoch 031 | Train Loss: 0.0711 Acc: 0.9796 | Val Loss: 0.1166 Acc: 0.9591                                               \n",
      "Epoch 032 | Train Loss: 0.0653 Acc: 0.9811 | Val Loss: 0.0603 Acc: 0.9827                                               \n",
      "Epoch 033 | Train Loss: 0.0623 Acc: 0.9804 | Val Loss: 0.0944 Acc: 0.9773                                               \n",
      "Epoch 034 | Train Loss: 0.0629 Acc: 0.9817 | Val Loss: 0.1170 Acc: 0.9657                                               \n",
      "Epoch 035 | Train Loss: 0.0521 Acc: 0.9837 | Val Loss: 0.1104 Acc: 0.9696                                               \n",
      "Epoch 036 | Train Loss: 0.0529 Acc: 0.9836 | Val Loss: 0.0894 Acc: 0.9755                                               \n",
      "Epoch 037 | Train Loss: 0.0543 Acc: 0.9838 | Val Loss: 0.0575 Acc: 0.9830                                               \n",
      "Epoch 038 | Train Loss: 0.0523 Acc: 0.9848 | Val Loss: 0.0700 Acc: 0.9809                                               \n",
      "Epoch 039 | Train Loss: 0.0442 Acc: 0.9867 | Val Loss: 0.1207 Acc: 0.9663                                               \n",
      "Epoch 040 | Train Loss: 0.0463 Acc: 0.9860 | Val Loss: 0.0609 Acc: 0.9794                                               \n",
      "Epoch 041 | Train Loss: 0.0460 Acc: 0.9851 | Val Loss: 0.0971 Acc: 0.9722                                               \n",
      "Epoch 042 | Train Loss: 0.0387 Acc: 0.9887 | Val Loss: 0.0812 Acc: 0.9791                                               \n",
      "Epoch 043 | Train Loss: 0.0405 Acc: 0.9881 | Val Loss: 0.0754 Acc: 0.9797                                               \n",
      "Epoch 044 | Train Loss: 0.0376 Acc: 0.9881 | Val Loss: 0.0802 Acc: 0.9770                                               \n",
      "Epoch 045 | Train Loss: 0.0383 Acc: 0.9879 | Val Loss: 0.0734 Acc: 0.9770                                               \n",
      "Epoch 046 | Train Loss: 0.0368 Acc: 0.9891 | Val Loss: 0.0604 Acc: 0.9794                                               \n",
      "Epoch 047 | Train Loss: 0.0326 Acc: 0.9898 | Val Loss: 0.0709 Acc: 0.9815                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 36, 'cnn_dense': 256, 'cnn_dropout': 0.5635841179157474, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 32, 'cnn_kernels_2': 16, 'learning_rate': 0.0009216690425795263, 'lstm_dense': 128, 'lstm_hidden_size': 96, 'lstm_layers': 6, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 5.2871 Acc: 0.4400 | Val Loss: 1.2470 Acc: 0.4421                                               \n",
      "Epoch 002 | Train Loss: 1.2443 Acc: 0.4422 | Val Loss: 1.2426 Acc: 0.4421                                               \n",
      "Epoch 003 | Train Loss: 1.2424 Acc: 0.4422 | Val Loss: 1.2427 Acc: 0.4421                                               \n",
      "Epoch 004 | Train Loss: 1.2423 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 005 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2423 Acc: 0.4421                                               \n",
      "Epoch 006 | Train Loss: 1.2422 Acc: 0.4422 | Val Loss: 1.2420 Acc: 0.4421                                               \n",
      "Epoch 007 | Train Loss: 1.2423 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 008 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 009 | Train Loss: 1.2422 Acc: 0.4422 | Val Loss: 1.2420 Acc: 0.4421                                               \n",
      "Epoch 010 | Train Loss: 1.2422 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 011 | Train Loss: 1.2422 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 012 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2420 Acc: 0.4421                                               \n",
      "Epoch 013 | Train Loss: 1.2422 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 014 | Train Loss: 1.2422 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 015 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2427 Acc: 0.4421                                               \n",
      "Epoch 016 | Train Loss: 1.2423 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 017 | Train Loss: 1.2423 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 018 | Train Loss: 1.2424 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 019 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2420 Acc: 0.4421                                               \n",
      "Epoch 020 | Train Loss: 1.2424 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 021 | Train Loss: 1.2425 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 022 | Train Loss: 1.2422 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 023 | Train Loss: 1.2423 Acc: 0.4422 | Val Loss: 1.2420 Acc: 0.4421                                               \n",
      "Epoch 024 | Train Loss: 1.2422 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 025 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 026 | Train Loss: 1.2422 Acc: 0.4422 | Val Loss: 1.2422 Acc: 0.4421                                               \n",
      "Epoch 027 | Train Loss: 1.2422 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 028 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 029 | Train Loss: 1.2422 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 030 | Train Loss: 1.2422 Acc: 0.4422 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 031 | Train Loss: 1.2423 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 032 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 033 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 034 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2422 Acc: 0.4421                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 32, 'cnn_dropout': 0.5291870018840162, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 32, 'learning_rate': 0.004222081705148959, 'lstm_dense': 32, 'lstm_hidden_size': 64, 'lstm_layers': 2, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 6152.4608 Acc: 0.4281 | Val Loss: 1.2567 Acc: 0.4421                                            \n",
      "Epoch 002 | Train Loss: 1.2484 Acc: 0.4313 | Val Loss: 1.2386 Acc: 0.4421                                               \n",
      "Epoch 003 | Train Loss: 1.2395 Acc: 0.4422 | Val Loss: 1.2361 Acc: 0.4421                                               \n",
      "Epoch 004 | Train Loss: 1.3353 Acc: 0.4323 | Val Loss: 1.2592 Acc: 0.4421                                               \n",
      "Epoch 005 | Train Loss: 1.2469 Acc: 0.4422 | Val Loss: 1.2421 Acc: 0.4421                                               \n",
      "Epoch 006 | Train Loss: 1.2443 Acc: 0.4422 | Val Loss: 1.2440 Acc: 0.4421                                               \n",
      "Epoch 007 | Train Loss: 1.2436 Acc: 0.4422 | Val Loss: 1.2443 Acc: 0.4421                                               \n",
      "Epoch 008 | Train Loss: 1.2454 Acc: 0.4422 | Val Loss: 1.2426 Acc: 0.4421                                               \n",
      "Epoch 009 | Train Loss: 1.2463 Acc: 0.4422 | Val Loss: 1.2431 Acc: 0.4421                                               \n",
      "Epoch 010 | Train Loss: 1.2445 Acc: 0.4422 | Val Loss: 1.2421 Acc: 0.4421                                               \n",
      "Epoch 011 | Train Loss: 1.2447 Acc: 0.4422 | Val Loss: 1.2436 Acc: 0.4421                                               \n",
      "Epoch 012 | Train Loss: 1.2467 Acc: 0.4422 | Val Loss: 1.2514 Acc: 0.4421                                               \n",
      "Epoch 013 | Train Loss: 1.2439 Acc: 0.4422 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 64, 'cnn_dropout': 0.45762349207655006, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 64, 'learning_rate': 0.0003070780719393061, 'lstm_dense': 64, 'lstm_hidden_size': 128, 'lstm_layers': 5, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 8.7856 Acc: 0.4027 | Val Loss: 3.3018 Acc: 0.5215                                               \n",
      "Epoch 002 | Train Loss: 2.9091 Acc: 0.4778 | Val Loss: 1.6710 Acc: 0.5657                                               \n",
      "Epoch 003 | Train Loss: 1.6515 Acc: 0.5657 | Val Loss: 0.9129 Acc: 0.6490                                               \n",
      "Epoch 004 | Train Loss: 1.1250 Acc: 0.6173 | Val Loss: 0.8280 Acc: 0.6442                                               \n",
      "Epoch 005 | Train Loss: 0.8277 Acc: 0.6821 | Val Loss: 0.5896 Acc: 0.7501                                               \n",
      "Epoch 006 | Train Loss: 0.6536 Acc: 0.7396 | Val Loss: 0.4561 Acc: 0.8107                                               \n",
      "Epoch 007 | Train Loss: 0.5401 Acc: 0.7874 | Val Loss: 0.5784 Acc: 0.7104                                               \n",
      "Epoch 008 | Train Loss: 0.4605 Acc: 0.8269 | Val Loss: 0.3545 Acc: 0.8704                                               \n",
      "Epoch 009 | Train Loss: 0.3832 Acc: 0.8601 | Val Loss: 0.2310 Acc: 0.9182                                               \n",
      "Epoch 010 | Train Loss: 0.3204 Acc: 0.8836 | Val Loss: 0.2304 Acc: 0.9030                                               \n",
      "Epoch 011 | Train Loss: 0.2628 Acc: 0.9056 | Val Loss: 0.2320 Acc: 0.9006                                               \n",
      "Epoch 012 | Train Loss: 0.2351 Acc: 0.9165 | Val Loss: 0.1692 Acc: 0.9334                                               \n",
      "Epoch 013 | Train Loss: 0.2012 Acc: 0.9289 | Val Loss: 0.1630 Acc: 0.9263                                               \n",
      "Epoch 014 | Train Loss: 0.1768 Acc: 0.9374 | Val Loss: 0.1599 Acc: 0.9349                                               \n",
      "Epoch 015 | Train Loss: 0.1719 Acc: 0.9425 | Val Loss: 0.1120 Acc: 0.9534                                               \n",
      "Epoch 016 | Train Loss: 0.1444 Acc: 0.9489 | Val Loss: 0.1426 Acc: 0.9355                                               \n",
      "Epoch 017 | Train Loss: 0.1321 Acc: 0.9559 | Val Loss: 0.0979 Acc: 0.9600                                               \n",
      "Epoch 018 | Train Loss: 0.1191 Acc: 0.9583 | Val Loss: 0.1645 Acc: 0.9406                                               \n",
      "Epoch 019 | Train Loss: 0.1022 Acc: 0.9637 | Val Loss: 0.0992 Acc: 0.9579                                               \n",
      "Epoch 020 | Train Loss: 0.0999 Acc: 0.9665 | Val Loss: 0.1674 Acc: 0.9373                                               \n",
      "Epoch 021 | Train Loss: 0.0982 Acc: 0.9670 | Val Loss: 0.0630 Acc: 0.9776                                               \n",
      "Epoch 022 | Train Loss: 0.0860 Acc: 0.9703 | Val Loss: 0.0788 Acc: 0.9734                                               \n",
      "Epoch 023 | Train Loss: 0.0891 Acc: 0.9721 | Val Loss: 0.0814 Acc: 0.9663                                               \n",
      "Epoch 024 | Train Loss: 0.0833 Acc: 0.9729 | Val Loss: 0.0771 Acc: 0.9755                                               \n",
      "Epoch 025 | Train Loss: 0.0704 Acc: 0.9778 | Val Loss: 0.1014 Acc: 0.9654                                               \n",
      "Epoch 026 | Train Loss: 0.0675 Acc: 0.9787 | Val Loss: 0.0649 Acc: 0.9821                                               \n",
      "Epoch 027 | Train Loss: 0.0701 Acc: 0.9787 | Val Loss: 0.0476 Acc: 0.9854                                               \n",
      "Epoch 028 | Train Loss: 0.0665 Acc: 0.9805 | Val Loss: 0.0639 Acc: 0.9812                                               \n",
      "Epoch 029 | Train Loss: 0.0643 Acc: 0.9809 | Val Loss: 0.0536 Acc: 0.9827                                               \n",
      "Epoch 030 | Train Loss: 0.0593 Acc: 0.9802 | Val Loss: 0.0472 Acc: 0.9830                                               \n",
      "Epoch 031 | Train Loss: 0.0595 Acc: 0.9804 | Val Loss: 0.0449 Acc: 0.9851                                               \n",
      "Epoch 032 | Train Loss: 0.0559 Acc: 0.9800 | Val Loss: 0.0516 Acc: 0.9827                                               \n",
      "Epoch 033 | Train Loss: 0.0489 Acc: 0.9846 | Val Loss: 0.0847 Acc: 0.9696                                               \n",
      "Epoch 034 | Train Loss: 0.0545 Acc: 0.9828 | Val Loss: 0.0841 Acc: 0.9743                                               \n",
      "Epoch 035 | Train Loss: 0.0511 Acc: 0.9843 | Val Loss: 0.0714 Acc: 0.9815                                               \n",
      "Epoch 036 | Train Loss: 0.0470 Acc: 0.9854 | Val Loss: 0.0472 Acc: 0.9857                                               \n",
      "Epoch 037 | Train Loss: 0.0498 Acc: 0.9843 | Val Loss: 0.0464 Acc: 0.9854                                               \n",
      "Epoch 038 | Train Loss: 0.0456 Acc: 0.9845 | Val Loss: 0.0383 Acc: 0.9863                                               \n",
      "Epoch 039 | Train Loss: 0.0428 Acc: 0.9864 | Val Loss: 0.0532 Acc: 0.9830                                               \n",
      "Epoch 040 | Train Loss: 0.0453 Acc: 0.9858 | Val Loss: 0.0429 Acc: 0.9860                                               \n",
      "Epoch 041 | Train Loss: 0.0379 Acc: 0.9875 | Val Loss: 0.0525 Acc: 0.9854                                               \n",
      "Epoch 042 | Train Loss: 0.0442 Acc: 0.9865 | Val Loss: 0.0466 Acc: 0.9857                                               \n",
      "Epoch 043 | Train Loss: 0.0417 Acc: 0.9870 | Val Loss: 0.0375 Acc: 0.9875                                               \n",
      "Epoch 044 | Train Loss: 0.0405 Acc: 0.9877 | Val Loss: 0.0311 Acc: 0.9896                                               \n",
      "Epoch 045 | Train Loss: 0.0390 Acc: 0.9879 | Val Loss: 0.0338 Acc: 0.9884                                               \n",
      "Epoch 046 | Train Loss: 0.0373 Acc: 0.9882 | Val Loss: 0.0362 Acc: 0.9878                                               \n",
      "Epoch 047 | Train Loss: 0.0341 Acc: 0.9887 | Val Loss: 0.0464 Acc: 0.9872                                               \n",
      "Epoch 048 | Train Loss: 0.0324 Acc: 0.9906 | Val Loss: 0.0609 Acc: 0.9851                                               \n",
      "Epoch 049 | Train Loss: 0.0404 Acc: 0.9868 | Val Loss: 0.0486 Acc: 0.9863                                               \n",
      "Epoch 050 | Train Loss: 0.0304 Acc: 0.9905 | Val Loss: 0.0478 Acc: 0.9839                                               \n",
      "Epoch 051 | Train Loss: 0.0305 Acc: 0.9904 | Val Loss: 0.0365 Acc: 0.9881                                               \n",
      "Epoch 052 | Train Loss: 0.0312 Acc: 0.9905 | Val Loss: 0.0451 Acc: 0.9845                                               \n",
      "Epoch 053 | Train Loss: 0.0351 Acc: 0.9896 | Val Loss: 0.0511 Acc: 0.9839                                               \n",
      "Epoch 054 | Train Loss: 0.0278 Acc: 0.9914 | Val Loss: 0.1095 Acc: 0.9719                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 128, 'cnn_dropout': 0.4220239099295649, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 64, 'learning_rate': 0.002563810917531924, 'lstm_dense': 64, 'lstm_hidden_size': 128, 'lstm_layers': 5, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 4.7252 Acc: 0.5097 | Val Loss: 0.9205 Acc: 0.5699                                               \n",
      "Epoch 002 | Train Loss: 0.8096 Acc: 0.6246 | Val Loss: 0.7398 Acc: 0.6731                                               \n",
      "Epoch 003 | Train Loss: 0.8108 Acc: 0.6041 | Val Loss: 0.7576 Acc: 0.6242                                               \n",
      "Epoch 004 | Train Loss: 0.7933 Acc: 0.6139 | Val Loss: 0.7569 Acc: 0.5767                                               \n",
      "Epoch 005 | Train Loss: 0.7727 Acc: 0.6253 | Val Loss: 0.6805 Acc: 0.7096                                               \n",
      "Epoch 006 | Train Loss: 0.7747 Acc: 0.6264 | Val Loss: 0.7445 Acc: 0.6164                                               \n",
      "Epoch 007 | Train Loss: 0.7909 Acc: 0.6257 | Val Loss: 0.7347 Acc: 0.6621                                               \n",
      "Epoch 008 | Train Loss: 0.8131 Acc: 0.6078 | Val Loss: 0.7616 Acc: 0.6179                                               \n",
      "Epoch 009 | Train Loss: 0.8276 Acc: 0.6065 | Val Loss: 0.8618 Acc: 0.5824                                               \n",
      "Epoch 010 | Train Loss: 0.8296 Acc: 0.5936 | Val Loss: 0.7939 Acc: 0.5818                                               \n",
      "Epoch 011 | Train Loss: 0.8269 Acc: 0.6002 | Val Loss: 0.8461 Acc: 0.5722                                               \n",
      "Epoch 012 | Train Loss: 0.8332 Acc: 0.6020 | Val Loss: 0.7091 Acc: 0.6573                                               \n",
      "Epoch 013 | Train Loss: 0.8144 Acc: 0.6109 | Val Loss: 0.7741 Acc: 0.6370                                               \n",
      "Epoch 014 | Train Loss: 0.8124 Acc: 0.6022 | Val Loss: 0.7961 Acc: 0.5881                                               \n",
      "Epoch 015 | Train Loss: 0.8122 Acc: 0.6071 | Val Loss: 0.7476 Acc: 0.6322                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 64, 'cnn_dropout': 0.4025444662336499, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 64, 'learning_rate': 0.0003865880854854608, 'lstm_dense': 64, 'lstm_hidden_size': 128, 'lstm_layers': 5, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 8.6566 Acc: 0.4286 | Val Loss: 4.1656 Acc: 0.4558                                               \n",
      "Epoch 002 | Train Loss: 2.9427 Acc: 0.4843 | Val Loss: 1.3467 Acc: 0.5567                                               \n",
      "Epoch 003 | Train Loss: 1.7628 Acc: 0.5215 | Val Loss: 0.8808 Acc: 0.6063                                               \n",
      "Epoch 004 | Train Loss: 1.2221 Acc: 0.5536 | Val Loss: 0.8151 Acc: 0.5478                                               \n",
      "Epoch 005 | Train Loss: 0.8910 Acc: 0.6073 | Val Loss: 0.6281 Acc: 0.7355                                               \n",
      "Epoch 006 | Train Loss: 0.7483 Acc: 0.6539 | Val Loss: 0.6526 Acc: 0.7090                                               \n",
      "Epoch 007 | Train Loss: 0.6656 Acc: 0.6906 | Val Loss: 0.4954 Acc: 0.7621                                               \n",
      "Epoch 008 | Train Loss: 0.6082 Acc: 0.7222 | Val Loss: 0.5173 Acc: 0.7496                                               \n",
      "Epoch 009 | Train Loss: 0.5577 Acc: 0.7472 | Val Loss: 0.5162 Acc: 0.7624                                               \n",
      "Epoch 010 | Train Loss: 0.5219 Acc: 0.7733 | Val Loss: 0.4256 Acc: 0.7848                                               \n",
      "Epoch 011 | Train Loss: 0.4851 Acc: 0.7871 | Val Loss: 0.4602 Acc: 0.7758                                               \n",
      "Epoch 012 | Train Loss: 0.4747 Acc: 0.7925 | Val Loss: 0.4492 Acc: 0.8093                                               \n",
      "Epoch 013 | Train Loss: 0.4489 Acc: 0.8048 | Val Loss: 0.4206 Acc: 0.8101                                               \n",
      "Epoch 014 | Train Loss: 0.4333 Acc: 0.8133 | Val Loss: 0.4195 Acc: 0.8161                                               \n",
      "Epoch 015 | Train Loss: 0.4245 Acc: 0.8166 | Val Loss: 0.3941 Acc: 0.8388                                               \n",
      "Epoch 016 | Train Loss: 0.3995 Acc: 0.8280 | Val Loss: 0.4616 Acc: 0.8033                                               \n",
      "Epoch 017 | Train Loss: 0.3824 Acc: 0.8338 | Val Loss: 0.4517 Acc: 0.8125                                               \n",
      "Epoch 018 | Train Loss: 0.3862 Acc: 0.8365 | Val Loss: 0.5189 Acc: 0.8042                                               \n",
      "Epoch 019 | Train Loss: 0.3762 Acc: 0.8389 | Val Loss: 0.4420 Acc: 0.8122                                               \n",
      "Epoch 020 | Train Loss: 0.3598 Acc: 0.8440 | Val Loss: 0.5121 Acc: 0.8000                                               \n",
      "Epoch 021 | Train Loss: 0.3495 Acc: 0.8518 | Val Loss: 0.4341 Acc: 0.8397                                               \n",
      "Epoch 022 | Train Loss: 0.3442 Acc: 0.8539 | Val Loss: 0.4838 Acc: 0.7925                                               \n",
      "Epoch 023 | Train Loss: 0.3466 Acc: 0.8585 | Val Loss: 0.4398 Acc: 0.8525                                               \n",
      "Epoch 024 | Train Loss: 0.3335 Acc: 0.8632 | Val Loss: 0.4899 Acc: 0.7961                                               \n",
      "Epoch 025 | Train Loss: 0.3274 Acc: 0.8626 | Val Loss: 0.3598 Acc: 0.8436                                               \n",
      "Epoch 026 | Train Loss: 0.3124 Acc: 0.8690 | Val Loss: 0.3204 Acc: 0.8722                                               \n",
      "Epoch 027 | Train Loss: 0.3124 Acc: 0.8671 | Val Loss: 0.3705 Acc: 0.8690                                               \n",
      "Epoch 028 | Train Loss: 0.3031 Acc: 0.8726 | Val Loss: 0.4442 Acc: 0.8006                                               \n",
      "Epoch 029 | Train Loss: 0.2938 Acc: 0.8787 | Val Loss: 0.4120 Acc: 0.8328                                               \n",
      "Epoch 030 | Train Loss: 0.2877 Acc: 0.8810 | Val Loss: 0.3852 Acc: 0.8645                                               \n",
      "Epoch 031 | Train Loss: 0.2910 Acc: 0.8818 | Val Loss: 0.5405 Acc: 0.8146                                               \n",
      "Epoch 032 | Train Loss: 0.2984 Acc: 0.8794 | Val Loss: 0.5125 Acc: 0.8012                                               \n",
      "Epoch 033 | Train Loss: 0.2896 Acc: 0.8813 | Val Loss: 0.3603 Acc: 0.8582                                               \n",
      "Epoch 034 | Train Loss: 0.2817 Acc: 0.8865 | Val Loss: 0.3583 Acc: 0.8197                                               \n",
      "Epoch 035 | Train Loss: 0.2774 Acc: 0.8876 | Val Loss: 0.4394 Acc: 0.8027                                               \n",
      "Epoch 036 | Train Loss: 0.2747 Acc: 0.8876 | Val Loss: 0.4398 Acc: 0.8284                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 64, 'cnn_dropout': 0.6043421507585027, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 64, 'learning_rate': 0.0017504172374938026, 'lstm_dense': 64, 'lstm_hidden_size': 128, 'lstm_layers': 3, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 4.8790 Acc: 0.4903 | Val Loss: 0.7548 Acc: 0.6170                                               \n",
      "Epoch 002 | Train Loss: 0.8424 Acc: 0.6100 | Val Loss: 0.8161 Acc: 0.6233                                               \n",
      "Epoch 003 | Train Loss: 0.7025 Acc: 0.6757 | Val Loss: 0.5981 Acc: 0.7104                                               \n",
      "Epoch 004 | Train Loss: 0.6424 Acc: 0.7087 | Val Loss: 0.5682 Acc: 0.7376                                               \n",
      "Epoch 005 | Train Loss: 0.6086 Acc: 0.7266 | Val Loss: 0.5627 Acc: 0.7594                                               \n",
      "Epoch 006 | Train Loss: 0.5682 Acc: 0.7489 | Val Loss: 0.4738 Acc: 0.8027                                               \n",
      "Epoch 007 | Train Loss: 0.5539 Acc: 0.7637 | Val Loss: 0.5042 Acc: 0.7701                                               \n",
      "Epoch 008 | Train Loss: 0.5505 Acc: 0.7628 | Val Loss: 0.4945 Acc: 0.7896                                               \n",
      "Epoch 009 | Train Loss: 0.5528 Acc: 0.7644 | Val Loss: 0.4888 Acc: 0.7645                                               \n",
      "Epoch 010 | Train Loss: 0.5410 Acc: 0.7751 | Val Loss: 0.4668 Acc: 0.7851                                               \n",
      "Epoch 011 | Train Loss: 0.5278 Acc: 0.7714 | Val Loss: 0.5320 Acc: 0.7749                                               \n",
      "Epoch 012 | Train Loss: 0.5288 Acc: 0.7713 | Val Loss: 0.5557 Acc: 0.8012                                               \n",
      "Epoch 013 | Train Loss: 0.5160 Acc: 0.7719 | Val Loss: 0.5016 Acc: 0.8084                                               \n",
      "Epoch 014 | Train Loss: 0.5652 Acc: 0.7609 | Val Loss: 0.6373 Acc: 0.7406                                               \n",
      "Epoch 015 | Train Loss: 0.5187 Acc: 0.7721 | Val Loss: 0.4309 Acc: 0.8197                                               \n",
      "Epoch 016 | Train Loss: 0.5132 Acc: 0.7839 | Val Loss: 0.4846 Acc: 0.7809                                               \n",
      "Epoch 017 | Train Loss: 0.5444 Acc: 0.7681 | Val Loss: 0.4195 Acc: 0.8149                                               \n",
      "Epoch 018 | Train Loss: 0.5209 Acc: 0.7740 | Val Loss: 0.6119 Acc: 0.7519                                               \n",
      "Epoch 019 | Train Loss: 0.5266 Acc: 0.7745 | Val Loss: 0.4605 Acc: 0.7621                                               \n",
      "Epoch 020 | Train Loss: 0.5492 Acc: 0.7611 | Val Loss: 0.4727 Acc: 0.8200                                               \n",
      "Epoch 021 | Train Loss: 0.5822 Acc: 0.7504 | Val Loss: 0.4605 Acc: 0.7731                                               \n",
      "Epoch 022 | Train Loss: 0.5214 Acc: 0.7695 | Val Loss: 0.4082 Acc: 0.8140                                               \n",
      "Epoch 023 | Train Loss: 0.5633 Acc: 0.7668 | Val Loss: 0.4639 Acc: 0.8027                                               \n",
      "Epoch 024 | Train Loss: 0.5401 Acc: 0.7656 | Val Loss: 0.4836 Acc: 0.7958                                               \n",
      "Epoch 025 | Train Loss: 0.5233 Acc: 0.7736 | Val Loss: 0.5902 Acc: 0.7397                                               \n",
      "Epoch 026 | Train Loss: 0.4992 Acc: 0.7844 | Val Loss: 0.4304 Acc: 0.8161                                               \n",
      "Epoch 027 | Train Loss: 0.5281 Acc: 0.7668 | Val Loss: 0.5366 Acc: 0.7415                                               \n",
      "Epoch 028 | Train Loss: 0.5969 Acc: 0.7324 | Val Loss: 0.5442 Acc: 0.7618                                               \n",
      "Epoch 029 | Train Loss: 0.6178 Acc: 0.7328 | Val Loss: 0.5294 Acc: 0.7337                                               \n",
      "Epoch 030 | Train Loss: 0.5586 Acc: 0.7553 | Val Loss: 0.5424 Acc: 0.7307                                               \n",
      "Epoch 031 | Train Loss: 0.5733 Acc: 0.7522 | Val Loss: 0.6241 Acc: 0.7800                                               \n",
      "Epoch 032 | Train Loss: 0.5618 Acc: 0.7535 | Val Loss: 0.4902 Acc: 0.7737                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 128, 'cnn_dropout': 0.46910651326576697, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 16, 'learning_rate': 5.262278102073734e-05, 'lstm_dense': 64, 'lstm_hidden_size': 128, 'lstm_layers': 5, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 22.9718 Acc: 0.3413 | Val Loss: 4.4690 Acc: 0.4522                                              \n",
      "Epoch 002 | Train Loss: 7.0770 Acc: 0.4214 | Val Loss: 1.5577 Acc: 0.5191                                               \n",
      "Epoch 003 | Train Loss: 4.1672 Acc: 0.4556 | Val Loss: 1.3271 Acc: 0.5875                                               \n",
      "Epoch 004 | Train Loss: 2.8965 Acc: 0.4709 | Val Loss: 1.0611 Acc: 0.5507                                               \n",
      "Epoch 005 | Train Loss: 2.3343 Acc: 0.4790 | Val Loss: 1.2523 Acc: 0.5967                                               \n",
      "Epoch 006 | Train Loss: 1.9136 Acc: 0.4995 | Val Loss: 1.4085 Acc: 0.4946                                               \n",
      "Epoch 007 | Train Loss: 1.6257 Acc: 0.5301 | Val Loss: 1.3484 Acc: 0.5397                                               \n",
      "Epoch 008 | Train Loss: 1.3779 Acc: 0.5630 | Val Loss: 0.8497 Acc: 0.6585                                               \n",
      "Epoch 009 | Train Loss: 1.2377 Acc: 0.5905 | Val Loss: 1.0227 Acc: 0.5773                                               \n",
      "Epoch 010 | Train Loss: 1.1163 Acc: 0.6111 | Val Loss: 0.8506 Acc: 0.6863                                               \n",
      "Epoch 011 | Train Loss: 0.9802 Acc: 0.6342 | Val Loss: 1.1951 Acc: 0.5722                                               \n",
      "Epoch 012 | Train Loss: 0.9320 Acc: 0.6466 | Val Loss: 0.7834 Acc: 0.6848                                               \n",
      "Epoch 013 | Train Loss: 0.8603 Acc: 0.6717 | Val Loss: 0.5227 Acc: 0.7887                                               \n",
      "Epoch 014 | Train Loss: 0.7951 Acc: 0.6907 | Val Loss: 0.7953 Acc: 0.7101                                               \n",
      "Epoch 015 | Train Loss: 0.7383 Acc: 0.7091 | Val Loss: 0.5578 Acc: 0.7621                                               \n",
      "Epoch 016 | Train Loss: 0.6959 Acc: 0.7251 | Val Loss: 0.6392 Acc: 0.6899                                               \n",
      "Epoch 017 | Train Loss: 0.6843 Acc: 0.7280 | Val Loss: 0.5299 Acc: 0.7857                                               \n",
      "Epoch 018 | Train Loss: 0.6576 Acc: 0.7412 | Val Loss: 0.4182 Acc: 0.8421                                               \n",
      "Epoch 019 | Train Loss: 0.6242 Acc: 0.7533 | Val Loss: 0.5099 Acc: 0.7752                                               \n",
      "Epoch 020 | Train Loss: 0.5786 Acc: 0.7690 | Val Loss: 0.7005 Acc: 0.7230                                               \n",
      "Epoch 021 | Train Loss: 0.5872 Acc: 0.7733 | Val Loss: 0.5098 Acc: 0.8146                                               \n",
      "Epoch 022 | Train Loss: 0.5467 Acc: 0.7872 | Val Loss: 0.6556 Acc: 0.7167                                               \n",
      "Epoch 023 | Train Loss: 0.5197 Acc: 0.7974 | Val Loss: 0.3937 Acc: 0.8496                                               \n",
      "Epoch 024 | Train Loss: 0.4980 Acc: 0.8098 | Val Loss: 0.5147 Acc: 0.8042                                               \n",
      "Epoch 025 | Train Loss: 0.4752 Acc: 0.8187 | Val Loss: 0.4674 Acc: 0.8191                                               \n",
      "Epoch 026 | Train Loss: 0.4551 Acc: 0.8290 | Val Loss: 0.4164 Acc: 0.8490                                               \n",
      "Epoch 027 | Train Loss: 0.4252 Acc: 0.8422 | Val Loss: 0.2930 Acc: 0.8934                                               \n",
      "Epoch 028 | Train Loss: 0.4068 Acc: 0.8472 | Val Loss: 0.3479 Acc: 0.8722                                               \n",
      "Epoch 029 | Train Loss: 0.3912 Acc: 0.8550 | Val Loss: 0.4016 Acc: 0.8537                                               \n",
      "Epoch 030 | Train Loss: 0.3693 Acc: 0.8638 | Val Loss: 0.2790 Acc: 0.8991                                               \n",
      "Epoch 031 | Train Loss: 0.3525 Acc: 0.8733 | Val Loss: 0.3491 Acc: 0.8746                                               \n",
      "Epoch 032 | Train Loss: 0.3376 Acc: 0.8763 | Val Loss: 0.3120 Acc: 0.8779                                               \n",
      "Epoch 033 | Train Loss: 0.3223 Acc: 0.8813 | Val Loss: 0.2268 Acc: 0.9185                                               \n",
      "Epoch 034 | Train Loss: 0.3093 Acc: 0.8908 | Val Loss: 0.3824 Acc: 0.8701                                               \n",
      "Epoch 035 | Train Loss: 0.2904 Acc: 0.8980 | Val Loss: 0.2310 Acc: 0.9227                                               \n",
      "Epoch 036 | Train Loss: 0.2751 Acc: 0.9035 | Val Loss: 0.1960 Acc: 0.9325                                               \n",
      "Epoch 037 | Train Loss: 0.2598 Acc: 0.9066 | Val Loss: 0.2979 Acc: 0.8979                                               \n",
      "Epoch 038 | Train Loss: 0.2562 Acc: 0.9088 | Val Loss: 0.1992 Acc: 0.9284                                               \n",
      "Epoch 039 | Train Loss: 0.2326 Acc: 0.9169 | Val Loss: 0.3315 Acc: 0.8934                                               \n",
      "Epoch 040 | Train Loss: 0.2274 Acc: 0.9204 | Val Loss: 0.2381 Acc: 0.9170                                               \n",
      "Epoch 041 | Train Loss: 0.2087 Acc: 0.9257 | Val Loss: 0.2183 Acc: 0.9278                                               \n",
      "Epoch 042 | Train Loss: 0.2023 Acc: 0.9300 | Val Loss: 0.1782 Acc: 0.9409                                               \n",
      "Epoch 043 | Train Loss: 0.1977 Acc: 0.9305 | Val Loss: 0.1713 Acc: 0.9412                                               \n",
      "Epoch 044 | Train Loss: 0.1846 Acc: 0.9375 | Val Loss: 0.1881 Acc: 0.9367                                               \n",
      "Epoch 045 | Train Loss: 0.1727 Acc: 0.9416 | Val Loss: 0.2674 Acc: 0.9113                                               \n",
      "Epoch 046 | Train Loss: 0.1705 Acc: 0.9399 | Val Loss: 0.1209 Acc: 0.9567                                               \n",
      "Epoch 047 | Train Loss: 0.1542 Acc: 0.9463 | Val Loss: 0.2119 Acc: 0.9257                                               \n",
      "Epoch 048 | Train Loss: 0.1513 Acc: 0.9466 | Val Loss: 0.1128 Acc: 0.9567                                               \n",
      "Epoch 049 | Train Loss: 0.1507 Acc: 0.9497 | Val Loss: 0.1493 Acc: 0.9487                                               \n",
      "Epoch 050 | Train Loss: 0.1363 Acc: 0.9546 | Val Loss: 0.1467 Acc: 0.9501                                               \n",
      "Epoch 051 | Train Loss: 0.1410 Acc: 0.9546 | Val Loss: 0.0995 Acc: 0.9612                                               \n",
      "Epoch 052 | Train Loss: 0.1252 Acc: 0.9580 | Val Loss: 0.3285 Acc: 0.8985                                               \n",
      "Epoch 053 | Train Loss: 0.1202 Acc: 0.9601 | Val Loss: 0.1501 Acc: 0.9442                                               \n",
      "Epoch 054 | Train Loss: 0.1242 Acc: 0.9602 | Val Loss: 0.2276 Acc: 0.9230                                               \n",
      "Epoch 055 | Train Loss: 0.1101 Acc: 0.9637 | Val Loss: 0.0887 Acc: 0.9678                                               \n",
      "Epoch 056 | Train Loss: 0.1070 Acc: 0.9669 | Val Loss: 0.0914 Acc: 0.9669                                               \n",
      "Epoch 057 | Train Loss: 0.1056 Acc: 0.9657 | Val Loss: 0.1112 Acc: 0.9621                                               \n",
      "Epoch 058 | Train Loss: 0.0939 Acc: 0.9684 | Val Loss: 0.0943 Acc: 0.9707                                               \n",
      "Epoch 059 | Train Loss: 0.0957 Acc: 0.9690 | Val Loss: 0.0872 Acc: 0.9710                                               \n",
      "Epoch 060 | Train Loss: 0.0915 Acc: 0.9698 | Val Loss: 0.1975 Acc: 0.9379                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 128, 'cnn_dropout': 0.3540316626065999, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 64, 'learning_rate': 0.00027133795570118135, 'lstm_dense': 64, 'lstm_hidden_size': 128, 'lstm_layers': 5, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 8.8288 Acc: 0.4035 | Val Loss: 1.6855 Acc: 0.5072                                               \n",
      "Epoch 002 | Train Loss: 2.9595 Acc: 0.4844 | Val Loss: 1.3475 Acc: 0.6176                                               \n",
      "Epoch 003 | Train Loss: 1.7939 Acc: 0.5386 | Val Loss: 0.8752 Acc: 0.7018                                               \n",
      "Epoch 004 | Train Loss: 1.2846 Acc: 0.5930 | Val Loss: 0.6948 Acc: 0.7143                                               \n",
      "Epoch 005 | Train Loss: 0.9488 Acc: 0.6543 | Val Loss: 0.7128 Acc: 0.6746                                               \n",
      "Epoch 006 | Train Loss: 0.7494 Acc: 0.7107 | Val Loss: 0.5329 Acc: 0.7887                                               \n",
      "Epoch 007 | Train Loss: 0.6276 Acc: 0.7577 | Val Loss: 0.4042 Acc: 0.8507                                               \n",
      "Epoch 008 | Train Loss: 0.5205 Acc: 0.8028 | Val Loss: 0.5322 Acc: 0.8003                                               \n",
      "Epoch 009 | Train Loss: 0.4454 Acc: 0.8330 | Val Loss: 0.3079 Acc: 0.8970                                               \n",
      "Epoch 010 | Train Loss: 0.3780 Acc: 0.8618 | Val Loss: 0.2495 Acc: 0.9110                                               \n",
      "Epoch 011 | Train Loss: 0.3094 Acc: 0.8895 | Val Loss: 0.2425 Acc: 0.9122                                               \n",
      "Epoch 012 | Train Loss: 0.2590 Acc: 0.9081 | Val Loss: 0.2187 Acc: 0.9269                                               \n",
      "Epoch 013 | Train Loss: 0.2184 Acc: 0.9265 | Val Loss: 0.2041 Acc: 0.9215                                               \n",
      "Epoch 014 | Train Loss: 0.1821 Acc: 0.9381 | Val Loss: 0.1956 Acc: 0.9158                                               \n",
      "Epoch 015 | Train Loss: 0.1603 Acc: 0.9445 | Val Loss: 0.1088 Acc: 0.9672                                               \n",
      "Epoch 016 | Train Loss: 0.1292 Acc: 0.9553 | Val Loss: 0.1049 Acc: 0.9552                                               \n",
      "Epoch 017 | Train Loss: 0.1212 Acc: 0.9598 | Val Loss: 0.1155 Acc: 0.9525                                               \n",
      "Epoch 018 | Train Loss: 0.1052 Acc: 0.9679 | Val Loss: 0.1303 Acc: 0.9484                                               \n",
      "Epoch 019 | Train Loss: 0.1033 Acc: 0.9672 | Val Loss: 0.1826 Acc: 0.9439                                               \n",
      "Epoch 020 | Train Loss: 0.0813 Acc: 0.9734 | Val Loss: 0.0947 Acc: 0.9633                                               \n",
      "Epoch 021 | Train Loss: 0.0779 Acc: 0.9767 | Val Loss: 0.0997 Acc: 0.9570                                               \n",
      "Epoch 022 | Train Loss: 0.0770 Acc: 0.9754 | Val Loss: 0.1018 Acc: 0.9537                                               \n",
      "Epoch 023 | Train Loss: 0.0689 Acc: 0.9796 | Val Loss: 0.0893 Acc: 0.9719                                               \n",
      "Epoch 024 | Train Loss: 0.0664 Acc: 0.9788 | Val Loss: 0.0893 Acc: 0.9675                                               \n",
      "Epoch 025 | Train Loss: 0.0650 Acc: 0.9807 | Val Loss: 0.0779 Acc: 0.9666                                               \n",
      "Epoch 026 | Train Loss: 0.0536 Acc: 0.9837 | Val Loss: 0.0739 Acc: 0.9743                                               \n",
      "Epoch 027 | Train Loss: 0.0512 Acc: 0.9854 | Val Loss: 0.0668 Acc: 0.9788                                               \n",
      "Epoch 028 | Train Loss: 0.0464 Acc: 0.9861 | Val Loss: 0.0660 Acc: 0.9779                                               \n",
      "Epoch 029 | Train Loss: 0.0502 Acc: 0.9838 | Val Loss: 0.0722 Acc: 0.9770                                               \n",
      "Epoch 030 | Train Loss: 0.0439 Acc: 0.9869 | Val Loss: 0.0996 Acc: 0.9615                                               \n",
      "Epoch 031 | Train Loss: 0.0475 Acc: 0.9854 | Val Loss: 0.0762 Acc: 0.9755                                               \n",
      "Epoch 032 | Train Loss: 0.0428 Acc: 0.9867 | Val Loss: 0.0762 Acc: 0.9779                                               \n",
      "Epoch 033 | Train Loss: 0.0466 Acc: 0.9873 | Val Loss: 0.0675 Acc: 0.9767                                               \n",
      "Epoch 034 | Train Loss: 0.0410 Acc: 0.9875 | Val Loss: 0.0686 Acc: 0.9812                                               \n",
      "Epoch 035 | Train Loss: 0.0349 Acc: 0.9898 | Val Loss: 0.0556 Acc: 0.9752                                               \n",
      "Epoch 036 | Train Loss: 0.0407 Acc: 0.9875 | Val Loss: 0.1080 Acc: 0.9690                                               \n",
      "Epoch 037 | Train Loss: 0.0370 Acc: 0.9897 | Val Loss: 0.0826 Acc: 0.9746                                               \n",
      "Epoch 038 | Train Loss: 0.0357 Acc: 0.9898 | Val Loss: 0.0651 Acc: 0.9773                                               \n",
      "Epoch 039 | Train Loss: 0.0361 Acc: 0.9896 | Val Loss: 0.0596 Acc: 0.9812                                               \n",
      "Epoch 040 | Train Loss: 0.0342 Acc: 0.9898 | Val Loss: 0.0788 Acc: 0.9690                                               \n",
      "Epoch 041 | Train Loss: 0.0336 Acc: 0.9896 | Val Loss: 0.0520 Acc: 0.9833                                               \n",
      "Epoch 042 | Train Loss: 0.0268 Acc: 0.9923 | Val Loss: 0.0471 Acc: 0.9869                                               \n",
      "Epoch 043 | Train Loss: 0.0325 Acc: 0.9897 | Val Loss: 0.0854 Acc: 0.9761                                               \n",
      "Epoch 044 | Train Loss: 0.0323 Acc: 0.9904 | Val Loss: 0.0989 Acc: 0.9704                                               \n",
      "Epoch 045 | Train Loss: 0.0285 Acc: 0.9917 | Val Loss: 0.0814 Acc: 0.9696                                               \n",
      "Epoch 046 | Train Loss: 0.0238 Acc: 0.9928 | Val Loss: 0.0842 Acc: 0.9731                                               \n",
      "Epoch 047 | Train Loss: 0.0301 Acc: 0.9899 | Val Loss: 0.0819 Acc: 0.9672                                               \n",
      "Epoch 048 | Train Loss: 0.0276 Acc: 0.9905 | Val Loss: 0.1483 Acc: 0.9463                                               \n",
      "Epoch 049 | Train Loss: 0.0229 Acc: 0.9925 | Val Loss: 0.1014 Acc: 0.9618                                               \n",
      "Epoch 050 | Train Loss: 0.0275 Acc: 0.9921 | Val Loss: 0.0508 Acc: 0.9800                                               \n",
      "Epoch 051 | Train Loss: 0.0236 Acc: 0.9927 | Val Loss: 0.0665 Acc: 0.9806                                               \n",
      "Epoch 052 | Train Loss: 0.0248 Acc: 0.9930 | Val Loss: 0.0717 Acc: 0.9821                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 64, 'cnn_dropout': 0.6096029514270094, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 16, 'learning_rate': 0.0005795244246232464, 'lstm_dense': 64, 'lstm_hidden_size': 128, 'lstm_layers': 5, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 6.0903 Acc: 0.4653 | Val Loss: 1.0580 Acc: 0.5857                                               \n",
      "Epoch 002 | Train Loss: 1.4143 Acc: 0.5544 | Val Loss: 0.7633 Acc: 0.6299                                               \n",
      "Epoch 003 | Train Loss: 0.8193 Acc: 0.6525 | Val Loss: 0.6012 Acc: 0.7331                                               \n",
      "Epoch 004 | Train Loss: 0.6538 Acc: 0.7236 | Val Loss: 0.5154 Acc: 0.7684                                               \n",
      "Epoch 005 | Train Loss: 0.5278 Acc: 0.7804 | Val Loss: 0.4259 Acc: 0.8060                                               \n",
      "Epoch 006 | Train Loss: 0.4639 Acc: 0.8131 | Val Loss: 0.3439 Acc: 0.8564                                               \n",
      "Epoch 007 | Train Loss: 0.3962 Acc: 0.8434 | Val Loss: 0.3552 Acc: 0.8576                                               \n",
      "Epoch 008 | Train Loss: 0.3532 Acc: 0.8669 | Val Loss: 0.3764 Acc: 0.8501                                               \n",
      "Epoch 009 | Train Loss: 0.3195 Acc: 0.8768 | Val Loss: 0.2320 Acc: 0.8958                                               \n",
      "Epoch 010 | Train Loss: 0.3081 Acc: 0.8848 | Val Loss: 0.2064 Acc: 0.9093                                               \n",
      "Epoch 011 | Train Loss: 0.2869 Acc: 0.8910 | Val Loss: 0.2279 Acc: 0.9042                                               \n",
      "Epoch 012 | Train Loss: 0.2692 Acc: 0.8992 | Val Loss: 0.1763 Acc: 0.9296                                               \n",
      "Epoch 013 | Train Loss: 0.2434 Acc: 0.9111 | Val Loss: 0.2369 Acc: 0.8940                                               \n",
      "Epoch 014 | Train Loss: 0.2320 Acc: 0.9146 | Val Loss: 0.1596 Acc: 0.9421                                               \n",
      "Epoch 015 | Train Loss: 0.2166 Acc: 0.9237 | Val Loss: 0.2252 Acc: 0.8991                                               \n",
      "Epoch 016 | Train Loss: 0.2116 Acc: 0.9248 | Val Loss: 0.2509 Acc: 0.8896                                               \n",
      "Epoch 017 | Train Loss: 0.2040 Acc: 0.9310 | Val Loss: 0.2596 Acc: 0.9030                                               \n",
      "Epoch 018 | Train Loss: 0.1748 Acc: 0.9381 | Val Loss: 0.2042 Acc: 0.9254                                               \n",
      "Epoch 019 | Train Loss: 0.1962 Acc: 0.9304 | Val Loss: 0.1945 Acc: 0.9206                                               \n",
      "Epoch 020 | Train Loss: 0.1653 Acc: 0.9401 | Val Loss: 0.1282 Acc: 0.9519                                               \n",
      "Epoch 021 | Train Loss: 0.1516 Acc: 0.9478 | Val Loss: 0.1392 Acc: 0.9412                                               \n",
      "Epoch 022 | Train Loss: 0.1503 Acc: 0.9485 | Val Loss: 0.1316 Acc: 0.9430                                               \n",
      "Epoch 023 | Train Loss: 0.1339 Acc: 0.9534 | Val Loss: 0.1763 Acc: 0.9301                                               \n",
      "Epoch 024 | Train Loss: 0.1477 Acc: 0.9501 | Val Loss: 0.1746 Acc: 0.9382                                               \n",
      "Epoch 025 | Train Loss: 0.1312 Acc: 0.9563 | Val Loss: 0.1218 Acc: 0.9451                                               \n",
      "Epoch 026 | Train Loss: 0.1307 Acc: 0.9572 | Val Loss: 0.1355 Acc: 0.9603                                               \n",
      "Epoch 027 | Train Loss: 0.1408 Acc: 0.9545 | Val Loss: 0.3420 Acc: 0.8669                                               \n",
      "Epoch 028 | Train Loss: 0.1199 Acc: 0.9604 | Val Loss: 0.0708 Acc: 0.9815                                               \n",
      "Epoch 029 | Train Loss: 0.1253 Acc: 0.9586 | Val Loss: 0.1706 Acc: 0.9370                                               \n",
      "Epoch 030 | Train Loss: 0.1056 Acc: 0.9637 | Val Loss: 0.1492 Acc: 0.9409                                               \n",
      "Epoch 031 | Train Loss: 0.1240 Acc: 0.9588 | Val Loss: 0.1659 Acc: 0.9457                                               \n",
      "Epoch 032 | Train Loss: 0.1126 Acc: 0.9616 | Val Loss: 0.1202 Acc: 0.9382                                               \n",
      "Epoch 033 | Train Loss: 0.1155 Acc: 0.9640 | Val Loss: 0.1684 Acc: 0.9397                                               \n",
      "Epoch 034 | Train Loss: 0.0996 Acc: 0.9676 | Val Loss: 0.0772 Acc: 0.9779                                               \n",
      "Epoch 035 | Train Loss: 0.1073 Acc: 0.9659 | Val Loss: 0.0799 Acc: 0.9707                                               \n",
      "Epoch 036 | Train Loss: 0.1003 Acc: 0.9674 | Val Loss: 0.1035 Acc: 0.9645                                               \n",
      "Epoch 037 | Train Loss: 0.1076 Acc: 0.9658 | Val Loss: 0.1114 Acc: 0.9672                                               \n",
      "Epoch 038 | Train Loss: 0.0958 Acc: 0.9686 | Val Loss: 0.1327 Acc: 0.9573                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 64, 'cnn_dropout': 0.36790754488801325, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 32, 'cnn_kernels_2': 64, 'learning_rate': 2.7026787731480528e-05, 'lstm_dense': 64, 'lstm_hidden_size': 128, 'lstm_layers': 4, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 38.0031 Acc: 0.2750 | Val Loss: 8.5409 Acc: 0.5042                                              \n",
      "Epoch 002 | Train Loss: 20.0030 Acc: 0.3479 | Val Loss: 12.4136 Acc: 0.4146                                             \n",
      "Epoch 003 | Train Loss: 15.1191 Acc: 0.3802 | Val Loss: 8.7412 Acc: 0.4466                                              \n",
      "Epoch 004 | Train Loss: 12.3116 Acc: 0.3882 | Val Loss: 6.6438 Acc: 0.5009                                              \n",
      "Epoch 005 | Train Loss: 10.1045 Acc: 0.4061 | Val Loss: 5.4561 Acc: 0.4899                                              \n",
      "Epoch 006 | Train Loss: 8.8047 Acc: 0.4041 | Val Loss: 2.9382 Acc: 0.5233                                               \n",
      "Epoch 007 | Train Loss: 7.7041 Acc: 0.4126 | Val Loss: 2.2873 Acc: 0.5564                                               \n",
      "Epoch 008 | Train Loss: 6.5072 Acc: 0.4348 | Val Loss: 2.5472 Acc: 0.5101                                               \n",
      "Epoch 009 | Train Loss: 5.7344 Acc: 0.4480 | Val Loss: 2.3815 Acc: 0.5701                                               \n",
      "Epoch 010 | Train Loss: 5.3184 Acc: 0.4516 | Val Loss: 1.8335 Acc: 0.5609                                               \n",
      "Epoch 011 | Train Loss: 4.6029 Acc: 0.4738 | Val Loss: 2.0629 Acc: 0.5573                                               \n",
      "Epoch 012 | Train Loss: 4.2216 Acc: 0.4903 | Val Loss: 1.4520 Acc: 0.5946                                               \n",
      "Epoch 013 | Train Loss: 3.9654 Acc: 0.4880 | Val Loss: 1.4645 Acc: 0.6281                                               \n",
      "Epoch 014 | Train Loss: 3.6417 Acc: 0.4979 | Val Loss: 1.2475 Acc: 0.5997                                               \n",
      "Epoch 015 | Train Loss: 3.3875 Acc: 0.5135 | Val Loss: 1.0980 Acc: 0.6427                                               \n",
      "Epoch 016 | Train Loss: 3.1255 Acc: 0.5177 | Val Loss: 1.2389 Acc: 0.6493                                               \n",
      "Epoch 017 | Train Loss: 3.0339 Acc: 0.5231 | Val Loss: 1.4800 Acc: 0.6090                                               \n",
      "Epoch 018 | Train Loss: 2.8069 Acc: 0.5360 | Val Loss: 1.5341 Acc: 0.5857                                               \n",
      "Epoch 019 | Train Loss: 2.6305 Acc: 0.5388 | Val Loss: 1.3220 Acc: 0.6090                                               \n",
      "Epoch 020 | Train Loss: 2.4479 Acc: 0.5555 | Val Loss: 0.9177 Acc: 0.6525                                               \n",
      "Epoch 021 | Train Loss: 2.3887 Acc: 0.5587 | Val Loss: 1.3370 Acc: 0.6310                                               \n",
      "Epoch 022 | Train Loss: 2.2027 Acc: 0.5750 | Val Loss: 1.1896 Acc: 0.6116                                               \n",
      "Epoch 023 | Train Loss: 2.0439 Acc: 0.5900 | Val Loss: 1.3678 Acc: 0.6137                                               \n",
      "Epoch 024 | Train Loss: 1.9971 Acc: 0.5917 | Val Loss: 1.2786 Acc: 0.6227                                               \n",
      "Epoch 025 | Train Loss: 1.9091 Acc: 0.5965 | Val Loss: 0.7999 Acc: 0.7167                                               \n",
      "Epoch 026 | Train Loss: 1.8101 Acc: 0.6084 | Val Loss: 0.8198 Acc: 0.7110                                               \n",
      "Epoch 027 | Train Loss: 1.6780 Acc: 0.6243 | Val Loss: 0.6985 Acc: 0.7349                                               \n",
      "Epoch 028 | Train Loss: 1.6445 Acc: 0.6342 | Val Loss: 0.9152 Acc: 0.6833                                               \n",
      "Epoch 029 | Train Loss: 1.5146 Acc: 0.6524 | Val Loss: 0.7966 Acc: 0.7290                                               \n",
      "Epoch 030 | Train Loss: 1.4564 Acc: 0.6580 | Val Loss: 0.5795 Acc: 0.7699                                               \n",
      "Epoch 031 | Train Loss: 1.3259 Acc: 0.6766 | Val Loss: 0.6504 Acc: 0.7642                                               \n",
      "Epoch 032 | Train Loss: 1.2688 Acc: 0.6806 | Val Loss: 0.5966 Acc: 0.7875                                               \n",
      "Epoch 033 | Train Loss: 1.2116 Acc: 0.6964 | Val Loss: 0.5898 Acc: 0.7890                                               \n",
      "Epoch 034 | Train Loss: 1.1863 Acc: 0.7105 | Val Loss: 0.9360 Acc: 0.7206                                               \n",
      "Epoch 035 | Train Loss: 1.1372 Acc: 0.7145 | Val Loss: 0.8227 Acc: 0.7597                                               \n",
      "Epoch 036 | Train Loss: 1.1050 Acc: 0.7263 | Val Loss: 0.6024 Acc: 0.8152                                               \n",
      "Epoch 037 | Train Loss: 1.0506 Acc: 0.7310 | Val Loss: 1.0206 Acc: 0.7316                                               \n",
      "Epoch 038 | Train Loss: 0.9901 Acc: 0.7338 | Val Loss: 1.1418 Acc: 0.7101                                               \n",
      "Epoch 039 | Train Loss: 0.9427 Acc: 0.7503 | Val Loss: 0.7700 Acc: 0.7830                                               \n",
      "Epoch 040 | Train Loss: 0.9214 Acc: 0.7557 | Val Loss: 0.6408 Acc: 0.8051                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 32, 'cnn_dropout': 0.48481715182775814, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 16, 'learning_rate': 0.000189855767280898, 'lstm_dense': 256, 'lstm_hidden_size': 128, 'lstm_layers': 5, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 12.2849 Acc: 0.4004 | Val Loss: 2.4285 Acc: 0.4976                                              \n",
      "Epoch 002 | Train Loss: 3.9064 Acc: 0.4676 | Val Loss: 1.5061 Acc: 0.5857                                               \n",
      "Epoch 003 | Train Loss: 2.2820 Acc: 0.5312 | Val Loss: 1.1430 Acc: 0.6761                                               \n",
      "Epoch 004 | Train Loss: 1.6509 Acc: 0.5774 | Val Loss: 2.4416 Acc: 0.5785                                               \n",
      "Epoch 005 | Train Loss: 1.2935 Acc: 0.6192 | Val Loss: 1.4681 Acc: 0.5899                                               \n",
      "Epoch 006 | Train Loss: 1.0682 Acc: 0.6529 | Val Loss: 1.2295 Acc: 0.5690                                               \n",
      "Epoch 007 | Train Loss: 0.8816 Acc: 0.6941 | Val Loss: 0.6702 Acc: 0.7122                                               \n",
      "Epoch 008 | Train Loss: 0.8066 Acc: 0.7174 | Val Loss: 0.7681 Acc: 0.7319                                               \n",
      "Epoch 009 | Train Loss: 0.7081 Acc: 0.7456 | Val Loss: 0.6928 Acc: 0.7570                                               \n",
      "Epoch 010 | Train Loss: 0.6395 Acc: 0.7707 | Val Loss: 0.6881 Acc: 0.7719                                               \n",
      "Epoch 011 | Train Loss: 0.5816 Acc: 0.7869 | Val Loss: 0.4352 Acc: 0.8325                                               \n",
      "Epoch 012 | Train Loss: 0.5151 Acc: 0.8089 | Val Loss: 0.4914 Acc: 0.8003                                               \n",
      "Epoch 013 | Train Loss: 0.4651 Acc: 0.8299 | Val Loss: 0.3597 Acc: 0.8696                                               \n",
      "Epoch 014 | Train Loss: 0.4313 Acc: 0.8422 | Val Loss: 0.2803 Acc: 0.8946                                               \n",
      "Epoch 015 | Train Loss: 0.3931 Acc: 0.8603 | Val Loss: 0.3456 Acc: 0.8645                                               \n",
      "Epoch 016 | Train Loss: 0.3522 Acc: 0.8717 | Val Loss: 0.2758 Acc: 0.9093                                               \n",
      "Epoch 017 | Train Loss: 0.3091 Acc: 0.8874 | Val Loss: 0.1879 Acc: 0.9382                                               \n",
      "Epoch 018 | Train Loss: 0.2715 Acc: 0.9066 | Val Loss: 0.3393 Acc: 0.8943                                               \n",
      "Epoch 019 | Train Loss: 0.2393 Acc: 0.9175 | Val Loss: 0.2357 Acc: 0.9128                                               \n",
      "Epoch 020 | Train Loss: 0.2289 Acc: 0.9202 | Val Loss: 0.1794 Acc: 0.9400                                               \n",
      "Epoch 021 | Train Loss: 0.2003 Acc: 0.9284 | Val Loss: 0.1881 Acc: 0.9293                                               \n",
      "Epoch 022 | Train Loss: 0.1829 Acc: 0.9375 | Val Loss: 0.2143 Acc: 0.9236                                               \n",
      "Epoch 023 | Train Loss: 0.1713 Acc: 0.9397 | Val Loss: 0.1303 Acc: 0.9457                                               \n",
      "Epoch 024 | Train Loss: 0.1588 Acc: 0.9444 | Val Loss: 0.1285 Acc: 0.9496                                               \n",
      "Epoch 025 | Train Loss: 0.1409 Acc: 0.9525 | Val Loss: 0.1228 Acc: 0.9540                                               \n",
      "Epoch 026 | Train Loss: 0.1375 Acc: 0.9539 | Val Loss: 0.1888 Acc: 0.9361                                               \n",
      "Epoch 027 | Train Loss: 0.1261 Acc: 0.9584 | Val Loss: 0.1480 Acc: 0.9475                                               \n",
      "Epoch 028 | Train Loss: 0.1164 Acc: 0.9590 | Val Loss: 0.1302 Acc: 0.9522                                               \n",
      "Epoch 029 | Train Loss: 0.1123 Acc: 0.9642 | Val Loss: 0.1276 Acc: 0.9531                                               \n",
      "Epoch 030 | Train Loss: 0.0989 Acc: 0.9676 | Val Loss: 0.1578 Acc: 0.9460                                               \n",
      "Epoch 031 | Train Loss: 0.0960 Acc: 0.9685 | Val Loss: 0.0968 Acc: 0.9731                                               \n",
      "Epoch 032 | Train Loss: 0.0902 Acc: 0.9693 | Val Loss: 0.0960 Acc: 0.9627                                               \n",
      "Epoch 033 | Train Loss: 0.0927 Acc: 0.9696 | Val Loss: 0.0844 Acc: 0.9660                                               \n",
      "Epoch 034 | Train Loss: 0.0810 Acc: 0.9758 | Val Loss: 0.0889 Acc: 0.9639                                               \n",
      "Epoch 035 | Train Loss: 0.0863 Acc: 0.9717 | Val Loss: 0.0928 Acc: 0.9669                                               \n",
      "Epoch 036 | Train Loss: 0.0749 Acc: 0.9763 | Val Loss: 0.0971 Acc: 0.9603                                               \n",
      "Epoch 037 | Train Loss: 0.0721 Acc: 0.9758 | Val Loss: 0.0674 Acc: 0.9806                                               \n",
      "Epoch 038 | Train Loss: 0.0712 Acc: 0.9763 | Val Loss: 0.0652 Acc: 0.9788                                               \n",
      "Epoch 039 | Train Loss: 0.0640 Acc: 0.9795 | Val Loss: 0.0613 Acc: 0.9758                                               \n",
      "Epoch 040 | Train Loss: 0.0636 Acc: 0.9803 | Val Loss: 0.0774 Acc: 0.9693                                               \n",
      "Epoch 041 | Train Loss: 0.0653 Acc: 0.9792 | Val Loss: 0.0682 Acc: 0.9788                                               \n",
      "Epoch 042 | Train Loss: 0.0598 Acc: 0.9807 | Val Loss: 0.1008 Acc: 0.9654                                               \n",
      "Epoch 043 | Train Loss: 0.0541 Acc: 0.9828 | Val Loss: 0.0803 Acc: 0.9713                                               \n",
      "Epoch 044 | Train Loss: 0.0558 Acc: 0.9834 | Val Loss: 0.1250 Acc: 0.9549                                               \n",
      "Epoch 045 | Train Loss: 0.0493 Acc: 0.9840 | Val Loss: 0.0549 Acc: 0.9782                                               \n",
      "Epoch 046 | Train Loss: 0.0494 Acc: 0.9841 | Val Loss: 0.0686 Acc: 0.9746                                               \n",
      "Epoch 047 | Train Loss: 0.0500 Acc: 0.9842 | Val Loss: 0.0730 Acc: 0.9794                                               \n",
      "Epoch 048 | Train Loss: 0.0461 Acc: 0.9856 | Val Loss: 0.0695 Acc: 0.9737                                               \n",
      "Epoch 049 | Train Loss: 0.0441 Acc: 0.9861 | Val Loss: 0.0560 Acc: 0.9839                                               \n",
      "Epoch 050 | Train Loss: 0.0433 Acc: 0.9860 | Val Loss: 0.0803 Acc: 0.9791                                               \n",
      "Epoch 051 | Train Loss: 0.0448 Acc: 0.9853 | Val Loss: 0.0695 Acc: 0.9812                                               \n",
      "Epoch 052 | Train Loss: 0.0432 Acc: 0.9865 | Val Loss: 0.1187 Acc: 0.9654                                               \n",
      "Epoch 053 | Train Loss: 0.0390 Acc: 0.9872 | Val Loss: 0.0662 Acc: 0.9815                                               \n",
      "Epoch 054 | Train Loss: 0.0391 Acc: 0.9875 | Val Loss: 0.0561 Acc: 0.9830                                               \n",
      "Epoch 055 | Train Loss: 0.0403 Acc: 0.9869 | Val Loss: 0.0818 Acc: 0.9755                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 128, 'cnn_dropout': 0.29893155287441653, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 32, 'cnn_kernels_2': 64, 'learning_rate': 6.294524940717796e-05, 'lstm_dense': 32, 'lstm_hidden_size': 128, 'lstm_layers': 3, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 27.4380 Acc: 0.3194 | Val Loss: 7.2457 Acc: 0.4484                                              \n",
      "Epoch 002 | Train Loss: 11.2100 Acc: 0.3789 | Val Loss: 3.2733 Acc: 0.5042                                              \n",
      "Epoch 003 | Train Loss: 7.8403 Acc: 0.4041 | Val Loss: 2.6194 Acc: 0.5391                                               \n",
      "Epoch 004 | Train Loss: 6.1295 Acc: 0.4335 | Val Loss: 2.7752 Acc: 0.5385                                               \n",
      "Epoch 005 | Train Loss: 4.5156 Acc: 0.4715 | Val Loss: 2.7302 Acc: 0.5842                                               \n",
      "Epoch 006 | Train Loss: 3.6547 Acc: 0.5012 | Val Loss: 1.2191 Acc: 0.6406                                               \n",
      "Epoch 007 | Train Loss: 3.1013 Acc: 0.5185 | Val Loss: 1.1242 Acc: 0.6033                                               \n",
      "Epoch 008 | Train Loss: 2.5679 Acc: 0.5397 | Val Loss: 1.3851 Acc: 0.6513                                               \n",
      "Epoch 009 | Train Loss: 2.2767 Acc: 0.5615 | Val Loss: 1.1987 Acc: 0.6534                                               \n",
      "Epoch 010 | Train Loss: 1.9735 Acc: 0.5861 | Val Loss: 1.0343 Acc: 0.6806                                               \n",
      "Epoch 011 | Train Loss: 1.7671 Acc: 0.6011 | Val Loss: 0.7386 Acc: 0.6964                                               \n",
      "Epoch 012 | Train Loss: 1.5592 Acc: 0.6162 | Val Loss: 1.4203 Acc: 0.6140                                               \n",
      "Epoch 013 | Train Loss: 1.3923 Acc: 0.6386 | Val Loss: 0.8080 Acc: 0.7093                                               \n",
      "Epoch 014 | Train Loss: 1.2307 Acc: 0.6615 | Val Loss: 0.7004 Acc: 0.7472                                               \n",
      "Epoch 015 | Train Loss: 1.0853 Acc: 0.6828 | Val Loss: 1.3554 Acc: 0.6018                                               \n",
      "Epoch 016 | Train Loss: 1.0379 Acc: 0.6992 | Val Loss: 0.6417 Acc: 0.7645                                               \n",
      "Epoch 017 | Train Loss: 0.9147 Acc: 0.7196 | Val Loss: 0.5237 Acc: 0.8173                                               \n",
      "Epoch 018 | Train Loss: 0.8224 Acc: 0.7445 | Val Loss: 0.4939 Acc: 0.8194                                               \n",
      "Epoch 019 | Train Loss: 0.7431 Acc: 0.7630 | Val Loss: 0.6855 Acc: 0.7875                                               \n",
      "Epoch 020 | Train Loss: 0.6670 Acc: 0.7877 | Val Loss: 0.5058 Acc: 0.8522                                               \n",
      "Epoch 021 | Train Loss: 0.6028 Acc: 0.8048 | Val Loss: 0.3845 Acc: 0.8675                                               \n",
      "Epoch 022 | Train Loss: 0.5415 Acc: 0.8230 | Val Loss: 0.5779 Acc: 0.8042                                               \n",
      "Epoch 023 | Train Loss: 0.4922 Acc: 0.8411 | Val Loss: 0.3456 Acc: 0.8857                                               \n",
      "Epoch 024 | Train Loss: 0.4505 Acc: 0.8511 | Val Loss: 0.2562 Acc: 0.9113                                               \n",
      "Epoch 025 | Train Loss: 0.4174 Acc: 0.8620 | Val Loss: 0.3256 Acc: 0.8967                                               \n",
      "Epoch 026 | Train Loss: 0.3702 Acc: 0.8772 | Val Loss: 0.2925 Acc: 0.8994                                               \n",
      "Epoch 027 | Train Loss: 0.3464 Acc: 0.8855 | Val Loss: 0.3857 Acc: 0.8752                                               \n",
      "Epoch 028 | Train Loss: 0.3045 Acc: 0.8983 | Val Loss: 0.2962 Acc: 0.9081                                               \n",
      "Epoch 029 | Train Loss: 0.2819 Acc: 0.9055 | Val Loss: 0.3594 Acc: 0.8916                                               \n",
      "Epoch 030 | Train Loss: 0.2586 Acc: 0.9154 | Val Loss: 0.1953 Acc: 0.9364                                               \n",
      "Epoch 031 | Train Loss: 0.2398 Acc: 0.9226 | Val Loss: 0.1937 Acc: 0.9349                                               \n",
      "Epoch 032 | Train Loss: 0.2222 Acc: 0.9268 | Val Loss: 0.2354 Acc: 0.9149                                               \n",
      "Epoch 033 | Train Loss: 0.2100 Acc: 0.9321 | Val Loss: 0.2103 Acc: 0.9301                                               \n",
      "Epoch 034 | Train Loss: 0.2012 Acc: 0.9357 | Val Loss: 0.2878 Acc: 0.9006                                               \n",
      "Epoch 035 | Train Loss: 0.1805 Acc: 0.9404 | Val Loss: 0.1229 Acc: 0.9591                                               \n",
      "Epoch 036 | Train Loss: 0.1681 Acc: 0.9424 | Val Loss: 0.1836 Acc: 0.9376                                               \n",
      "Epoch 037 | Train Loss: 0.1479 Acc: 0.9491 | Val Loss: 0.2490 Acc: 0.9119                                               \n",
      "Epoch 038 | Train Loss: 0.1456 Acc: 0.9492 | Val Loss: 0.2446 Acc: 0.9176                                               \n",
      "Epoch 039 | Train Loss: 0.1329 Acc: 0.9566 | Val Loss: 0.1430 Acc: 0.9513                                               \n",
      "Epoch 040 | Train Loss: 0.1230 Acc: 0.9598 | Val Loss: 0.1147 Acc: 0.9612                                               \n",
      "Epoch 041 | Train Loss: 0.1104 Acc: 0.9640 | Val Loss: 0.0772 Acc: 0.9764                                               \n",
      "Epoch 042 | Train Loss: 0.1061 Acc: 0.9663 | Val Loss: 0.2469 Acc: 0.9170                                               \n",
      "Epoch 043 | Train Loss: 0.0999 Acc: 0.9680 | Val Loss: 0.1013 Acc: 0.9687                                               \n",
      "Epoch 044 | Train Loss: 0.0910 Acc: 0.9690 | Val Loss: 0.0889 Acc: 0.9713                                               \n",
      "Epoch 045 | Train Loss: 0.0821 Acc: 0.9724 | Val Loss: 0.1408 Acc: 0.9555                                               \n",
      "Epoch 046 | Train Loss: 0.0797 Acc: 0.9744 | Val Loss: 0.2999 Acc: 0.9116                                               \n",
      "Epoch 047 | Train Loss: 0.0899 Acc: 0.9714 | Val Loss: 0.1303 Acc: 0.9576                                               \n",
      "Epoch 048 | Train Loss: 0.0739 Acc: 0.9757 | Val Loss: 0.0934 Acc: 0.9699                                               \n",
      "Epoch 049 | Train Loss: 0.0743 Acc: 0.9760 | Val Loss: 0.0865 Acc: 0.9725                                               \n",
      "Epoch 050 | Train Loss: 0.0585 Acc: 0.9798 | Val Loss: 0.1193 Acc: 0.9639                                               \n",
      "Epoch 051 | Train Loss: 0.0677 Acc: 0.9776 | Val Loss: 0.0912 Acc: 0.9734                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "100%|████████████████████████████████████████████████| 30/30 [31:39<00:00, 63.31s/trial, best loss: 0.03113365327967192]\n",
      "Best hyperparameters: {'batch_size': np.int64(0), 'cnn_dense': np.int64(1), 'cnn_dropout': np.float64(0.45762349207655006), 'cnn_kernel_size_1': np.int64(0), 'cnn_kernel_size_2': np.int64(1), 'cnn_kernels_1': np.int64(2), 'cnn_kernels_2': np.int64(2), 'learning_rate': np.float64(0.0003070780719393061), 'lstm_dense': np.int64(1), 'lstm_hidden_size': np.int64(3), 'lstm_layers': np.int64(4), 'optimizer': np.int64(1)}\n",
      "TPE search finished in 1899.28 seconds\n",
      "Best (raw indices): {'batch_size': np.int64(0), 'cnn_dense': np.int64(1), 'cnn_dropout': np.float64(0.45762349207655006), 'cnn_kernel_size_1': np.int64(0), 'cnn_kernel_size_2': np.int64(1), 'cnn_kernels_1': np.int64(2), 'cnn_kernels_2': np.int64(2), 'learning_rate': np.float64(0.0003070780719393061), 'lstm_dense': np.int64(1), 'lstm_hidden_size': np.int64(3), 'lstm_layers': np.int64(4), 'optimizer': np.int64(1)}\n",
      "Best (interpreted): {'batch_size': 32, 'cnn_dense': 64, 'cnn_dropout': np.float64(0.45762349207655006), 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 64, 'learning_rate': np.float64(0.0003070780719393061), 'lstm_dense': 64, 'lstm_hidden_size': 128, 'lstm_layers': 5, 'optimizer': 'rmsprop'}\n",
      "Starting TPE search...\n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 256, 'cnn_dropout': 0.6935586453104956, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 64, 'learning_rate': 0.0005448265626764874, 'lstm_dense': 256, 'lstm_hidden_size': 96, 'lstm_layers': 4, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 10.0428 Acc: 0.4519 | Val Loss: 2.6106 Acc: 0.5896                                              \n",
      "Epoch 002 | Train Loss: 2.7168 Acc: 0.5155 | Val Loss: 1.5251 Acc: 0.5645                                               \n",
      "Epoch 003 | Train Loss: 1.2775 Acc: 0.5813 | Val Loss: 1.0428 Acc: 0.6400                                               \n",
      "Epoch 004 | Train Loss: 0.7858 Acc: 0.6645 | Val Loss: 0.5351 Acc: 0.8018                                               \n",
      "Epoch 005 | Train Loss: 0.6190 Acc: 0.7366 | Val Loss: 0.5717 Acc: 0.7349                                               \n",
      "Epoch 006 | Train Loss: 0.5267 Acc: 0.7886 | Val Loss: 0.4608 Acc: 0.7958                                               \n",
      "Epoch 007 | Train Loss: 0.4476 Acc: 0.8287 | Val Loss: 0.3584 Acc: 0.8609                                               \n",
      "Epoch 008 | Train Loss: 0.3858 Acc: 0.8576 | Val Loss: 0.2550 Acc: 0.9101                                               \n",
      "Epoch 009 | Train Loss: 0.3238 Acc: 0.8850 | Val Loss: 0.2931 Acc: 0.8839                                               \n",
      "Epoch 010 | Train Loss: 0.2775 Acc: 0.9016 | Val Loss: 0.2841 Acc: 0.8878                                               \n",
      "Epoch 011 | Train Loss: 0.2494 Acc: 0.9166 | Val Loss: 0.2301 Acc: 0.8896                                               \n",
      "Epoch 012 | Train Loss: 0.2122 Acc: 0.9279 | Val Loss: 0.1631 Acc: 0.9454                                               \n",
      "Epoch 013 | Train Loss: 0.1979 Acc: 0.9337 | Val Loss: 0.1682 Acc: 0.9287                                               \n",
      "Epoch 014 | Train Loss: 0.1698 Acc: 0.9435 | Val Loss: 0.1609 Acc: 0.9424                                               \n",
      "Epoch 015 | Train Loss: 0.1623 Acc: 0.9481 | Val Loss: 0.2479 Acc: 0.9310                                               \n",
      "Epoch 016 | Train Loss: 0.1551 Acc: 0.9486 | Val Loss: 0.0939 Acc: 0.9693                                               \n",
      "Epoch 017 | Train Loss: 0.1364 Acc: 0.9532 | Val Loss: 0.1202 Acc: 0.9633                                               \n",
      "Epoch 018 | Train Loss: 0.1251 Acc: 0.9598 | Val Loss: 0.1115 Acc: 0.9624                                               \n",
      "Epoch 019 | Train Loss: 0.1113 Acc: 0.9637 | Val Loss: 0.1112 Acc: 0.9591                                               \n",
      "Epoch 020 | Train Loss: 0.1069 Acc: 0.9666 | Val Loss: 0.1533 Acc: 0.9391                                               \n",
      "Epoch 021 | Train Loss: 0.1158 Acc: 0.9655 | Val Loss: 0.1457 Acc: 0.9543                                               \n",
      "Epoch 022 | Train Loss: 0.0972 Acc: 0.9690 | Val Loss: 0.0640 Acc: 0.9797                                               \n",
      "Epoch 023 | Train Loss: 0.0974 Acc: 0.9684 | Val Loss: 0.0600 Acc: 0.9830                                               \n",
      "Epoch 024 | Train Loss: 0.0891 Acc: 0.9696 | Val Loss: 0.0920 Acc: 0.9725                                               \n",
      "Epoch 025 | Train Loss: 0.0917 Acc: 0.9710 | Val Loss: 0.1188 Acc: 0.9666                                               \n",
      "Epoch 026 | Train Loss: 0.0826 Acc: 0.9743 | Val Loss: 0.0737 Acc: 0.9767                                               \n",
      "Epoch 027 | Train Loss: 0.0774 Acc: 0.9742 | Val Loss: 0.0730 Acc: 0.9773                                               \n",
      "Epoch 028 | Train Loss: 0.0799 Acc: 0.9749 | Val Loss: 0.1163 Acc: 0.9678                                               \n",
      "Epoch 029 | Train Loss: 0.0709 Acc: 0.9778 | Val Loss: 0.0627 Acc: 0.9839                                               \n",
      "Epoch 030 | Train Loss: 0.0711 Acc: 0.9771 | Val Loss: 0.1440 Acc: 0.9567                                               \n",
      "Epoch 031 | Train Loss: 0.0790 Acc: 0.9757 | Val Loss: 0.0889 Acc: 0.9710                                               \n",
      "Epoch 032 | Train Loss: 0.0710 Acc: 0.9782 | Val Loss: 0.0766 Acc: 0.9758                                               \n",
      "Epoch 033 | Train Loss: 0.0669 Acc: 0.9790 | Val Loss: 0.0592 Acc: 0.9791                                               \n",
      "Epoch 034 | Train Loss: 0.0736 Acc: 0.9766 | Val Loss: 0.1099 Acc: 0.9516                                               \n",
      "Epoch 035 | Train Loss: 0.0730 Acc: 0.9769 | Val Loss: 0.0589 Acc: 0.9821                                               \n",
      "Epoch 036 | Train Loss: 0.0620 Acc: 0.9805 | Val Loss: 0.1164 Acc: 0.9666                                               \n",
      "Epoch 037 | Train Loss: 0.0567 Acc: 0.9823 | Val Loss: 0.0724 Acc: 0.9818                                               \n",
      "Epoch 038 | Train Loss: 0.0563 Acc: 0.9815 | Val Loss: 0.2366 Acc: 0.9379                                               \n",
      "Epoch 039 | Train Loss: 0.0699 Acc: 0.9789 | Val Loss: 0.0926 Acc: 0.9737                                               \n",
      "Epoch 040 | Train Loss: 0.0629 Acc: 0.9804 | Val Loss: 0.0859 Acc: 0.9701                                               \n",
      "Epoch 041 | Train Loss: 0.0582 Acc: 0.9818 | Val Loss: 0.1045 Acc: 0.9693                                               \n",
      "Epoch 042 | Train Loss: 0.0598 Acc: 0.9816 | Val Loss: 0.0822 Acc: 0.9740                                               \n",
      "Epoch 043 | Train Loss: 0.0742 Acc: 0.9773 | Val Loss: 0.0876 Acc: 0.9737                                               \n",
      "Epoch 044 | Train Loss: 0.0602 Acc: 0.9819 | Val Loss: 0.0797 Acc: 0.9684                                               \n",
      "Epoch 045 | Train Loss: 0.0533 Acc: 0.9834 | Val Loss: 0.0698 Acc: 0.9785                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 128, 'cnn_dropout': 0.16467462151922643, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 48, 'cnn_kernels_2': 32, 'learning_rate': 0.00043049610125090555, 'lstm_dense': 256, 'lstm_hidden_size': 32, 'lstm_layers': 1, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 41.4513 Acc: 0.3674 | Val Loss: 3.3659 Acc: 0.4490                                              \n",
      "Epoch 002 | Train Loss: 8.5813 Acc: 0.4320 | Val Loss: 2.0073 Acc: 0.4740                                               \n",
      "Epoch 003 | Train Loss: 5.2684 Acc: 0.4656 | Val Loss: 1.6997 Acc: 0.5842                                               \n",
      "Epoch 004 | Train Loss: 3.8114 Acc: 0.4698 | Val Loss: 1.0971 Acc: 0.6373                                               \n",
      "Epoch 005 | Train Loss: 3.0161 Acc: 0.4778 | Val Loss: 1.0839 Acc: 0.5433                                               \n",
      "Epoch 006 | Train Loss: 2.4292 Acc: 0.4932 | Val Loss: 1.4094 Acc: 0.5657                                               \n",
      "Epoch 007 | Train Loss: 2.2235 Acc: 0.5054 | Val Loss: 0.8337 Acc: 0.6364                                               \n",
      "Epoch 008 | Train Loss: 1.8125 Acc: 0.5177 | Val Loss: 0.9119 Acc: 0.6296                                               \n",
      "Epoch 009 | Train Loss: 1.6622 Acc: 0.5306 | Val Loss: 1.9914 Acc: 0.5749                                               \n",
      "Epoch 010 | Train Loss: 1.4924 Acc: 0.5369 | Val Loss: 1.0777 Acc: 0.5740                                               \n",
      "Epoch 011 | Train Loss: 1.4541 Acc: 0.5374 | Val Loss: 0.9334 Acc: 0.6143                                               \n",
      "Epoch 012 | Train Loss: 1.3193 Acc: 0.5524 | Val Loss: 1.1009 Acc: 0.5588                                               \n",
      "Epoch 013 | Train Loss: 1.1489 Acc: 0.5649 | Val Loss: 0.7173 Acc: 0.6899                                               \n",
      "Epoch 014 | Train Loss: 1.0921 Acc: 0.5734 | Val Loss: 0.7280 Acc: 0.6788                                               \n",
      "Epoch 015 | Train Loss: 1.0583 Acc: 0.5853 | Val Loss: 0.7911 Acc: 0.6257                                               \n",
      "Epoch 016 | Train Loss: 1.0496 Acc: 0.5803 | Val Loss: 0.7988 Acc: 0.6301                                               \n",
      "Epoch 017 | Train Loss: 0.9825 Acc: 0.6039 | Val Loss: 0.7907 Acc: 0.6857                                               \n",
      "Epoch 018 | Train Loss: 0.8901 Acc: 0.6300 | Val Loss: 0.5814 Acc: 0.7012                                               \n",
      "Epoch 019 | Train Loss: 0.7867 Acc: 0.6695 | Val Loss: 0.5183 Acc: 0.7710                                               \n",
      "Epoch 020 | Train Loss: 0.7550 Acc: 0.7000 | Val Loss: 0.4899 Acc: 0.7824                                               \n",
      "Epoch 021 | Train Loss: 0.6973 Acc: 0.7214 | Val Loss: 0.5937 Acc: 0.7481                                               \n",
      "Epoch 022 | Train Loss: 0.6203 Acc: 0.7565 | Val Loss: 0.4341 Acc: 0.8191                                               \n",
      "Epoch 023 | Train Loss: 0.5557 Acc: 0.7854 | Val Loss: 0.4927 Acc: 0.7809                                               \n",
      "Epoch 024 | Train Loss: 0.5285 Acc: 0.8054 | Val Loss: 0.4766 Acc: 0.7949                                               \n",
      "Epoch 025 | Train Loss: 0.4307 Acc: 0.8398 | Val Loss: 0.2831 Acc: 0.8976                                               \n",
      "Epoch 026 | Train Loss: 0.3826 Acc: 0.8575 | Val Loss: 0.2387 Acc: 0.9113                                               \n",
      "Epoch 027 | Train Loss: 0.3230 Acc: 0.8834 | Val Loss: 0.2586 Acc: 0.8991                                               \n",
      "Epoch 028 | Train Loss: 0.2971 Acc: 0.8951 | Val Loss: 0.3581 Acc: 0.8704                                               \n",
      "Epoch 029 | Train Loss: 0.2666 Acc: 0.9081 | Val Loss: 0.3870 Acc: 0.8555                                               \n",
      "Epoch 030 | Train Loss: 0.2565 Acc: 0.9078 | Val Loss: 0.2010 Acc: 0.9299                                               \n",
      "Epoch 031 | Train Loss: 0.2139 Acc: 0.9237 | Val Loss: 0.2016 Acc: 0.9284                                               \n",
      "Epoch 032 | Train Loss: 0.2005 Acc: 0.9292 | Val Loss: 0.1628 Acc: 0.9436                                               \n",
      "Epoch 033 | Train Loss: 0.1835 Acc: 0.9355 | Val Loss: 0.1731 Acc: 0.9427                                               \n",
      "Epoch 034 | Train Loss: 0.1637 Acc: 0.9444 | Val Loss: 0.1634 Acc: 0.9418                                               \n",
      "Epoch 035 | Train Loss: 0.1582 Acc: 0.9445 | Val Loss: 0.1386 Acc: 0.9558                                               \n",
      "Epoch 036 | Train Loss: 0.1586 Acc: 0.9472 | Val Loss: 0.1389 Acc: 0.9516                                               \n",
      "Epoch 037 | Train Loss: 0.1471 Acc: 0.9476 | Val Loss: 0.1250 Acc: 0.9579                                               \n",
      "Epoch 038 | Train Loss: 0.1190 Acc: 0.9591 | Val Loss: 0.1454 Acc: 0.9501                                               \n",
      "Epoch 039 | Train Loss: 0.1124 Acc: 0.9623 | Val Loss: 0.1410 Acc: 0.9540                                               \n",
      "Epoch 040 | Train Loss: 0.1136 Acc: 0.9583 | Val Loss: 0.1014 Acc: 0.9651                                               \n",
      "Epoch 041 | Train Loss: 0.1014 Acc: 0.9655 | Val Loss: 0.1308 Acc: 0.9555                                               \n",
      "Epoch 042 | Train Loss: 0.1034 Acc: 0.9648 | Val Loss: 0.1308 Acc: 0.9567                                               \n",
      "Epoch 043 | Train Loss: 0.0961 Acc: 0.9656 | Val Loss: 0.0849 Acc: 0.9713                                               \n",
      "Epoch 044 | Train Loss: 0.0880 Acc: 0.9707 | Val Loss: 0.1379 Acc: 0.9504                                               \n",
      "Epoch 045 | Train Loss: 0.0982 Acc: 0.9648 | Val Loss: 0.1084 Acc: 0.9654                                               \n",
      "Epoch 046 | Train Loss: 0.0726 Acc: 0.9754 | Val Loss: 0.1280 Acc: 0.9633                                               \n",
      "Epoch 047 | Train Loss: 0.0830 Acc: 0.9725 | Val Loss: 0.1285 Acc: 0.9591                                               \n",
      "Epoch 048 | Train Loss: 0.0760 Acc: 0.9752 | Val Loss: 0.1346 Acc: 0.9555                                               \n",
      "Epoch 049 | Train Loss: 0.0761 Acc: 0.9748 | Val Loss: 0.1179 Acc: 0.9609                                               \n",
      "Epoch 050 | Train Loss: 0.0699 Acc: 0.9764 | Val Loss: 0.1140 Acc: 0.9624                                               \n",
      "Epoch 051 | Train Loss: 0.0609 Acc: 0.9800 | Val Loss: 0.1308 Acc: 0.9552                                               \n",
      "Epoch 052 | Train Loss: 0.0598 Acc: 0.9810 | Val Loss: 0.0915 Acc: 0.9731                                               \n",
      "Epoch 053 | Train Loss: 0.0687 Acc: 0.9768 | Val Loss: 0.1448 Acc: 0.9543                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 256, 'cnn_dropout': 0.4814336944680211, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 48, 'cnn_kernels_2': 64, 'learning_rate': 0.007084973943694177, 'lstm_dense': 64, 'lstm_hidden_size': 64, 'lstm_layers': 3, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 5.6779 Acc: 0.4685 | Val Loss: 0.7813 Acc: 0.6445                                               \n",
      "Epoch 002 | Train Loss: 0.7769 Acc: 0.6412 | Val Loss: 0.6000 Acc: 0.7651                                               \n",
      "Epoch 003 | Train Loss: 0.6844 Acc: 0.6917 | Val Loss: 0.5630 Acc: 0.7896                                               \n",
      "Epoch 004 | Train Loss: 0.6440 Acc: 0.7149 | Val Loss: 0.5588 Acc: 0.7591                                               \n",
      "Epoch 005 | Train Loss: 0.6509 Acc: 0.6997 | Val Loss: 0.6415 Acc: 0.6770                                               \n",
      "Epoch 006 | Train Loss: 0.6151 Acc: 0.7176 | Val Loss: 0.4918 Acc: 0.8272                                               \n",
      "Epoch 007 | Train Loss: 0.6354 Acc: 0.7119 | Val Loss: 0.4909 Acc: 0.8027                                               \n",
      "Epoch 008 | Train Loss: 0.6102 Acc: 0.7216 | Val Loss: 0.5268 Acc: 0.7875                                               \n",
      "Epoch 009 | Train Loss: 0.6492 Acc: 0.7014 | Val Loss: 0.5741 Acc: 0.7743                                               \n",
      "Epoch 010 | Train Loss: 0.6887 Acc: 0.6742 | Val Loss: 0.6534 Acc: 0.6713                                               \n",
      "Epoch 011 | Train Loss: 0.6669 Acc: 0.6754 | Val Loss: 0.5586 Acc: 0.7355                                               \n",
      "Epoch 012 | Train Loss: 0.6345 Acc: 0.6965 | Val Loss: 0.5631 Acc: 0.7263                                               \n",
      "Epoch 013 | Train Loss: 0.7492 Acc: 0.6365 | Val Loss: 0.7249 Acc: 0.6701                                               \n",
      "Epoch 014 | Train Loss: 0.7760 Acc: 0.6218 | Val Loss: 0.7841 Acc: 0.5988                                               \n",
      "Epoch 015 | Train Loss: 0.8106 Acc: 0.6056 | Val Loss: 0.7194 Acc: 0.6528                                               \n",
      "Epoch 016 | Train Loss: 0.7835 Acc: 0.6164 | Val Loss: 0.7980 Acc: 0.5746                                               \n",
      "Epoch 017 | Train Loss: 0.8021 Acc: 0.6027 | Val Loss: 0.9311 Acc: 0.6203                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 128, 'cnn_dropout': 0.09013625918935793, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 32, 'learning_rate': 0.000567922458618094, 'lstm_dense': 128, 'lstm_hidden_size': 32, 'lstm_layers': 5, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 8.9900 Acc: 0.4096 | Val Loss: 1.2779 Acc: 0.4421                                               \n",
      "Epoch 002 | Train Loss: 1.2624 Acc: 0.4422 | Val Loss: 1.2525 Acc: 0.4421                                               \n",
      "Epoch 003 | Train Loss: 1.2484 Acc: 0.4422 | Val Loss: 1.2454 Acc: 0.4421                                               \n",
      "Epoch 004 | Train Loss: 1.2444 Acc: 0.4422 | Val Loss: 1.2432 Acc: 0.4421                                               \n",
      "Epoch 005 | Train Loss: 1.2430 Acc: 0.4422 | Val Loss: 1.2424 Acc: 0.4421                                               \n",
      "Epoch 006 | Train Loss: 1.2424 Acc: 0.4422 | Val Loss: 1.2421 Acc: 0.4421                                               \n",
      "Epoch 007 | Train Loss: 1.2422 Acc: 0.4422 | Val Loss: 1.2421 Acc: 0.4421                                               \n",
      "Epoch 008 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 009 | Train Loss: 1.2422 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 010 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 011 | Train Loss: 1.2422 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 012 | Train Loss: 1.2422 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 013 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 014 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 015 | Train Loss: 1.2422 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 016 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 017 | Train Loss: 1.2422 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 018 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 019 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 020 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2422 Acc: 0.4421                                               \n",
      "Epoch 021 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 022 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2420 Acc: 0.4421                                               \n",
      "Epoch 023 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2420 Acc: 0.4421                                               \n",
      "Epoch 024 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 025 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 026 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 027 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 028 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 029 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2423 Acc: 0.4421                                               \n",
      "Epoch 030 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 031 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 032 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 033 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 034 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 035 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 036 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 037 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 038 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 039 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 040 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 041 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 042 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 043 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2424 Acc: 0.4421                                               \n",
      "Epoch 044 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 045 | Train Loss: 1.2417 Acc: 0.4422 | Val Loss: 1.2420 Acc: 0.4421                                               \n",
      "Epoch 046 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 047 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 048 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 049 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2415 Acc: 0.4421                                               \n",
      "Epoch 050 | Train Loss: 1.2418 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 051 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2415 Acc: 0.4421                                               \n",
      "Epoch 052 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2415 Acc: 0.4421                                               \n",
      "Epoch 053 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2415 Acc: 0.4421                                               \n",
      "Epoch 054 | Train Loss: 1.2418 Acc: 0.4422 | Val Loss: 1.2415 Acc: 0.4421                                               \n",
      "Epoch 055 | Train Loss: 1.2418 Acc: 0.4422 | Val Loss: 1.2413 Acc: 0.4421                                               \n",
      "Epoch 056 | Train Loss: 1.2416 Acc: 0.4422 | Val Loss: 1.2415 Acc: 0.4421                                               \n",
      "Epoch 057 | Train Loss: 1.2417 Acc: 0.4422 | Val Loss: 1.2413 Acc: 0.4421                                               \n",
      "Epoch 058 | Train Loss: 1.2416 Acc: 0.4422 | Val Loss: 1.2413 Acc: 0.4421                                               \n",
      "Epoch 059 | Train Loss: 1.2415 Acc: 0.4422 | Val Loss: 1.2413 Acc: 0.4421                                               \n",
      "Epoch 060 | Train Loss: 1.2415 Acc: 0.4422 | Val Loss: 1.2411 Acc: 0.4421                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 256, 'cnn_dropout': 0.12364274331101975, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 64, 'cnn_kernels_2': 32, 'learning_rate': 1.0166729379614166e-05, 'lstm_dense': 64, 'lstm_hidden_size': 32, 'lstm_layers': 6, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 16.4393 Acc: 0.4059 | Val Loss: 1.7621 Acc: 0.4806                                              \n",
      "Epoch 002 | Train Loss: 3.2183 Acc: 0.4426 | Val Loss: 1.0346 Acc: 0.5946                                               \n",
      "Epoch 003 | Train Loss: 1.9953 Acc: 0.4681 | Val Loss: 0.8736 Acc: 0.5591                                               \n",
      "Epoch 004 | Train Loss: 1.4719 Acc: 0.4804 | Val Loss: 0.8256 Acc: 0.5758                                               \n",
      "Epoch 005 | Train Loss: 1.2750 Acc: 0.4919 | Val Loss: 0.8401 Acc: 0.6176                                               \n",
      "Epoch 006 | Train Loss: 1.1445 Acc: 0.5057 | Val Loss: 0.7982 Acc: 0.5316                                               \n",
      "Epoch 007 | Train Loss: 1.0942 Acc: 0.5127 | Val Loss: 0.8640 Acc: 0.5958                                               \n",
      "Epoch 008 | Train Loss: 1.0504 Acc: 0.5106 | Val Loss: 0.7815 Acc: 0.6791                                               \n",
      "Epoch 009 | Train Loss: 1.0174 Acc: 0.5205 | Val Loss: 0.7759 Acc: 0.5893                                               \n",
      "Epoch 010 | Train Loss: 0.9925 Acc: 0.5256 | Val Loss: 0.7935 Acc: 0.6606                                               \n",
      "Epoch 011 | Train Loss: 0.9575 Acc: 0.5302 | Val Loss: 0.7601 Acc: 0.5979                                               \n",
      "Epoch 012 | Train Loss: 0.9404 Acc: 0.5378 | Val Loss: 0.8512 Acc: 0.5200                                               \n",
      "Epoch 013 | Train Loss: 0.9280 Acc: 0.5435 | Val Loss: 0.7564 Acc: 0.6215                                               \n",
      "Epoch 014 | Train Loss: 0.9451 Acc: 0.5454 | Val Loss: 0.7671 Acc: 0.6800                                               \n",
      "Epoch 015 | Train Loss: 0.9192 Acc: 0.5510 | Val Loss: 0.7534 Acc: 0.6409                                               \n",
      "Epoch 016 | Train Loss: 0.8960 Acc: 0.5617 | Val Loss: 0.7461 Acc: 0.6400                                               \n",
      "Epoch 017 | Train Loss: 0.8935 Acc: 0.5696 | Val Loss: 0.7615 Acc: 0.6221                                               \n",
      "Epoch 018 | Train Loss: 0.8797 Acc: 0.5701 | Val Loss: 0.7438 Acc: 0.6701                                               \n",
      "Epoch 019 | Train Loss: 0.8690 Acc: 0.5771 | Val Loss: 0.7209 Acc: 0.6851                                               \n",
      "Epoch 020 | Train Loss: 0.8587 Acc: 0.5764 | Val Loss: 0.7291 Acc: 0.6785                                               \n",
      "Epoch 021 | Train Loss: 0.8597 Acc: 0.5767 | Val Loss: 0.7378 Acc: 0.6266                                               \n",
      "Epoch 022 | Train Loss: 0.8559 Acc: 0.5842 | Val Loss: 0.6984 Acc: 0.7018                                               \n",
      "Epoch 023 | Train Loss: 0.8601 Acc: 0.5835 | Val Loss: 0.7061 Acc: 0.6881                                               \n",
      "Epoch 024 | Train Loss: 0.8370 Acc: 0.5964 | Val Loss: 0.7124 Acc: 0.6618                                               \n",
      "Epoch 025 | Train Loss: 0.8309 Acc: 0.5999 | Val Loss: 0.6930 Acc: 0.6815                                               \n",
      "Epoch 026 | Train Loss: 0.8289 Acc: 0.5995 | Val Loss: 0.6803 Acc: 0.7439                                               \n",
      "Epoch 027 | Train Loss: 0.8309 Acc: 0.6026 | Val Loss: 0.6831 Acc: 0.7176                                               \n",
      "Epoch 028 | Train Loss: 0.8240 Acc: 0.6104 | Val Loss: 0.6690 Acc: 0.6985                                               \n",
      "Epoch 029 | Train Loss: 0.8167 Acc: 0.6130 | Val Loss: 0.6924 Acc: 0.7057                                               \n",
      "Epoch 030 | Train Loss: 0.8098 Acc: 0.6130 | Val Loss: 0.6676 Acc: 0.7039                                               \n",
      "Epoch 031 | Train Loss: 0.8176 Acc: 0.6150 | Val Loss: 0.6618 Acc: 0.7161                                               \n",
      "Epoch 032 | Train Loss: 0.8003 Acc: 0.6204 | Val Loss: 0.6618 Acc: 0.7639                                               \n",
      "Epoch 033 | Train Loss: 0.8042 Acc: 0.6177 | Val Loss: 0.6599 Acc: 0.7379                                               \n",
      "Epoch 034 | Train Loss: 0.7958 Acc: 0.6207 | Val Loss: 0.6575 Acc: 0.7146                                               \n",
      "Epoch 035 | Train Loss: 0.7777 Acc: 0.6335 | Val Loss: 0.6791 Acc: 0.7009                                               \n",
      "Epoch 036 | Train Loss: 0.7937 Acc: 0.6264 | Val Loss: 0.6471 Acc: 0.7415                                               \n",
      "Epoch 037 | Train Loss: 0.7855 Acc: 0.6315 | Val Loss: 0.6433 Acc: 0.7818                                               \n",
      "Epoch 038 | Train Loss: 0.7821 Acc: 0.6371 | Val Loss: 0.6460 Acc: 0.7367                                               \n",
      "Epoch 039 | Train Loss: 0.7909 Acc: 0.6289 | Val Loss: 0.6500 Acc: 0.7257                                               \n",
      "Epoch 040 | Train Loss: 0.7803 Acc: 0.6346 | Val Loss: 0.6516 Acc: 0.7385                                               \n",
      "Epoch 041 | Train Loss: 0.7776 Acc: 0.6428 | Val Loss: 0.6541 Acc: 0.7215                                               \n",
      "Epoch 042 | Train Loss: 0.7681 Acc: 0.6430 | Val Loss: 0.6332 Acc: 0.7188                                               \n",
      "Epoch 043 | Train Loss: 0.7628 Acc: 0.6496 | Val Loss: 0.6170 Acc: 0.7490                                               \n",
      "Epoch 044 | Train Loss: 0.7635 Acc: 0.6526 | Val Loss: 0.6333 Acc: 0.7585                                               \n",
      "Epoch 045 | Train Loss: 0.7548 Acc: 0.6556 | Val Loss: 0.6177 Acc: 0.8081                                               \n",
      "Epoch 046 | Train Loss: 0.7593 Acc: 0.6572 | Val Loss: 0.6179 Acc: 0.7615                                               \n",
      "Epoch 047 | Train Loss: 0.7571 Acc: 0.6474 | Val Loss: 0.6231 Acc: 0.7588                                               \n",
      "Epoch 048 | Train Loss: 0.7560 Acc: 0.6494 | Val Loss: 0.6098 Acc: 0.7761                                               \n",
      "Epoch 049 | Train Loss: 0.7448 Acc: 0.6586 | Val Loss: 0.6043 Acc: 0.7785                                               \n",
      "Epoch 050 | Train Loss: 0.7400 Acc: 0.6695 | Val Loss: 0.6070 Acc: 0.7242                                               \n",
      "Epoch 051 | Train Loss: 0.7407 Acc: 0.6643 | Val Loss: 0.6077 Acc: 0.7696                                               \n",
      "Epoch 052 | Train Loss: 0.7263 Acc: 0.6715 | Val Loss: 0.6122 Acc: 0.7591                                               \n",
      "Epoch 053 | Train Loss: 0.7319 Acc: 0.6712 | Val Loss: 0.5929 Acc: 0.7731                                               \n",
      "Epoch 054 | Train Loss: 0.7232 Acc: 0.6715 | Val Loss: 0.5862 Acc: 0.7788                                               \n",
      "Epoch 055 | Train Loss: 0.7335 Acc: 0.6714 | Val Loss: 0.5828 Acc: 0.7946                                               \n",
      "Epoch 056 | Train Loss: 0.7294 Acc: 0.6686 | Val Loss: 0.5836 Acc: 0.7916                                               \n",
      "Epoch 057 | Train Loss: 0.7224 Acc: 0.6748 | Val Loss: 0.5940 Acc: 0.7615                                               \n",
      "Epoch 058 | Train Loss: 0.7218 Acc: 0.6743 | Val Loss: 0.5786 Acc: 0.7878                                               \n",
      "Epoch 059 | Train Loss: 0.7190 Acc: 0.6741 | Val Loss: 0.5766 Acc: 0.7627                                               \n",
      "Epoch 060 | Train Loss: 0.7103 Acc: 0.6841 | Val Loss: 0.5705 Acc: 0.8060                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 32, 'cnn_dropout': 0.39196977747536244, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 16, 'cnn_kernels_2': 64, 'learning_rate': 0.0005890530830478573, 'lstm_dense': 32, 'lstm_hidden_size': 96, 'lstm_layers': 2, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 18.1827 Acc: 0.3703 | Val Loss: 3.1266 Acc: 0.5113                                              \n",
      "Epoch 002 | Train Loss: 4.1192 Acc: 0.4317 | Val Loss: 1.2400 Acc: 0.5442                                               \n",
      "Epoch 003 | Train Loss: 2.5073 Acc: 0.4956 | Val Loss: 1.0995 Acc: 0.5904                                               \n",
      "Epoch 004 | Train Loss: 1.8103 Acc: 0.5397 | Val Loss: 0.9802 Acc: 0.5913                                               \n",
      "Epoch 005 | Train Loss: 1.3036 Acc: 0.6039 | Val Loss: 0.8988 Acc: 0.6457                                               \n",
      "Epoch 006 | Train Loss: 1.0505 Acc: 0.6556 | Val Loss: 0.6370 Acc: 0.7263                                               \n",
      "Epoch 007 | Train Loss: 0.8946 Acc: 0.6879 | Val Loss: 0.6236 Acc: 0.7621                                               \n",
      "Epoch 008 | Train Loss: 0.7676 Acc: 0.7212 | Val Loss: 0.6230 Acc: 0.7391                                               \n",
      "Epoch 009 | Train Loss: 0.6857 Acc: 0.7532 | Val Loss: 0.5575 Acc: 0.8101                                               \n",
      "Epoch 010 | Train Loss: 0.5839 Acc: 0.7849 | Val Loss: 0.3473 Acc: 0.8576                                               \n",
      "Epoch 011 | Train Loss: 0.5427 Acc: 0.8063 | Val Loss: 0.3541 Acc: 0.8678                                               \n",
      "Epoch 012 | Train Loss: 0.4087 Acc: 0.8525 | Val Loss: 0.3608 Acc: 0.8660                                               \n",
      "Epoch 013 | Train Loss: 0.3772 Acc: 0.8626 | Val Loss: 0.3102 Acc: 0.8719                                               \n",
      "Epoch 014 | Train Loss: 0.3351 Acc: 0.8781 | Val Loss: 0.3391 Acc: 0.8666                                               \n",
      "Epoch 015 | Train Loss: 0.2894 Acc: 0.8975 | Val Loss: 0.2364 Acc: 0.9048                                               \n",
      "Epoch 016 | Train Loss: 0.2580 Acc: 0.9097 | Val Loss: 0.2127 Acc: 0.9161                                               \n",
      "Epoch 017 | Train Loss: 0.2350 Acc: 0.9168 | Val Loss: 0.2020 Acc: 0.9307                                               \n",
      "Epoch 018 | Train Loss: 0.2084 Acc: 0.9304 | Val Loss: 0.1954 Acc: 0.9334                                               \n",
      "Epoch 019 | Train Loss: 0.1981 Acc: 0.9314 | Val Loss: 0.1810 Acc: 0.9355                                               \n",
      "Epoch 020 | Train Loss: 0.1675 Acc: 0.9404 | Val Loss: 0.1712 Acc: 0.9418                                               \n",
      "Epoch 021 | Train Loss: 0.1521 Acc: 0.9486 | Val Loss: 0.1427 Acc: 0.9469                                               \n",
      "Epoch 022 | Train Loss: 0.1564 Acc: 0.9475 | Val Loss: 0.1407 Acc: 0.9504                                               \n",
      "Epoch 023 | Train Loss: 0.1543 Acc: 0.9492 | Val Loss: 0.2116 Acc: 0.9284                                               \n",
      "Epoch 024 | Train Loss: 0.1278 Acc: 0.9577 | Val Loss: 0.1892 Acc: 0.9328                                               \n",
      "Epoch 025 | Train Loss: 0.1311 Acc: 0.9561 | Val Loss: 0.1351 Acc: 0.9570                                               \n",
      "Epoch 026 | Train Loss: 0.1123 Acc: 0.9640 | Val Loss: 0.1634 Acc: 0.9484                                               \n",
      "Epoch 027 | Train Loss: 0.1049 Acc: 0.9666 | Val Loss: 0.2285 Acc: 0.9278                                               \n",
      "Epoch 028 | Train Loss: 0.0955 Acc: 0.9693 | Val Loss: 0.1535 Acc: 0.9510                                               \n",
      "Epoch 029 | Train Loss: 0.0944 Acc: 0.9685 | Val Loss: 0.1536 Acc: 0.9519                                               \n",
      "Epoch 030 | Train Loss: 0.0867 Acc: 0.9713 | Val Loss: 0.0976 Acc: 0.9675                                               \n",
      "Epoch 031 | Train Loss: 0.0950 Acc: 0.9675 | Val Loss: 0.1632 Acc: 0.9451                                               \n",
      "Epoch 032 | Train Loss: 0.0819 Acc: 0.9735 | Val Loss: 0.1143 Acc: 0.9630                                               \n",
      "Epoch 033 | Train Loss: 0.0723 Acc: 0.9754 | Val Loss: 0.1435 Acc: 0.9549                                               \n",
      "Epoch 034 | Train Loss: 0.0791 Acc: 0.9738 | Val Loss: 0.1253 Acc: 0.9603                                               \n",
      "Epoch 035 | Train Loss: 0.0728 Acc: 0.9764 | Val Loss: 0.1262 Acc: 0.9600                                               \n",
      "Epoch 036 | Train Loss: 0.0677 Acc: 0.9779 | Val Loss: 0.0824 Acc: 0.9746                                               \n",
      "Epoch 037 | Train Loss: 0.0665 Acc: 0.9784 | Val Loss: 0.0826 Acc: 0.9743                                               \n",
      "Epoch 038 | Train Loss: 0.0668 Acc: 0.9767 | Val Loss: 0.1784 Acc: 0.9445                                               \n",
      "Epoch 039 | Train Loss: 0.0594 Acc: 0.9810 | Val Loss: 0.1436 Acc: 0.9558                                               \n",
      "Epoch 040 | Train Loss: 0.0572 Acc: 0.9810 | Val Loss: 0.0910 Acc: 0.9716                                               \n",
      "Epoch 041 | Train Loss: 0.0530 Acc: 0.9831 | Val Loss: 0.1075 Acc: 0.9699                                               \n",
      "Epoch 042 | Train Loss: 0.0576 Acc: 0.9810 | Val Loss: 0.1411 Acc: 0.9615                                               \n",
      "Epoch 043 | Train Loss: 0.0582 Acc: 0.9812 | Val Loss: 0.0988 Acc: 0.9731                                               \n",
      "Epoch 044 | Train Loss: 0.0500 Acc: 0.9848 | Val Loss: 0.1034 Acc: 0.9713                                               \n",
      "Epoch 045 | Train Loss: 0.0502 Acc: 0.9848 | Val Loss: 0.1520 Acc: 0.9603                                               \n",
      "Epoch 046 | Train Loss: 0.0597 Acc: 0.9795 | Val Loss: 0.1099 Acc: 0.9660                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 128, 'cnn_dropout': 0.49216696235217666, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 32, 'learning_rate': 0.002691767196684427, 'lstm_dense': 32, 'lstm_hidden_size': 32, 'lstm_layers': 1, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 5.3525 Acc: 0.4073 | Val Loss: 1.1228 Acc: 0.4901                                               \n",
      "Epoch 002 | Train Loss: 1.0931 Acc: 0.4632 | Val Loss: 0.9014 Acc: 0.5397                                               \n",
      "Epoch 003 | Train Loss: 0.9430 Acc: 0.5035 | Val Loss: 0.8575 Acc: 0.5340                                               \n",
      "Epoch 004 | Train Loss: 0.9326 Acc: 0.5124 | Val Loss: 0.8354 Acc: 0.5197                                               \n",
      "Epoch 005 | Train Loss: 0.8914 Acc: 0.5315 | Val Loss: 0.8159 Acc: 0.6373                                               \n",
      "Epoch 006 | Train Loss: 0.8537 Acc: 0.5713 | Val Loss: 0.7842 Acc: 0.5681                                               \n",
      "Epoch 007 | Train Loss: 0.8291 Acc: 0.5759 | Val Loss: 0.7769 Acc: 0.5672                                               \n",
      "Epoch 008 | Train Loss: 0.8114 Acc: 0.5990 | Val Loss: 0.7966 Acc: 0.5851                                               \n",
      "Epoch 009 | Train Loss: 0.8141 Acc: 0.5962 | Val Loss: 0.7346 Acc: 0.6654                                               \n",
      "Epoch 010 | Train Loss: 0.8209 Acc: 0.5943 | Val Loss: 0.7746 Acc: 0.5961                                               \n",
      "Epoch 011 | Train Loss: 0.8141 Acc: 0.6061 | Val Loss: 0.8528 Acc: 0.4854                                               \n",
      "Epoch 012 | Train Loss: 0.8422 Acc: 0.5872 | Val Loss: 0.7391 Acc: 0.6048                                               \n",
      "Epoch 013 | Train Loss: 0.8164 Acc: 0.6000 | Val Loss: 0.8217 Acc: 0.6313                                               \n",
      "Epoch 014 | Train Loss: 0.8519 Acc: 0.5964 | Val Loss: 0.7659 Acc: 0.6352                                               \n",
      "Epoch 015 | Train Loss: 0.8000 Acc: 0.6144 | Val Loss: 0.7186 Acc: 0.6507                                               \n",
      "Epoch 016 | Train Loss: 0.7965 Acc: 0.6048 | Val Loss: 0.7907 Acc: 0.6161                                               \n",
      "Epoch 017 | Train Loss: 0.8049 Acc: 0.6086 | Val Loss: 0.9860 Acc: 0.5266                                               \n",
      "Epoch 018 | Train Loss: 0.8371 Acc: 0.5885 | Val Loss: 0.8346 Acc: 0.5994                                               \n",
      "Epoch 019 | Train Loss: 0.8029 Acc: 0.6106 | Val Loss: 1.4131 Acc: 0.5537                                               \n",
      "Epoch 020 | Train Loss: 0.8192 Acc: 0.5916 | Val Loss: 0.7772 Acc: 0.6099                                               \n",
      "Epoch 021 | Train Loss: 0.8340 Acc: 0.5951 | Val Loss: 0.7170 Acc: 0.6893                                               \n",
      "Epoch 022 | Train Loss: 0.8397 Acc: 0.5918 | Val Loss: 0.7927 Acc: 0.6236                                               \n",
      "Epoch 023 | Train Loss: 0.8597 Acc: 0.5806 | Val Loss: 0.7726 Acc: 0.6170                                               \n",
      "Epoch 024 | Train Loss: 0.8669 Acc: 0.5903 | Val Loss: 0.7595 Acc: 0.6325                                               \n",
      "Epoch 025 | Train Loss: 0.8462 Acc: 0.5824 | Val Loss: 0.7718 Acc: 0.6293                                               \n",
      "Epoch 026 | Train Loss: 0.8633 Acc: 0.5773 | Val Loss: 0.8367 Acc: 0.6343                                               \n",
      "Epoch 027 | Train Loss: 0.8495 Acc: 0.5906 | Val Loss: 0.7488 Acc: 0.6594                                               \n",
      "Epoch 028 | Train Loss: 0.8608 Acc: 0.5903 | Val Loss: 1.0663 Acc: 0.5009                                               \n",
      "Epoch 029 | Train Loss: 0.8483 Acc: 0.5815 | Val Loss: 0.8245 Acc: 0.5734                                               \n",
      "Epoch 030 | Train Loss: 0.8508 Acc: 0.5793 | Val Loss: 0.7580 Acc: 0.6099                                               \n",
      "Epoch 031 | Train Loss: 0.8602 Acc: 0.5814 | Val Loss: 0.7962 Acc: 0.6245                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 36, 'cnn_dense': 128, 'cnn_dropout': 0.5969617985521046, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 64, 'cnn_kernels_2': 32, 'learning_rate': 0.0031124774590143077, 'lstm_dense': 128, 'lstm_hidden_size': 128, 'lstm_layers': 5, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 7.5938 Acc: 0.4373 | Val Loss: 1.2798 Acc: 0.4421                                               \n",
      "Epoch 002 | Train Loss: 1.2579 Acc: 0.4422 | Val Loss: 1.2970 Acc: 0.4421                                               \n",
      "Epoch 003 | Train Loss: 1.2503 Acc: 0.4422 | Val Loss: 1.2425 Acc: 0.4421                                               \n",
      "Epoch 004 | Train Loss: 1.2429 Acc: 0.4422 | Val Loss: 1.2426 Acc: 0.4421                                               \n",
      "Epoch 005 | Train Loss: 1.2430 Acc: 0.4422 | Val Loss: 1.2427 Acc: 0.4421                                               \n",
      "Epoch 006 | Train Loss: 1.2429 Acc: 0.4422 | Val Loss: 1.2415 Acc: 0.4421                                               \n",
      "Epoch 007 | Train Loss: 1.2431 Acc: 0.4422 | Val Loss: 1.2415 Acc: 0.4421                                               \n",
      "Epoch 008 | Train Loss: 1.2428 Acc: 0.4422 | Val Loss: 1.2435 Acc: 0.4421                                               \n",
      "Epoch 009 | Train Loss: 1.2429 Acc: 0.4422 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 010 | Train Loss: 1.2424 Acc: 0.4422 | Val Loss: 1.2409 Acc: 0.4421                                               \n",
      "Epoch 011 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2407 Acc: 0.4421                                               \n",
      "Epoch 012 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2411 Acc: 0.4421                                               \n",
      "Epoch 013 | Train Loss: 1.2417 Acc: 0.4422 | Val Loss: 1.2398 Acc: 0.4421                                               \n",
      "Epoch 014 | Train Loss: 1.2403 Acc: 0.4422 | Val Loss: 1.2380 Acc: 0.4421                                               \n",
      "Epoch 015 | Train Loss: 1.2360 Acc: 0.4422 | Val Loss: 1.2381 Acc: 0.4421                                               \n",
      "Epoch 016 | Train Loss: 1.1909 Acc: 0.4606 | Val Loss: 1.0982 Acc: 0.5167                                               \n",
      "Epoch 017 | Train Loss: 1.1241 Acc: 0.4905 | Val Loss: 1.0777 Acc: 0.5048                                               \n",
      "Epoch 018 | Train Loss: 1.0777 Acc: 0.4953 | Val Loss: 1.0924 Acc: 0.5179                                               \n",
      "Epoch 019 | Train Loss: 1.0398 Acc: 0.5032 | Val Loss: 0.9734 Acc: 0.5316                                               \n",
      "Epoch 020 | Train Loss: 1.0187 Acc: 0.5255 | Val Loss: 0.9578 Acc: 0.5424                                               \n",
      "Epoch 021 | Train Loss: 0.9924 Acc: 0.5450 | Val Loss: 0.9908 Acc: 0.5260                                               \n",
      "Epoch 022 | Train Loss: 0.9580 Acc: 0.5585 | Val Loss: 1.1221 Acc: 0.5379                                               \n",
      "Epoch 023 | Train Loss: 0.9467 Acc: 0.5653 | Val Loss: 0.8927 Acc: 0.5994                                               \n",
      "Epoch 024 | Train Loss: 0.9256 Acc: 0.5769 | Val Loss: 0.9319 Acc: 0.5907                                               \n",
      "Epoch 025 | Train Loss: 0.9190 Acc: 0.5793 | Val Loss: 0.9770 Acc: 0.5504                                               \n",
      "Epoch 026 | Train Loss: 0.8956 Acc: 0.6009 | Val Loss: 0.9355 Acc: 0.5899                                               \n",
      "Epoch 027 | Train Loss: 0.8808 Acc: 0.6074 | Val Loss: 1.0595 Acc: 0.5537                                               \n",
      "Epoch 028 | Train Loss: 0.8615 Acc: 0.6197 | Val Loss: 1.3226 Acc: 0.5325                                               \n",
      "Epoch 029 | Train Loss: 0.8445 Acc: 0.6313 | Val Loss: 1.0990 Acc: 0.5513                                               \n",
      "Epoch 030 | Train Loss: 0.8402 Acc: 0.6343 | Val Loss: 0.8508 Acc: 0.6537                                               \n",
      "Epoch 031 | Train Loss: 0.8284 Acc: 0.6453 | Val Loss: 1.0646 Acc: 0.5934                                               \n",
      "Epoch 032 | Train Loss: 0.8101 Acc: 0.6584 | Val Loss: 1.1284 Acc: 0.5570                                               \n",
      "Epoch 033 | Train Loss: 0.8148 Acc: 0.6517 | Val Loss: 0.9532 Acc: 0.5848                                               \n",
      "Epoch 034 | Train Loss: 0.8086 Acc: 0.6582 | Val Loss: 0.7788 Acc: 0.6701                                               \n",
      "Epoch 035 | Train Loss: 0.8022 Acc: 0.6656 | Val Loss: 0.8949 Acc: 0.6576                                               \n",
      "Epoch 036 | Train Loss: 0.7980 Acc: 0.6696 | Val Loss: 0.7821 Acc: 0.6812                                               \n",
      "Epoch 037 | Train Loss: 0.7620 Acc: 0.6815 | Val Loss: 0.8540 Acc: 0.6675                                               \n",
      "Epoch 038 | Train Loss: 0.7322 Acc: 0.6973 | Val Loss: 0.7203 Acc: 0.7137                                               \n",
      "Epoch 039 | Train Loss: 0.6935 Acc: 0.7093 | Val Loss: 0.8536 Acc: 0.6896                                               \n",
      "Epoch 040 | Train Loss: 0.6848 Acc: 0.7197 | Val Loss: 0.7143 Acc: 0.7313                                               \n",
      "Epoch 041 | Train Loss: 0.7251 Acc: 0.6985 | Val Loss: 0.8588 Acc: 0.6860                                               \n",
      "Epoch 042 | Train Loss: 0.7350 Acc: 0.7006 | Val Loss: 0.8373 Acc: 0.7104                                               \n",
      "Epoch 043 | Train Loss: 0.7012 Acc: 0.7192 | Val Loss: 0.7053 Acc: 0.7233                                               \n",
      "Epoch 044 | Train Loss: 0.6384 Acc: 0.7472 | Val Loss: 0.7679 Acc: 0.7101                                               \n",
      "Epoch 045 | Train Loss: 0.6312 Acc: 0.7478 | Val Loss: 0.5506 Acc: 0.7884                                               \n",
      "Epoch 046 | Train Loss: 0.6200 Acc: 0.7564 | Val Loss: 0.8039 Acc: 0.7385                                               \n",
      "Epoch 047 | Train Loss: 0.5961 Acc: 0.7689 | Val Loss: 0.6211 Acc: 0.7767                                               \n",
      "Epoch 048 | Train Loss: 0.5932 Acc: 0.7717 | Val Loss: 0.6120 Acc: 0.7937                                               \n",
      "Epoch 049 | Train Loss: 0.5733 Acc: 0.7757 | Val Loss: 0.6980 Acc: 0.7415                                               \n",
      "Epoch 050 | Train Loss: 0.5653 Acc: 0.7768 | Val Loss: 0.6061 Acc: 0.7919                                               \n",
      "Epoch 051 | Train Loss: 0.5998 Acc: 0.7650 | Val Loss: 0.6820 Acc: 0.7657                                               \n",
      "Epoch 052 | Train Loss: 0.6315 Acc: 0.7504 | Val Loss: 0.7090 Acc: 0.7716                                               \n",
      "Epoch 053 | Train Loss: 0.6945 Acc: 0.7217 | Val Loss: 0.6861 Acc: 0.7579                                               \n",
      "Epoch 054 | Train Loss: 0.6308 Acc: 0.7550 | Val Loss: 0.7472 Acc: 0.7316                                               \n",
      "Epoch 055 | Train Loss: 0.6571 Acc: 0.7421 | Val Loss: 0.5511 Acc: 0.8042                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 64, 'cnn_dropout': 0.5492031565369982, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 32, 'cnn_kernels_2': 96, 'learning_rate': 0.00011793159316662311, 'lstm_dense': 64, 'lstm_hidden_size': 96, 'lstm_layers': 3, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 19.2164 Acc: 0.3525 | Val Loss: 2.9355 Acc: 0.5233                                              \n",
      "Epoch 002 | Train Loss: 5.3052 Acc: 0.4357 | Val Loss: 1.3302 Acc: 0.5397                                               \n",
      "Epoch 003 | Train Loss: 3.2168 Acc: 0.4653 | Val Loss: 1.1394 Acc: 0.6045                                               \n",
      "Epoch 004 | Train Loss: 2.5003 Acc: 0.5046 | Val Loss: 1.0712 Acc: 0.6164                                               \n",
      "Epoch 005 | Train Loss: 1.9991 Acc: 0.5235 | Val Loss: 0.8839 Acc: 0.6516                                               \n",
      "Epoch 006 | Train Loss: 1.5114 Acc: 0.5792 | Val Loss: 0.7709 Acc: 0.6896                                               \n",
      "Epoch 007 | Train Loss: 1.2754 Acc: 0.6285 | Val Loss: 0.7216 Acc: 0.7254                                               \n",
      "Epoch 008 | Train Loss: 1.0946 Acc: 0.6675 | Val Loss: 0.6893 Acc: 0.7534                                               \n",
      "Epoch 009 | Train Loss: 0.9487 Acc: 0.6971 | Val Loss: 0.6859 Acc: 0.7615                                               \n",
      "Epoch 010 | Train Loss: 0.8048 Acc: 0.7308 | Val Loss: 0.5970 Acc: 0.7636                                               \n",
      "Epoch 011 | Train Loss: 0.7222 Acc: 0.7604 | Val Loss: 0.4373 Acc: 0.8430                                               \n",
      "Epoch 012 | Train Loss: 0.5971 Acc: 0.7922 | Val Loss: 0.4385 Acc: 0.8433                                               \n",
      "Epoch 013 | Train Loss: 0.5258 Acc: 0.8215 | Val Loss: 0.3852 Acc: 0.8576                                               \n",
      "Epoch 014 | Train Loss: 0.4572 Acc: 0.8436 | Val Loss: 0.4300 Acc: 0.8463                                               \n",
      "Epoch 015 | Train Loss: 0.4222 Acc: 0.8531 | Val Loss: 0.3566 Acc: 0.8758                                               \n",
      "Epoch 016 | Train Loss: 0.3535 Acc: 0.8763 | Val Loss: 0.2895 Acc: 0.9066                                               \n",
      "Epoch 017 | Train Loss: 0.3227 Acc: 0.8921 | Val Loss: 0.2476 Acc: 0.9200                                               \n",
      "Epoch 018 | Train Loss: 0.3023 Acc: 0.8984 | Val Loss: 0.2427 Acc: 0.9188                                               \n",
      "Epoch 019 | Train Loss: 0.2727 Acc: 0.9075 | Val Loss: 0.2428 Acc: 0.9104                                               \n",
      "Epoch 020 | Train Loss: 0.2466 Acc: 0.9167 | Val Loss: 0.2867 Acc: 0.9072                                               \n",
      "Epoch 021 | Train Loss: 0.2231 Acc: 0.9272 | Val Loss: 0.2472 Acc: 0.9170                                               \n",
      "Epoch 022 | Train Loss: 0.2093 Acc: 0.9331 | Val Loss: 0.1867 Acc: 0.9379                                               \n",
      "Epoch 023 | Train Loss: 0.1952 Acc: 0.9378 | Val Loss: 0.1902 Acc: 0.9370                                               \n",
      "Epoch 024 | Train Loss: 0.1893 Acc: 0.9391 | Val Loss: 0.3724 Acc: 0.8869                                               \n",
      "Epoch 025 | Train Loss: 0.1797 Acc: 0.9445 | Val Loss: 0.1759 Acc: 0.9382                                               \n",
      "Epoch 026 | Train Loss: 0.1481 Acc: 0.9528 | Val Loss: 0.2099 Acc: 0.9349                                               \n",
      "Epoch 027 | Train Loss: 0.1514 Acc: 0.9522 | Val Loss: 0.1561 Acc: 0.9463                                               \n",
      "Epoch 028 | Train Loss: 0.1363 Acc: 0.9552 | Val Loss: 0.2304 Acc: 0.9275                                               \n",
      "Epoch 029 | Train Loss: 0.1339 Acc: 0.9565 | Val Loss: 0.2031 Acc: 0.9361                                               \n",
      "Epoch 030 | Train Loss: 0.1171 Acc: 0.9617 | Val Loss: 0.1314 Acc: 0.9552                                               \n",
      "Epoch 031 | Train Loss: 0.1119 Acc: 0.9637 | Val Loss: 0.1321 Acc: 0.9531                                               \n",
      "Epoch 032 | Train Loss: 0.1047 Acc: 0.9663 | Val Loss: 0.1059 Acc: 0.9663                                               \n",
      "Epoch 033 | Train Loss: 0.1012 Acc: 0.9664 | Val Loss: 0.1313 Acc: 0.9561                                               \n",
      "Epoch 034 | Train Loss: 0.0875 Acc: 0.9711 | Val Loss: 0.1213 Acc: 0.9633                                               \n",
      "Epoch 035 | Train Loss: 0.0923 Acc: 0.9710 | Val Loss: 0.1078 Acc: 0.9699                                               \n",
      "Epoch 036 | Train Loss: 0.0812 Acc: 0.9741 | Val Loss: 0.1128 Acc: 0.9636                                               \n",
      "Epoch 037 | Train Loss: 0.0753 Acc: 0.9758 | Val Loss: 0.0979 Acc: 0.9728                                               \n",
      "Epoch 038 | Train Loss: 0.0742 Acc: 0.9755 | Val Loss: 0.1030 Acc: 0.9704                                               \n",
      "Epoch 039 | Train Loss: 0.0785 Acc: 0.9747 | Val Loss: 0.1030 Acc: 0.9707                                               \n",
      "Epoch 040 | Train Loss: 0.0655 Acc: 0.9787 | Val Loss: 0.1281 Acc: 0.9663                                               \n",
      "Epoch 041 | Train Loss: 0.0634 Acc: 0.9810 | Val Loss: 0.0776 Acc: 0.9791                                               \n",
      "Epoch 042 | Train Loss: 0.0554 Acc: 0.9829 | Val Loss: 0.0911 Acc: 0.9749                                               \n",
      "Epoch 043 | Train Loss: 0.0583 Acc: 0.9819 | Val Loss: 0.0922 Acc: 0.9767                                               \n",
      "Epoch 044 | Train Loss: 0.0588 Acc: 0.9825 | Val Loss: 0.0830 Acc: 0.9758                                               \n",
      "Epoch 045 | Train Loss: 0.0464 Acc: 0.9848 | Val Loss: 0.0799 Acc: 0.9797                                               \n",
      "Epoch 046 | Train Loss: 0.0457 Acc: 0.9851 | Val Loss: 0.1042 Acc: 0.9701                                               \n",
      "Epoch 047 | Train Loss: 0.0451 Acc: 0.9869 | Val Loss: 0.0784 Acc: 0.9773                                               \n",
      "Epoch 048 | Train Loss: 0.0452 Acc: 0.9853 | Val Loss: 0.0889 Acc: 0.9752                                               \n",
      "Epoch 049 | Train Loss: 0.0467 Acc: 0.9860 | Val Loss: 0.0705 Acc: 0.9809                                               \n",
      "Epoch 050 | Train Loss: 0.0538 Acc: 0.9826 | Val Loss: 0.0698 Acc: 0.9791                                               \n",
      "Epoch 051 | Train Loss: 0.0300 Acc: 0.9910 | Val Loss: 0.0738 Acc: 0.9833                                               \n",
      "Epoch 052 | Train Loss: 0.0364 Acc: 0.9893 | Val Loss: 0.0771 Acc: 0.9764                                               \n",
      "Epoch 053 | Train Loss: 0.0354 Acc: 0.9894 | Val Loss: 0.0789 Acc: 0.9803                                               \n",
      "Epoch 054 | Train Loss: 0.0369 Acc: 0.9879 | Val Loss: 0.1033 Acc: 0.9713                                               \n",
      "Epoch 055 | Train Loss: 0.0481 Acc: 0.9854 | Val Loss: 0.0718 Acc: 0.9800                                               \n",
      "Epoch 056 | Train Loss: 0.0367 Acc: 0.9879 | Val Loss: 0.0736 Acc: 0.9809                                               \n",
      "Epoch 057 | Train Loss: 0.0330 Acc: 0.9895 | Val Loss: 0.1329 Acc: 0.9722                                               \n",
      "Epoch 058 | Train Loss: 0.0369 Acc: 0.9891 | Val Loss: 0.0760 Acc: 0.9806                                               \n",
      "Epoch 059 | Train Loss: 0.0272 Acc: 0.9920 | Val Loss: 0.0796 Acc: 0.9788                                               \n",
      "Epoch 060 | Train Loss: 0.0325 Acc: 0.9904 | Val Loss: 0.0736 Acc: 0.9782                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 256, 'cnn_dropout': 0.4174490848345807, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 16, 'learning_rate': 0.0002449168240896652, 'lstm_dense': 32, 'lstm_hidden_size': 64, 'lstm_layers': 1, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 18.7754 Acc: 0.3878 | Val Loss: 2.3416 Acc: 0.5421                                              \n",
      "Epoch 002 | Train Loss: 4.0392 Acc: 0.4659 | Val Loss: 1.5019 Acc: 0.5510                                               \n",
      "Epoch 003 | Train Loss: 2.3977 Acc: 0.4944 | Val Loss: 0.8814 Acc: 0.6427                                               \n",
      "Epoch 004 | Train Loss: 1.8911 Acc: 0.5133 | Val Loss: 0.9821 Acc: 0.6000                                               \n",
      "Epoch 005 | Train Loss: 1.4562 Acc: 0.5356 | Val Loss: 0.8618 Acc: 0.6355                                               \n",
      "Epoch 006 | Train Loss: 1.3100 Acc: 0.5423 | Val Loss: 1.0399 Acc: 0.5866                                               \n",
      "Epoch 007 | Train Loss: 1.1257 Acc: 0.5684 | Val Loss: 1.0373 Acc: 0.5815                                               \n",
      "Epoch 008 | Train Loss: 1.0360 Acc: 0.5777 | Val Loss: 0.7138 Acc: 0.7191                                               \n",
      "Epoch 009 | Train Loss: 1.0448 Acc: 0.5704 | Val Loss: 0.9533 Acc: 0.5803                                               \n",
      "Epoch 010 | Train Loss: 0.9614 Acc: 0.5870 | Val Loss: 0.7246 Acc: 0.6600                                               \n",
      "Epoch 011 | Train Loss: 0.9109 Acc: 0.6068 | Val Loss: 0.8174 Acc: 0.6457                                               \n",
      "Epoch 012 | Train Loss: 0.9463 Acc: 0.5994 | Val Loss: 0.6646 Acc: 0.7143                                               \n",
      "Epoch 013 | Train Loss: 0.8953 Acc: 0.6094 | Val Loss: 0.6298 Acc: 0.7490                                               \n",
      "Epoch 014 | Train Loss: 0.8588 Acc: 0.6185 | Val Loss: 0.7294 Acc: 0.6343                                               \n",
      "Epoch 015 | Train Loss: 0.8498 Acc: 0.6110 | Val Loss: 0.8074 Acc: 0.6293                                               \n",
      "Epoch 016 | Train Loss: 0.7729 Acc: 0.6466 | Val Loss: 0.7583 Acc: 0.6812                                               \n",
      "Epoch 017 | Train Loss: 0.6989 Acc: 0.6863 | Val Loss: 0.5784 Acc: 0.7334                                               \n",
      "Epoch 018 | Train Loss: 0.6612 Acc: 0.7131 | Val Loss: 0.5362 Acc: 0.7543                                               \n",
      "Epoch 019 | Train Loss: 0.6331 Acc: 0.7331 | Val Loss: 0.5248 Acc: 0.7484                                               \n",
      "Epoch 020 | Train Loss: 0.5715 Acc: 0.7639 | Val Loss: 0.4434 Acc: 0.8176                                               \n",
      "Epoch 021 | Train Loss: 0.5101 Acc: 0.7914 | Val Loss: 0.4674 Acc: 0.7988                                               \n",
      "Epoch 022 | Train Loss: 0.4847 Acc: 0.8048 | Val Loss: 0.3456 Acc: 0.8516                                               \n",
      "Epoch 023 | Train Loss: 0.4602 Acc: 0.8198 | Val Loss: 0.5170 Acc: 0.7884                                               \n",
      "Epoch 024 | Train Loss: 0.4272 Acc: 0.8343 | Val Loss: 0.3297 Acc: 0.8463                                               \n",
      "Epoch 025 | Train Loss: 0.4093 Acc: 0.8408 | Val Loss: 0.3230 Acc: 0.8707                                               \n",
      "Epoch 026 | Train Loss: 0.3584 Acc: 0.8620 | Val Loss: 0.2605 Acc: 0.8973                                               \n",
      "Epoch 027 | Train Loss: 0.3494 Acc: 0.8678 | Val Loss: 0.3051 Acc: 0.8845                                               \n",
      "Epoch 028 | Train Loss: 0.3237 Acc: 0.8795 | Val Loss: 0.3872 Acc: 0.8466                                               \n",
      "Epoch 029 | Train Loss: 0.2962 Acc: 0.8897 | Val Loss: 0.2689 Acc: 0.8952                                               \n",
      "Epoch 030 | Train Loss: 0.2751 Acc: 0.9014 | Val Loss: 0.2977 Acc: 0.8884                                               \n",
      "Epoch 031 | Train Loss: 0.2628 Acc: 0.9034 | Val Loss: 0.2364 Acc: 0.9164                                               \n",
      "Epoch 032 | Train Loss: 0.2455 Acc: 0.9097 | Val Loss: 0.1470 Acc: 0.9516                                               \n",
      "Epoch 033 | Train Loss: 0.2246 Acc: 0.9202 | Val Loss: 0.2983 Acc: 0.8875                                               \n",
      "Epoch 034 | Train Loss: 0.2160 Acc: 0.9230 | Val Loss: 0.1533 Acc: 0.9466                                               \n",
      "Epoch 035 | Train Loss: 0.2022 Acc: 0.9278 | Val Loss: 0.2004 Acc: 0.9230                                               \n",
      "Epoch 036 | Train Loss: 0.1779 Acc: 0.9392 | Val Loss: 0.1552 Acc: 0.9481                                               \n",
      "Epoch 037 | Train Loss: 0.1843 Acc: 0.9348 | Val Loss: 0.2148 Acc: 0.9257                                               \n",
      "Epoch 038 | Train Loss: 0.1621 Acc: 0.9460 | Val Loss: 0.1086 Acc: 0.9618                                               \n",
      "Epoch 039 | Train Loss: 0.1465 Acc: 0.9485 | Val Loss: 0.1011 Acc: 0.9642                                               \n",
      "Epoch 040 | Train Loss: 0.1367 Acc: 0.9543 | Val Loss: 0.1496 Acc: 0.9439                                               \n",
      "Epoch 041 | Train Loss: 0.1397 Acc: 0.9500 | Val Loss: 0.1178 Acc: 0.9609                                               \n",
      "Epoch 042 | Train Loss: 0.1287 Acc: 0.9548 | Val Loss: 0.1528 Acc: 0.9427                                               \n",
      "Epoch 043 | Train Loss: 0.1240 Acc: 0.9567 | Val Loss: 0.0954 Acc: 0.9681                                               \n",
      "Epoch 044 | Train Loss: 0.1109 Acc: 0.9634 | Val Loss: 0.1130 Acc: 0.9567                                               \n",
      "Epoch 045 | Train Loss: 0.1112 Acc: 0.9597 | Val Loss: 0.0906 Acc: 0.9687                                               \n",
      "Epoch 046 | Train Loss: 0.1016 Acc: 0.9643 | Val Loss: 0.0674 Acc: 0.9752                                               \n",
      "Epoch 047 | Train Loss: 0.1120 Acc: 0.9627 | Val Loss: 0.0899 Acc: 0.9669                                               \n",
      "Epoch 048 | Train Loss: 0.1120 Acc: 0.9617 | Val Loss: 0.0852 Acc: 0.9710                                               \n",
      "Epoch 049 | Train Loss: 0.0910 Acc: 0.9669 | Val Loss: 0.0938 Acc: 0.9660                                               \n",
      "Epoch 050 | Train Loss: 0.0827 Acc: 0.9710 | Val Loss: 0.0732 Acc: 0.9743                                               \n",
      "Epoch 051 | Train Loss: 0.0844 Acc: 0.9732 | Val Loss: 0.0747 Acc: 0.9734                                               \n",
      "Epoch 052 | Train Loss: 0.0845 Acc: 0.9712 | Val Loss: 0.0820 Acc: 0.9696                                               \n",
      "Epoch 053 | Train Loss: 0.0962 Acc: 0.9664 | Val Loss: 0.0642 Acc: 0.9791                                               \n",
      "Epoch 054 | Train Loss: 0.0746 Acc: 0.9756 | Val Loss: 0.0665 Acc: 0.9752                                               \n",
      "Epoch 055 | Train Loss: 0.0624 Acc: 0.9798 | Val Loss: 0.0703 Acc: 0.9779                                               \n",
      "Epoch 056 | Train Loss: 0.0649 Acc: 0.9778 | Val Loss: 0.0785 Acc: 0.9752                                               \n",
      "Epoch 057 | Train Loss: 0.0533 Acc: 0.9813 | Val Loss: 0.0792 Acc: 0.9764                                               \n",
      "Epoch 058 | Train Loss: 0.0602 Acc: 0.9796 | Val Loss: 0.0732 Acc: 0.9767                                               \n",
      "Epoch 059 | Train Loss: 0.0659 Acc: 0.9775 | Val Loss: 0.0723 Acc: 0.9767                                               \n",
      "Epoch 060 | Train Loss: 0.0591 Acc: 0.9797 | Val Loss: 0.0616 Acc: 0.9788                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 64, 'cnn_dropout': 0.5718861008638834, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 64, 'learning_rate': 1.6137321388010126e-05, 'lstm_dense': 32, 'lstm_hidden_size': 32, 'lstm_layers': 6, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 98.0674 Acc: 0.2892 | Val Loss: 57.9374 Acc: 0.4421                                             \n",
      "Epoch 002 | Train Loss: 72.6846 Acc: 0.2992 | Val Loss: 43.0982 Acc: 0.4421                                             \n",
      "Epoch 003 | Train Loss: 57.4555 Acc: 0.2935 | Val Loss: 28.5005 Acc: 0.4421                                             \n",
      "Epoch 004 | Train Loss: 47.6546 Acc: 0.3085 | Val Loss: 21.6894 Acc: 0.4421                                             \n",
      "Epoch 005 | Train Loss: 38.9804 Acc: 0.3207 | Val Loss: 16.1832 Acc: 0.4421                                             \n",
      "Epoch 006 | Train Loss: 32.6779 Acc: 0.3334 | Val Loss: 10.3950 Acc: 0.4466                                             \n",
      "Epoch 007 | Train Loss: 27.5954 Acc: 0.3443 | Val Loss: 5.2322 Acc: 0.4710                                              \n",
      "Epoch 008 | Train Loss: 24.0228 Acc: 0.3531 | Val Loss: 5.1373 Acc: 0.4755                                              \n",
      "Epoch 009 | Train Loss: 21.2824 Acc: 0.3573 | Val Loss: 3.9606 Acc: 0.4460                                              \n",
      "Epoch 010 | Train Loss: 19.2464 Acc: 0.3608 | Val Loss: 3.4352 Acc: 0.3946                                              \n",
      "Epoch 011 | Train Loss: 17.4213 Acc: 0.3696 | Val Loss: 3.9316 Acc: 0.4069                                              \n",
      "Epoch 012 | Train Loss: 16.0153 Acc: 0.3663 | Val Loss: 3.4191 Acc: 0.4433                                              \n",
      "Epoch 013 | Train Loss: 14.4181 Acc: 0.3778 | Val Loss: 3.3554 Acc: 0.4636                                              \n",
      "Epoch 014 | Train Loss: 13.1352 Acc: 0.3870 | Val Loss: 3.2491 Acc: 0.4496                                              \n",
      "Epoch 015 | Train Loss: 12.2428 Acc: 0.3814 | Val Loss: 2.9281 Acc: 0.4767                                              \n",
      "Epoch 016 | Train Loss: 11.1693 Acc: 0.3979 | Val Loss: 3.0791 Acc: 0.4890                                              \n",
      "Epoch 017 | Train Loss: 10.3602 Acc: 0.4004 | Val Loss: 3.0255 Acc: 0.4767                                              \n",
      "Epoch 018 | Train Loss: 9.6275 Acc: 0.4026 | Val Loss: 3.5441 Acc: 0.4478                                               \n",
      "Epoch 019 | Train Loss: 8.9412 Acc: 0.4097 | Val Loss: 3.5076 Acc: 0.4609                                               \n",
      "Epoch 020 | Train Loss: 8.2384 Acc: 0.4168 | Val Loss: 3.8536 Acc: 0.4785                                               \n",
      "Epoch 021 | Train Loss: 8.0226 Acc: 0.4137 | Val Loss: 2.7633 Acc: 0.5358                                               \n",
      "Epoch 022 | Train Loss: 7.1793 Acc: 0.4283 | Val Loss: 2.6111 Acc: 0.5319                                               \n",
      "Epoch 023 | Train Loss: 6.8393 Acc: 0.4253 | Val Loss: 2.7447 Acc: 0.5200                                               \n",
      "Epoch 024 | Train Loss: 6.4330 Acc: 0.4317 | Val Loss: 2.3955 Acc: 0.5382                                               \n",
      "Epoch 025 | Train Loss: 6.0660 Acc: 0.4341 | Val Loss: 2.6468 Acc: 0.5433                                               \n",
      "Epoch 026 | Train Loss: 5.6510 Acc: 0.4423 | Val Loss: 1.9965 Acc: 0.5433                                               \n",
      "Epoch 027 | Train Loss: 5.2557 Acc: 0.4485 | Val Loss: 1.8594 Acc: 0.5045                                               \n",
      "Epoch 028 | Train Loss: 5.1084 Acc: 0.4450 | Val Loss: 1.9066 Acc: 0.5424                                               \n",
      "Epoch 029 | Train Loss: 4.7385 Acc: 0.4570 | Val Loss: 2.0859 Acc: 0.5546                                               \n",
      "Epoch 030 | Train Loss: 4.4278 Acc: 0.4625 | Val Loss: 1.6761 Acc: 0.5597                                               \n",
      "Epoch 031 | Train Loss: 4.3006 Acc: 0.4685 | Val Loss: 1.8995 Acc: 0.5299                                               \n",
      "Epoch 032 | Train Loss: 4.1402 Acc: 0.4685 | Val Loss: 1.8529 Acc: 0.5445                                               \n",
      "Epoch 033 | Train Loss: 3.9135 Acc: 0.4764 | Val Loss: 1.2775 Acc: 0.5881                                               \n",
      "Epoch 034 | Train Loss: 3.6423 Acc: 0.4879 | Val Loss: 1.3192 Acc: 0.5931                                               \n",
      "Epoch 035 | Train Loss: 3.4979 Acc: 0.4849 | Val Loss: 1.1116 Acc: 0.6113                                               \n",
      "Epoch 036 | Train Loss: 3.3638 Acc: 0.4928 | Val Loss: 1.5073 Acc: 0.5704                                               \n",
      "Epoch 037 | Train Loss: 3.1650 Acc: 0.5023 | Val Loss: 1.2881 Acc: 0.6143                                               \n",
      "Epoch 038 | Train Loss: 3.1150 Acc: 0.5027 | Val Loss: 1.4901 Acc: 0.5567                                               \n",
      "Epoch 039 | Train Loss: 2.9620 Acc: 0.5067 | Val Loss: 1.1880 Acc: 0.6000                                               \n",
      "Epoch 040 | Train Loss: 2.8007 Acc: 0.5152 | Val Loss: 1.0210 Acc: 0.6648                                               \n",
      "Epoch 041 | Train Loss: 2.7903 Acc: 0.5191 | Val Loss: 0.9883 Acc: 0.6830                                               \n",
      "Epoch 042 | Train Loss: 2.5133 Acc: 0.5298 | Val Loss: 1.1367 Acc: 0.6549                                               \n",
      "Epoch 043 | Train Loss: 2.4810 Acc: 0.5323 | Val Loss: 0.9563 Acc: 0.6836                                               \n",
      "Epoch 044 | Train Loss: 2.4021 Acc: 0.5391 | Val Loss: 0.9326 Acc: 0.7045                                               \n",
      "Epoch 045 | Train Loss: 2.3283 Acc: 0.5461 | Val Loss: 1.0070 Acc: 0.6734                                               \n",
      "Epoch 046 | Train Loss: 2.3061 Acc: 0.5458 | Val Loss: 1.1132 Acc: 0.6910                                               \n",
      "Epoch 047 | Train Loss: 2.1719 Acc: 0.5558 | Val Loss: 0.9225 Acc: 0.6922                                               \n",
      "Epoch 048 | Train Loss: 2.0621 Acc: 0.5709 | Val Loss: 0.8197 Acc: 0.6925                                               \n",
      "Epoch 049 | Train Loss: 2.0690 Acc: 0.5702 | Val Loss: 0.8745 Acc: 0.7137                                               \n",
      "Epoch 050 | Train Loss: 1.9265 Acc: 0.5750 | Val Loss: 0.9920 Acc: 0.6749                                               \n",
      "Epoch 051 | Train Loss: 1.8795 Acc: 0.5757 | Val Loss: 0.7816 Acc: 0.7045                                               \n",
      "Epoch 052 | Train Loss: 1.8290 Acc: 0.5883 | Val Loss: 0.8912 Acc: 0.6499                                               \n",
      "Epoch 053 | Train Loss: 1.8124 Acc: 0.5856 | Val Loss: 0.9164 Acc: 0.6666                                               \n",
      "Epoch 054 | Train Loss: 1.6983 Acc: 0.6032 | Val Loss: 0.8113 Acc: 0.7042                                               \n",
      "Epoch 055 | Train Loss: 1.6807 Acc: 0.5955 | Val Loss: 0.9257 Acc: 0.7078                                               \n",
      "Epoch 056 | Train Loss: 1.6230 Acc: 0.6018 | Val Loss: 0.7939 Acc: 0.7197                                               \n",
      "Epoch 057 | Train Loss: 1.5893 Acc: 0.6158 | Val Loss: 0.8952 Acc: 0.7057                                               \n",
      "Epoch 058 | Train Loss: 1.5805 Acc: 0.6203 | Val Loss: 0.7430 Acc: 0.6931                                               \n",
      "Epoch 059 | Train Loss: 1.5072 Acc: 0.6243 | Val Loss: 0.9060 Acc: 0.6827                                               \n",
      "Epoch 060 | Train Loss: 1.4996 Acc: 0.6216 | Val Loss: 0.8323 Acc: 0.6893                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 64, 'cnn_dropout': 0.06762422345413452, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 32, 'cnn_kernels_2': 16, 'learning_rate': 0.00015730866465380597, 'lstm_dense': 256, 'lstm_hidden_size': 128, 'lstm_layers': 3, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 2.8774 Acc: 0.4500 | Val Loss: 0.8681 Acc: 0.5869                                               \n",
      "Epoch 002 | Train Loss: 0.9539 Acc: 0.5107 | Val Loss: 0.8364 Acc: 0.5525                                               \n",
      "Epoch 003 | Train Loss: 0.9135 Acc: 0.5302 | Val Loss: 0.7974 Acc: 0.5913                                               \n",
      "Epoch 004 | Train Loss: 0.8849 Acc: 0.5536 | Val Loss: 0.8082 Acc: 0.6084                                               \n",
      "Epoch 005 | Train Loss: 0.8599 Acc: 0.5678 | Val Loss: 0.7554 Acc: 0.6412                                               \n",
      "Epoch 006 | Train Loss: 0.8578 Acc: 0.5611 | Val Loss: 0.8143 Acc: 0.5645                                               \n",
      "Epoch 007 | Train Loss: 0.8505 Acc: 0.5714 | Val Loss: 0.7429 Acc: 0.6421                                               \n",
      "Epoch 008 | Train Loss: 0.8303 Acc: 0.5833 | Val Loss: 0.7193 Acc: 0.6979                                               \n",
      "Epoch 009 | Train Loss: 0.8250 Acc: 0.5880 | Val Loss: 0.7260 Acc: 0.6173                                               \n",
      "Epoch 010 | Train Loss: 0.7976 Acc: 0.6015 | Val Loss: 0.6723 Acc: 0.6940                                               \n",
      "Epoch 011 | Train Loss: 0.8335 Acc: 0.5915 | Val Loss: 0.7005 Acc: 0.6484                                               \n",
      "Epoch 012 | Train Loss: 0.7816 Acc: 0.6187 | Val Loss: 0.6776 Acc: 0.7281                                               \n",
      "Epoch 013 | Train Loss: 0.7818 Acc: 0.6192 | Val Loss: 0.6486 Acc: 0.7015                                               \n",
      "Epoch 014 | Train Loss: 0.7792 Acc: 0.6201 | Val Loss: 0.7216 Acc: 0.6278                                               \n",
      "Epoch 015 | Train Loss: 0.7852 Acc: 0.6206 | Val Loss: 0.6855 Acc: 0.6487                                               \n",
      "Epoch 016 | Train Loss: 0.7941 Acc: 0.6174 | Val Loss: 0.6400 Acc: 0.7382                                               \n",
      "Epoch 017 | Train Loss: 0.7813 Acc: 0.6178 | Val Loss: 0.6595 Acc: 0.7328                                               \n",
      "Epoch 018 | Train Loss: 0.7719 Acc: 0.6276 | Val Loss: 0.6714 Acc: 0.6794                                               \n",
      "Epoch 019 | Train Loss: 0.7834 Acc: 0.6209 | Val Loss: 0.6613 Acc: 0.7176                                               \n",
      "Epoch 020 | Train Loss: 0.7760 Acc: 0.6262 | Val Loss: 0.6775 Acc: 0.6833                                               \n",
      "Epoch 021 | Train Loss: 0.8013 Acc: 0.6071 | Val Loss: 0.7635 Acc: 0.5773                                               \n",
      "Epoch 022 | Train Loss: 0.8507 Acc: 0.5859 | Val Loss: 0.6917 Acc: 0.6964                                               \n",
      "Epoch 023 | Train Loss: 0.8054 Acc: 0.6083 | Val Loss: 0.6881 Acc: 0.7170                                               \n",
      "Epoch 024 | Train Loss: 0.8290 Acc: 0.6018 | Val Loss: 0.7097 Acc: 0.6221                                               \n",
      "Epoch 025 | Train Loss: 0.8225 Acc: 0.6126 | Val Loss: 0.7089 Acc: 0.6675                                               \n",
      "Epoch 026 | Train Loss: 0.8098 Acc: 0.6216 | Val Loss: 0.7013 Acc: 0.6421                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 32, 'cnn_dropout': 0.2878987867741044, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 16, 'cnn_kernels_2': 96, 'learning_rate': 2.4608063438046444e-05, 'lstm_dense': 64, 'lstm_hidden_size': 96, 'lstm_layers': 6, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 79.7706 Acc: 0.1867 | Val Loss: 11.1581 Acc: 0.3042                                             \n",
      "Epoch 002 | Train Loss: 28.8778 Acc: 0.3284 | Val Loss: 7.7713 Acc: 0.4666                                              \n",
      "Epoch 003 | Train Loss: 21.2741 Acc: 0.3632 | Val Loss: 6.9720 Acc: 0.4493                                              \n",
      "Epoch 004 | Train Loss: 17.3479 Acc: 0.3782 | Val Loss: 5.4325 Acc: 0.4612                                              \n",
      "Epoch 005 | Train Loss: 13.7848 Acc: 0.3888 | Val Loss: 5.4826 Acc: 0.4349                                              \n",
      "Epoch 006 | Train Loss: 12.1081 Acc: 0.3959 | Val Loss: 4.2255 Acc: 0.4382                                              \n",
      "Epoch 007 | Train Loss: 10.2261 Acc: 0.4003 | Val Loss: 3.4621 Acc: 0.5161                                              \n",
      "Epoch 008 | Train Loss: 9.2236 Acc: 0.4017 | Val Loss: 3.2641 Acc: 0.5200                                               \n",
      "Epoch 009 | Train Loss: 8.1623 Acc: 0.4153 | Val Loss: 3.2638 Acc: 0.5304                                               \n",
      "Epoch 010 | Train Loss: 7.4293 Acc: 0.4224 | Val Loss: 2.3990 Acc: 0.5161                                               \n",
      "Epoch 011 | Train Loss: 6.7280 Acc: 0.4315 | Val Loss: 2.0636 Acc: 0.5803                                               \n",
      "Epoch 012 | Train Loss: 6.1657 Acc: 0.4482 | Val Loss: 1.9721 Acc: 0.5893                                               \n",
      "Epoch 013 | Train Loss: 5.4953 Acc: 0.4538 | Val Loss: 1.8353 Acc: 0.6024                                               \n",
      "Epoch 014 | Train Loss: 5.1387 Acc: 0.4533 | Val Loss: 1.8023 Acc: 0.6215                                               \n",
      "Epoch 015 | Train Loss: 4.6171 Acc: 0.4746 | Val Loss: 1.6143 Acc: 0.6660                                               \n",
      "Epoch 016 | Train Loss: 4.3382 Acc: 0.4853 | Val Loss: 1.4071 Acc: 0.6651                                               \n",
      "Epoch 017 | Train Loss: 4.1916 Acc: 0.4892 | Val Loss: 1.4343 Acc: 0.6510                                               \n",
      "Epoch 018 | Train Loss: 3.8189 Acc: 0.4968 | Val Loss: 1.4676 Acc: 0.6842                                               \n",
      "Epoch 019 | Train Loss: 3.5479 Acc: 0.5072 | Val Loss: 1.3652 Acc: 0.6761                                               \n",
      "Epoch 020 | Train Loss: 3.2805 Acc: 0.5205 | Val Loss: 1.2099 Acc: 0.6734                                               \n",
      "Epoch 021 | Train Loss: 3.1221 Acc: 0.5388 | Val Loss: 1.2479 Acc: 0.6860                                               \n",
      "Epoch 022 | Train Loss: 2.9961 Acc: 0.5387 | Val Loss: 1.0813 Acc: 0.6645                                               \n",
      "Epoch 023 | Train Loss: 2.8048 Acc: 0.5468 | Val Loss: 1.1554 Acc: 0.7033                                               \n",
      "Epoch 024 | Train Loss: 2.6777 Acc: 0.5488 | Val Loss: 1.0790 Acc: 0.7027                                               \n",
      "Epoch 025 | Train Loss: 2.5161 Acc: 0.5601 | Val Loss: 0.9535 Acc: 0.6866                                               \n",
      "Epoch 026 | Train Loss: 2.3904 Acc: 0.5673 | Val Loss: 0.9274 Acc: 0.6693                                               \n",
      "Epoch 027 | Train Loss: 2.2156 Acc: 0.5788 | Val Loss: 0.9619 Acc: 0.7304                                               \n",
      "Epoch 028 | Train Loss: 2.1737 Acc: 0.5814 | Val Loss: 0.9028 Acc: 0.7149                                               \n",
      "Epoch 029 | Train Loss: 2.0789 Acc: 0.5908 | Val Loss: 0.9470 Acc: 0.7227                                               \n",
      "Epoch 030 | Train Loss: 1.9519 Acc: 0.5986 | Val Loss: 1.1405 Acc: 0.6907                                               \n",
      "Epoch 031 | Train Loss: 1.8896 Acc: 0.6070 | Val Loss: 0.9586 Acc: 0.7227                                               \n",
      "Epoch 032 | Train Loss: 1.8368 Acc: 0.6132 | Val Loss: 0.8025 Acc: 0.7299                                               \n",
      "Epoch 033 | Train Loss: 1.7538 Acc: 0.6268 | Val Loss: 0.8334 Acc: 0.7516                                               \n",
      "Epoch 034 | Train Loss: 1.6667 Acc: 0.6368 | Val Loss: 0.8954 Acc: 0.7475                                               \n",
      "Epoch 035 | Train Loss: 1.6317 Acc: 0.6423 | Val Loss: 0.7560 Acc: 0.7687                                               \n",
      "Epoch 036 | Train Loss: 1.5427 Acc: 0.6553 | Val Loss: 0.7233 Acc: 0.7913                                               \n",
      "Epoch 037 | Train Loss: 1.4280 Acc: 0.6656 | Val Loss: 0.7249 Acc: 0.7746                                               \n",
      "Epoch 038 | Train Loss: 1.4286 Acc: 0.6678 | Val Loss: 0.7530 Acc: 0.7728                                               \n",
      "Epoch 039 | Train Loss: 1.3306 Acc: 0.6811 | Val Loss: 0.7581 Acc: 0.7824                                               \n",
      "Epoch 040 | Train Loss: 1.2980 Acc: 0.6839 | Val Loss: 0.7556 Acc: 0.7800                                               \n",
      "Epoch 041 | Train Loss: 1.2802 Acc: 0.6851 | Val Loss: 0.6596 Acc: 0.7919                                               \n",
      "Epoch 042 | Train Loss: 1.1764 Acc: 0.7025 | Val Loss: 0.6845 Acc: 0.7940                                               \n",
      "Epoch 043 | Train Loss: 1.2036 Acc: 0.7027 | Val Loss: 0.5653 Acc: 0.8096                                               \n",
      "Epoch 044 | Train Loss: 1.1245 Acc: 0.7098 | Val Loss: 0.7350 Acc: 0.7848                                               \n",
      "Epoch 045 | Train Loss: 1.0619 Acc: 0.7195 | Val Loss: 0.5868 Acc: 0.8194                                               \n",
      "Epoch 046 | Train Loss: 1.0316 Acc: 0.7307 | Val Loss: 0.5643 Acc: 0.8149                                               \n",
      "Epoch 047 | Train Loss: 1.0379 Acc: 0.7203 | Val Loss: 0.8260 Acc: 0.7642                                               \n",
      "Epoch 048 | Train Loss: 1.0009 Acc: 0.7354 | Val Loss: 0.6369 Acc: 0.8045                                               \n",
      "Epoch 049 | Train Loss: 0.9223 Acc: 0.7504 | Val Loss: 0.5431 Acc: 0.8248                                               \n",
      "Epoch 050 | Train Loss: 0.9368 Acc: 0.7441 | Val Loss: 0.5612 Acc: 0.8263                                               \n",
      "Epoch 051 | Train Loss: 0.8786 Acc: 0.7632 | Val Loss: 0.8676 Acc: 0.7388                                               \n",
      "Epoch 052 | Train Loss: 0.8693 Acc: 0.7633 | Val Loss: 0.4788 Acc: 0.8487                                               \n",
      "Epoch 053 | Train Loss: 0.8770 Acc: 0.7585 | Val Loss: 0.4960 Acc: 0.8460                                               \n",
      "Epoch 054 | Train Loss: 0.8087 Acc: 0.7693 | Val Loss: 0.5578 Acc: 0.8316                                               \n",
      "Epoch 055 | Train Loss: 0.7969 Acc: 0.7781 | Val Loss: 0.4768 Acc: 0.8510                                               \n",
      "Epoch 056 | Train Loss: 0.7620 Acc: 0.7816 | Val Loss: 0.4655 Acc: 0.8618                                               \n",
      "Epoch 057 | Train Loss: 0.7359 Acc: 0.7852 | Val Loss: 0.4411 Acc: 0.8618                                               \n",
      "Epoch 058 | Train Loss: 0.7345 Acc: 0.7931 | Val Loss: 0.4259 Acc: 0.8606                                               \n",
      "Epoch 059 | Train Loss: 0.7026 Acc: 0.7957 | Val Loss: 0.4533 Acc: 0.8558                                               \n",
      "Epoch 060 | Train Loss: 0.6926 Acc: 0.7998 | Val Loss: 0.4806 Acc: 0.8436                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 36, 'cnn_dense': 128, 'cnn_dropout': 0.5596177194248717, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 64, 'cnn_kernels_2': 16, 'learning_rate': 0.002173880886909515, 'lstm_dense': 128, 'lstm_hidden_size': 64, 'lstm_layers': 4, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 8.7755 Acc: 0.4722 | Val Loss: 1.7641 Acc: 0.5531                                               \n",
      "Epoch 002 | Train Loss: 1.1739 Acc: 0.5844 | Val Loss: 0.7341 Acc: 0.6800                                               \n",
      "Epoch 003 | Train Loss: 0.7971 Acc: 0.6450 | Val Loss: 0.6054 Acc: 0.7764                                               \n",
      "Epoch 004 | Train Loss: 0.7351 Acc: 0.6671 | Val Loss: 0.5491 Acc: 0.7555                                               \n",
      "Epoch 005 | Train Loss: 0.6799 Acc: 0.6948 | Val Loss: 0.6405 Acc: 0.6836                                               \n",
      "Epoch 006 | Train Loss: 0.6616 Acc: 0.7018 | Val Loss: 0.5906 Acc: 0.7287                                               \n",
      "Epoch 007 | Train Loss: 0.6381 Acc: 0.7091 | Val Loss: 0.6036 Acc: 0.7409                                               \n",
      "Epoch 008 | Train Loss: 0.6538 Acc: 0.7139 | Val Loss: 0.7810 Acc: 0.6451                                               \n",
      "Epoch 009 | Train Loss: 0.6323 Acc: 0.7195 | Val Loss: 0.6851 Acc: 0.6916                                               \n",
      "Epoch 010 | Train Loss: 0.6423 Acc: 0.7213 | Val Loss: 0.5983 Acc: 0.7355                                               \n",
      "Epoch 011 | Train Loss: 0.6242 Acc: 0.7260 | Val Loss: 0.5670 Acc: 0.7854                                               \n",
      "Epoch 012 | Train Loss: 0.6276 Acc: 0.7261 | Val Loss: 0.7009 Acc: 0.7206                                               \n",
      "Epoch 013 | Train Loss: 0.6077 Acc: 0.7347 | Val Loss: 0.5335 Acc: 0.7615                                               \n",
      "Epoch 014 | Train Loss: 0.5874 Acc: 0.7438 | Val Loss: 0.5435 Acc: 0.7731                                               \n",
      "Epoch 015 | Train Loss: 0.6038 Acc: 0.7338 | Val Loss: 0.5496 Acc: 0.7851                                               \n",
      "Epoch 016 | Train Loss: 0.6540 Acc: 0.7116 | Val Loss: 0.5497 Acc: 0.8075                                               \n",
      "Epoch 017 | Train Loss: 0.6290 Acc: 0.7245 | Val Loss: 0.7001 Acc: 0.7003                                               \n",
      "Epoch 018 | Train Loss: 0.6302 Acc: 0.7296 | Val Loss: 0.5596 Acc: 0.7794                                               \n",
      "Epoch 019 | Train Loss: 0.5900 Acc: 0.7452 | Val Loss: 0.4889 Acc: 0.8003                                               \n",
      "Epoch 020 | Train Loss: 0.5934 Acc: 0.7401 | Val Loss: 0.6325 Acc: 0.7185                                               \n",
      "Epoch 021 | Train Loss: 0.6092 Acc: 0.7319 | Val Loss: 0.6005 Acc: 0.7206                                               \n",
      "Epoch 022 | Train Loss: 0.6042 Acc: 0.7385 | Val Loss: 0.6978 Acc: 0.7003                                               \n",
      "Epoch 023 | Train Loss: 0.6411 Acc: 0.7128 | Val Loss: 0.7011 Acc: 0.7185                                               \n",
      "Epoch 024 | Train Loss: 0.6110 Acc: 0.7372 | Val Loss: 0.6697 Acc: 0.6660                                               \n",
      "Epoch 025 | Train Loss: 0.6131 Acc: 0.7314 | Val Loss: 0.5906 Acc: 0.7648                                               \n",
      "Epoch 026 | Train Loss: 0.6091 Acc: 0.7339 | Val Loss: 0.7793 Acc: 0.6185                                               \n",
      "Epoch 027 | Train Loss: 0.6567 Acc: 0.7054 | Val Loss: 0.7865 Acc: 0.6340                                               \n",
      "Epoch 028 | Train Loss: 0.5950 Acc: 0.7379 | Val Loss: 0.7174 Acc: 0.6746                                               \n",
      "Epoch 029 | Train Loss: 0.5957 Acc: 0.7395 | Val Loss: 0.7516 Acc: 0.6588                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 64, 'cnn_dropout': 0.5642870120996387, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 48, 'cnn_kernels_2': 64, 'learning_rate': 0.0003430974499751907, 'lstm_dense': 64, 'lstm_hidden_size': 32, 'lstm_layers': 4, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 22.8955 Acc: 0.4119 | Val Loss: 4.1573 Acc: 0.5006                                              \n",
      "Epoch 002 | Train Loss: 8.4617 Acc: 0.4328 | Val Loss: 2.5846 Acc: 0.5236                                               \n",
      "Epoch 003 | Train Loss: 5.6369 Acc: 0.4391 | Val Loss: 3.0184 Acc: 0.4567                                               \n",
      "Epoch 004 | Train Loss: 3.9781 Acc: 0.4612 | Val Loss: 2.4343 Acc: 0.4552                                               \n",
      "Epoch 005 | Train Loss: 2.9846 Acc: 0.4757 | Val Loss: 7.2865 Acc: 0.4621                                               \n",
      "Epoch 006 | Train Loss: 2.2521 Acc: 0.5004 | Val Loss: 1.0514 Acc: 0.6278                                               \n",
      "Epoch 007 | Train Loss: 1.7035 Acc: 0.5471 | Val Loss: 0.8183 Acc: 0.6615                                               \n",
      "Epoch 008 | Train Loss: 1.3694 Acc: 0.5765 | Val Loss: 0.8688 Acc: 0.6349                                               \n",
      "Epoch 009 | Train Loss: 1.1445 Acc: 0.5980 | Val Loss: 1.1088 Acc: 0.6567                                               \n",
      "Epoch 010 | Train Loss: 0.9111 Acc: 0.6520 | Val Loss: 0.5738 Acc: 0.7436                                               \n",
      "Epoch 011 | Train Loss: 0.8116 Acc: 0.6821 | Val Loss: 0.7051 Acc: 0.7230                                               \n",
      "Epoch 012 | Train Loss: 0.7318 Acc: 0.7130 | Val Loss: 0.6880 Acc: 0.6952                                               \n",
      "Epoch 013 | Train Loss: 0.6427 Acc: 0.7409 | Val Loss: 0.5708 Acc: 0.7394                                               \n",
      "Epoch 014 | Train Loss: 0.5989 Acc: 0.7615 | Val Loss: 0.4092 Acc: 0.8382                                               \n",
      "Epoch 015 | Train Loss: 0.5227 Acc: 0.7927 | Val Loss: 0.3773 Acc: 0.8472                                               \n",
      "Epoch 016 | Train Loss: 0.4921 Acc: 0.8091 | Val Loss: 0.8120 Acc: 0.6794                                               \n",
      "Epoch 017 | Train Loss: 0.4343 Acc: 0.8385 | Val Loss: 0.3231 Acc: 0.8821                                               \n",
      "Epoch 018 | Train Loss: 0.3805 Acc: 0.8632 | Val Loss: 0.2726 Acc: 0.9078                                               \n",
      "Epoch 019 | Train Loss: 0.3436 Acc: 0.8794 | Val Loss: 0.3584 Acc: 0.8660                                               \n",
      "Epoch 020 | Train Loss: 0.3113 Acc: 0.8897 | Val Loss: 0.4608 Acc: 0.8218                                               \n",
      "Epoch 021 | Train Loss: 0.2823 Acc: 0.9022 | Val Loss: 0.5041 Acc: 0.8299                                               \n",
      "Epoch 022 | Train Loss: 0.2704 Acc: 0.9059 | Val Loss: 0.3002 Acc: 0.9003                                               \n",
      "Epoch 023 | Train Loss: 0.2403 Acc: 0.9172 | Val Loss: 0.1982 Acc: 0.9304                                               \n",
      "Epoch 024 | Train Loss: 0.2249 Acc: 0.9251 | Val Loss: 0.2058 Acc: 0.9269                                               \n",
      "Epoch 025 | Train Loss: 0.2046 Acc: 0.9290 | Val Loss: 0.2080 Acc: 0.9349                                               \n",
      "Epoch 026 | Train Loss: 0.1911 Acc: 0.9363 | Val Loss: 0.1122 Acc: 0.9636                                               \n",
      "Epoch 027 | Train Loss: 0.1771 Acc: 0.9396 | Val Loss: 0.1224 Acc: 0.9570                                               \n",
      "Epoch 028 | Train Loss: 0.1681 Acc: 0.9450 | Val Loss: 0.2732 Acc: 0.9075                                               \n",
      "Epoch 029 | Train Loss: 0.1708 Acc: 0.9457 | Val Loss: 0.1808 Acc: 0.9331                                               \n",
      "Epoch 030 | Train Loss: 0.1482 Acc: 0.9478 | Val Loss: 0.1600 Acc: 0.9454                                               \n",
      "Epoch 031 | Train Loss: 0.1366 Acc: 0.9534 | Val Loss: 0.1473 Acc: 0.9493                                               \n",
      "Epoch 032 | Train Loss: 0.1373 Acc: 0.9536 | Val Loss: 0.1270 Acc: 0.9475                                               \n",
      "Epoch 033 | Train Loss: 0.1238 Acc: 0.9584 | Val Loss: 0.0896 Acc: 0.9696                                               \n",
      "Epoch 034 | Train Loss: 0.1288 Acc: 0.9570 | Val Loss: 0.1086 Acc: 0.9624                                               \n",
      "Epoch 035 | Train Loss: 0.1200 Acc: 0.9600 | Val Loss: 0.0972 Acc: 0.9672                                               \n",
      "Epoch 036 | Train Loss: 0.1170 Acc: 0.9616 | Val Loss: 0.0791 Acc: 0.9752                                               \n",
      "Epoch 037 | Train Loss: 0.0995 Acc: 0.9666 | Val Loss: 0.1207 Acc: 0.9645                                               \n",
      "Epoch 038 | Train Loss: 0.0975 Acc: 0.9669 | Val Loss: 0.0903 Acc: 0.9710                                               \n",
      "Epoch 039 | Train Loss: 0.0932 Acc: 0.9701 | Val Loss: 0.2142 Acc: 0.9406                                               \n",
      "Epoch 040 | Train Loss: 0.0982 Acc: 0.9675 | Val Loss: 0.1334 Acc: 0.9609                                               \n",
      "Epoch 041 | Train Loss: 0.0931 Acc: 0.9696 | Val Loss: 0.2392 Acc: 0.9278                                               \n",
      "Epoch 042 | Train Loss: 0.0874 Acc: 0.9724 | Val Loss: 0.1178 Acc: 0.9615                                               \n",
      "Epoch 043 | Train Loss: 0.0805 Acc: 0.9740 | Val Loss: 0.0685 Acc: 0.9761                                               \n",
      "Epoch 044 | Train Loss: 0.0808 Acc: 0.9748 | Val Loss: 0.1028 Acc: 0.9666                                               \n",
      "Epoch 045 | Train Loss: 0.0696 Acc: 0.9781 | Val Loss: 0.1783 Acc: 0.9463                                               \n",
      "Epoch 046 | Train Loss: 0.0780 Acc: 0.9747 | Val Loss: 0.0554 Acc: 0.9818                                               \n",
      "Epoch 047 | Train Loss: 0.0648 Acc: 0.9778 | Val Loss: 0.0820 Acc: 0.9707                                               \n",
      "Epoch 048 | Train Loss: 0.0730 Acc: 0.9749 | Val Loss: 0.0648 Acc: 0.9803                                               \n",
      "Epoch 049 | Train Loss: 0.0627 Acc: 0.9789 | Val Loss: 0.0802 Acc: 0.9764                                               \n",
      "Epoch 050 | Train Loss: 0.0674 Acc: 0.9789 | Val Loss: 0.0554 Acc: 0.9845                                               \n",
      "Epoch 051 | Train Loss: 0.0620 Acc: 0.9794 | Val Loss: 0.0577 Acc: 0.9818                                               \n",
      "Epoch 052 | Train Loss: 0.0657 Acc: 0.9799 | Val Loss: 0.0672 Acc: 0.9794                                               \n",
      "Epoch 053 | Train Loss: 0.0605 Acc: 0.9798 | Val Loss: 0.0864 Acc: 0.9758                                               \n",
      "Epoch 054 | Train Loss: 0.0572 Acc: 0.9821 | Val Loss: 0.1382 Acc: 0.9546                                               \n",
      "Epoch 055 | Train Loss: 0.0595 Acc: 0.9807 | Val Loss: 0.0556 Acc: 0.9812                                               \n",
      "Epoch 056 | Train Loss: 0.0541 Acc: 0.9831 | Val Loss: 0.0562 Acc: 0.9842                                               \n",
      "Epoch 057 | Train Loss: 0.0450 Acc: 0.9849 | Val Loss: 0.0521 Acc: 0.9809                                               \n",
      "Epoch 058 | Train Loss: 0.0597 Acc: 0.9811 | Val Loss: 0.0936 Acc: 0.9693                                               \n",
      "Epoch 059 | Train Loss: 0.0496 Acc: 0.9848 | Val Loss: 0.1333 Acc: 0.9603                                               \n",
      "Epoch 060 | Train Loss: 0.0464 Acc: 0.9846 | Val Loss: 0.0485 Acc: 0.9833                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 128, 'cnn_dropout': 0.20948156393427975, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 64, 'learning_rate': 0.001232408568178715, 'lstm_dense': 32, 'lstm_hidden_size': 64, 'lstm_layers': 6, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 16.6253 Acc: 0.3941 | Val Loss: 2.5196 Acc: 0.5564                                              \n",
      "Epoch 002 | Train Loss: 2.8458 Acc: 0.4756 | Val Loss: 1.9714 Acc: 0.5430                                               \n",
      "Epoch 003 | Train Loss: 2.1403 Acc: 0.4979 | Val Loss: 0.7811 Acc: 0.6379                                               \n",
      "Epoch 004 | Train Loss: 1.2847 Acc: 0.5507 | Val Loss: 1.1263 Acc: 0.5460                                               \n",
      "Epoch 005 | Train Loss: 1.0863 Acc: 0.5723 | Val Loss: 0.8446 Acc: 0.6890                                               \n",
      "Epoch 006 | Train Loss: 0.9714 Acc: 0.5872 | Val Loss: 0.7412 Acc: 0.6236                                               \n",
      "Epoch 007 | Train Loss: 0.8837 Acc: 0.6077 | Val Loss: 0.8533 Acc: 0.5899                                               \n",
      "Epoch 008 | Train Loss: 0.8013 Acc: 0.6304 | Val Loss: 0.6148 Acc: 0.6669                                               \n",
      "Epoch 009 | Train Loss: 0.7590 Acc: 0.6397 | Val Loss: 0.6164 Acc: 0.7155                                               \n",
      "Epoch 010 | Train Loss: 0.7376 Acc: 0.6495 | Val Loss: 0.6746 Acc: 0.6675                                               \n",
      "Epoch 011 | Train Loss: 0.7121 Acc: 0.6598 | Val Loss: 0.6000 Acc: 0.7063                                               \n",
      "Epoch 012 | Train Loss: 0.6789 Acc: 0.6751 | Val Loss: 0.5756 Acc: 0.7185                                               \n",
      "Epoch 013 | Train Loss: 0.6707 Acc: 0.6848 | Val Loss: 0.5824 Acc: 0.7107                                               \n",
      "Epoch 014 | Train Loss: 0.6337 Acc: 0.7067 | Val Loss: 0.5899 Acc: 0.6940                                               \n",
      "Epoch 015 | Train Loss: 0.6126 Acc: 0.7225 | Val Loss: 0.5148 Acc: 0.7510                                               \n",
      "Epoch 016 | Train Loss: 0.5796 Acc: 0.7345 | Val Loss: 0.4943 Acc: 0.7860                                               \n",
      "Epoch 017 | Train Loss: 0.5629 Acc: 0.7501 | Val Loss: 0.5589 Acc: 0.6719                                               \n",
      "Epoch 018 | Train Loss: 0.5389 Acc: 0.7645 | Val Loss: 0.5815 Acc: 0.6925                                               \n",
      "Epoch 019 | Train Loss: 0.5224 Acc: 0.7728 | Val Loss: 0.5526 Acc: 0.6806                                               \n",
      "Epoch 020 | Train Loss: 0.5045 Acc: 0.7834 | Val Loss: 0.4941 Acc: 0.7516                                               \n",
      "Epoch 021 | Train Loss: 0.4990 Acc: 0.7870 | Val Loss: 0.5250 Acc: 0.7367                                               \n",
      "Epoch 022 | Train Loss: 0.4856 Acc: 0.7918 | Val Loss: 0.5256 Acc: 0.7260                                               \n",
      "Epoch 023 | Train Loss: 0.4992 Acc: 0.7855 | Val Loss: 0.4736 Acc: 0.7937                                               \n",
      "Epoch 024 | Train Loss: 0.4607 Acc: 0.8042 | Val Loss: 0.4652 Acc: 0.7603                                               \n",
      "Epoch 025 | Train Loss: 0.4715 Acc: 0.7969 | Val Loss: 0.4632 Acc: 0.7773                                               \n",
      "Epoch 026 | Train Loss: 0.4460 Acc: 0.8095 | Val Loss: 0.5068 Acc: 0.7403                                               \n",
      "Epoch 027 | Train Loss: 0.4586 Acc: 0.8024 | Val Loss: 0.4959 Acc: 0.7576                                               \n",
      "Epoch 028 | Train Loss: 0.4512 Acc: 0.8069 | Val Loss: 0.4449 Acc: 0.7576                                               \n",
      "Epoch 029 | Train Loss: 0.4411 Acc: 0.8140 | Val Loss: 0.5225 Acc: 0.7188                                               \n",
      "Epoch 030 | Train Loss: 0.4410 Acc: 0.8126 | Val Loss: 0.7361 Acc: 0.6812                                               \n",
      "Epoch 031 | Train Loss: 0.4371 Acc: 0.8141 | Val Loss: 0.4437 Acc: 0.7487                                               \n",
      "Epoch 032 | Train Loss: 0.4065 Acc: 0.8259 | Val Loss: 0.4680 Acc: 0.7522                                               \n",
      "Epoch 033 | Train Loss: 0.4058 Acc: 0.8289 | Val Loss: 0.4486 Acc: 0.7696                                               \n",
      "Epoch 034 | Train Loss: 0.4111 Acc: 0.8266 | Val Loss: 0.4230 Acc: 0.7916                                               \n",
      "Epoch 035 | Train Loss: 0.4158 Acc: 0.8295 | Val Loss: 0.4450 Acc: 0.7979                                               \n",
      "Epoch 036 | Train Loss: 0.4110 Acc: 0.8287 | Val Loss: 0.4968 Acc: 0.7636                                               \n",
      "Epoch 037 | Train Loss: 0.4139 Acc: 0.8251 | Val Loss: 0.5202 Acc: 0.7296                                               \n",
      "Epoch 038 | Train Loss: 0.4050 Acc: 0.8319 | Val Loss: 0.5381 Acc: 0.7615                                               \n",
      "Epoch 039 | Train Loss: 0.4179 Acc: 0.8239 | Val Loss: 0.4904 Acc: 0.7546                                               \n",
      "Epoch 040 | Train Loss: 0.3963 Acc: 0.8357 | Val Loss: 0.4331 Acc: 0.7973                                               \n",
      "Epoch 041 | Train Loss: 0.4009 Acc: 0.8321 | Val Loss: 0.4923 Acc: 0.7725                                               \n",
      "Epoch 042 | Train Loss: 0.4115 Acc: 0.8259 | Val Loss: 0.4318 Acc: 0.8272                                               \n",
      "Epoch 043 | Train Loss: 0.3867 Acc: 0.8396 | Val Loss: 0.5741 Acc: 0.7457                                               \n",
      "Epoch 044 | Train Loss: 0.4232 Acc: 0.8195 | Val Loss: 0.4795 Acc: 0.7955                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 36, 'cnn_dense': 64, 'cnn_dropout': 0.6994693831734867, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 32, 'cnn_kernels_2': 64, 'learning_rate': 0.0002601190919777001, 'lstm_dense': 64, 'lstm_hidden_size': 128, 'lstm_layers': 6, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 2.8319 Acc: 0.3981 | Val Loss: 1.0972 Acc: 0.4504                                               \n",
      "Epoch 002 | Train Loss: 1.1146 Acc: 0.4458 | Val Loss: 1.0187 Acc: 0.4206                                               \n",
      "Epoch 003 | Train Loss: 1.0977 Acc: 0.4497 | Val Loss: 1.0024 Acc: 0.5093                                               \n",
      "Epoch 004 | Train Loss: 1.0717 Acc: 0.4563 | Val Loss: 1.0907 Acc: 0.4107                                               \n",
      "Epoch 005 | Train Loss: 1.2364 Acc: 0.3555 | Val Loss: 1.3179 Acc: 0.2743                                               \n",
      "Epoch 006 | Train Loss: 1.2764 Acc: 0.3850 | Val Loss: 1.2946 Acc: 0.4421                                               \n",
      "Epoch 007 | Train Loss: 1.2819 Acc: 0.4422 | Val Loss: 1.2772 Acc: 0.4421                                               \n",
      "Epoch 008 | Train Loss: 1.2689 Acc: 0.4422 | Val Loss: 1.2663 Acc: 0.4421                                               \n",
      "Epoch 009 | Train Loss: 1.2607 Acc: 0.4422 | Val Loss: 1.2594 Acc: 0.4421                                               \n",
      "Epoch 010 | Train Loss: 1.2560 Acc: 0.4422 | Val Loss: 1.2549 Acc: 0.4421                                               \n",
      "Epoch 011 | Train Loss: 1.2529 Acc: 0.4422 | Val Loss: 1.2517 Acc: 0.4421                                               \n",
      "Epoch 012 | Train Loss: 1.2508 Acc: 0.4422 | Val Loss: 1.2495 Acc: 0.4421                                               \n",
      "Epoch 013 | Train Loss: 1.2485 Acc: 0.4422 | Val Loss: 1.2479 Acc: 0.4421                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 64, 'cnn_dropout': 0.6196994422888118, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 32, 'cnn_kernels_2': 16, 'learning_rate': 0.00066254178603668, 'lstm_dense': 32, 'lstm_hidden_size': 64, 'lstm_layers': 4, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 4.3806 Acc: 0.4410 | Val Loss: 1.2653 Acc: 0.4421                                               \n",
      "Epoch 002 | Train Loss: 1.2557 Acc: 0.4422 | Val Loss: 1.2487 Acc: 0.4421                                               \n",
      "Epoch 003 | Train Loss: 1.2463 Acc: 0.4422 | Val Loss: 1.2440 Acc: 0.4421                                               \n",
      "Epoch 004 | Train Loss: 1.2435 Acc: 0.4422 | Val Loss: 1.2425 Acc: 0.4421                                               \n",
      "Epoch 005 | Train Loss: 1.2425 Acc: 0.4422 | Val Loss: 1.2420 Acc: 0.4421                                               \n",
      "Epoch 006 | Train Loss: 1.2423 Acc: 0.4422 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 007 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 008 | Train Loss: 1.2422 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 009 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 010 | Train Loss: 1.2422 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 011 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 012 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 013 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2421 Acc: 0.4421                                               \n",
      "Epoch 014 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 015 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 016 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 017 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 018 | Train Loss: 1.2422 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 019 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 020 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 021 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 022 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 023 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 024 | Train Loss: 1.2422 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 025 | Train Loss: 1.2422 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 026 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 027 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 028 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 029 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 030 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 031 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 032 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 033 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 034 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 035 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 036 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 037 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 038 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2415 Acc: 0.4421                                               \n",
      "Epoch 039 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 040 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 041 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2417 Acc: 0.4421                                               \n",
      "Epoch 042 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 043 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 044 | Train Loss: 1.2420 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 045 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 046 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 047 | Train Loss: 1.2419 Acc: 0.4422 | Val Loss: 1.2415 Acc: 0.4421                                               \n",
      "Epoch 048 | Train Loss: 1.2418 Acc: 0.4422 | Val Loss: 1.2415 Acc: 0.4421                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 256, 'cnn_dropout': 0.2888897302753447, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 96, 'learning_rate': 0.0005169009351730779, 'lstm_dense': 64, 'lstm_hidden_size': 32, 'lstm_layers': 4, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 30.4736 Acc: 0.3679 | Val Loss: 3.2384 Acc: 0.3881                                              \n",
      "Epoch 002 | Train Loss: 8.3396 Acc: 0.4073 | Val Loss: 2.2018 Acc: 0.4170                                               \n",
      "Epoch 003 | Train Loss: 4.9080 Acc: 0.4130 | Val Loss: 1.6309 Acc: 0.4713                                               \n",
      "Epoch 004 | Train Loss: 3.3434 Acc: 0.4367 | Val Loss: 1.2062 Acc: 0.5313                                               \n",
      "Epoch 005 | Train Loss: 2.4843 Acc: 0.4626 | Val Loss: 1.3218 Acc: 0.5346                                               \n",
      "Epoch 006 | Train Loss: 2.0787 Acc: 0.4750 | Val Loss: 0.9799 Acc: 0.5940                                               \n",
      "Epoch 007 | Train Loss: 1.7067 Acc: 0.5003 | Val Loss: 0.9490 Acc: 0.6200                                               \n",
      "Epoch 008 | Train Loss: 1.5827 Acc: 0.5076 | Val Loss: 1.4676 Acc: 0.4836                                               \n",
      "Epoch 009 | Train Loss: 1.4423 Acc: 0.5195 | Val Loss: 0.8114 Acc: 0.6749                                               \n",
      "Epoch 010 | Train Loss: 1.3288 Acc: 0.5310 | Val Loss: 0.8749 Acc: 0.6785                                               \n",
      "Epoch 011 | Train Loss: 1.2389 Acc: 0.5447 | Val Loss: 1.2538 Acc: 0.5510                                               \n",
      "Epoch 012 | Train Loss: 1.1438 Acc: 0.5603 | Val Loss: 0.6925 Acc: 0.6528                                               \n",
      "Epoch 013 | Train Loss: 1.0377 Acc: 0.5884 | Val Loss: 0.8388 Acc: 0.6310                                               \n",
      "Epoch 014 | Train Loss: 0.9471 Acc: 0.6182 | Val Loss: 0.6943 Acc: 0.6645                                               \n",
      "Epoch 015 | Train Loss: 0.9081 Acc: 0.6216 | Val Loss: 0.8514 Acc: 0.7066                                               \n",
      "Epoch 016 | Train Loss: 0.8927 Acc: 0.6434 | Val Loss: 0.8354 Acc: 0.6561                                               \n",
      "Epoch 017 | Train Loss: 0.7962 Acc: 0.6705 | Val Loss: 0.6132 Acc: 0.7460                                               \n",
      "Epoch 018 | Train Loss: 0.7917 Acc: 0.6833 | Val Loss: 0.6866 Acc: 0.6719                                               \n",
      "Epoch 019 | Train Loss: 0.7505 Acc: 0.7002 | Val Loss: 0.5630 Acc: 0.7433                                               \n",
      "Epoch 020 | Train Loss: 0.7218 Acc: 0.7104 | Val Loss: 0.4905 Acc: 0.7991                                               \n",
      "Epoch 021 | Train Loss: 0.6741 Acc: 0.7324 | Val Loss: 0.5453 Acc: 0.7439                                               \n",
      "Epoch 022 | Train Loss: 0.6805 Acc: 0.7340 | Val Loss: 0.5191 Acc: 0.7558                                               \n",
      "Epoch 023 | Train Loss: 0.6285 Acc: 0.7468 | Val Loss: 0.4506 Acc: 0.8081                                               \n",
      "Epoch 024 | Train Loss: 0.5721 Acc: 0.7744 | Val Loss: 0.4700 Acc: 0.7687                                               \n",
      "Epoch 025 | Train Loss: 0.5561 Acc: 0.7789 | Val Loss: 0.4363 Acc: 0.8087                                               \n",
      "Epoch 026 | Train Loss: 0.5124 Acc: 0.7968 | Val Loss: 0.3846 Acc: 0.8499                                               \n",
      "Epoch 027 | Train Loss: 0.4813 Acc: 0.8154 | Val Loss: 0.4307 Acc: 0.8307                                               \n",
      "Epoch 028 | Train Loss: 0.4532 Acc: 0.8315 | Val Loss: 0.3605 Acc: 0.8728                                               \n",
      "Epoch 029 | Train Loss: 0.3937 Acc: 0.8578 | Val Loss: 0.3475 Acc: 0.8639                                               \n",
      "Epoch 030 | Train Loss: 0.3454 Acc: 0.8771 | Val Loss: 0.2391 Acc: 0.9116                                               \n",
      "Epoch 031 | Train Loss: 0.3245 Acc: 0.8857 | Val Loss: 0.3120 Acc: 0.8797                                               \n",
      "Epoch 032 | Train Loss: 0.2902 Acc: 0.8986 | Val Loss: 0.2571 Acc: 0.9069                                               \n",
      "Epoch 033 | Train Loss: 0.2553 Acc: 0.9100 | Val Loss: 0.2225 Acc: 0.9266                                               \n",
      "Epoch 034 | Train Loss: 0.2240 Acc: 0.9236 | Val Loss: 0.2273 Acc: 0.9200                                               \n",
      "Epoch 035 | Train Loss: 0.2189 Acc: 0.9222 | Val Loss: 0.2392 Acc: 0.9155                                               \n",
      "Epoch 036 | Train Loss: 0.2071 Acc: 0.9300 | Val Loss: 0.2688 Acc: 0.9078                                               \n",
      "Epoch 037 | Train Loss: 0.1882 Acc: 0.9354 | Val Loss: 0.1755 Acc: 0.9382                                               \n",
      "Epoch 038 | Train Loss: 0.1655 Acc: 0.9416 | Val Loss: 0.1398 Acc: 0.9475                                               \n",
      "Epoch 039 | Train Loss: 0.1715 Acc: 0.9424 | Val Loss: 0.1577 Acc: 0.9394                                               \n",
      "Epoch 040 | Train Loss: 0.1474 Acc: 0.9498 | Val Loss: 0.1466 Acc: 0.9507                                               \n",
      "Epoch 041 | Train Loss: 0.1532 Acc: 0.9486 | Val Loss: 0.1586 Acc: 0.9436                                               \n",
      "Epoch 042 | Train Loss: 0.1296 Acc: 0.9563 | Val Loss: 0.1162 Acc: 0.9636                                               \n",
      "Epoch 043 | Train Loss: 0.1346 Acc: 0.9556 | Val Loss: 0.2429 Acc: 0.9218                                               \n",
      "Epoch 044 | Train Loss: 0.1379 Acc: 0.9526 | Val Loss: 0.1390 Acc: 0.9504                                               \n",
      "Epoch 045 | Train Loss: 0.1158 Acc: 0.9614 | Val Loss: 0.1969 Acc: 0.9331                                               \n",
      "Epoch 046 | Train Loss: 0.1095 Acc: 0.9645 | Val Loss: 0.1058 Acc: 0.9615                                               \n",
      "Epoch 047 | Train Loss: 0.0978 Acc: 0.9679 | Val Loss: 0.0916 Acc: 0.9678                                               \n",
      "Epoch 048 | Train Loss: 0.0940 Acc: 0.9707 | Val Loss: 0.0897 Acc: 0.9657                                               \n",
      "Epoch 049 | Train Loss: 0.0946 Acc: 0.9697 | Val Loss: 0.0950 Acc: 0.9693                                               \n",
      "Epoch 050 | Train Loss: 0.0996 Acc: 0.9676 | Val Loss: 0.0961 Acc: 0.9687                                               \n",
      "Epoch 051 | Train Loss: 0.0809 Acc: 0.9726 | Val Loss: 0.0791 Acc: 0.9746                                               \n",
      "Epoch 052 | Train Loss: 0.0795 Acc: 0.9743 | Val Loss: 0.0735 Acc: 0.9773                                               \n",
      "Epoch 053 | Train Loss: 0.0836 Acc: 0.9740 | Val Loss: 0.1150 Acc: 0.9603                                               \n",
      "Epoch 054 | Train Loss: 0.0658 Acc: 0.9794 | Val Loss: 0.0762 Acc: 0.9743                                               \n",
      "Epoch 055 | Train Loss: 0.0707 Acc: 0.9769 | Val Loss: 0.0749 Acc: 0.9788                                               \n",
      "Epoch 056 | Train Loss: 0.0714 Acc: 0.9778 | Val Loss: 0.0825 Acc: 0.9758                                               \n",
      "Epoch 057 | Train Loss: 0.0678 Acc: 0.9777 | Val Loss: 0.0842 Acc: 0.9722                                               \n",
      "Epoch 058 | Train Loss: 0.0708 Acc: 0.9772 | Val Loss: 0.0849 Acc: 0.9752                                               \n",
      "Epoch 059 | Train Loss: 0.0626 Acc: 0.9790 | Val Loss: 0.0886 Acc: 0.9701                                               \n",
      "Epoch 060 | Train Loss: 0.0562 Acc: 0.9818 | Val Loss: 0.0575 Acc: 0.9833                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 64, 'cnn_dropout': 0.5722445634009317, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 16, 'cnn_kernels_2': 16, 'learning_rate': 0.009119915608947607, 'lstm_dense': 32, 'lstm_hidden_size': 96, 'lstm_layers': 5, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 65.3978 Acc: 0.4362 | Val Loss: 1.2422 Acc: 0.4421                                              \n",
      "Epoch 002 | Train Loss: 1.2425 Acc: 0.4422 | Val Loss: 1.2430 Acc: 0.4421                                               \n",
      "Epoch 003 | Train Loss: 1.2421 Acc: 0.4422 | Val Loss: 1.2444 Acc: 0.4421                                               \n",
      "Epoch 004 | Train Loss: 1.2428 Acc: 0.4422 | Val Loss: 1.2421 Acc: 0.4421                                               \n",
      "Epoch 005 | Train Loss: 1.2423 Acc: 0.4422 | Val Loss: 1.2420 Acc: 0.4421                                               \n",
      "Epoch 006 | Train Loss: 1.2424 Acc: 0.4422 | Val Loss: 1.2425 Acc: 0.4421                                               \n",
      "Epoch 007 | Train Loss: 1.2424 Acc: 0.4422 | Val Loss: 1.2420 Acc: 0.4421                                               \n",
      "Epoch 008 | Train Loss: 1.2427 Acc: 0.4422 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 009 | Train Loss: 1.2424 Acc: 0.4422 | Val Loss: 1.2426 Acc: 0.4421                                               \n",
      "Epoch 010 | Train Loss: 1.2427 Acc: 0.4422 | Val Loss: 1.2420 Acc: 0.4421                                               \n",
      "Epoch 011 | Train Loss: 1.2426 Acc: 0.4422 | Val Loss: 1.2421 Acc: 0.4421                                               \n",
      "Epoch 012 | Train Loss: 1.2426 Acc: 0.4422 | Val Loss: 1.2420 Acc: 0.4421                                               \n",
      "Epoch 013 | Train Loss: 1.2426 Acc: 0.4422 | Val Loss: 1.2420 Acc: 0.4421                                               \n",
      "Epoch 014 | Train Loss: 1.2423 Acc: 0.4422 | Val Loss: 1.2427 Acc: 0.4421                                               \n",
      "Epoch 015 | Train Loss: 1.2423 Acc: 0.4422 | Val Loss: 1.2427 Acc: 0.4421                                               \n",
      "Epoch 016 | Train Loss: 1.2426 Acc: 0.4422 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 017 | Train Loss: 1.2422 Acc: 0.4422 | Val Loss: 1.2422 Acc: 0.4421                                               \n",
      "Epoch 018 | Train Loss: 1.2423 Acc: 0.4422 | Val Loss: 1.2429 Acc: 0.4421                                               \n",
      "Epoch 019 | Train Loss: 1.2427 Acc: 0.4422 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 020 | Train Loss: 1.2423 Acc: 0.4422 | Val Loss: 1.2422 Acc: 0.4421                                               \n",
      "Epoch 021 | Train Loss: 1.2423 Acc: 0.4422 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 022 | Train Loss: 1.2424 Acc: 0.4422 | Val Loss: 1.2420 Acc: 0.4421                                               \n",
      "Epoch 023 | Train Loss: 1.2424 Acc: 0.4422 | Val Loss: 1.2422 Acc: 0.4421                                               \n",
      "Epoch 024 | Train Loss: 1.2425 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 025 | Train Loss: 1.2425 Acc: 0.4422 | Val Loss: 1.2423 Acc: 0.4421                                               \n",
      "Epoch 026 | Train Loss: 1.2425 Acc: 0.4422 | Val Loss: 1.2420 Acc: 0.4421                                               \n",
      "Epoch 027 | Train Loss: 1.2424 Acc: 0.4422 | Val Loss: 1.2423 Acc: 0.4421                                               \n",
      "Epoch 028 | Train Loss: 1.2424 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 029 | Train Loss: 1.2424 Acc: 0.4422 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 030 | Train Loss: 1.2425 Acc: 0.4422 | Val Loss: 1.2422 Acc: 0.4421                                               \n",
      "Epoch 031 | Train Loss: 1.2422 Acc: 0.4422 | Val Loss: 1.2429 Acc: 0.4421                                               \n",
      "Epoch 032 | Train Loss: 1.2424 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 033 | Train Loss: 1.2424 Acc: 0.4422 | Val Loss: 1.2423 Acc: 0.4421                                               \n",
      "Epoch 034 | Train Loss: 1.2423 Acc: 0.4422 | Val Loss: 1.2427 Acc: 0.4421                                               \n",
      "Epoch 035 | Train Loss: 1.2426 Acc: 0.4422 | Val Loss: 1.2423 Acc: 0.4421                                               \n",
      "Epoch 036 | Train Loss: 1.2425 Acc: 0.4422 | Val Loss: 1.2422 Acc: 0.4421                                               \n",
      "Epoch 037 | Train Loss: 1.2425 Acc: 0.4422 | Val Loss: 1.2423 Acc: 0.4421                                               \n",
      "Epoch 038 | Train Loss: 1.2425 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 256, 'cnn_dropout': 0.28515577290718663, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 48, 'cnn_kernels_2': 96, 'learning_rate': 5.900102059263463e-05, 'lstm_dense': 64, 'lstm_hidden_size': 32, 'lstm_layers': 4, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 72.6285 Acc: 0.3161 | Val Loss: 28.0690 Acc: 0.3409                                             \n",
      "Epoch 002 | Train Loss: 38.0412 Acc: 0.3431 | Val Loss: 22.2154 Acc: 0.3696                                             \n",
      "Epoch 003 | Train Loss: 27.1633 Acc: 0.3691 | Val Loss: 17.1513 Acc: 0.3621                                             \n",
      "Epoch 004 | Train Loss: 20.8378 Acc: 0.3735 | Val Loss: 10.5211 Acc: 0.3776                                             \n",
      "Epoch 005 | Train Loss: 16.5980 Acc: 0.3926 | Val Loss: 8.6422 Acc: 0.3460                                              \n",
      "Epoch 006 | Train Loss: 13.4226 Acc: 0.4014 | Val Loss: 5.0233 Acc: 0.4281                                              \n",
      "Epoch 007 | Train Loss: 11.0021 Acc: 0.4103 | Val Loss: 3.2671 Acc: 0.4257                                              \n",
      "Epoch 008 | Train Loss: 9.3122 Acc: 0.4164 | Val Loss: 2.8290 Acc: 0.4466                                               \n",
      "Epoch 009 | Train Loss: 7.7461 Acc: 0.4251 | Val Loss: 1.9319 Acc: 0.4728                                               \n",
      "Epoch 010 | Train Loss: 6.6954 Acc: 0.4264 | Val Loss: 1.7532 Acc: 0.4687                                               \n",
      "Epoch 011 | Train Loss: 5.9174 Acc: 0.4290 | Val Loss: 1.4301 Acc: 0.4967                                               \n",
      "Epoch 012 | Train Loss: 5.0577 Acc: 0.4386 | Val Loss: 1.4396 Acc: 0.5203                                               \n",
      "Epoch 013 | Train Loss: 4.5336 Acc: 0.4506 | Val Loss: 1.3743 Acc: 0.5424                                               \n",
      "Epoch 014 | Train Loss: 3.8655 Acc: 0.4596 | Val Loss: 1.5099 Acc: 0.5161                                               \n",
      "Epoch 015 | Train Loss: 3.5350 Acc: 0.4704 | Val Loss: 0.9678 Acc: 0.6033                                               \n",
      "Epoch 016 | Train Loss: 3.1446 Acc: 0.4756 | Val Loss: 1.2387 Acc: 0.5561                                               \n",
      "Epoch 017 | Train Loss: 2.8425 Acc: 0.4873 | Val Loss: 0.9744 Acc: 0.6176                                               \n",
      "Epoch 018 | Train Loss: 2.6352 Acc: 0.4864 | Val Loss: 1.0329 Acc: 0.6078                                               \n",
      "Epoch 019 | Train Loss: 2.3835 Acc: 0.5094 | Val Loss: 0.8412 Acc: 0.6218                                               \n",
      "Epoch 020 | Train Loss: 2.1697 Acc: 0.5238 | Val Loss: 0.8169 Acc: 0.6367                                               \n",
      "Epoch 021 | Train Loss: 2.0173 Acc: 0.5337 | Val Loss: 1.2839 Acc: 0.5716                                               \n",
      "Epoch 022 | Train Loss: 1.8007 Acc: 0.5560 | Val Loss: 0.7580 Acc: 0.6985                                               \n",
      "Epoch 023 | Train Loss: 1.7845 Acc: 0.5532 | Val Loss: 0.7302 Acc: 0.6693                                               \n",
      "Epoch 024 | Train Loss: 1.5677 Acc: 0.5902 | Val Loss: 0.6315 Acc: 0.6678                                               \n",
      "Epoch 025 | Train Loss: 1.4682 Acc: 0.6021 | Val Loss: 0.7887 Acc: 0.6818                                               \n",
      "Epoch 026 | Train Loss: 1.3926 Acc: 0.6147 | Val Loss: 0.6217 Acc: 0.7427                                               \n",
      "Epoch 027 | Train Loss: 1.2996 Acc: 0.6312 | Val Loss: 0.7096 Acc: 0.7364                                               \n",
      "Epoch 028 | Train Loss: 1.2146 Acc: 0.6449 | Val Loss: 0.8582 Acc: 0.6606                                               \n",
      "Epoch 029 | Train Loss: 1.1591 Acc: 0.6530 | Val Loss: 0.6793 Acc: 0.7546                                               \n",
      "Epoch 030 | Train Loss: 1.0923 Acc: 0.6710 | Val Loss: 0.6871 Acc: 0.7149                                               \n",
      "Epoch 031 | Train Loss: 1.0438 Acc: 0.6783 | Val Loss: 0.5207 Acc: 0.7863                                               \n",
      "Epoch 032 | Train Loss: 0.9830 Acc: 0.6976 | Val Loss: 0.5317 Acc: 0.8176                                               \n",
      "Epoch 033 | Train Loss: 0.9535 Acc: 0.7041 | Val Loss: 1.0046 Acc: 0.6516                                               \n",
      "Epoch 034 | Train Loss: 0.8913 Acc: 0.7185 | Val Loss: 0.8968 Acc: 0.7137                                               \n",
      "Epoch 035 | Train Loss: 0.8204 Acc: 0.7347 | Val Loss: 0.5474 Acc: 0.7931                                               \n",
      "Epoch 036 | Train Loss: 0.8009 Acc: 0.7401 | Val Loss: 0.5228 Acc: 0.8173                                               \n",
      "Epoch 037 | Train Loss: 0.7688 Acc: 0.7560 | Val Loss: 0.4621 Acc: 0.8466                                               \n",
      "Epoch 038 | Train Loss: 0.7170 Acc: 0.7673 | Val Loss: 0.6112 Acc: 0.7842                                               \n",
      "Epoch 039 | Train Loss: 0.6833 Acc: 0.7851 | Val Loss: 0.4165 Acc: 0.8591                                               \n",
      "Epoch 040 | Train Loss: 0.6457 Acc: 0.7883 | Val Loss: 0.4000 Acc: 0.8743                                               \n",
      "Epoch 041 | Train Loss: 0.5941 Acc: 0.8053 | Val Loss: 0.4327 Acc: 0.8493                                               \n",
      "Epoch 042 | Train Loss: 0.5893 Acc: 0.8095 | Val Loss: 0.6342 Acc: 0.7925                                               \n",
      "Epoch 043 | Train Loss: 0.5597 Acc: 0.8172 | Val Loss: 0.4205 Acc: 0.8642                                               \n",
      "Epoch 044 | Train Loss: 0.5289 Acc: 0.8269 | Val Loss: 0.4458 Acc: 0.8501                                               \n",
      "Epoch 045 | Train Loss: 0.4952 Acc: 0.8427 | Val Loss: 0.4234 Acc: 0.8651                                               \n",
      "Epoch 046 | Train Loss: 0.4733 Acc: 0.8491 | Val Loss: 0.8193 Acc: 0.7636                                               \n",
      "Epoch 047 | Train Loss: 0.4590 Acc: 0.8513 | Val Loss: 0.3191 Acc: 0.8893                                               \n",
      "Epoch 048 | Train Loss: 0.4363 Acc: 0.8607 | Val Loss: 0.4500 Acc: 0.8490                                               \n",
      "Epoch 049 | Train Loss: 0.4293 Acc: 0.8610 | Val Loss: 0.5016 Acc: 0.8293                                               \n",
      "Epoch 050 | Train Loss: 0.4172 Acc: 0.8654 | Val Loss: 0.2973 Acc: 0.8949                                               \n",
      "Epoch 051 | Train Loss: 0.3832 Acc: 0.8751 | Val Loss: 0.3666 Acc: 0.8675                                               \n",
      "Epoch 052 | Train Loss: 0.3752 Acc: 0.8799 | Val Loss: 0.2768 Acc: 0.9090                                               \n",
      "Epoch 053 | Train Loss: 0.3552 Acc: 0.8877 | Val Loss: 0.3253 Acc: 0.8884                                               \n",
      "Epoch 054 | Train Loss: 0.3431 Acc: 0.8878 | Val Loss: 0.2945 Acc: 0.8979                                               \n",
      "Epoch 055 | Train Loss: 0.3313 Acc: 0.8924 | Val Loss: 0.4294 Acc: 0.8496                                               \n",
      "Epoch 056 | Train Loss: 0.3150 Acc: 0.9011 | Val Loss: 0.3233 Acc: 0.8958                                               \n",
      "Epoch 057 | Train Loss: 0.3075 Acc: 0.9021 | Val Loss: 0.2705 Acc: 0.8994                                               \n",
      "Epoch 058 | Train Loss: 0.3005 Acc: 0.9075 | Val Loss: 0.2427 Acc: 0.9155                                               \n",
      "Epoch 059 | Train Loss: 0.2775 Acc: 0.9110 | Val Loss: 0.2386 Acc: 0.9266                                               \n",
      "Epoch 060 | Train Loss: 0.2806 Acc: 0.9108 | Val Loss: 0.2158 Acc: 0.9278                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 32, 'cnn_dropout': 0.3057098536802707, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 96, 'learning_rate': 5.8587963251717315e-05, 'lstm_dense': 64, 'lstm_hidden_size': 32, 'lstm_layers': 4, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 24.7277 Acc: 0.3645 | Val Loss: 3.5969 Acc: 0.5185                                              \n",
      "Epoch 002 | Train Loss: 13.3342 Acc: 0.3991 | Val Loss: 2.7929 Acc: 0.5299                                              \n",
      "Epoch 003 | Train Loss: 8.9475 Acc: 0.4271 | Val Loss: 2.2087 Acc: 0.5540                                               \n",
      "Epoch 004 | Train Loss: 6.5908 Acc: 0.4397 | Val Loss: 2.0608 Acc: 0.5039                                               \n",
      "Epoch 005 | Train Loss: 4.7585 Acc: 0.4585 | Val Loss: 1.6318 Acc: 0.5537                                               \n",
      "Epoch 006 | Train Loss: 3.5577 Acc: 0.4900 | Val Loss: 1.4248 Acc: 0.5131                                               \n",
      "Epoch 007 | Train Loss: 2.9515 Acc: 0.5038 | Val Loss: 1.0914 Acc: 0.6128                                               \n",
      "Epoch 008 | Train Loss: 2.4024 Acc: 0.5185 | Val Loss: 1.0771 Acc: 0.5949                                               \n",
      "Epoch 009 | Train Loss: 2.0496 Acc: 0.5357 | Val Loss: 0.9694 Acc: 0.6379                                               \n",
      "Epoch 010 | Train Loss: 1.8304 Acc: 0.5513 | Val Loss: 0.8887 Acc: 0.6501                                               \n",
      "Epoch 011 | Train Loss: 1.5921 Acc: 0.5708 | Val Loss: 0.8576 Acc: 0.6710                                               \n",
      "Epoch 012 | Train Loss: 1.3884 Acc: 0.5937 | Val Loss: 0.9071 Acc: 0.6424                                               \n",
      "Epoch 013 | Train Loss: 1.2479 Acc: 0.6122 | Val Loss: 0.8246 Acc: 0.6513                                               \n",
      "Epoch 014 | Train Loss: 1.1683 Acc: 0.6235 | Val Loss: 0.7133 Acc: 0.7033                                               \n",
      "Epoch 015 | Train Loss: 1.1007 Acc: 0.6368 | Val Loss: 0.8929 Acc: 0.6113                                               \n",
      "Epoch 016 | Train Loss: 1.0063 Acc: 0.6630 | Val Loss: 0.7212 Acc: 0.7266                                               \n",
      "Epoch 017 | Train Loss: 0.9587 Acc: 0.6718 | Val Loss: 0.6218 Acc: 0.7454                                               \n",
      "Epoch 018 | Train Loss: 0.9000 Acc: 0.6846 | Val Loss: 0.6612 Acc: 0.6937                                               \n",
      "Epoch 019 | Train Loss: 0.8624 Acc: 0.6959 | Val Loss: 0.6228 Acc: 0.7481                                               \n",
      "Epoch 020 | Train Loss: 0.7887 Acc: 0.7124 | Val Loss: 0.7271 Acc: 0.7101                                               \n",
      "Epoch 021 | Train Loss: 0.7862 Acc: 0.7216 | Val Loss: 0.6970 Acc: 0.7290                                               \n",
      "Epoch 022 | Train Loss: 0.7422 Acc: 0.7298 | Val Loss: 0.7680 Acc: 0.7125                                               \n",
      "Epoch 023 | Train Loss: 0.7270 Acc: 0.7380 | Val Loss: 0.6459 Acc: 0.7406                                               \n",
      "Epoch 024 | Train Loss: 0.6957 Acc: 0.7453 | Val Loss: 0.7347 Acc: 0.7075                                               \n",
      "Epoch 025 | Train Loss: 0.6558 Acc: 0.7530 | Val Loss: 0.9576 Acc: 0.6690                                               \n",
      "Epoch 026 | Train Loss: 0.6493 Acc: 0.7616 | Val Loss: 0.8173 Acc: 0.7263                                               \n",
      "Epoch 027 | Train Loss: 0.6101 Acc: 0.7749 | Val Loss: 0.4954 Acc: 0.8113                                               \n",
      "Epoch 028 | Train Loss: 0.6133 Acc: 0.7748 | Val Loss: 0.7842 Acc: 0.7319                                               \n",
      "Epoch 029 | Train Loss: 0.5962 Acc: 0.7811 | Val Loss: 0.5427 Acc: 0.7890                                               \n",
      "Epoch 030 | Train Loss: 0.5783 Acc: 0.7883 | Val Loss: 0.5287 Acc: 0.7666                                               \n",
      "Epoch 031 | Train Loss: 0.5550 Acc: 0.7979 | Val Loss: 0.8527 Acc: 0.7206                                               \n",
      "Epoch 032 | Train Loss: 0.5451 Acc: 0.8015 | Val Loss: 0.5055 Acc: 0.8152                                               \n",
      "Epoch 033 | Train Loss: 0.5339 Acc: 0.8036 | Val Loss: 0.6530 Acc: 0.7806                                               \n",
      "Epoch 034 | Train Loss: 0.5162 Acc: 0.8137 | Val Loss: 0.6332 Acc: 0.7594                                               \n",
      "Epoch 035 | Train Loss: 0.5075 Acc: 0.8138 | Val Loss: 0.4391 Acc: 0.8278                                               \n",
      "Epoch 036 | Train Loss: 0.4975 Acc: 0.8189 | Val Loss: 0.5099 Acc: 0.7901                                               \n",
      "Epoch 037 | Train Loss: 0.4907 Acc: 0.8217 | Val Loss: 0.4491 Acc: 0.8421                                               \n",
      "Epoch 038 | Train Loss: 0.4722 Acc: 0.8284 | Val Loss: 0.5611 Acc: 0.7970                                               \n",
      "Epoch 039 | Train Loss: 0.4696 Acc: 0.8310 | Val Loss: 0.4919 Acc: 0.8284                                               \n",
      "Epoch 040 | Train Loss: 0.4588 Acc: 0.8334 | Val Loss: 0.4554 Acc: 0.8430                                               \n",
      "Epoch 041 | Train Loss: 0.4453 Acc: 0.8366 | Val Loss: 0.5638 Acc: 0.8036                                               \n",
      "Epoch 042 | Train Loss: 0.4396 Acc: 0.8418 | Val Loss: 0.4565 Acc: 0.8406                                               \n",
      "Epoch 043 | Train Loss: 0.4181 Acc: 0.8483 | Val Loss: 0.4211 Acc: 0.8442                                               \n",
      "Epoch 044 | Train Loss: 0.4154 Acc: 0.8498 | Val Loss: 0.4872 Acc: 0.8251                                               \n",
      "Epoch 045 | Train Loss: 0.4058 Acc: 0.8564 | Val Loss: 0.3735 Acc: 0.8609                                               \n",
      "Epoch 046 | Train Loss: 0.3953 Acc: 0.8596 | Val Loss: 0.8312 Acc: 0.7555                                               \n",
      "Epoch 047 | Train Loss: 0.3851 Acc: 0.8632 | Val Loss: 0.3735 Acc: 0.8612                                               \n",
      "Epoch 048 | Train Loss: 0.3898 Acc: 0.8592 | Val Loss: 0.4364 Acc: 0.8472                                               \n",
      "Epoch 049 | Train Loss: 0.3703 Acc: 0.8713 | Val Loss: 0.3503 Acc: 0.8806                                               \n",
      "Epoch 050 | Train Loss: 0.3832 Acc: 0.8685 | Val Loss: 0.3900 Acc: 0.8591                                               \n",
      "Epoch 051 | Train Loss: 0.3601 Acc: 0.8710 | Val Loss: 0.3538 Acc: 0.8812                                               \n",
      "Epoch 052 | Train Loss: 0.3451 Acc: 0.8795 | Val Loss: 0.3176 Acc: 0.8901                                               \n",
      "Epoch 053 | Train Loss: 0.3396 Acc: 0.8838 | Val Loss: 0.4048 Acc: 0.8478                                               \n",
      "Epoch 054 | Train Loss: 0.3327 Acc: 0.8862 | Val Loss: 0.3153 Acc: 0.8955                                               \n",
      "Epoch 055 | Train Loss: 0.3218 Acc: 0.8886 | Val Loss: 0.4049 Acc: 0.8513                                               \n",
      "Epoch 056 | Train Loss: 0.3175 Acc: 0.8896 | Val Loss: 0.6397 Acc: 0.8015                                               \n",
      "Epoch 057 | Train Loss: 0.2986 Acc: 0.8962 | Val Loss: 0.3991 Acc: 0.8567                                               \n",
      "Epoch 058 | Train Loss: 0.3039 Acc: 0.8925 | Val Loss: 0.5496 Acc: 0.8030                                               \n",
      "Epoch 059 | Train Loss: 0.2976 Acc: 0.8993 | Val Loss: 0.3488 Acc: 0.8693                                               \n",
      "Epoch 060 | Train Loss: 0.2799 Acc: 0.9036 | Val Loss: 0.3262 Acc: 0.8901                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 256, 'cnn_dropout': 0.39256299605068357, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 96, 'learning_rate': 0.001110796895340612, 'lstm_dense': 64, 'lstm_hidden_size': 32, 'lstm_layers': 2, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 16.4984 Acc: 0.4089 | Val Loss: 5.2179 Acc: 0.5436                                              \n",
      "Epoch 002 | Train Loss: 3.1409 Acc: 0.4568 | Val Loss: 0.9892 Acc: 0.6099                                               \n",
      "Epoch 003 | Train Loss: 1.3296 Acc: 0.5265 | Val Loss: 0.7670 Acc: 0.5612                                               \n",
      "Epoch 004 | Train Loss: 0.8721 Acc: 0.6093 | Val Loss: 0.7618 Acc: 0.7131                                               \n",
      "Epoch 005 | Train Loss: 0.7932 Acc: 0.6405 | Val Loss: 0.5672 Acc: 0.7424                                               \n",
      "Epoch 006 | Train Loss: 0.7247 Acc: 0.6673 | Val Loss: 0.7126 Acc: 0.7221                                               \n",
      "Epoch 007 | Train Loss: 0.6835 Acc: 0.6891 | Val Loss: 0.5517 Acc: 0.7997                                               \n",
      "Epoch 008 | Train Loss: 0.6267 Acc: 0.7198 | Val Loss: 0.5355 Acc: 0.7803                                               \n",
      "Epoch 009 | Train Loss: 0.6038 Acc: 0.7348 | Val Loss: 0.4402 Acc: 0.8003                                               \n",
      "Epoch 010 | Train Loss: 0.5697 Acc: 0.7460 | Val Loss: 0.4683 Acc: 0.8200                                               \n",
      "Epoch 011 | Train Loss: 0.5489 Acc: 0.7610 | Val Loss: 0.5317 Acc: 0.7857                                               \n",
      "Epoch 012 | Train Loss: 0.5356 Acc: 0.7688 | Val Loss: 0.4581 Acc: 0.8081                                               \n",
      "Epoch 013 | Train Loss: 0.5089 Acc: 0.7857 | Val Loss: 0.4326 Acc: 0.8236                                               \n",
      "Epoch 014 | Train Loss: 0.5049 Acc: 0.7874 | Val Loss: 0.4069 Acc: 0.8206                                               \n",
      "Epoch 015 | Train Loss: 0.4811 Acc: 0.8001 | Val Loss: 0.4772 Acc: 0.7830                                               \n",
      "Epoch 016 | Train Loss: 0.4669 Acc: 0.8031 | Val Loss: 0.4359 Acc: 0.8346                                               \n",
      "Epoch 017 | Train Loss: 0.4615 Acc: 0.8030 | Val Loss: 0.3480 Acc: 0.8594                                               \n",
      "Epoch 018 | Train Loss: 0.4516 Acc: 0.8094 | Val Loss: 0.5991 Acc: 0.7678                                               \n",
      "Epoch 019 | Train Loss: 0.4403 Acc: 0.8109 | Val Loss: 0.3273 Acc: 0.8609                                               \n",
      "Epoch 020 | Train Loss: 0.4356 Acc: 0.8166 | Val Loss: 0.3884 Acc: 0.8290                                               \n",
      "Epoch 021 | Train Loss: 0.4235 Acc: 0.8218 | Val Loss: 0.6406 Acc: 0.7970                                               \n",
      "Epoch 022 | Train Loss: 0.4222 Acc: 0.8213 | Val Loss: 0.4174 Acc: 0.8352                                               \n",
      "Epoch 023 | Train Loss: 0.4146 Acc: 0.8314 | Val Loss: 0.3203 Acc: 0.8845                                               \n",
      "Epoch 024 | Train Loss: 0.4096 Acc: 0.8277 | Val Loss: 0.3413 Acc: 0.8696                                               \n",
      "Epoch 025 | Train Loss: 0.4002 Acc: 0.8336 | Val Loss: 0.5542 Acc: 0.7857                                               \n",
      "Epoch 026 | Train Loss: 0.3874 Acc: 0.8388 | Val Loss: 0.3455 Acc: 0.8582                                               \n",
      "Epoch 027 | Train Loss: 0.3889 Acc: 0.8363 | Val Loss: 0.3884 Acc: 0.8322                                               \n",
      "Epoch 028 | Train Loss: 0.3869 Acc: 0.8428 | Val Loss: 0.5259 Acc: 0.8206                                               \n",
      "Epoch 029 | Train Loss: 0.3833 Acc: 0.8421 | Val Loss: 0.4773 Acc: 0.8081                                               \n",
      "Epoch 030 | Train Loss: 0.3794 Acc: 0.8450 | Val Loss: 0.3993 Acc: 0.8684                                               \n",
      "Epoch 031 | Train Loss: 0.3796 Acc: 0.8436 | Val Loss: 0.4480 Acc: 0.7982                                               \n",
      "Epoch 032 | Train Loss: 0.3717 Acc: 0.8462 | Val Loss: 0.5713 Acc: 0.7857                                               \n",
      "Epoch 033 | Train Loss: 0.3648 Acc: 0.8490 | Val Loss: 0.4853 Acc: 0.7881                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 256, 'cnn_dropout': 0.21499273636349403, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 48, 'cnn_kernels_2': 96, 'learning_rate': 8.008480159937984e-05, 'lstm_dense': 64, 'lstm_hidden_size': 32, 'lstm_layers': 4, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 55.2377 Acc: 0.3649 | Val Loss: 11.8069 Acc: 0.4642                                             \n",
      "Epoch 002 | Train Loss: 24.8037 Acc: 0.4025 | Val Loss: 15.4005 Acc: 0.3439                                             \n",
      "Epoch 003 | Train Loss: 17.6350 Acc: 0.4188 | Val Loss: 7.1288 Acc: 0.4854                                              \n",
      "Epoch 004 | Train Loss: 13.6284 Acc: 0.4350 | Val Loss: 4.7720 Acc: 0.5322                                              \n",
      "Epoch 005 | Train Loss: 11.1029 Acc: 0.4459 | Val Loss: 5.0152 Acc: 0.5478                                              \n",
      "Epoch 006 | Train Loss: 9.1729 Acc: 0.4520 | Val Loss: 3.7630 Acc: 0.5860                                               \n",
      "Epoch 007 | Train Loss: 7.7779 Acc: 0.4597 | Val Loss: 2.5902 Acc: 0.5794                                               \n",
      "Epoch 008 | Train Loss: 6.5457 Acc: 0.4719 | Val Loss: 2.0340 Acc: 0.5722                                               \n",
      "Epoch 009 | Train Loss: 5.7882 Acc: 0.4698 | Val Loss: 2.0582 Acc: 0.5540                                               \n",
      "Epoch 010 | Train Loss: 5.1848 Acc: 0.4731 | Val Loss: 1.5779 Acc: 0.6081                                               \n",
      "Epoch 011 | Train Loss: 4.4183 Acc: 0.4849 | Val Loss: 1.5753 Acc: 0.5758                                               \n",
      "Epoch 012 | Train Loss: 3.9312 Acc: 0.4938 | Val Loss: 1.8051 Acc: 0.5699                                               \n",
      "Epoch 013 | Train Loss: 3.6075 Acc: 0.4924 | Val Loss: 2.4980 Acc: 0.5063                                               \n",
      "Epoch 014 | Train Loss: 3.2035 Acc: 0.5053 | Val Loss: 1.7087 Acc: 0.5869                                               \n",
      "Epoch 015 | Train Loss: 2.8946 Acc: 0.5037 | Val Loss: 1.5171 Acc: 0.6236                                               \n",
      "Epoch 016 | Train Loss: 2.7070 Acc: 0.5179 | Val Loss: 1.6029 Acc: 0.5322                                               \n",
      "Epoch 017 | Train Loss: 2.4907 Acc: 0.5100 | Val Loss: 1.4883 Acc: 0.6212                                               \n",
      "Epoch 018 | Train Loss: 2.3592 Acc: 0.5170 | Val Loss: 1.2080 Acc: 0.6734                                               \n",
      "Epoch 019 | Train Loss: 2.1287 Acc: 0.5291 | Val Loss: 0.9410 Acc: 0.6612                                               \n",
      "Epoch 020 | Train Loss: 1.9779 Acc: 0.5468 | Val Loss: 2.3965 Acc: 0.5370                                               \n",
      "Epoch 021 | Train Loss: 1.7920 Acc: 0.5573 | Val Loss: 0.9679 Acc: 0.6439                                               \n",
      "Epoch 022 | Train Loss: 1.6459 Acc: 0.5652 | Val Loss: 0.9337 Acc: 0.6501                                               \n",
      "Epoch 023 | Train Loss: 1.5802 Acc: 0.5715 | Val Loss: 0.7958 Acc: 0.6728                                               \n",
      "Epoch 024 | Train Loss: 1.4673 Acc: 0.5783 | Val Loss: 0.8533 Acc: 0.6436                                               \n",
      "Epoch 025 | Train Loss: 1.3398 Acc: 0.5922 | Val Loss: 0.7878 Acc: 0.6609                                               \n",
      "Epoch 026 | Train Loss: 1.2825 Acc: 0.6002 | Val Loss: 0.8996 Acc: 0.6481                                               \n",
      "Epoch 027 | Train Loss: 1.1985 Acc: 0.6065 | Val Loss: 1.4264 Acc: 0.5836                                               \n",
      "Epoch 028 | Train Loss: 1.1212 Acc: 0.6285 | Val Loss: 0.7017 Acc: 0.7188                                               \n",
      "Epoch 029 | Train Loss: 1.1036 Acc: 0.6374 | Val Loss: 0.7006 Acc: 0.6988                                               \n",
      "Epoch 030 | Train Loss: 1.0090 Acc: 0.6553 | Val Loss: 1.4546 Acc: 0.5896                                               \n",
      "Epoch 031 | Train Loss: 0.9582 Acc: 0.6637 | Val Loss: 0.9007 Acc: 0.6307                                               \n",
      "Epoch 032 | Train Loss: 0.8828 Acc: 0.6851 | Val Loss: 0.5275 Acc: 0.7881                                               \n",
      "Epoch 033 | Train Loss: 0.8205 Acc: 0.6998 | Val Loss: 0.6573 Acc: 0.7018                                               \n",
      "Epoch 034 | Train Loss: 0.7918 Acc: 0.7071 | Val Loss: 0.8710 Acc: 0.6639                                               \n",
      "Epoch 035 | Train Loss: 0.7469 Acc: 0.7219 | Val Loss: 0.6288 Acc: 0.7690                                               \n",
      "Epoch 036 | Train Loss: 0.7102 Acc: 0.7277 | Val Loss: 0.5390 Acc: 0.7863                                               \n",
      "Epoch 037 | Train Loss: 0.6842 Acc: 0.7407 | Val Loss: 0.4204 Acc: 0.8236                                               \n",
      "Epoch 038 | Train Loss: 0.6442 Acc: 0.7515 | Val Loss: 0.4581 Acc: 0.8346                                               \n",
      "Epoch 039 | Train Loss: 0.6158 Acc: 0.7591 | Val Loss: 0.4161 Acc: 0.8233                                               \n",
      "Epoch 040 | Train Loss: 0.5755 Acc: 0.7849 | Val Loss: 0.4167 Acc: 0.8284                                               \n",
      "Epoch 041 | Train Loss: 0.5753 Acc: 0.7746 | Val Loss: 0.4027 Acc: 0.8313                                               \n",
      "Epoch 042 | Train Loss: 0.5410 Acc: 0.7909 | Val Loss: 0.6678 Acc: 0.6660                                               \n",
      "Epoch 043 | Train Loss: 0.5412 Acc: 0.7914 | Val Loss: 0.4705 Acc: 0.7970                                               \n",
      "Epoch 044 | Train Loss: 0.4978 Acc: 0.8119 | Val Loss: 0.5153 Acc: 0.8128                                               \n",
      "Epoch 045 | Train Loss: 0.4969 Acc: 0.8120 | Val Loss: 0.5559 Acc: 0.7510                                               \n",
      "Epoch 046 | Train Loss: 0.4615 Acc: 0.8197 | Val Loss: 0.4469 Acc: 0.8248                                               \n",
      "Epoch 047 | Train Loss: 0.4456 Acc: 0.8327 | Val Loss: 0.3872 Acc: 0.8782                                               \n",
      "Epoch 048 | Train Loss: 0.4247 Acc: 0.8411 | Val Loss: 0.5336 Acc: 0.7887                                               \n",
      "Epoch 049 | Train Loss: 0.4132 Acc: 0.8461 | Val Loss: 0.3224 Acc: 0.8797                                               \n",
      "Epoch 050 | Train Loss: 0.3980 Acc: 0.8555 | Val Loss: 0.2847 Acc: 0.9033                                               \n",
      "Epoch 051 | Train Loss: 0.3799 Acc: 0.8630 | Val Loss: 0.3200 Acc: 0.8806                                               \n",
      "Epoch 052 | Train Loss: 0.3563 Acc: 0.8680 | Val Loss: 0.5230 Acc: 0.7961                                               \n",
      "Epoch 053 | Train Loss: 0.3391 Acc: 0.8777 | Val Loss: 0.3223 Acc: 0.8791                                               \n",
      "Epoch 054 | Train Loss: 0.3339 Acc: 0.8790 | Val Loss: 0.3386 Acc: 0.8597                                               \n",
      "Epoch 055 | Train Loss: 0.3131 Acc: 0.8886 | Val Loss: 0.3843 Acc: 0.8463                                               \n",
      "Epoch 056 | Train Loss: 0.2933 Acc: 0.8966 | Val Loss: 0.3889 Acc: 0.8510                                               \n",
      "Epoch 057 | Train Loss: 0.2973 Acc: 0.8978 | Val Loss: 0.2524 Acc: 0.9140                                               \n",
      "Epoch 058 | Train Loss: 0.2769 Acc: 0.9059 | Val Loss: 0.8758 Acc: 0.6988                                               \n",
      "Epoch 059 | Train Loss: 0.2550 Acc: 0.9145 | Val Loss: 0.2372 Acc: 0.9137                                               \n",
      "Epoch 060 | Train Loss: 0.2560 Acc: 0.9121 | Val Loss: 0.2487 Acc: 0.9087                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 64, 'cnn_dropout': 0.4669921977445243, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 16, 'cnn_kernels_2': 64, 'learning_rate': 0.0012199876771474378, 'lstm_dense': 64, 'lstm_hidden_size': 32, 'lstm_layers': 4, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 24.8256 Acc: 0.3901 | Val Loss: 6.5415 Acc: 0.4719                                              \n",
      "Epoch 002 | Train Loss: 4.6661 Acc: 0.4467 | Val Loss: 1.6848 Acc: 0.5433                                               \n",
      "Epoch 003 | Train Loss: 2.3071 Acc: 0.4897 | Val Loss: 2.2254 Acc: 0.5272                                               \n",
      "Epoch 004 | Train Loss: 1.3988 Acc: 0.5365 | Val Loss: 1.0046 Acc: 0.6594                                               \n",
      "Epoch 005 | Train Loss: 0.9335 Acc: 0.5933 | Val Loss: 0.6497 Acc: 0.6863                                               \n",
      "Epoch 006 | Train Loss: 0.7821 Acc: 0.6429 | Val Loss: 0.6612 Acc: 0.6866                                               \n",
      "Epoch 007 | Train Loss: 0.7033 Acc: 0.6766 | Val Loss: 0.5893 Acc: 0.7107                                               \n",
      "Epoch 008 | Train Loss: 0.6709 Acc: 0.6948 | Val Loss: 0.6699 Acc: 0.6484                                               \n",
      "Epoch 009 | Train Loss: 0.6540 Acc: 0.6967 | Val Loss: 0.5550 Acc: 0.7367                                               \n",
      "Epoch 010 | Train Loss: 0.6208 Acc: 0.7119 | Val Loss: 0.8702 Acc: 0.5976                                               \n",
      "Epoch 011 | Train Loss: 0.5869 Acc: 0.7276 | Val Loss: 0.4479 Acc: 0.7896                                               \n",
      "Epoch 012 | Train Loss: 0.5843 Acc: 0.7301 | Val Loss: 0.5320 Acc: 0.7836                                               \n",
      "Epoch 013 | Train Loss: 0.5656 Acc: 0.7403 | Val Loss: 0.5040 Acc: 0.7487                                               \n",
      "Epoch 014 | Train Loss: 0.5423 Acc: 0.7536 | Val Loss: 0.4756 Acc: 0.7976                                               \n",
      "Epoch 015 | Train Loss: 0.5156 Acc: 0.7636 | Val Loss: 0.4154 Acc: 0.7842                                               \n",
      "Epoch 016 | Train Loss: 0.5386 Acc: 0.7580 | Val Loss: 0.5182 Acc: 0.7666                                               \n",
      "Epoch 017 | Train Loss: 0.5109 Acc: 0.7670 | Val Loss: 0.4950 Acc: 0.7767                                               \n",
      "Epoch 018 | Train Loss: 0.5019 Acc: 0.7708 | Val Loss: 0.6064 Acc: 0.7400                                               \n",
      "Epoch 019 | Train Loss: 0.4863 Acc: 0.7823 | Val Loss: 0.5491 Acc: 0.7710                                               \n",
      "Epoch 020 | Train Loss: 0.4707 Acc: 0.7883 | Val Loss: 0.6656 Acc: 0.6722                                               \n",
      "Epoch 021 | Train Loss: 0.4787 Acc: 0.7866 | Val Loss: 0.4054 Acc: 0.7937                                               \n",
      "Epoch 022 | Train Loss: 0.4694 Acc: 0.7939 | Val Loss: 0.4988 Acc: 0.7406                                               \n",
      "Epoch 023 | Train Loss: 0.4564 Acc: 0.8008 | Val Loss: 0.5010 Acc: 0.7615                                               \n",
      "Epoch 024 | Train Loss: 0.4664 Acc: 0.7973 | Val Loss: 0.5899 Acc: 0.6869                                               \n",
      "Epoch 025 | Train Loss: 0.4790 Acc: 0.7849 | Val Loss: 0.4419 Acc: 0.8376                                               \n",
      "Epoch 026 | Train Loss: 0.4287 Acc: 0.8107 | Val Loss: 0.5155 Acc: 0.7594                                               \n",
      "Epoch 027 | Train Loss: 0.4548 Acc: 0.7980 | Val Loss: 0.7134 Acc: 0.7119                                               \n",
      "Epoch 028 | Train Loss: 0.4573 Acc: 0.8052 | Val Loss: 0.6083 Acc: 0.7334                                               \n",
      "Epoch 029 | Train Loss: 0.4337 Acc: 0.8101 | Val Loss: 0.3947 Acc: 0.8290                                               \n",
      "Epoch 030 | Train Loss: 0.4596 Acc: 0.7977 | Val Loss: 0.5285 Acc: 0.7746                                               \n",
      "Epoch 031 | Train Loss: 0.4298 Acc: 0.8066 | Val Loss: 0.5396 Acc: 0.7710                                               \n",
      "Epoch 032 | Train Loss: 0.4310 Acc: 0.8119 | Val Loss: 0.4965 Acc: 0.7761                                               \n",
      "Epoch 033 | Train Loss: 0.4358 Acc: 0.8141 | Val Loss: 0.6342 Acc: 0.7224                                               \n",
      "Epoch 034 | Train Loss: 0.4342 Acc: 0.8092 | Val Loss: 0.4036 Acc: 0.8179                                               \n",
      "Epoch 035 | Train Loss: 0.3903 Acc: 0.8304 | Val Loss: 0.5382 Acc: 0.7654                                               \n",
      "Epoch 036 | Train Loss: 0.4119 Acc: 0.8194 | Val Loss: 0.4281 Acc: 0.8236                                               \n",
      "Epoch 037 | Train Loss: 0.4127 Acc: 0.8174 | Val Loss: 0.8073 Acc: 0.7066                                               \n",
      "Epoch 038 | Train Loss: 0.3922 Acc: 0.8339 | Val Loss: 0.4240 Acc: 0.8307                                               \n",
      "Epoch 039 | Train Loss: 0.4074 Acc: 0.8264 | Val Loss: 0.6494 Acc: 0.7427                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 32, 'cnn_dropout': 0.32690936786727415, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 96, 'learning_rate': 0.00019710210054910197, 'lstm_dense': 64, 'lstm_hidden_size': 32, 'lstm_layers': 4, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 68.5139 Acc: 0.3080 | Val Loss: 8.5723 Acc: 0.3719                                              \n",
      "Epoch 002 | Train Loss: 19.8632 Acc: 0.3706 | Val Loss: 3.4587 Acc: 0.4364                                              \n",
      "Epoch 003 | Train Loss: 13.3595 Acc: 0.3854 | Val Loss: 3.0866 Acc: 0.4266                                              \n",
      "Epoch 004 | Train Loss: 10.0042 Acc: 0.3892 | Val Loss: 2.3546 Acc: 0.4636                                              \n",
      "Epoch 005 | Train Loss: 7.7290 Acc: 0.3991 | Val Loss: 1.7837 Acc: 0.5746                                               \n",
      "Epoch 006 | Train Loss: 6.0080 Acc: 0.4450 | Val Loss: 2.3873 Acc: 0.5143                                               \n",
      "Epoch 007 | Train Loss: 4.8855 Acc: 0.4582 | Val Loss: 1.5053 Acc: 0.5827                                               \n",
      "Epoch 008 | Train Loss: 4.0279 Acc: 0.4797 | Val Loss: 1.2480 Acc: 0.5806                                               \n",
      "Epoch 009 | Train Loss: 3.4322 Acc: 0.5038 | Val Loss: 1.1755 Acc: 0.6448                                               \n",
      "Epoch 010 | Train Loss: 2.9991 Acc: 0.5235 | Val Loss: 1.0144 Acc: 0.6609                                               \n",
      "Epoch 011 | Train Loss: 2.4606 Acc: 0.5544 | Val Loss: 0.9579 Acc: 0.7185                                               \n",
      "Epoch 012 | Train Loss: 2.2068 Acc: 0.5879 | Val Loss: 0.9415 Acc: 0.7239                                               \n",
      "Epoch 013 | Train Loss: 1.8398 Acc: 0.6320 | Val Loss: 0.8831 Acc: 0.7299                                               \n",
      "Epoch 014 | Train Loss: 1.6304 Acc: 0.6534 | Val Loss: 0.8061 Acc: 0.7469                                               \n",
      "Epoch 015 | Train Loss: 1.5365 Acc: 0.6651 | Val Loss: 0.8896 Acc: 0.7421                                               \n",
      "Epoch 016 | Train Loss: 1.2973 Acc: 0.6995 | Val Loss: 0.8279 Acc: 0.7672                                               \n",
      "Epoch 017 | Train Loss: 1.1798 Acc: 0.7212 | Val Loss: 0.5190 Acc: 0.8191                                               \n",
      "Epoch 018 | Train Loss: 1.0884 Acc: 0.7355 | Val Loss: 0.5925 Acc: 0.8090                                               \n",
      "Epoch 019 | Train Loss: 0.9760 Acc: 0.7577 | Val Loss: 0.5639 Acc: 0.8137                                               \n",
      "Epoch 020 | Train Loss: 0.9265 Acc: 0.7752 | Val Loss: 0.5276 Acc: 0.8310                                               \n",
      "Epoch 021 | Train Loss: 0.8274 Acc: 0.7843 | Val Loss: 0.5247 Acc: 0.8451                                               \n",
      "Epoch 022 | Train Loss: 0.7979 Acc: 0.7824 | Val Loss: 0.4823 Acc: 0.8409                                               \n",
      "Epoch 023 | Train Loss: 0.6973 Acc: 0.8107 | Val Loss: 0.4809 Acc: 0.8558                                               \n",
      "Epoch 024 | Train Loss: 0.6644 Acc: 0.8181 | Val Loss: 0.4819 Acc: 0.8493                                               \n",
      "Epoch 025 | Train Loss: 0.6105 Acc: 0.8266 | Val Loss: 0.4998 Acc: 0.8484                                               \n",
      "Epoch 026 | Train Loss: 0.5611 Acc: 0.8366 | Val Loss: 0.3628 Acc: 0.8967                                               \n",
      "Epoch 027 | Train Loss: 0.5421 Acc: 0.8458 | Val Loss: 0.5420 Acc: 0.8439                                               \n",
      "Epoch 028 | Train Loss: 0.5176 Acc: 0.8492 | Val Loss: 0.4011 Acc: 0.8952                                               \n",
      "Epoch 029 | Train Loss: 0.4825 Acc: 0.8630 | Val Loss: 0.5131 Acc: 0.8433                                               \n",
      "Epoch 030 | Train Loss: 0.4254 Acc: 0.8736 | Val Loss: 0.6007 Acc: 0.8182                                               \n",
      "Epoch 031 | Train Loss: 0.4262 Acc: 0.8695 | Val Loss: 0.3209 Acc: 0.9099                                               \n",
      "Epoch 032 | Train Loss: 0.4022 Acc: 0.8789 | Val Loss: 0.3469 Acc: 0.9081                                               \n",
      "Epoch 033 | Train Loss: 0.3759 Acc: 0.8856 | Val Loss: 0.3779 Acc: 0.8901                                               \n",
      "Epoch 034 | Train Loss: 0.3603 Acc: 0.8896 | Val Loss: 0.4199 Acc: 0.8704                                               \n",
      "Epoch 035 | Train Loss: 0.3375 Acc: 0.8934 | Val Loss: 0.3632 Acc: 0.8970                                               \n",
      "Epoch 036 | Train Loss: 0.3428 Acc: 0.8951 | Val Loss: 0.5791 Acc: 0.8194                                               \n",
      "Epoch 037 | Train Loss: 0.3218 Acc: 0.9039 | Val Loss: 0.4206 Acc: 0.8737                                               \n",
      "Epoch 038 | Train Loss: 0.3374 Acc: 0.8940 | Val Loss: 0.4968 Acc: 0.8507                                               \n",
      "Epoch 039 | Train Loss: 0.3033 Acc: 0.9101 | Val Loss: 0.2370 Acc: 0.9293                                               \n",
      "Epoch 040 | Train Loss: 0.2879 Acc: 0.9145 | Val Loss: 0.3858 Acc: 0.8845                                               \n",
      "Epoch 041 | Train Loss: 0.2628 Acc: 0.9242 | Val Loss: 0.3376 Acc: 0.8854                                               \n",
      "Epoch 042 | Train Loss: 0.2530 Acc: 0.9261 | Val Loss: 0.2634 Acc: 0.9251                                               \n",
      "Epoch 043 | Train Loss: 0.2517 Acc: 0.9289 | Val Loss: 0.3132 Acc: 0.9101                                               \n",
      "Epoch 044 | Train Loss: 0.2402 Acc: 0.9289 | Val Loss: 0.2791 Acc: 0.9263                                               \n",
      "Epoch 045 | Train Loss: 0.2393 Acc: 0.9304 | Val Loss: 0.3007 Acc: 0.9134                                               \n",
      "Epoch 046 | Train Loss: 0.2193 Acc: 0.9351 | Val Loss: 0.3004 Acc: 0.9272                                               \n",
      "Epoch 047 | Train Loss: 0.2232 Acc: 0.9355 | Val Loss: 0.3798 Acc: 0.8779                                               \n",
      "Epoch 048 | Train Loss: 0.2189 Acc: 0.9362 | Val Loss: 0.3135 Acc: 0.8979                                               \n",
      "Epoch 049 | Train Loss: 0.2097 Acc: 0.9416 | Val Loss: 0.2157 Acc: 0.9370                                               \n",
      "Epoch 050 | Train Loss: 0.1982 Acc: 0.9442 | Val Loss: 0.2605 Acc: 0.9188                                               \n",
      "Epoch 051 | Train Loss: 0.1839 Acc: 0.9498 | Val Loss: 0.2316 Acc: 0.9328                                               \n",
      "Epoch 052 | Train Loss: 0.1730 Acc: 0.9525 | Val Loss: 0.2198 Acc: 0.9433                                               \n",
      "Epoch 053 | Train Loss: 0.1821 Acc: 0.9477 | Val Loss: 0.2376 Acc: 0.9248                                               \n",
      "Epoch 054 | Train Loss: 0.1571 Acc: 0.9548 | Val Loss: 0.2841 Acc: 0.8949                                               \n",
      "Epoch 055 | Train Loss: 0.1350 Acc: 0.9621 | Val Loss: 0.2366 Acc: 0.9221                                               \n",
      "Epoch 056 | Train Loss: 0.1385 Acc: 0.9593 | Val Loss: 0.3776 Acc: 0.9122                                               \n",
      "Epoch 057 | Train Loss: 0.1447 Acc: 0.9585 | Val Loss: 0.1910 Acc: 0.9469                                               \n",
      "Epoch 058 | Train Loss: 0.1274 Acc: 0.9611 | Val Loss: 0.2919 Acc: 0.9245                                               \n",
      "Epoch 059 | Train Loss: 0.1339 Acc: 0.9605 | Val Loss: 0.1417 Acc: 0.9582                                               \n",
      "Epoch 060 | Train Loss: 0.1287 Acc: 0.9630 | Val Loss: 0.2113 Acc: 0.9346                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 256, 'cnn_dropout': 0.0043059108208103325, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 64, 'learning_rate': 3.3659604386413445e-05, 'lstm_dense': 256, 'lstm_hidden_size': 128, 'lstm_layers': 4, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 39.4760 Acc: 0.2961 | Val Loss: 13.5165 Acc: 0.4322                                             \n",
      "Epoch 002 | Train Loss: 20.6947 Acc: 0.3591 | Val Loss: 9.3970 Acc: 0.4460                                              \n",
      "Epoch 003 | Train Loss: 13.8716 Acc: 0.3775 | Val Loss: 4.9268 Acc: 0.4964                                              \n",
      "Epoch 004 | Train Loss: 10.1177 Acc: 0.3925 | Val Loss: 4.4336 Acc: 0.5570                                              \n",
      "Epoch 005 | Train Loss: 7.7071 Acc: 0.4110 | Val Loss: 3.3301 Acc: 0.5657                                               \n",
      "Epoch 006 | Train Loss: 6.2351 Acc: 0.4267 | Val Loss: 2.7988 Acc: 0.5627                                               \n",
      "Epoch 007 | Train Loss: 5.4734 Acc: 0.4460 | Val Loss: 2.1118 Acc: 0.5594                                               \n",
      "Epoch 008 | Train Loss: 4.6298 Acc: 0.4644 | Val Loss: 2.9595 Acc: 0.5740                                               \n",
      "Epoch 009 | Train Loss: 3.9745 Acc: 0.4847 | Val Loss: 1.5168 Acc: 0.6382                                               \n",
      "Epoch 010 | Train Loss: 3.5681 Acc: 0.5005 | Val Loss: 1.2397 Acc: 0.6734                                               \n",
      "Epoch 011 | Train Loss: 3.3070 Acc: 0.5147 | Val Loss: 1.5308 Acc: 0.6576                                               \n",
      "Epoch 012 | Train Loss: 2.9425 Acc: 0.5321 | Val Loss: 1.0731 Acc: 0.5857                                               \n",
      "Epoch 013 | Train Loss: 2.6481 Acc: 0.5509 | Val Loss: 1.3632 Acc: 0.5910                                               \n",
      "Epoch 014 | Train Loss: 2.4408 Acc: 0.5734 | Val Loss: 0.9997 Acc: 0.6257                                               \n",
      "Epoch 015 | Train Loss: 2.2962 Acc: 0.5727 | Val Loss: 1.0787 Acc: 0.6397                                               \n",
      "Epoch 016 | Train Loss: 2.0419 Acc: 0.5926 | Val Loss: 1.2332 Acc: 0.6585                                               \n",
      "Epoch 017 | Train Loss: 1.8733 Acc: 0.6012 | Val Loss: 0.8125 Acc: 0.6818                                               \n",
      "Epoch 018 | Train Loss: 1.7679 Acc: 0.6100 | Val Loss: 1.3957 Acc: 0.6361                                               \n",
      "Epoch 019 | Train Loss: 1.5788 Acc: 0.6325 | Val Loss: 0.8042 Acc: 0.6696                                               \n",
      "Epoch 020 | Train Loss: 1.5781 Acc: 0.6290 | Val Loss: 1.0246 Acc: 0.6752                                               \n",
      "Epoch 021 | Train Loss: 1.4110 Acc: 0.6503 | Val Loss: 0.8901 Acc: 0.7155                                               \n",
      "Epoch 022 | Train Loss: 1.3234 Acc: 0.6589 | Val Loss: 0.8141 Acc: 0.7128                                               \n",
      "Epoch 023 | Train Loss: 1.2432 Acc: 0.6706 | Val Loss: 0.6225 Acc: 0.7475                                               \n",
      "Epoch 024 | Train Loss: 1.2078 Acc: 0.6830 | Val Loss: 0.6321 Acc: 0.7591                                               \n",
      "Epoch 025 | Train Loss: 1.1538 Acc: 0.6906 | Val Loss: 0.5009 Acc: 0.7734                                               \n",
      "Epoch 026 | Train Loss: 1.0649 Acc: 0.7075 | Val Loss: 0.6259 Acc: 0.7427                                               \n",
      "Epoch 027 | Train Loss: 0.9971 Acc: 0.7197 | Val Loss: 0.6680 Acc: 0.7221                                               \n",
      "Epoch 028 | Train Loss: 0.9195 Acc: 0.7322 | Val Loss: 0.4122 Acc: 0.8475                                               \n",
      "Epoch 029 | Train Loss: 0.8417 Acc: 0.7555 | Val Loss: 0.3772 Acc: 0.8516                                               \n",
      "Epoch 030 | Train Loss: 0.7890 Acc: 0.7686 | Val Loss: 0.5772 Acc: 0.8045                                               \n",
      "Epoch 031 | Train Loss: 0.7542 Acc: 0.7867 | Val Loss: 0.3749 Acc: 0.8663                                               \n",
      "Epoch 032 | Train Loss: 0.7330 Acc: 0.7951 | Val Loss: 0.3987 Acc: 0.8603                                               \n",
      "Epoch 033 | Train Loss: 0.6632 Acc: 0.8131 | Val Loss: 0.3765 Acc: 0.8684                                               \n",
      "Epoch 034 | Train Loss: 0.6193 Acc: 0.8214 | Val Loss: 0.7462 Acc: 0.7758                                               \n",
      "Epoch 035 | Train Loss: 0.5818 Acc: 0.8360 | Val Loss: 0.4130 Acc: 0.8579                                               \n",
      "Epoch 036 | Train Loss: 0.5563 Acc: 0.8446 | Val Loss: 0.4027 Acc: 0.8707                                               \n",
      "Epoch 037 | Train Loss: 0.4973 Acc: 0.8603 | Val Loss: 0.3909 Acc: 0.8657                                               \n",
      "Epoch 038 | Train Loss: 0.4948 Acc: 0.8633 | Val Loss: 0.2776 Acc: 0.9099                                               \n",
      "Epoch 039 | Train Loss: 0.4807 Acc: 0.8659 | Val Loss: 0.2952 Acc: 0.9003                                               \n",
      "Epoch 040 | Train Loss: 0.4167 Acc: 0.8811 | Val Loss: 0.3084 Acc: 0.8973                                               \n",
      "Epoch 041 | Train Loss: 0.4008 Acc: 0.8857 | Val Loss: 0.3144 Acc: 0.8961                                               \n",
      "Epoch 042 | Train Loss: 0.3842 Acc: 0.8922 | Val Loss: 0.3583 Acc: 0.9009                                               \n",
      "Epoch 043 | Train Loss: 0.3751 Acc: 0.8960 | Val Loss: 0.5553 Acc: 0.8373                                               \n",
      "Epoch 044 | Train Loss: 0.3358 Acc: 0.9041 | Val Loss: 0.3600 Acc: 0.8875                                               \n",
      "Epoch 045 | Train Loss: 0.3226 Acc: 0.9072 | Val Loss: 0.4787 Acc: 0.8546                                               \n",
      "Epoch 046 | Train Loss: 0.3031 Acc: 0.9137 | Val Loss: 0.2995 Acc: 0.9054                                               \n",
      "Epoch 047 | Train Loss: 0.2846 Acc: 0.9162 | Val Loss: 0.2821 Acc: 0.9099                                               \n",
      "Epoch 048 | Train Loss: 0.2786 Acc: 0.9201 | Val Loss: 0.2461 Acc: 0.9281                                               \n",
      "Epoch 049 | Train Loss: 0.2782 Acc: 0.9238 | Val Loss: 0.4262 Acc: 0.8800                                               \n",
      "Epoch 050 | Train Loss: 0.2513 Acc: 0.9267 | Val Loss: 0.3525 Acc: 0.8901                                               \n",
      "Epoch 051 | Train Loss: 0.2281 Acc: 0.9357 | Val Loss: 0.2515 Acc: 0.9254                                               \n",
      "Epoch 052 | Train Loss: 0.2262 Acc: 0.9351 | Val Loss: 0.2722 Acc: 0.9296                                               \n",
      "Epoch 053 | Train Loss: 0.2269 Acc: 0.9369 | Val Loss: 0.2469 Acc: 0.9242                                               \n",
      "Epoch 054 | Train Loss: 0.2066 Acc: 0.9408 | Val Loss: 0.4107 Acc: 0.9012                                               \n",
      "Epoch 055 | Train Loss: 0.1838 Acc: 0.9451 | Val Loss: 0.2164 Acc: 0.9376                                               \n",
      "Epoch 056 | Train Loss: 0.1752 Acc: 0.9473 | Val Loss: 0.2854 Acc: 0.9182                                               \n",
      "Epoch 057 | Train Loss: 0.1700 Acc: 0.9507 | Val Loss: 0.6327 Acc: 0.8388                                               \n",
      "Epoch 058 | Train Loss: 0.1730 Acc: 0.9480 | Val Loss: 0.1873 Acc: 0.9457                                               \n",
      "Epoch 059 | Train Loss: 0.1491 Acc: 0.9560 | Val Loss: 0.2201 Acc: 0.9361                                               \n",
      "Epoch 060 | Train Loss: 0.1447 Acc: 0.9585 | Val Loss: 0.2145 Acc: 0.9412                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 64, 'cnn_dropout': 0.23339513610697651, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 16, 'cnn_kernels_2': 96, 'learning_rate': 0.0003075933013474904, 'lstm_dense': 64, 'lstm_hidden_size': 32, 'lstm_layers': 2, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 22.1913 Acc: 0.3854 | Val Loss: 9.1791 Acc: 0.4009                                              \n",
      "Epoch 002 | Train Loss: 8.8154 Acc: 0.4128 | Val Loss: 5.2435 Acc: 0.3069                                               \n",
      "Epoch 003 | Train Loss: 5.9908 Acc: 0.4273 | Val Loss: 3.8783 Acc: 0.4319                                               \n",
      "Epoch 004 | Train Loss: 4.5757 Acc: 0.4428 | Val Loss: 2.3242 Acc: 0.5275                                               \n",
      "Epoch 005 | Train Loss: 3.7831 Acc: 0.4523 | Val Loss: 1.7747 Acc: 0.5919                                               \n",
      "Epoch 006 | Train Loss: 3.1916 Acc: 0.4673 | Val Loss: 2.5876 Acc: 0.5370                                               \n",
      "Epoch 007 | Train Loss: 2.5939 Acc: 0.4893 | Val Loss: 1.0170 Acc: 0.6385                                               \n",
      "Epoch 008 | Train Loss: 2.3437 Acc: 0.5076 | Val Loss: 2.1204 Acc: 0.5501                                               \n",
      "Epoch 009 | Train Loss: 2.0185 Acc: 0.5297 | Val Loss: 0.9439 Acc: 0.5979                                               \n",
      "Epoch 010 | Train Loss: 1.7467 Acc: 0.5460 | Val Loss: 1.2304 Acc: 0.6054                                               \n",
      "Epoch 011 | Train Loss: 1.5092 Acc: 0.5791 | Val Loss: 1.4839 Acc: 0.6209                                               \n",
      "Epoch 012 | Train Loss: 1.3276 Acc: 0.6012 | Val Loss: 2.0817 Acc: 0.6075                                               \n",
      "Epoch 013 | Train Loss: 1.2044 Acc: 0.6206 | Val Loss: 0.8415 Acc: 0.6478                                               \n",
      "Epoch 014 | Train Loss: 1.0770 Acc: 0.6478 | Val Loss: 0.9986 Acc: 0.6567                                               \n",
      "Epoch 015 | Train Loss: 0.9594 Acc: 0.6795 | Val Loss: 0.6988 Acc: 0.6979                                               \n",
      "Epoch 016 | Train Loss: 0.8505 Acc: 0.7007 | Val Loss: 0.6221 Acc: 0.7633                                               \n",
      "Epoch 017 | Train Loss: 0.7788 Acc: 0.7251 | Val Loss: 0.9811 Acc: 0.6979                                               \n",
      "Epoch 018 | Train Loss: 0.7046 Acc: 0.7522 | Val Loss: 1.0374 Acc: 0.6884                                               \n",
      "Epoch 019 | Train Loss: 0.6470 Acc: 0.7695 | Val Loss: 0.8700 Acc: 0.7254                                               \n",
      "Epoch 020 | Train Loss: 0.5816 Acc: 0.7848 | Val Loss: 0.7067 Acc: 0.7618                                               \n",
      "Epoch 021 | Train Loss: 0.5542 Acc: 0.8013 | Val Loss: 0.3818 Acc: 0.8448                                               \n",
      "Epoch 022 | Train Loss: 0.4840 Acc: 0.8221 | Val Loss: 0.3282 Acc: 0.8848                                               \n",
      "Epoch 023 | Train Loss: 0.4426 Acc: 0.8376 | Val Loss: 0.3044 Acc: 0.8809                                               \n",
      "Epoch 024 | Train Loss: 0.4030 Acc: 0.8530 | Val Loss: 0.3056 Acc: 0.8866                                               \n",
      "Epoch 025 | Train Loss: 0.3578 Acc: 0.8674 | Val Loss: 0.4241 Acc: 0.8597                                               \n",
      "Epoch 026 | Train Loss: 0.3342 Acc: 0.8769 | Val Loss: 0.2824 Acc: 0.8928                                               \n",
      "Epoch 027 | Train Loss: 0.3185 Acc: 0.8881 | Val Loss: 0.3931 Acc: 0.8558                                               \n",
      "Epoch 028 | Train Loss: 0.2775 Acc: 0.9013 | Val Loss: 0.2898 Acc: 0.9030                                               \n",
      "Epoch 029 | Train Loss: 0.2647 Acc: 0.9042 | Val Loss: 0.3110 Acc: 0.8848                                               \n",
      "Epoch 030 | Train Loss: 0.2653 Acc: 0.9087 | Val Loss: 0.1894 Acc: 0.9233                                               \n",
      "Epoch 031 | Train Loss: 0.2345 Acc: 0.9193 | Val Loss: 0.2446 Acc: 0.9119                                               \n",
      "Epoch 032 | Train Loss: 0.2283 Acc: 0.9203 | Val Loss: 0.2457 Acc: 0.9128                                               \n",
      "Epoch 033 | Train Loss: 0.2162 Acc: 0.9250 | Val Loss: 0.3312 Acc: 0.8761                                               \n",
      "Epoch 034 | Train Loss: 0.1994 Acc: 0.9306 | Val Loss: 0.2453 Acc: 0.9266                                               \n",
      "Epoch 035 | Train Loss: 0.1774 Acc: 0.9386 | Val Loss: 0.2261 Acc: 0.9224                                               \n",
      "Epoch 036 | Train Loss: 0.1733 Acc: 0.9407 | Val Loss: 0.1864 Acc: 0.9293                                               \n",
      "Epoch 037 | Train Loss: 0.1709 Acc: 0.9421 | Val Loss: 0.1718 Acc: 0.9373                                               \n",
      "Epoch 038 | Train Loss: 0.1535 Acc: 0.9482 | Val Loss: 0.1424 Acc: 0.9484                                               \n",
      "Epoch 039 | Train Loss: 0.1549 Acc: 0.9462 | Val Loss: 0.1397 Acc: 0.9466                                               \n",
      "Epoch 040 | Train Loss: 0.1432 Acc: 0.9514 | Val Loss: 0.1861 Acc: 0.9319                                               \n",
      "Epoch 041 | Train Loss: 0.1265 Acc: 0.9563 | Val Loss: 0.1343 Acc: 0.9490                                               \n",
      "Epoch 042 | Train Loss: 0.1298 Acc: 0.9533 | Val Loss: 0.1574 Acc: 0.9484                                               \n",
      "Epoch 043 | Train Loss: 0.1250 Acc: 0.9572 | Val Loss: 0.1906 Acc: 0.9355                                               \n",
      "Epoch 044 | Train Loss: 0.1198 Acc: 0.9613 | Val Loss: 0.1142 Acc: 0.9618                                               \n",
      "Epoch 045 | Train Loss: 0.1073 Acc: 0.9637 | Val Loss: 0.1638 Acc: 0.9439                                               \n",
      "Epoch 046 | Train Loss: 0.1064 Acc: 0.9656 | Val Loss: 0.1131 Acc: 0.9633                                               \n",
      "Epoch 047 | Train Loss: 0.1005 Acc: 0.9674 | Val Loss: 0.1837 Acc: 0.9391                                               \n",
      "Epoch 048 | Train Loss: 0.1001 Acc: 0.9667 | Val Loss: 0.3302 Acc: 0.9203                                               \n",
      "Epoch 049 | Train Loss: 0.0931 Acc: 0.9698 | Val Loss: 0.1335 Acc: 0.9615                                               \n",
      "Epoch 050 | Train Loss: 0.0931 Acc: 0.9694 | Val Loss: 0.1070 Acc: 0.9630                                               \n",
      "Epoch 051 | Train Loss: 0.0866 Acc: 0.9709 | Val Loss: 0.0887 Acc: 0.9707                                               \n",
      "Epoch 052 | Train Loss: 0.0859 Acc: 0.9734 | Val Loss: 0.1233 Acc: 0.9576                                               \n",
      "Epoch 053 | Train Loss: 0.0852 Acc: 0.9709 | Val Loss: 0.1146 Acc: 0.9663                                               \n",
      "Epoch 054 | Train Loss: 0.0829 Acc: 0.9731 | Val Loss: 0.0812 Acc: 0.9716                                               \n",
      "Epoch 055 | Train Loss: 0.0741 Acc: 0.9774 | Val Loss: 0.1526 Acc: 0.9451                                               \n",
      "Epoch 056 | Train Loss: 0.0751 Acc: 0.9760 | Val Loss: 0.0767 Acc: 0.9743                                               \n",
      "Epoch 057 | Train Loss: 0.0677 Acc: 0.9775 | Val Loss: 0.1144 Acc: 0.9612                                               \n",
      "Epoch 058 | Train Loss: 0.0643 Acc: 0.9801 | Val Loss: 0.0899 Acc: 0.9728                                               \n",
      "Epoch 059 | Train Loss: 0.0621 Acc: 0.9810 | Val Loss: 0.1781 Acc: 0.9475                                               \n",
      "Epoch 060 | Train Loss: 0.0653 Acc: 0.9799 | Val Loss: 0.0817 Acc: 0.9767                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 256, 'cnn_dropout': 0.6642209223166103, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 64, 'learning_rate': 0.0001045973070905621, 'lstm_dense': 128, 'lstm_hidden_size': 32, 'lstm_layers': 4, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 65.3875 Acc: 0.3141 | Val Loss: 11.7254 Acc: 0.4812                                             \n",
      "Epoch 002 | Train Loss: 27.2946 Acc: 0.3593 | Val Loss: 9.4817 Acc: 0.4657                                              \n",
      "Epoch 003 | Train Loss: 17.5721 Acc: 0.3795 | Val Loss: 4.3553 Acc: 0.4681                                              \n",
      "Epoch 004 | Train Loss: 12.6137 Acc: 0.3948 | Val Loss: 5.0235 Acc: 0.4588                                              \n",
      "Epoch 005 | Train Loss: 10.0258 Acc: 0.4051 | Val Loss: 2.9660 Acc: 0.5233                                              \n",
      "Epoch 006 | Train Loss: 7.7121 Acc: 0.4287 | Val Loss: 2.4455 Acc: 0.5140                                               \n",
      "Epoch 007 | Train Loss: 6.6108 Acc: 0.4312 | Val Loss: 2.3375 Acc: 0.5197                                               \n",
      "Epoch 008 | Train Loss: 5.5508 Acc: 0.4534 | Val Loss: 2.0347 Acc: 0.5293                                               \n",
      "Epoch 009 | Train Loss: 4.6223 Acc: 0.4775 | Val Loss: 1.4451 Acc: 0.6248                                               \n",
      "Epoch 010 | Train Loss: 4.0451 Acc: 0.4938 | Val Loss: 1.5908 Acc: 0.5794                                               \n",
      "Epoch 011 | Train Loss: 3.5501 Acc: 0.5060 | Val Loss: 1.7416 Acc: 0.6487                                               \n",
      "Epoch 012 | Train Loss: 3.1537 Acc: 0.5263 | Val Loss: 1.4317 Acc: 0.6131                                               \n",
      "Epoch 013 | Train Loss: 2.8472 Acc: 0.5459 | Val Loss: 1.6038 Acc: 0.6057                                               \n",
      "Epoch 014 | Train Loss: 2.5155 Acc: 0.5697 | Val Loss: 1.2560 Acc: 0.6734                                               \n",
      "Epoch 015 | Train Loss: 2.2366 Acc: 0.5884 | Val Loss: 1.0865 Acc: 0.7158                                               \n",
      "Epoch 016 | Train Loss: 2.0512 Acc: 0.6023 | Val Loss: 1.1515 Acc: 0.7242                                               \n",
      "Epoch 017 | Train Loss: 1.8222 Acc: 0.6189 | Val Loss: 0.8685 Acc: 0.7621                                               \n",
      "Epoch 018 | Train Loss: 1.6709 Acc: 0.6439 | Val Loss: 0.8778 Acc: 0.7710                                               \n",
      "Epoch 019 | Train Loss: 1.5262 Acc: 0.6543 | Val Loss: 0.8036 Acc: 0.7899                                               \n",
      "Epoch 020 | Train Loss: 1.3916 Acc: 0.6615 | Val Loss: 0.8560 Acc: 0.7809                                               \n",
      "Epoch 021 | Train Loss: 1.2507 Acc: 0.6848 | Val Loss: 0.8405 Acc: 0.7773                                               \n",
      "Epoch 022 | Train Loss: 1.1767 Acc: 0.6993 | Val Loss: 0.6908 Acc: 0.8033                                               \n",
      "Epoch 023 | Train Loss: 1.1140 Acc: 0.7077 | Val Loss: 0.7649 Acc: 0.7937                                               \n",
      "Epoch 024 | Train Loss: 1.0242 Acc: 0.7290 | Val Loss: 0.6561 Acc: 0.8137                                               \n",
      "Epoch 025 | Train Loss: 0.9658 Acc: 0.7475 | Val Loss: 0.7625 Acc: 0.8096                                               \n",
      "Epoch 026 | Train Loss: 0.8870 Acc: 0.7575 | Val Loss: 0.6389 Acc: 0.8269                                               \n",
      "Epoch 027 | Train Loss: 0.8299 Acc: 0.7659 | Val Loss: 0.6852 Acc: 0.8075                                               \n",
      "Epoch 028 | Train Loss: 0.7927 Acc: 0.7757 | Val Loss: 0.5919 Acc: 0.8364                                               \n",
      "Epoch 029 | Train Loss: 0.7586 Acc: 0.7821 | Val Loss: 0.6208 Acc: 0.8328                                               \n",
      "Epoch 030 | Train Loss: 0.7039 Acc: 0.7954 | Val Loss: 0.5076 Acc: 0.8499                                               \n",
      "Epoch 031 | Train Loss: 0.6721 Acc: 0.8033 | Val Loss: 0.4692 Acc: 0.8713                                               \n",
      "Epoch 032 | Train Loss: 0.6449 Acc: 0.8027 | Val Loss: 0.4513 Acc: 0.8588                                               \n",
      "Epoch 033 | Train Loss: 0.5931 Acc: 0.8183 | Val Loss: 0.4266 Acc: 0.8707                                               \n",
      "Epoch 034 | Train Loss: 0.5599 Acc: 0.8272 | Val Loss: 0.4705 Acc: 0.8493                                               \n",
      "Epoch 035 | Train Loss: 0.5271 Acc: 0.8394 | Val Loss: 0.4318 Acc: 0.8534                                               \n",
      "Epoch 036 | Train Loss: 0.4969 Acc: 0.8469 | Val Loss: 0.3488 Acc: 0.8934                                               \n",
      "Epoch 037 | Train Loss: 0.4719 Acc: 0.8500 | Val Loss: 0.3648 Acc: 0.8707                                               \n",
      "Epoch 038 | Train Loss: 0.4344 Acc: 0.8643 | Val Loss: 0.3563 Acc: 0.8839                                               \n",
      "Epoch 039 | Train Loss: 0.4413 Acc: 0.8647 | Val Loss: 0.3310 Acc: 0.8851                                               \n",
      "Epoch 040 | Train Loss: 0.3945 Acc: 0.8769 | Val Loss: 0.2839 Acc: 0.9167                                               \n",
      "Epoch 041 | Train Loss: 0.3632 Acc: 0.8814 | Val Loss: 0.3121 Acc: 0.8928                                               \n",
      "Epoch 042 | Train Loss: 0.3453 Acc: 0.8895 | Val Loss: 0.2848 Acc: 0.9072                                               \n",
      "Epoch 043 | Train Loss: 0.3360 Acc: 0.8982 | Val Loss: 0.2277 Acc: 0.9140                                               \n",
      "Epoch 044 | Train Loss: 0.3142 Acc: 0.9024 | Val Loss: 0.3174 Acc: 0.8806                                               \n",
      "Epoch 045 | Train Loss: 0.2890 Acc: 0.9084 | Val Loss: 0.2764 Acc: 0.9018                                               \n",
      "Epoch 046 | Train Loss: 0.3048 Acc: 0.9007 | Val Loss: 0.2396 Acc: 0.9134                                               \n",
      "Epoch 047 | Train Loss: 0.2827 Acc: 0.9139 | Val Loss: 0.3243 Acc: 0.8848                                               \n",
      "Epoch 048 | Train Loss: 0.2586 Acc: 0.9214 | Val Loss: 0.2668 Acc: 0.8958                                               \n",
      "Epoch 049 | Train Loss: 0.2377 Acc: 0.9268 | Val Loss: 0.2882 Acc: 0.8976                                               \n",
      "Epoch 050 | Train Loss: 0.2504 Acc: 0.9228 | Val Loss: 0.1851 Acc: 0.9358                                               \n",
      "Epoch 051 | Train Loss: 0.2308 Acc: 0.9287 | Val Loss: 0.2264 Acc: 0.9072                                               \n",
      "Epoch 052 | Train Loss: 0.2076 Acc: 0.9370 | Val Loss: 0.2116 Acc: 0.9212                                               \n",
      "Epoch 053 | Train Loss: 0.2154 Acc: 0.9348 | Val Loss: 0.2605 Acc: 0.8943                                               \n",
      "Epoch 054 | Train Loss: 0.1961 Acc: 0.9384 | Val Loss: 0.2683 Acc: 0.8952                                               \n",
      "Epoch 055 | Train Loss: 0.2056 Acc: 0.9349 | Val Loss: 0.2005 Acc: 0.9179                                               \n",
      "Epoch 056 | Train Loss: 0.1919 Acc: 0.9428 | Val Loss: 0.2187 Acc: 0.9161                                               \n",
      "Epoch 057 | Train Loss: 0.1796 Acc: 0.9428 | Val Loss: 0.2119 Acc: 0.9218                                               \n",
      "Epoch 058 | Train Loss: 0.1718 Acc: 0.9475 | Val Loss: 0.2482 Acc: 0.9087                                               \n",
      "Epoch 059 | Train Loss: 0.1788 Acc: 0.9458 | Val Loss: 0.1778 Acc: 0.9319                                               \n",
      "Epoch 060 | Train Loss: 0.1530 Acc: 0.9557 | Val Loss: 0.1710 Acc: 0.9337                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 64, 'cnn_dropout': 0.36562693556841097, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 48, 'cnn_kernels_2': 96, 'learning_rate': 0.0008312320901819931, 'lstm_dense': 256, 'lstm_hidden_size': 32, 'lstm_layers': 4, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 17.0814 Acc: 0.4343 | Val Loss: 6.9972 Acc: 0.4687                                              \n",
      "Epoch 002 | Train Loss: 4.7165 Acc: 0.4875 | Val Loss: 1.6511 Acc: 0.5272                                               \n",
      "Epoch 003 | Train Loss: 2.3535 Acc: 0.5402 | Val Loss: 2.0234 Acc: 0.6036                                               \n",
      "Epoch 004 | Train Loss: 1.4559 Acc: 0.5797 | Val Loss: 1.2853 Acc: 0.5749                                               \n",
      "Epoch 005 | Train Loss: 1.0568 Acc: 0.6126 | Val Loss: 1.4654 Acc: 0.5439                                               \n",
      "Epoch 006 | Train Loss: 0.8368 Acc: 0.6474 | Val Loss: 0.7000 Acc: 0.6988                                               \n",
      "Epoch 007 | Train Loss: 0.7600 Acc: 0.6659 | Val Loss: 0.6609 Acc: 0.6985                                               \n",
      "Epoch 008 | Train Loss: 0.7376 Acc: 0.6824 | Val Loss: 0.8467 Acc: 0.6457                                               \n",
      "Epoch 009 | Train Loss: 0.6957 Acc: 0.7010 | Val Loss: 0.6711 Acc: 0.6943                                               \n",
      "Epoch 010 | Train Loss: 0.6414 Acc: 0.7278 | Val Loss: 0.5083 Acc: 0.7946                                               \n",
      "Epoch 011 | Train Loss: 0.6020 Acc: 0.7542 | Val Loss: 0.4552 Acc: 0.8284                                               \n",
      "Epoch 012 | Train Loss: 0.5691 Acc: 0.7782 | Val Loss: 0.4206 Acc: 0.8701                                               \n",
      "Epoch 013 | Train Loss: 0.5266 Acc: 0.8027 | Val Loss: 0.4988 Acc: 0.8531                                               \n",
      "Epoch 014 | Train Loss: 0.4846 Acc: 0.8219 | Val Loss: 0.4492 Acc: 0.7916                                               \n",
      "Epoch 015 | Train Loss: 0.4496 Acc: 0.8378 | Val Loss: 0.3419 Acc: 0.8439                                               \n",
      "Epoch 016 | Train Loss: 0.4364 Acc: 0.8462 | Val Loss: 0.5095 Acc: 0.7725                                               \n",
      "Epoch 017 | Train Loss: 0.4013 Acc: 0.8594 | Val Loss: 0.4088 Acc: 0.8579                                               \n",
      "Epoch 018 | Train Loss: 0.3732 Acc: 0.8710 | Val Loss: 0.3097 Acc: 0.8812                                               \n",
      "Epoch 019 | Train Loss: 0.3875 Acc: 0.8650 | Val Loss: 0.5008 Acc: 0.7803                                               \n",
      "Epoch 020 | Train Loss: 0.3520 Acc: 0.8758 | Val Loss: 0.3614 Acc: 0.8284                                               \n",
      "Epoch 021 | Train Loss: 0.3425 Acc: 0.8841 | Val Loss: 0.2857 Acc: 0.8791                                               \n",
      "Epoch 022 | Train Loss: 0.3414 Acc: 0.8813 | Val Loss: 0.2703 Acc: 0.9015                                               \n",
      "Epoch 023 | Train Loss: 0.3039 Acc: 0.8983 | Val Loss: 0.2391 Acc: 0.9066                                               \n",
      "Epoch 024 | Train Loss: 0.2914 Acc: 0.9001 | Val Loss: 0.3961 Acc: 0.8367                                               \n",
      "Epoch 025 | Train Loss: 0.2818 Acc: 0.9022 | Val Loss: 0.3813 Acc: 0.8555                                               \n",
      "Epoch 026 | Train Loss: 0.2687 Acc: 0.9119 | Val Loss: 0.2319 Acc: 0.9161                                               \n",
      "Epoch 027 | Train Loss: 0.2537 Acc: 0.9157 | Val Loss: 0.2788 Acc: 0.8845                                               \n",
      "Epoch 028 | Train Loss: 0.2410 Acc: 0.9201 | Val Loss: 0.2026 Acc: 0.9182                                               \n",
      "Epoch 029 | Train Loss: 0.2389 Acc: 0.9205 | Val Loss: 0.2385 Acc: 0.9251                                               \n",
      "Epoch 030 | Train Loss: 0.2283 Acc: 0.9244 | Val Loss: 0.2389 Acc: 0.9293                                               \n",
      "Epoch 031 | Train Loss: 0.2275 Acc: 0.9224 | Val Loss: 0.3799 Acc: 0.8630                                               \n",
      "Epoch 032 | Train Loss: 0.2188 Acc: 0.9292 | Val Loss: 0.4959 Acc: 0.8143                                               \n",
      "Epoch 033 | Train Loss: 0.2032 Acc: 0.9335 | Val Loss: 0.9222 Acc: 0.7287                                               \n",
      "Epoch 034 | Train Loss: 0.2016 Acc: 0.9329 | Val Loss: 0.1414 Acc: 0.9484                                               \n",
      "Epoch 035 | Train Loss: 0.2105 Acc: 0.9316 | Val Loss: 0.4208 Acc: 0.8490                                               \n",
      "Epoch 036 | Train Loss: 0.1778 Acc: 0.9404 | Val Loss: 0.3572 Acc: 0.8701                                               \n",
      "Epoch 037 | Train Loss: 0.1806 Acc: 0.9412 | Val Loss: 0.2750 Acc: 0.8839                                               \n",
      "Epoch 038 | Train Loss: 0.1704 Acc: 0.9457 | Val Loss: 0.7111 Acc: 0.7696                                               \n",
      "Epoch 039 | Train Loss: 0.1730 Acc: 0.9425 | Val Loss: 0.3654 Acc: 0.8746                                               \n",
      "Epoch 040 | Train Loss: 0.1611 Acc: 0.9471 | Val Loss: 0.2778 Acc: 0.9137                                               \n",
      "Epoch 041 | Train Loss: 0.1592 Acc: 0.9473 | Val Loss: 0.1970 Acc: 0.9310                                               \n",
      "Epoch 042 | Train Loss: 0.1619 Acc: 0.9500 | Val Loss: 0.2969 Acc: 0.9066                                               \n",
      "Epoch 043 | Train Loss: 0.1570 Acc: 0.9521 | Val Loss: 0.3343 Acc: 0.8713                                               \n",
      "Epoch 044 | Train Loss: 0.1358 Acc: 0.9571 | Val Loss: 0.1656 Acc: 0.9382                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "100%|████████████████████████████████████████████████| 30/30 [25:50<00:00, 51.67s/trial, best loss: 0.04853299623105063]\n",
      "Best hyperparameters: {'batch_size': np.int64(3), 'cnn_dense': np.int64(1), 'cnn_dropout': np.float64(0.5642870120996387), 'cnn_kernel_size_1': np.int64(0), 'cnn_kernel_size_2': np.int64(0), 'cnn_kernels_1': np.int64(2), 'cnn_kernels_2': np.int64(2), 'learning_rate': np.float64(0.0003430974499751907), 'lstm_dense': np.int64(1), 'lstm_hidden_size': np.int64(0), 'lstm_layers': np.int64(3), 'optimizer': np.int64(1)}\n",
      "TPE search finished in 1550.11 seconds\n",
      "Best (raw indices): {'batch_size': np.int64(3), 'cnn_dense': np.int64(1), 'cnn_dropout': np.float64(0.5642870120996387), 'cnn_kernel_size_1': np.int64(0), 'cnn_kernel_size_2': np.int64(0), 'cnn_kernels_1': np.int64(2), 'cnn_kernels_2': np.int64(2), 'learning_rate': np.float64(0.0003430974499751907), 'lstm_dense': np.int64(1), 'lstm_hidden_size': np.int64(0), 'lstm_layers': np.int64(3), 'optimizer': np.int64(1)}\n",
      "Best (interpreted): {'batch_size': 64, 'cnn_dense': 64, 'cnn_dropout': np.float64(0.5642870120996387), 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 48, 'cnn_kernels_2': 64, 'learning_rate': np.float64(0.0003430974499751907), 'lstm_dense': 64, 'lstm_hidden_size': 32, 'lstm_layers': 4, 'optimizer': 'rmsprop'}\n",
      "Starting TPE search...\n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 64, 'cnn_dropout': 0.12180186768884975, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 32, 'learning_rate': 0.0010647475145648584, 'lstm_dense': 64, 'lstm_hidden_size': 128, 'lstm_layers': 5, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 13.6460 Acc: 0.4273 | Val Loss: 2.5661 Acc: 0.4627                                              \n",
      "Epoch 002 | Train Loss: 2.6413 Acc: 0.4884 | Val Loss: 1.8337 Acc: 0.5182                                               \n",
      "Epoch 003 | Train Loss: 1.3566 Acc: 0.5429 | Val Loss: 0.8630 Acc: 0.6275                                               \n",
      "Epoch 004 | Train Loss: 0.9406 Acc: 0.5965 | Val Loss: 0.7001 Acc: 0.7179                                               \n",
      "Epoch 005 | Train Loss: 0.7514 Acc: 0.6533 | Val Loss: 0.7837 Acc: 0.6657                                               \n",
      "Epoch 006 | Train Loss: 0.6672 Acc: 0.6995 | Val Loss: 0.6922 Acc: 0.6639                                               \n",
      "Epoch 007 | Train Loss: 0.6085 Acc: 0.7309 | Val Loss: 0.6718 Acc: 0.7203                                               \n",
      "Epoch 008 | Train Loss: 0.5707 Acc: 0.7481 | Val Loss: 0.5087 Acc: 0.7660                                               \n",
      "Epoch 009 | Train Loss: 0.5216 Acc: 0.7721 | Val Loss: 0.4858 Acc: 0.7734                                               \n",
      "Epoch 010 | Train Loss: 0.4937 Acc: 0.7901 | Val Loss: 0.4499 Acc: 0.7869                                               \n",
      "Epoch 011 | Train Loss: 0.4768 Acc: 0.7986 | Val Loss: 0.9307 Acc: 0.6107                                               \n",
      "Epoch 012 | Train Loss: 0.4557 Acc: 0.8095 | Val Loss: 0.4625 Acc: 0.8215                                               \n",
      "Epoch 013 | Train Loss: 0.4350 Acc: 0.8154 | Val Loss: 0.5125 Acc: 0.7107                                               \n",
      "Epoch 014 | Train Loss: 0.4164 Acc: 0.8195 | Val Loss: 0.3968 Acc: 0.8033                                               \n",
      "Epoch 015 | Train Loss: 0.4148 Acc: 0.8243 | Val Loss: 0.5941 Acc: 0.7707                                               \n",
      "Epoch 016 | Train Loss: 0.4049 Acc: 0.8268 | Val Loss: 0.4350 Acc: 0.8236                                               \n",
      "Epoch 017 | Train Loss: 0.4246 Acc: 0.8283 | Val Loss: 0.5510 Acc: 0.7421                                               \n",
      "Epoch 018 | Train Loss: 0.4192 Acc: 0.8319 | Val Loss: 0.6504 Acc: 0.7212                                               \n",
      "Epoch 019 | Train Loss: 0.3868 Acc: 0.8416 | Val Loss: 0.6737 Acc: 0.7096                                               \n",
      "Epoch 020 | Train Loss: 0.3819 Acc: 0.8439 | Val Loss: 0.6040 Acc: 0.7839                                               \n",
      "Epoch 021 | Train Loss: 0.3681 Acc: 0.8513 | Val Loss: 0.4412 Acc: 0.8212                                               \n",
      "Epoch 022 | Train Loss: 0.3676 Acc: 0.8523 | Val Loss: 0.5120 Acc: 0.7549                                               \n",
      "Epoch 023 | Train Loss: 0.3647 Acc: 0.8538 | Val Loss: 0.6657 Acc: 0.7439                                               \n",
      "Epoch 024 | Train Loss: 0.3517 Acc: 0.8616 | Val Loss: 0.6397 Acc: 0.7248                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 64, 'cnn_dropout': 0.22260965179039174, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 16, 'cnn_kernels_2': 64, 'learning_rate': 0.00023367671711106908, 'lstm_dense': 64, 'lstm_hidden_size': 64, 'lstm_layers': 3, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 14.0307 Acc: 0.4095 | Val Loss: 2.9243 Acc: 0.4337                                              \n",
      "Epoch 002 | Train Loss: 4.9391 Acc: 0.4417 | Val Loss: 2.2584 Acc: 0.5245                                               \n",
      "Epoch 003 | Train Loss: 3.3834 Acc: 0.4826 | Val Loss: 1.3598 Acc: 0.5985                                               \n",
      "Epoch 004 | Train Loss: 2.4057 Acc: 0.5261 | Val Loss: 1.8889 Acc: 0.4991                                               \n",
      "Epoch 005 | Train Loss: 1.8177 Acc: 0.5727 | Val Loss: 2.3026 Acc: 0.5925                                               \n",
      "Epoch 006 | Train Loss: 1.3960 Acc: 0.6253 | Val Loss: 1.0356 Acc: 0.6833                                               \n",
      "Epoch 007 | Train Loss: 1.1004 Acc: 0.6776 | Val Loss: 1.0899 Acc: 0.7176                                               \n",
      "Epoch 008 | Train Loss: 0.9145 Acc: 0.7215 | Val Loss: 0.7011 Acc: 0.7755                                               \n",
      "Epoch 009 | Train Loss: 0.7556 Acc: 0.7610 | Val Loss: 0.5302 Acc: 0.8236                                               \n",
      "Epoch 010 | Train Loss: 0.6693 Acc: 0.7875 | Val Loss: 0.4316 Acc: 0.8615                                               \n",
      "Epoch 011 | Train Loss: 0.5442 Acc: 0.8195 | Val Loss: 0.8662 Acc: 0.7746                                               \n",
      "Epoch 012 | Train Loss: 0.4546 Acc: 0.8503 | Val Loss: 0.3426 Acc: 0.8779                                               \n",
      "Epoch 013 | Train Loss: 0.4009 Acc: 0.8699 | Val Loss: 0.3303 Acc: 0.8925                                               \n",
      "Epoch 014 | Train Loss: 0.3344 Acc: 0.8871 | Val Loss: 0.2929 Acc: 0.9039                                               \n",
      "Epoch 015 | Train Loss: 0.2946 Acc: 0.9036 | Val Loss: 0.4475 Acc: 0.8525                                               \n",
      "Epoch 016 | Train Loss: 0.2664 Acc: 0.9119 | Val Loss: 0.2407 Acc: 0.9167                                               \n",
      "Epoch 017 | Train Loss: 0.2287 Acc: 0.9244 | Val Loss: 0.2472 Acc: 0.9125                                               \n",
      "Epoch 018 | Train Loss: 0.1951 Acc: 0.9334 | Val Loss: 0.1480 Acc: 0.9448                                               \n",
      "Epoch 019 | Train Loss: 0.1686 Acc: 0.9449 | Val Loss: 0.1398 Acc: 0.9510                                               \n",
      "Epoch 020 | Train Loss: 0.1581 Acc: 0.9477 | Val Loss: 0.1217 Acc: 0.9552                                               \n",
      "Epoch 021 | Train Loss: 0.1449 Acc: 0.9564 | Val Loss: 0.2065 Acc: 0.9355                                               \n",
      "Epoch 022 | Train Loss: 0.1152 Acc: 0.9643 | Val Loss: 0.1451 Acc: 0.9522                                               \n",
      "Epoch 023 | Train Loss: 0.1151 Acc: 0.9627 | Val Loss: 0.1527 Acc: 0.9496                                               \n",
      "Epoch 024 | Train Loss: 0.0913 Acc: 0.9694 | Val Loss: 0.2469 Acc: 0.9313                                               \n",
      "Epoch 025 | Train Loss: 0.0882 Acc: 0.9709 | Val Loss: 0.1032 Acc: 0.9678                                               \n",
      "Epoch 026 | Train Loss: 0.0798 Acc: 0.9751 | Val Loss: 0.2010 Acc: 0.9385                                               \n",
      "Epoch 027 | Train Loss: 0.0795 Acc: 0.9746 | Val Loss: 0.0880 Acc: 0.9746                                               \n",
      "Epoch 028 | Train Loss: 0.0691 Acc: 0.9772 | Val Loss: 0.1065 Acc: 0.9687                                               \n",
      "Epoch 029 | Train Loss: 0.0621 Acc: 0.9805 | Val Loss: 0.1161 Acc: 0.9603                                               \n",
      "Epoch 030 | Train Loss: 0.0554 Acc: 0.9825 | Val Loss: 0.1044 Acc: 0.9687                                               \n",
      "Epoch 031 | Train Loss: 0.0554 Acc: 0.9822 | Val Loss: 0.1012 Acc: 0.9693                                               \n",
      "Epoch 032 | Train Loss: 0.0461 Acc: 0.9855 | Val Loss: 0.3650 Acc: 0.8997                                               \n",
      "Epoch 033 | Train Loss: 0.0449 Acc: 0.9848 | Val Loss: 0.2999 Acc: 0.9218                                               \n",
      "Epoch 034 | Train Loss: 0.0401 Acc: 0.9876 | Val Loss: 0.1849 Acc: 0.9445                                               \n",
      "Epoch 035 | Train Loss: 0.0453 Acc: 0.9861 | Val Loss: 0.0716 Acc: 0.9773                                               \n",
      "Epoch 036 | Train Loss: 0.0444 Acc: 0.9863 | Val Loss: 0.1260 Acc: 0.9594                                               \n",
      "Epoch 037 | Train Loss: 0.0393 Acc: 0.9878 | Val Loss: 0.0801 Acc: 0.9761                                               \n",
      "Epoch 038 | Train Loss: 0.0339 Acc: 0.9887 | Val Loss: 0.0823 Acc: 0.9743                                               \n",
      "Epoch 039 | Train Loss: 0.0326 Acc: 0.9906 | Val Loss: 0.0511 Acc: 0.9842                                               \n",
      "Epoch 040 | Train Loss: 0.0369 Acc: 0.9894 | Val Loss: 0.1626 Acc: 0.9516                                               \n",
      "Epoch 041 | Train Loss: 0.0339 Acc: 0.9893 | Val Loss: 0.0624 Acc: 0.9806                                               \n",
      "Epoch 042 | Train Loss: 0.0335 Acc: 0.9901 | Val Loss: 0.0777 Acc: 0.9761                                               \n",
      "Epoch 043 | Train Loss: 0.0316 Acc: 0.9910 | Val Loss: 0.0960 Acc: 0.9722                                               \n",
      "Epoch 044 | Train Loss: 0.0335 Acc: 0.9904 | Val Loss: 0.0692 Acc: 0.9815                                               \n",
      "Epoch 045 | Train Loss: 0.0263 Acc: 0.9910 | Val Loss: 0.0920 Acc: 0.9779                                               \n",
      "Epoch 046 | Train Loss: 0.0286 Acc: 0.9914 | Val Loss: 0.0857 Acc: 0.9758                                               \n",
      "Epoch 047 | Train Loss: 0.0267 Acc: 0.9910 | Val Loss: 0.0889 Acc: 0.9761                                               \n",
      "Epoch 048 | Train Loss: 0.0252 Acc: 0.9919 | Val Loss: 0.0860 Acc: 0.9788                                               \n",
      "Epoch 049 | Train Loss: 0.0266 Acc: 0.9919 | Val Loss: 0.0635 Acc: 0.9821                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 64, 'cnn_dropout': 0.5125662905721351, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 48, 'cnn_kernels_2': 16, 'learning_rate': 8.817431779836926e-05, 'lstm_dense': 128, 'lstm_hidden_size': 32, 'lstm_layers': 2, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 75.0076 Acc: 0.3432 | Val Loss: 25.1163 Acc: 0.3185                                             \n",
      "Epoch 002 | Train Loss: 25.8988 Acc: 0.3691 | Val Loss: 21.7407 Acc: 0.3176                                             \n",
      "Epoch 003 | Train Loss: 15.6246 Acc: 0.4038 | Val Loss: 10.8266 Acc: 0.4090                                             \n",
      "Epoch 004 | Train Loss: 11.2237 Acc: 0.4149 | Val Loss: 5.8232 Acc: 0.4433                                              \n",
      "Epoch 005 | Train Loss: 9.0737 Acc: 0.4153 | Val Loss: 2.1987 Acc: 0.5487                                               \n",
      "Epoch 006 | Train Loss: 7.5736 Acc: 0.4236 | Val Loss: 2.3632 Acc: 0.5442                                               \n",
      "Epoch 007 | Train Loss: 6.1561 Acc: 0.4364 | Val Loss: 1.9886 Acc: 0.5325                                               \n",
      "Epoch 008 | Train Loss: 5.3722 Acc: 0.4394 | Val Loss: 1.4709 Acc: 0.5307                                               \n",
      "Epoch 009 | Train Loss: 4.8744 Acc: 0.4431 | Val Loss: 1.2881 Acc: 0.5400                                               \n",
      "Epoch 010 | Train Loss: 4.2715 Acc: 0.4491 | Val Loss: 1.1525 Acc: 0.5388                                               \n",
      "Epoch 011 | Train Loss: 3.9088 Acc: 0.4556 | Val Loss: 1.2633 Acc: 0.5979                                               \n",
      "Epoch 012 | Train Loss: 3.3444 Acc: 0.4677 | Val Loss: 1.1831 Acc: 0.5904                                               \n",
      "Epoch 013 | Train Loss: 3.1673 Acc: 0.4653 | Val Loss: 1.0206 Acc: 0.6209                                               \n",
      "Epoch 014 | Train Loss: 2.8399 Acc: 0.4818 | Val Loss: 1.0178 Acc: 0.6057                                               \n",
      "Epoch 015 | Train Loss: 2.6707 Acc: 0.4832 | Val Loss: 1.1765 Acc: 0.6081                                               \n",
      "Epoch 016 | Train Loss: 2.5736 Acc: 0.4891 | Val Loss: 1.4043 Acc: 0.5451                                               \n",
      "Epoch 017 | Train Loss: 2.2532 Acc: 0.5046 | Val Loss: 0.9897 Acc: 0.6421                                               \n",
      "Epoch 018 | Train Loss: 2.1055 Acc: 0.5169 | Val Loss: 1.0666 Acc: 0.6487                                               \n",
      "Epoch 019 | Train Loss: 1.9740 Acc: 0.5277 | Val Loss: 0.8804 Acc: 0.6785                                               \n",
      "Epoch 020 | Train Loss: 1.9048 Acc: 0.5257 | Val Loss: 0.9311 Acc: 0.6391                                               \n",
      "Epoch 021 | Train Loss: 1.7595 Acc: 0.5409 | Val Loss: 0.9710 Acc: 0.6552                                               \n",
      "Epoch 022 | Train Loss: 1.7602 Acc: 0.5346 | Val Loss: 0.9470 Acc: 0.6549                                               \n",
      "Epoch 023 | Train Loss: 1.6039 Acc: 0.5571 | Val Loss: 0.7885 Acc: 0.6782                                               \n",
      "Epoch 024 | Train Loss: 1.5587 Acc: 0.5662 | Val Loss: 0.9497 Acc: 0.6642                                               \n",
      "Epoch 025 | Train Loss: 1.5305 Acc: 0.5662 | Val Loss: 0.8743 Acc: 0.6657                                               \n",
      "Epoch 026 | Train Loss: 1.4522 Acc: 0.5716 | Val Loss: 0.7910 Acc: 0.6716                                               \n",
      "Epoch 027 | Train Loss: 1.4070 Acc: 0.5800 | Val Loss: 0.8717 Acc: 0.6749                                               \n",
      "Epoch 028 | Train Loss: 1.3215 Acc: 0.5955 | Val Loss: 0.8841 Acc: 0.6675                                               \n",
      "Epoch 029 | Train Loss: 1.2955 Acc: 0.5905 | Val Loss: 0.7597 Acc: 0.7087                                               \n",
      "Epoch 030 | Train Loss: 1.2429 Acc: 0.5974 | Val Loss: 0.9187 Acc: 0.6570                                               \n",
      "Epoch 031 | Train Loss: 1.2257 Acc: 0.5998 | Val Loss: 0.7081 Acc: 0.6919                                               \n",
      "Epoch 032 | Train Loss: 1.1832 Acc: 0.6063 | Val Loss: 0.7627 Acc: 0.6749                                               \n",
      "Epoch 033 | Train Loss: 1.1496 Acc: 0.6068 | Val Loss: 0.8898 Acc: 0.6716                                               \n",
      "Epoch 034 | Train Loss: 1.0945 Acc: 0.6138 | Val Loss: 0.8870 Acc: 0.6612                                               \n",
      "Epoch 035 | Train Loss: 1.0449 Acc: 0.6221 | Val Loss: 0.8370 Acc: 0.6066                                               \n",
      "Epoch 036 | Train Loss: 1.0312 Acc: 0.6309 | Val Loss: 0.7423 Acc: 0.7018                                               \n",
      "Epoch 037 | Train Loss: 0.9769 Acc: 0.6427 | Val Loss: 0.7830 Acc: 0.6818                                               \n",
      "Epoch 038 | Train Loss: 0.9770 Acc: 0.6403 | Val Loss: 0.8028 Acc: 0.6669                                               \n",
      "Epoch 039 | Train Loss: 0.9051 Acc: 0.6509 | Val Loss: 0.7781 Acc: 0.6182                                               \n",
      "Epoch 040 | Train Loss: 0.9272 Acc: 0.6551 | Val Loss: 0.6557 Acc: 0.6949                                               \n",
      "Epoch 041 | Train Loss: 0.8815 Acc: 0.6583 | Val Loss: 0.7342 Acc: 0.7051                                               \n",
      "Epoch 042 | Train Loss: 0.8701 Acc: 0.6671 | Val Loss: 0.6754 Acc: 0.6910                                               \n",
      "Epoch 043 | Train Loss: 0.8486 Acc: 0.6668 | Val Loss: 0.7556 Acc: 0.6651                                               \n",
      "Epoch 044 | Train Loss: 0.8430 Acc: 0.6721 | Val Loss: 0.6569 Acc: 0.7304                                               \n",
      "Epoch 045 | Train Loss: 0.8336 Acc: 0.6775 | Val Loss: 0.7774 Acc: 0.6585                                               \n",
      "Epoch 046 | Train Loss: 0.7972 Acc: 0.6855 | Val Loss: 0.6230 Acc: 0.7060                                               \n",
      "Epoch 047 | Train Loss: 0.7799 Acc: 0.6924 | Val Loss: 0.5809 Acc: 0.7376                                               \n",
      "Epoch 048 | Train Loss: 0.7826 Acc: 0.6941 | Val Loss: 0.6296 Acc: 0.7173                                               \n",
      "Epoch 049 | Train Loss: 0.7476 Acc: 0.7001 | Val Loss: 0.5955 Acc: 0.7460                                               \n",
      "Epoch 050 | Train Loss: 0.7526 Acc: 0.7006 | Val Loss: 0.6339 Acc: 0.7110                                               \n",
      "Epoch 051 | Train Loss: 0.7403 Acc: 0.7045 | Val Loss: 0.7547 Acc: 0.6890                                               \n",
      "Epoch 052 | Train Loss: 0.7433 Acc: 0.7069 | Val Loss: 0.5791 Acc: 0.7376                                               \n",
      "Epoch 053 | Train Loss: 0.7132 Acc: 0.7211 | Val Loss: 0.6738 Acc: 0.7039                                               \n",
      "Epoch 054 | Train Loss: 0.6709 Acc: 0.7346 | Val Loss: 0.5697 Acc: 0.7561                                               \n",
      "Epoch 055 | Train Loss: 0.6797 Acc: 0.7236 | Val Loss: 0.5847 Acc: 0.7364                                               \n",
      "Epoch 056 | Train Loss: 0.6650 Acc: 0.7375 | Val Loss: 0.6616 Acc: 0.7101                                               \n",
      "Epoch 057 | Train Loss: 0.6456 Acc: 0.7457 | Val Loss: 0.7951 Acc: 0.6803                                               \n",
      "Epoch 058 | Train Loss: 0.6344 Acc: 0.7454 | Val Loss: 0.6134 Acc: 0.7376                                               \n",
      "Epoch 059 | Train Loss: 0.6261 Acc: 0.7559 | Val Loss: 0.6010 Acc: 0.7212                                               \n",
      "Epoch 060 | Train Loss: 0.5936 Acc: 0.7664 | Val Loss: 0.5221 Acc: 0.7800                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 36, 'cnn_dense': 64, 'cnn_dropout': 0.4611267033464219, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 16, 'cnn_kernels_2': 32, 'learning_rate': 0.00019275692678876534, 'lstm_dense': 32, 'lstm_hidden_size': 32, 'lstm_layers': 6, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 41.5503 Acc: 0.3721 | Val Loss: 5.0915 Acc: 0.4376                                              \n",
      "Epoch 002 | Train Loss: 8.3666 Acc: 0.4211 | Val Loss: 2.4767 Acc: 0.4678                                               \n",
      "Epoch 003 | Train Loss: 4.4951 Acc: 0.4541 | Val Loss: 1.6317 Acc: 0.5815                                               \n",
      "Epoch 004 | Train Loss: 2.7842 Acc: 0.5004 | Val Loss: 0.9966 Acc: 0.6239                                               \n",
      "Epoch 005 | Train Loss: 2.0343 Acc: 0.5227 | Val Loss: 1.2541 Acc: 0.5994                                               \n",
      "Epoch 006 | Train Loss: 1.5079 Acc: 0.5691 | Val Loss: 1.0138 Acc: 0.6218                                               \n",
      "Epoch 007 | Train Loss: 1.2970 Acc: 0.5891 | Val Loss: 0.7266 Acc: 0.6982                                               \n",
      "Epoch 008 | Train Loss: 1.1889 Acc: 0.6056 | Val Loss: 0.9394 Acc: 0.6469                                               \n",
      "Epoch 009 | Train Loss: 1.0314 Acc: 0.6477 | Val Loss: 0.5972 Acc: 0.7725                                               \n",
      "Epoch 010 | Train Loss: 0.9297 Acc: 0.6778 | Val Loss: 0.5270 Acc: 0.7896                                               \n",
      "Epoch 011 | Train Loss: 0.8455 Acc: 0.7128 | Val Loss: 0.5663 Acc: 0.7955                                               \n",
      "Epoch 012 | Train Loss: 0.7356 Acc: 0.7517 | Val Loss: 0.5124 Acc: 0.8107                                               \n",
      "Epoch 013 | Train Loss: 0.6754 Acc: 0.7683 | Val Loss: 0.5081 Acc: 0.8412                                               \n",
      "Epoch 014 | Train Loss: 0.6033 Acc: 0.7951 | Val Loss: 0.4062 Acc: 0.8600                                               \n",
      "Epoch 015 | Train Loss: 0.5431 Acc: 0.8127 | Val Loss: 0.4563 Acc: 0.8516                                               \n",
      "Epoch 016 | Train Loss: 0.4970 Acc: 0.8310 | Val Loss: 0.3534 Acc: 0.8752                                               \n",
      "Epoch 017 | Train Loss: 0.5048 Acc: 0.8307 | Val Loss: 0.5087 Acc: 0.8340                                               \n",
      "Epoch 018 | Train Loss: 0.4612 Acc: 0.8436 | Val Loss: 0.2997 Acc: 0.8937                                               \n",
      "Epoch 019 | Train Loss: 0.4289 Acc: 0.8593 | Val Loss: 0.2748 Acc: 0.9057                                               \n",
      "Epoch 020 | Train Loss: 0.3957 Acc: 0.8651 | Val Loss: 0.3332 Acc: 0.8872                                               \n",
      "Epoch 021 | Train Loss: 0.3756 Acc: 0.8761 | Val Loss: 0.2488 Acc: 0.9101                                               \n",
      "Epoch 022 | Train Loss: 0.3344 Acc: 0.8901 | Val Loss: 0.2586 Acc: 0.9107                                               \n",
      "Epoch 023 | Train Loss: 0.3041 Acc: 0.8998 | Val Loss: 0.2012 Acc: 0.9430                                               \n",
      "Epoch 024 | Train Loss: 0.3028 Acc: 0.9013 | Val Loss: 0.2014 Acc: 0.9325                                               \n",
      "Epoch 025 | Train Loss: 0.2793 Acc: 0.9091 | Val Loss: 0.3227 Acc: 0.8985                                               \n",
      "Epoch 026 | Train Loss: 0.2755 Acc: 0.9100 | Val Loss: 0.1950 Acc: 0.9340                                               \n",
      "Epoch 027 | Train Loss: 0.2592 Acc: 0.9184 | Val Loss: 0.2754 Acc: 0.9060                                               \n",
      "Epoch 028 | Train Loss: 0.2253 Acc: 0.9263 | Val Loss: 0.2079 Acc: 0.9316                                               \n",
      "Epoch 029 | Train Loss: 0.2358 Acc: 0.9226 | Val Loss: 0.1774 Acc: 0.9361                                               \n",
      "Epoch 030 | Train Loss: 0.2244 Acc: 0.9263 | Val Loss: 0.1488 Acc: 0.9513                                               \n",
      "Epoch 031 | Train Loss: 0.2013 Acc: 0.9329 | Val Loss: 0.1555 Acc: 0.9490                                               \n",
      "Epoch 032 | Train Loss: 0.1974 Acc: 0.9369 | Val Loss: 0.2097 Acc: 0.9269                                               \n",
      "Epoch 033 | Train Loss: 0.1645 Acc: 0.9453 | Val Loss: 0.1293 Acc: 0.9603                                               \n",
      "Epoch 034 | Train Loss: 0.1716 Acc: 0.9419 | Val Loss: 0.1662 Acc: 0.9391                                               \n",
      "Epoch 035 | Train Loss: 0.1706 Acc: 0.9445 | Val Loss: 0.1187 Acc: 0.9609                                               \n",
      "Epoch 036 | Train Loss: 0.1421 Acc: 0.9535 | Val Loss: 0.1097 Acc: 0.9624                                               \n",
      "Epoch 037 | Train Loss: 0.1501 Acc: 0.9529 | Val Loss: 0.1195 Acc: 0.9597                                               \n",
      "Epoch 038 | Train Loss: 0.1442 Acc: 0.9522 | Val Loss: 0.1170 Acc: 0.9639                                               \n",
      "Epoch 039 | Train Loss: 0.1314 Acc: 0.9568 | Val Loss: 0.1805 Acc: 0.9445                                               \n",
      "Epoch 040 | Train Loss: 0.1275 Acc: 0.9606 | Val Loss: 0.1204 Acc: 0.9624                                               \n",
      "Epoch 041 | Train Loss: 0.1242 Acc: 0.9598 | Val Loss: 0.0753 Acc: 0.9767                                               \n",
      "Epoch 042 | Train Loss: 0.1236 Acc: 0.9614 | Val Loss: 0.1017 Acc: 0.9669                                               \n",
      "Epoch 043 | Train Loss: 0.1035 Acc: 0.9649 | Val Loss: 0.1870 Acc: 0.9394                                               \n",
      "Epoch 044 | Train Loss: 0.1114 Acc: 0.9646 | Val Loss: 0.0906 Acc: 0.9701                                               \n",
      "Epoch 045 | Train Loss: 0.0980 Acc: 0.9680 | Val Loss: 0.1055 Acc: 0.9690                                               \n",
      "Epoch 046 | Train Loss: 0.1021 Acc: 0.9672 | Val Loss: 0.1179 Acc: 0.9642                                               \n",
      "Epoch 047 | Train Loss: 0.0969 Acc: 0.9698 | Val Loss: 0.0869 Acc: 0.9710                                               \n",
      "Epoch 048 | Train Loss: 0.0962 Acc: 0.9683 | Val Loss: 0.1608 Acc: 0.9519                                               \n",
      "Epoch 049 | Train Loss: 0.0922 Acc: 0.9715 | Val Loss: 0.1104 Acc: 0.9654                                               \n",
      "Epoch 050 | Train Loss: 0.0902 Acc: 0.9713 | Val Loss: 0.1169 Acc: 0.9642                                               \n",
      "Epoch 051 | Train Loss: 0.0819 Acc: 0.9741 | Val Loss: 0.0915 Acc: 0.9743                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 36, 'cnn_dense': 128, 'cnn_dropout': 0.24419197908858434, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 32, 'cnn_kernels_2': 32, 'learning_rate': 0.0002187536524582408, 'lstm_dense': 32, 'lstm_hidden_size': 128, 'lstm_layers': 5, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 18.7031 Acc: 0.3903 | Val Loss: 1.9063 Acc: 0.5266                                              \n",
      "Epoch 002 | Train Loss: 5.1332 Acc: 0.4373 | Val Loss: 2.0507 Acc: 0.5200                                               \n",
      "Epoch 003 | Train Loss: 3.1514 Acc: 0.4891 | Val Loss: 1.3188 Acc: 0.6534                                               \n",
      "Epoch 004 | Train Loss: 2.3115 Acc: 0.5403 | Val Loss: 1.3023 Acc: 0.6239                                               \n",
      "Epoch 005 | Train Loss: 1.6620 Acc: 0.5985 | Val Loss: 1.5003 Acc: 0.6015                                               \n",
      "Epoch 006 | Train Loss: 1.2578 Acc: 0.6593 | Val Loss: 0.8332 Acc: 0.7522                                               \n",
      "Epoch 007 | Train Loss: 1.0608 Acc: 0.6888 | Val Loss: 0.6840 Acc: 0.7543                                               \n",
      "Epoch 008 | Train Loss: 0.8662 Acc: 0.7250 | Val Loss: 0.6519 Acc: 0.7842                                               \n",
      "Epoch 009 | Train Loss: 0.7044 Acc: 0.7708 | Val Loss: 0.7188 Acc: 0.7875                                               \n",
      "Epoch 010 | Train Loss: 0.5922 Acc: 0.8007 | Val Loss: 0.5170 Acc: 0.8206                                               \n",
      "Epoch 011 | Train Loss: 0.4789 Acc: 0.8357 | Val Loss: 0.3292 Acc: 0.8946                                               \n",
      "Epoch 012 | Train Loss: 0.3703 Acc: 0.8715 | Val Loss: 0.3000 Acc: 0.9039                                               \n",
      "Epoch 013 | Train Loss: 0.3033 Acc: 0.8951 | Val Loss: 0.2396 Acc: 0.9269                                               \n",
      "Epoch 014 | Train Loss: 0.2572 Acc: 0.9120 | Val Loss: 0.2144 Acc: 0.9316                                               \n",
      "Epoch 015 | Train Loss: 0.2151 Acc: 0.9271 | Val Loss: 0.1904 Acc: 0.9364                                               \n",
      "Epoch 016 | Train Loss: 0.1597 Acc: 0.9459 | Val Loss: 0.1503 Acc: 0.9522                                               \n",
      "Epoch 017 | Train Loss: 0.1434 Acc: 0.9526 | Val Loss: 0.1786 Acc: 0.9472                                               \n",
      "Epoch 018 | Train Loss: 0.1227 Acc: 0.9608 | Val Loss: 0.1938 Acc: 0.9397                                               \n",
      "Epoch 019 | Train Loss: 0.1184 Acc: 0.9621 | Val Loss: 0.1285 Acc: 0.9558                                               \n",
      "Epoch 020 | Train Loss: 0.0912 Acc: 0.9700 | Val Loss: 0.1646 Acc: 0.9442                                               \n",
      "Epoch 021 | Train Loss: 0.0831 Acc: 0.9731 | Val Loss: 0.0825 Acc: 0.9761                                               \n",
      "Epoch 022 | Train Loss: 0.0831 Acc: 0.9728 | Val Loss: 0.1302 Acc: 0.9597                                               \n",
      "Epoch 023 | Train Loss: 0.0785 Acc: 0.9749 | Val Loss: 0.1626 Acc: 0.9540                                               \n",
      "Epoch 024 | Train Loss: 0.0630 Acc: 0.9799 | Val Loss: 0.0862 Acc: 0.9740                                               \n",
      "Epoch 025 | Train Loss: 0.0590 Acc: 0.9803 | Val Loss: 0.0880 Acc: 0.9734                                               \n",
      "Epoch 026 | Train Loss: 0.0532 Acc: 0.9826 | Val Loss: 0.0813 Acc: 0.9764                                               \n",
      "Epoch 027 | Train Loss: 0.0771 Acc: 0.9762 | Val Loss: 0.1235 Acc: 0.9606                                               \n",
      "Epoch 028 | Train Loss: 0.0672 Acc: 0.9778 | Val Loss: 0.0886 Acc: 0.9737                                               \n",
      "Epoch 029 | Train Loss: 0.0441 Acc: 0.9867 | Val Loss: 0.0866 Acc: 0.9767                                               \n",
      "Epoch 030 | Train Loss: 0.0478 Acc: 0.9849 | Val Loss: 0.1083 Acc: 0.9672                                               \n",
      "Epoch 031 | Train Loss: 0.0366 Acc: 0.9884 | Val Loss: 0.0934 Acc: 0.9731                                               \n",
      "Epoch 032 | Train Loss: 0.0445 Acc: 0.9852 | Val Loss: 0.1216 Acc: 0.9687                                               \n",
      "Epoch 033 | Train Loss: 0.0501 Acc: 0.9844 | Val Loss: 0.1032 Acc: 0.9699                                               \n",
      "Epoch 034 | Train Loss: 0.0341 Acc: 0.9887 | Val Loss: 0.1247 Acc: 0.9672                                               \n",
      "Epoch 035 | Train Loss: 0.0244 Acc: 0.9919 | Val Loss: 0.1144 Acc: 0.9758                                               \n",
      "Epoch 036 | Train Loss: 0.0403 Acc: 0.9863 | Val Loss: 0.1023 Acc: 0.9710                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 256, 'cnn_dropout': 0.4565829393242257, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 16, 'learning_rate': 0.0001331461679823622, 'lstm_dense': 128, 'lstm_hidden_size': 64, 'lstm_layers': 3, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 50.1485 Acc: 0.3623 | Val Loss: 7.6436 Acc: 0.3797                                              \n",
      "Epoch 002 | Train Loss: 12.6541 Acc: 0.3808 | Val Loss: 2.4308 Acc: 0.4069                                              \n",
      "Epoch 003 | Train Loss: 8.1264 Acc: 0.3919 | Val Loss: 1.7653 Acc: 0.4675                                               \n",
      "Epoch 004 | Train Loss: 6.0825 Acc: 0.4103 | Val Loss: 2.9314 Acc: 0.4845                                               \n",
      "Epoch 005 | Train Loss: 4.7925 Acc: 0.4179 | Val Loss: 2.6107 Acc: 0.5322                                               \n",
      "Epoch 006 | Train Loss: 3.8252 Acc: 0.4431 | Val Loss: 1.5601 Acc: 0.5415                                               \n",
      "Epoch 007 | Train Loss: 3.0658 Acc: 0.4788 | Val Loss: 1.7910 Acc: 0.5012                                               \n",
      "Epoch 008 | Train Loss: 2.5658 Acc: 0.5005 | Val Loss: 1.3396 Acc: 0.5976                                               \n",
      "Epoch 009 | Train Loss: 2.2144 Acc: 0.5202 | Val Loss: 1.0373 Acc: 0.6164                                               \n",
      "Epoch 010 | Train Loss: 1.8607 Acc: 0.5490 | Val Loss: 0.9681 Acc: 0.6373                                               \n",
      "Epoch 011 | Train Loss: 1.6212 Acc: 0.5827 | Val Loss: 1.5393 Acc: 0.5648                                               \n",
      "Epoch 012 | Train Loss: 1.4583 Acc: 0.6030 | Val Loss: 1.1572 Acc: 0.6066                                               \n",
      "Epoch 013 | Train Loss: 1.2724 Acc: 0.6403 | Val Loss: 0.9008 Acc: 0.7000                                               \n",
      "Epoch 014 | Train Loss: 1.1425 Acc: 0.6650 | Val Loss: 0.8362 Acc: 0.7430                                               \n",
      "Epoch 015 | Train Loss: 1.0374 Acc: 0.6864 | Val Loss: 0.7951 Acc: 0.7666                                               \n",
      "Epoch 016 | Train Loss: 0.9035 Acc: 0.7155 | Val Loss: 0.7184 Acc: 0.7669                                               \n",
      "Epoch 017 | Train Loss: 0.8490 Acc: 0.7336 | Val Loss: 0.6039 Acc: 0.8039                                               \n",
      "Epoch 018 | Train Loss: 0.7630 Acc: 0.7532 | Val Loss: 0.5431 Acc: 0.8227                                               \n",
      "Epoch 019 | Train Loss: 0.6580 Acc: 0.7810 | Val Loss: 0.5917 Acc: 0.8173                                               \n",
      "Epoch 020 | Train Loss: 0.6094 Acc: 0.7986 | Val Loss: 0.4386 Acc: 0.8373                                               \n",
      "Epoch 021 | Train Loss: 0.5292 Acc: 0.8267 | Val Loss: 0.5054 Acc: 0.8257                                               \n",
      "Epoch 022 | Train Loss: 0.4943 Acc: 0.8314 | Val Loss: 0.4143 Acc: 0.8484                                               \n",
      "Epoch 023 | Train Loss: 0.4394 Acc: 0.8478 | Val Loss: 0.4596 Acc: 0.8304                                               \n",
      "Epoch 024 | Train Loss: 0.3931 Acc: 0.8637 | Val Loss: 0.3192 Acc: 0.8797                                               \n",
      "Epoch 025 | Train Loss: 0.4053 Acc: 0.8673 | Val Loss: 0.3170 Acc: 0.8955                                               \n",
      "Epoch 026 | Train Loss: 0.3254 Acc: 0.8860 | Val Loss: 0.2346 Acc: 0.9206                                               \n",
      "Epoch 027 | Train Loss: 0.2947 Acc: 0.9006 | Val Loss: 0.2305 Acc: 0.9284                                               \n",
      "Epoch 028 | Train Loss: 0.2697 Acc: 0.9066 | Val Loss: 0.1891 Acc: 0.9403                                               \n",
      "Epoch 029 | Train Loss: 0.2380 Acc: 0.9186 | Val Loss: 0.2380 Acc: 0.9236                                               \n",
      "Epoch 030 | Train Loss: 0.2177 Acc: 0.9304 | Val Loss: 0.1852 Acc: 0.9388                                               \n",
      "Epoch 031 | Train Loss: 0.2210 Acc: 0.9264 | Val Loss: 0.1632 Acc: 0.9472                                               \n",
      "Epoch 032 | Train Loss: 0.1849 Acc: 0.9387 | Val Loss: 0.1877 Acc: 0.9421                                               \n",
      "Epoch 033 | Train Loss: 0.1838 Acc: 0.9387 | Val Loss: 0.2115 Acc: 0.9290                                               \n",
      "Epoch 034 | Train Loss: 0.1782 Acc: 0.9381 | Val Loss: 0.2103 Acc: 0.9343                                               \n",
      "Epoch 035 | Train Loss: 0.1655 Acc: 0.9448 | Val Loss: 0.1411 Acc: 0.9546                                               \n",
      "Epoch 036 | Train Loss: 0.1496 Acc: 0.9513 | Val Loss: 0.1650 Acc: 0.9454                                               \n",
      "Epoch 037 | Train Loss: 0.1415 Acc: 0.9518 | Val Loss: 0.1975 Acc: 0.9304                                               \n",
      "Epoch 038 | Train Loss: 0.1383 Acc: 0.9540 | Val Loss: 0.1395 Acc: 0.9546                                               \n",
      "Epoch 039 | Train Loss: 0.1314 Acc: 0.9543 | Val Loss: 0.1061 Acc: 0.9693                                               \n",
      "Epoch 040 | Train Loss: 0.1178 Acc: 0.9601 | Val Loss: 0.1262 Acc: 0.9618                                               \n",
      "Epoch 041 | Train Loss: 0.1057 Acc: 0.9659 | Val Loss: 0.1029 Acc: 0.9666                                               \n",
      "Epoch 042 | Train Loss: 0.1129 Acc: 0.9630 | Val Loss: 0.1307 Acc: 0.9588                                               \n",
      "Epoch 043 | Train Loss: 0.1033 Acc: 0.9662 | Val Loss: 0.1063 Acc: 0.9657                                               \n",
      "Epoch 044 | Train Loss: 0.1017 Acc: 0.9657 | Val Loss: 0.1065 Acc: 0.9693                                               \n",
      "Epoch 045 | Train Loss: 0.0996 Acc: 0.9669 | Val Loss: 0.1030 Acc: 0.9681                                               \n",
      "Epoch 046 | Train Loss: 0.0989 Acc: 0.9660 | Val Loss: 0.1348 Acc: 0.9603                                               \n",
      "Epoch 047 | Train Loss: 0.0767 Acc: 0.9754 | Val Loss: 0.0983 Acc: 0.9690                                               \n",
      "Epoch 048 | Train Loss: 0.0781 Acc: 0.9731 | Val Loss: 0.1007 Acc: 0.9663                                               \n",
      "Epoch 049 | Train Loss: 0.0814 Acc: 0.9716 | Val Loss: 0.1089 Acc: 0.9681                                               \n",
      "Epoch 050 | Train Loss: 0.0890 Acc: 0.9704 | Val Loss: 0.0827 Acc: 0.9755                                               \n",
      "Epoch 051 | Train Loss: 0.0633 Acc: 0.9787 | Val Loss: 0.0819 Acc: 0.9752                                               \n",
      "Epoch 052 | Train Loss: 0.0646 Acc: 0.9767 | Val Loss: 0.1240 Acc: 0.9648                                               \n",
      "Epoch 053 | Train Loss: 0.0612 Acc: 0.9795 | Val Loss: 0.0790 Acc: 0.9770                                               \n",
      "Epoch 054 | Train Loss: 0.0606 Acc: 0.9806 | Val Loss: 0.1055 Acc: 0.9654                                               \n",
      "Epoch 055 | Train Loss: 0.0648 Acc: 0.9788 | Val Loss: 0.0722 Acc: 0.9797                                               \n",
      "Epoch 056 | Train Loss: 0.0590 Acc: 0.9808 | Val Loss: 0.0933 Acc: 0.9737                                               \n",
      "Epoch 057 | Train Loss: 0.0602 Acc: 0.9793 | Val Loss: 0.0778 Acc: 0.9743                                               \n",
      "Epoch 058 | Train Loss: 0.0538 Acc: 0.9821 | Val Loss: 0.1179 Acc: 0.9678                                               \n",
      "Epoch 059 | Train Loss: 0.0487 Acc: 0.9838 | Val Loss: 0.0958 Acc: 0.9746                                               \n",
      "Epoch 060 | Train Loss: 0.0579 Acc: 0.9797 | Val Loss: 0.1143 Acc: 0.9645                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 36, 'cnn_dense': 64, 'cnn_dropout': 0.5631444193634152, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 32, 'cnn_kernels_2': 32, 'learning_rate': 0.0008090167731295054, 'lstm_dense': 256, 'lstm_hidden_size': 128, 'lstm_layers': 4, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 2.9434 Acc: 0.4365 | Val Loss: 1.2498 Acc: 0.4421                                               \n",
      "Epoch 002 | Train Loss: 1.2442 Acc: 0.4422 | Val Loss: 1.2390 Acc: 0.4421                                               \n",
      "Epoch 003 | Train Loss: 1.2388 Acc: 0.4422 | Val Loss: 1.2372 Acc: 0.4421                                               \n",
      "Epoch 004 | Train Loss: 1.2381 Acc: 0.4422 | Val Loss: 1.2370 Acc: 0.4421                                               \n",
      "Epoch 005 | Train Loss: 1.2380 Acc: 0.4422 | Val Loss: 1.2372 Acc: 0.4421                                               \n",
      "Epoch 006 | Train Loss: 1.2379 Acc: 0.4422 | Val Loss: 1.2372 Acc: 0.4421                                               \n",
      "Epoch 007 | Train Loss: 1.2383 Acc: 0.4422 | Val Loss: 1.2369 Acc: 0.4421                                               \n",
      "Epoch 008 | Train Loss: 1.2374 Acc: 0.4422 | Val Loss: 1.2370 Acc: 0.4421                                               \n",
      "Epoch 009 | Train Loss: 1.2374 Acc: 0.4422 | Val Loss: 1.2361 Acc: 0.4421                                               \n",
      "Epoch 010 | Train Loss: 1.2382 Acc: 0.4422 | Val Loss: 1.2359 Acc: 0.4421                                               \n",
      "Epoch 011 | Train Loss: 1.2377 Acc: 0.4422 | Val Loss: 1.2358 Acc: 0.4421                                               \n",
      "Epoch 012 | Train Loss: 1.2372 Acc: 0.4422 | Val Loss: 1.2352 Acc: 0.4421                                               \n",
      "Epoch 013 | Train Loss: 1.2371 Acc: 0.4422 | Val Loss: 1.2349 Acc: 0.4421                                               \n",
      "Epoch 014 | Train Loss: 1.2364 Acc: 0.4422 | Val Loss: 1.2342 Acc: 0.4421                                               \n",
      "Epoch 015 | Train Loss: 1.2353 Acc: 0.4422 | Val Loss: 1.2333 Acc: 0.4421                                               \n",
      "Epoch 016 | Train Loss: 1.2350 Acc: 0.4422 | Val Loss: 1.2324 Acc: 0.4421                                               \n",
      "Epoch 017 | Train Loss: 1.2337 Acc: 0.4422 | Val Loss: 1.2311 Acc: 0.4421                                               \n",
      "Epoch 018 | Train Loss: 1.2304 Acc: 0.4422 | Val Loss: 1.2271 Acc: 0.4421                                               \n",
      "Epoch 019 | Train Loss: 1.2252 Acc: 0.4422 | Val Loss: 1.2183 Acc: 0.4421                                               \n",
      "Epoch 020 | Train Loss: 1.2124 Acc: 0.4416 | Val Loss: 1.2146 Acc: 0.4648                                               \n",
      "Epoch 021 | Train Loss: 1.1891 Acc: 0.4572 | Val Loss: 1.1872 Acc: 0.4687                                               \n",
      "Epoch 022 | Train Loss: 1.1681 Acc: 0.4752 | Val Loss: 1.1694 Acc: 0.4585                                               \n",
      "Epoch 023 | Train Loss: 1.1530 Acc: 0.4854 | Val Loss: 1.1528 Acc: 0.4696                                               \n",
      "Epoch 024 | Train Loss: 1.1323 Acc: 0.4901 | Val Loss: 1.1450 Acc: 0.4797                                               \n",
      "Epoch 025 | Train Loss: 1.1232 Acc: 0.4978 | Val Loss: 1.1300 Acc: 0.4660                                               \n",
      "Epoch 026 | Train Loss: 1.1096 Acc: 0.5025 | Val Loss: 1.0846 Acc: 0.5122                                               \n",
      "Epoch 027 | Train Loss: 1.0928 Acc: 0.5112 | Val Loss: 1.1294 Acc: 0.4597                                               \n",
      "Epoch 028 | Train Loss: 1.0839 Acc: 0.5156 | Val Loss: 1.0496 Acc: 0.5304                                               \n",
      "Epoch 029 | Train Loss: 1.0623 Acc: 0.5295 | Val Loss: 1.0376 Acc: 0.5278                                               \n",
      "Epoch 030 | Train Loss: 1.0434 Acc: 0.5338 | Val Loss: 0.9943 Acc: 0.5582                                               \n",
      "Epoch 031 | Train Loss: 1.0147 Acc: 0.5463 | Val Loss: 1.0319 Acc: 0.5119                                               \n",
      "Epoch 032 | Train Loss: 0.9802 Acc: 0.5639 | Val Loss: 0.9623 Acc: 0.5776                                               \n",
      "Epoch 033 | Train Loss: 0.9466 Acc: 0.5797 | Val Loss: 0.9009 Acc: 0.5955                                               \n",
      "Epoch 034 | Train Loss: 0.9238 Acc: 0.5944 | Val Loss: 0.9489 Acc: 0.5657                                               \n",
      "Epoch 035 | Train Loss: 0.9021 Acc: 0.6037 | Val Loss: 0.8944 Acc: 0.6194                                               \n",
      "Epoch 036 | Train Loss: 0.8847 Acc: 0.6094 | Val Loss: 0.9113 Acc: 0.6128                                               \n",
      "Epoch 037 | Train Loss: 0.8603 Acc: 0.6253 | Val Loss: 0.8333 Acc: 0.6346                                               \n",
      "Epoch 038 | Train Loss: 0.8386 Acc: 0.6385 | Val Loss: 0.7881 Acc: 0.6716                                               \n",
      "Epoch 039 | Train Loss: 0.8304 Acc: 0.6425 | Val Loss: 0.9534 Acc: 0.5893                                               \n",
      "Epoch 040 | Train Loss: 0.8026 Acc: 0.6605 | Val Loss: 0.7693 Acc: 0.6815                                               \n",
      "Epoch 041 | Train Loss: 0.8045 Acc: 0.6619 | Val Loss: 0.7331 Acc: 0.7024                                               \n",
      "Epoch 042 | Train Loss: 0.7736 Acc: 0.6771 | Val Loss: 0.8117 Acc: 0.6642                                               \n",
      "Epoch 043 | Train Loss: 0.7510 Acc: 0.6839 | Val Loss: 0.7580 Acc: 0.6857                                               \n",
      "Epoch 044 | Train Loss: 0.7343 Acc: 0.6937 | Val Loss: 0.7240 Acc: 0.7128                                               \n",
      "Epoch 045 | Train Loss: 0.7062 Acc: 0.7152 | Val Loss: 0.7251 Acc: 0.7015                                               \n",
      "Epoch 046 | Train Loss: 0.6952 Acc: 0.7165 | Val Loss: 0.7170 Acc: 0.7081                                               \n",
      "Epoch 047 | Train Loss: 0.6796 Acc: 0.7287 | Val Loss: 0.6646 Acc: 0.7546                                               \n",
      "Epoch 048 | Train Loss: 0.6344 Acc: 0.7563 | Val Loss: 0.6330 Acc: 0.7636                                               \n",
      "Epoch 049 | Train Loss: 0.5954 Acc: 0.7713 | Val Loss: 0.5275 Acc: 0.7919                                               \n",
      "Epoch 050 | Train Loss: 0.5598 Acc: 0.7844 | Val Loss: 0.6256 Acc: 0.7710                                               \n",
      "Epoch 051 | Train Loss: 0.5332 Acc: 0.7979 | Val Loss: 0.4861 Acc: 0.8161                                               \n",
      "Epoch 052 | Train Loss: 0.5081 Acc: 0.8064 | Val Loss: 0.5227 Acc: 0.8051                                               \n",
      "Epoch 053 | Train Loss: 0.4851 Acc: 0.8142 | Val Loss: 0.4527 Acc: 0.8281                                               \n",
      "Epoch 054 | Train Loss: 0.4699 Acc: 0.8220 | Val Loss: 0.5141 Acc: 0.8197                                               \n",
      "Epoch 055 | Train Loss: 0.4595 Acc: 0.8233 | Val Loss: 0.4176 Acc: 0.8430                                               \n",
      "Epoch 056 | Train Loss: 0.4302 Acc: 0.8346 | Val Loss: 0.4133 Acc: 0.8433                                               \n",
      "Epoch 057 | Train Loss: 0.4101 Acc: 0.8413 | Val Loss: 0.5095 Acc: 0.8209                                               \n",
      "Epoch 058 | Train Loss: 0.3953 Acc: 0.8492 | Val Loss: 0.3837 Acc: 0.8597                                               \n",
      "Epoch 059 | Train Loss: 0.3765 Acc: 0.8532 | Val Loss: 0.3663 Acc: 0.8609                                               \n",
      "Epoch 060 | Train Loss: 0.3677 Acc: 0.8599 | Val Loss: 0.3504 Acc: 0.8704                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 64, 'cnn_dropout': 0.1296373361895236, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 32, 'learning_rate': 9.608774760099408e-05, 'lstm_dense': 256, 'lstm_hidden_size': 64, 'lstm_layers': 6, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 25.8433 Acc: 0.3696 | Val Loss: 5.4573 Acc: 0.4818                                              \n",
      "Epoch 002 | Train Loss: 11.5415 Acc: 0.4014 | Val Loss: 3.5892 Acc: 0.4779                                              \n",
      "Epoch 003 | Train Loss: 8.1470 Acc: 0.4088 | Val Loss: 2.7780 Acc: 0.4863                                               \n",
      "Epoch 004 | Train Loss: 6.0109 Acc: 0.4375 | Val Loss: 1.6754 Acc: 0.5707                                               \n",
      "Epoch 005 | Train Loss: 4.5975 Acc: 0.4594 | Val Loss: 1.9253 Acc: 0.5549                                               \n",
      "Epoch 006 | Train Loss: 3.8211 Acc: 0.4847 | Val Loss: 1.3354 Acc: 0.5666                                               \n",
      "Epoch 007 | Train Loss: 3.1774 Acc: 0.5030 | Val Loss: 1.3663 Acc: 0.6460                                               \n",
      "Epoch 008 | Train Loss: 2.7858 Acc: 0.5250 | Val Loss: 1.8943 Acc: 0.6152                                               \n",
      "Epoch 009 | Train Loss: 2.5179 Acc: 0.5394 | Val Loss: 0.8506 Acc: 0.6722                                               \n",
      "Epoch 010 | Train Loss: 2.2261 Acc: 0.5635 | Val Loss: 1.1796 Acc: 0.6072                                               \n",
      "Epoch 011 | Train Loss: 2.0596 Acc: 0.5761 | Val Loss: 1.1264 Acc: 0.6645                                               \n",
      "Epoch 012 | Train Loss: 1.8384 Acc: 0.5961 | Val Loss: 1.0548 Acc: 0.6215                                               \n",
      "Epoch 013 | Train Loss: 1.7065 Acc: 0.6099 | Val Loss: 1.0627 Acc: 0.6499                                               \n",
      "Epoch 014 | Train Loss: 1.5704 Acc: 0.6255 | Val Loss: 0.7206 Acc: 0.7116                                               \n",
      "Epoch 015 | Train Loss: 1.4537 Acc: 0.6409 | Val Loss: 0.9983 Acc: 0.6815                                               \n",
      "Epoch 016 | Train Loss: 1.3136 Acc: 0.6624 | Val Loss: 0.8754 Acc: 0.6821                                               \n",
      "Epoch 017 | Train Loss: 1.2711 Acc: 0.6685 | Val Loss: 0.8247 Acc: 0.7167                                               \n",
      "Epoch 018 | Train Loss: 1.1212 Acc: 0.6922 | Val Loss: 0.6946 Acc: 0.7430                                               \n",
      "Epoch 019 | Train Loss: 1.0967 Acc: 0.6959 | Val Loss: 1.0918 Acc: 0.6609                                               \n",
      "Epoch 020 | Train Loss: 0.9967 Acc: 0.7137 | Val Loss: 0.6707 Acc: 0.7818                                               \n",
      "Epoch 021 | Train Loss: 0.9496 Acc: 0.7342 | Val Loss: 2.1290 Acc: 0.5496                                               \n",
      "Epoch 022 | Train Loss: 0.9011 Acc: 0.7425 | Val Loss: 1.0873 Acc: 0.6994                                               \n",
      "Epoch 023 | Train Loss: 0.8481 Acc: 0.7544 | Val Loss: 1.1812 Acc: 0.6973                                               \n",
      "Epoch 024 | Train Loss: 0.8124 Acc: 0.7683 | Val Loss: 1.0363 Acc: 0.7206                                               \n",
      "Epoch 025 | Train Loss: 0.7962 Acc: 0.7763 | Val Loss: 0.5269 Acc: 0.8149                                               \n",
      "Epoch 026 | Train Loss: 0.6982 Acc: 0.7941 | Val Loss: 1.3508 Acc: 0.6976                                               \n",
      "Epoch 027 | Train Loss: 0.6639 Acc: 0.8033 | Val Loss: 0.3200 Acc: 0.8854                                               \n",
      "Epoch 028 | Train Loss: 0.6256 Acc: 0.8121 | Val Loss: 0.3138 Acc: 0.8955                                               \n",
      "Epoch 029 | Train Loss: 0.5923 Acc: 0.8204 | Val Loss: 0.4463 Acc: 0.8543                                               \n",
      "Epoch 030 | Train Loss: 0.5624 Acc: 0.8275 | Val Loss: 0.3510 Acc: 0.8764                                               \n",
      "Epoch 031 | Train Loss: 0.5383 Acc: 0.8365 | Val Loss: 0.3688 Acc: 0.8734                                               \n",
      "Epoch 032 | Train Loss: 0.4623 Acc: 0.8581 | Val Loss: 0.4757 Acc: 0.8439                                               \n",
      "Epoch 033 | Train Loss: 0.4702 Acc: 0.8584 | Val Loss: 0.3688 Acc: 0.8743                                               \n",
      "Epoch 034 | Train Loss: 0.4583 Acc: 0.8619 | Val Loss: 0.3214 Acc: 0.8985                                               \n",
      "Epoch 035 | Train Loss: 0.3997 Acc: 0.8787 | Val Loss: 0.2957 Acc: 0.9000                                               \n",
      "Epoch 036 | Train Loss: 0.3827 Acc: 0.8831 | Val Loss: 0.4950 Acc: 0.8681                                               \n",
      "Epoch 037 | Train Loss: 0.3509 Acc: 0.8901 | Val Loss: 0.2662 Acc: 0.9116                                               \n",
      "Epoch 038 | Train Loss: 0.3663 Acc: 0.8918 | Val Loss: 0.2098 Acc: 0.9379                                               \n",
      "Epoch 039 | Train Loss: 0.3025 Acc: 0.9045 | Val Loss: 0.2974 Acc: 0.8994                                               \n",
      "Epoch 040 | Train Loss: 0.3108 Acc: 0.9022 | Val Loss: 0.4291 Acc: 0.8678                                               \n",
      "Epoch 041 | Train Loss: 0.2864 Acc: 0.9098 | Val Loss: 0.2016 Acc: 0.9278                                               \n",
      "Epoch 042 | Train Loss: 0.2804 Acc: 0.9141 | Val Loss: 0.2339 Acc: 0.9263                                               \n",
      "Epoch 043 | Train Loss: 0.2473 Acc: 0.9256 | Val Loss: 0.2430 Acc: 0.9278                                               \n",
      "Epoch 044 | Train Loss: 0.2317 Acc: 0.9290 | Val Loss: 0.2462 Acc: 0.9284                                               \n",
      "Epoch 045 | Train Loss: 0.2340 Acc: 0.9276 | Val Loss: 0.3638 Acc: 0.8952                                               \n",
      "Epoch 046 | Train Loss: 0.2158 Acc: 0.9318 | Val Loss: 0.2308 Acc: 0.9343                                               \n",
      "Epoch 047 | Train Loss: 0.1951 Acc: 0.9409 | Val Loss: 0.3260 Acc: 0.9110                                               \n",
      "Epoch 048 | Train Loss: 0.1928 Acc: 0.9429 | Val Loss: 0.2197 Acc: 0.9328                                               \n",
      "Epoch 049 | Train Loss: 0.1872 Acc: 0.9446 | Val Loss: 0.2250 Acc: 0.9340                                               \n",
      "Epoch 050 | Train Loss: 0.1578 Acc: 0.9508 | Val Loss: 0.6012 Acc: 0.8534                                               \n",
      "Epoch 051 | Train Loss: 0.1899 Acc: 0.9451 | Val Loss: 0.3172 Acc: 0.9152                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 36, 'cnn_dense': 128, 'cnn_dropout': 0.6640979115190798, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 32, 'learning_rate': 4.432680984393944e-05, 'lstm_dense': 256, 'lstm_hidden_size': 32, 'lstm_layers': 4, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 61.2547 Acc: 0.2841 | Val Loss: 19.9951 Acc: 0.4194                                             \n",
      "Epoch 002 | Train Loss: 27.1047 Acc: 0.3385 | Val Loss: 6.5061 Acc: 0.3603                                              \n",
      "Epoch 003 | Train Loss: 18.2628 Acc: 0.3646 | Val Loss: 5.0434 Acc: 0.4182                                              \n",
      "Epoch 004 | Train Loss: 13.7506 Acc: 0.3902 | Val Loss: 3.0947 Acc: 0.4821                                              \n",
      "Epoch 005 | Train Loss: 10.7581 Acc: 0.4137 | Val Loss: 4.7552 Acc: 0.4699                                              \n",
      "Epoch 006 | Train Loss: 8.7121 Acc: 0.4329 | Val Loss: 3.5752 Acc: 0.5290                                               \n",
      "Epoch 007 | Train Loss: 7.3003 Acc: 0.4364 | Val Loss: 2.9066 Acc: 0.5510                                               \n",
      "Epoch 008 | Train Loss: 6.1180 Acc: 0.4464 | Val Loss: 2.0237 Acc: 0.5588                                               \n",
      "Epoch 009 | Train Loss: 5.2796 Acc: 0.4568 | Val Loss: 1.9341 Acc: 0.5940                                               \n",
      "Epoch 010 | Train Loss: 4.6023 Acc: 0.4612 | Val Loss: 1.5417 Acc: 0.5170                                               \n",
      "Epoch 011 | Train Loss: 4.0382 Acc: 0.4735 | Val Loss: 1.3593 Acc: 0.5576                                               \n",
      "Epoch 012 | Train Loss: 3.6144 Acc: 0.4827 | Val Loss: 2.5341 Acc: 0.5119                                               \n",
      "Epoch 013 | Train Loss: 3.1676 Acc: 0.4891 | Val Loss: 0.9747 Acc: 0.6284                                               \n",
      "Epoch 014 | Train Loss: 2.7868 Acc: 0.4967 | Val Loss: 0.9630 Acc: 0.6272                                               \n",
      "Epoch 015 | Train Loss: 2.5441 Acc: 0.5159 | Val Loss: 1.3075 Acc: 0.6161                                               \n",
      "Epoch 016 | Train Loss: 2.2642 Acc: 0.5259 | Val Loss: 1.6447 Acc: 0.5710                                               \n",
      "Epoch 017 | Train Loss: 2.0612 Acc: 0.5370 | Val Loss: 0.9881 Acc: 0.6499                                               \n",
      "Epoch 018 | Train Loss: 1.8969 Acc: 0.5418 | Val Loss: 1.1404 Acc: 0.5696                                               \n",
      "Epoch 019 | Train Loss: 1.7260 Acc: 0.5607 | Val Loss: 1.5868 Acc: 0.5493                                               \n",
      "Epoch 020 | Train Loss: 1.5910 Acc: 0.5668 | Val Loss: 0.8112 Acc: 0.6976                                               \n",
      "Epoch 021 | Train Loss: 1.4773 Acc: 0.5809 | Val Loss: 1.0475 Acc: 0.6313                                               \n",
      "Epoch 022 | Train Loss: 1.3808 Acc: 0.5812 | Val Loss: 1.5712 Acc: 0.5684                                               \n",
      "Epoch 023 | Train Loss: 1.2848 Acc: 0.6036 | Val Loss: 0.7666 Acc: 0.7015                                               \n",
      "Epoch 024 | Train Loss: 1.2107 Acc: 0.6063 | Val Loss: 1.0059 Acc: 0.6042                                               \n",
      "Epoch 025 | Train Loss: 1.1504 Acc: 0.6162 | Val Loss: 1.0980 Acc: 0.6478                                               \n",
      "Epoch 026 | Train Loss: 1.1101 Acc: 0.6233 | Val Loss: 0.9167 Acc: 0.6439                                               \n",
      "Epoch 027 | Train Loss: 1.0669 Acc: 0.6239 | Val Loss: 0.8593 Acc: 0.6504                                               \n",
      "Epoch 028 | Train Loss: 1.0087 Acc: 0.6371 | Val Loss: 0.8018 Acc: 0.7179                                               \n",
      "Epoch 029 | Train Loss: 0.9766 Acc: 0.6450 | Val Loss: 0.7316 Acc: 0.6704                                               \n",
      "Epoch 030 | Train Loss: 0.9496 Acc: 0.6583 | Val Loss: 0.8333 Acc: 0.6630                                               \n",
      "Epoch 031 | Train Loss: 0.9031 Acc: 0.6672 | Val Loss: 0.7509 Acc: 0.7200                                               \n",
      "Epoch 032 | Train Loss: 0.8955 Acc: 0.6665 | Val Loss: 0.9091 Acc: 0.6854                                               \n",
      "Epoch 033 | Train Loss: 0.8600 Acc: 0.6745 | Val Loss: 0.7859 Acc: 0.6597                                               \n",
      "Epoch 034 | Train Loss: 0.8282 Acc: 0.6867 | Val Loss: 0.8916 Acc: 0.6576                                               \n",
      "Epoch 035 | Train Loss: 0.8024 Acc: 0.6929 | Val Loss: 0.9359 Acc: 0.6158                                               \n",
      "Epoch 036 | Train Loss: 0.7794 Acc: 0.7030 | Val Loss: 0.6646 Acc: 0.6958                                               \n",
      "Epoch 037 | Train Loss: 0.7588 Acc: 0.7089 | Val Loss: 0.9254 Acc: 0.6821                                               \n",
      "Epoch 038 | Train Loss: 0.7556 Acc: 0.7112 | Val Loss: 0.7022 Acc: 0.6576                                               \n",
      "Epoch 039 | Train Loss: 0.7144 Acc: 0.7262 | Val Loss: 0.5663 Acc: 0.7761                                               \n",
      "Epoch 040 | Train Loss: 0.7051 Acc: 0.7342 | Val Loss: 0.7355 Acc: 0.6522                                               \n",
      "Epoch 041 | Train Loss: 0.6824 Acc: 0.7398 | Val Loss: 0.8213 Acc: 0.6316                                               \n",
      "Epoch 042 | Train Loss: 0.6613 Acc: 0.7507 | Val Loss: 0.9331 Acc: 0.6913                                               \n",
      "Epoch 043 | Train Loss: 0.6607 Acc: 0.7514 | Val Loss: 0.8002 Acc: 0.7015                                               \n",
      "Epoch 044 | Train Loss: 0.6299 Acc: 0.7657 | Val Loss: 0.6304 Acc: 0.7713                                               \n",
      "Epoch 045 | Train Loss: 0.6304 Acc: 0.7702 | Val Loss: 0.9218 Acc: 0.6916                                               \n",
      "Epoch 046 | Train Loss: 0.5991 Acc: 0.7739 | Val Loss: 1.0148 Acc: 0.6600                                               \n",
      "Epoch 047 | Train Loss: 0.5931 Acc: 0.7803 | Val Loss: 0.7957 Acc: 0.6710                                               \n",
      "Epoch 048 | Train Loss: 0.5810 Acc: 0.7869 | Val Loss: 0.8260 Acc: 0.6997                                               \n",
      "Epoch 049 | Train Loss: 0.5762 Acc: 0.7902 | Val Loss: 0.7118 Acc: 0.6860                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 36, 'cnn_dense': 256, 'cnn_dropout': 0.35605825826117204, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 64, 'learning_rate': 5.634934235712131e-05, 'lstm_dense': 128, 'lstm_hidden_size': 96, 'lstm_layers': 1, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 38.3586 Acc: 0.3393 | Val Loss: 7.1411 Acc: 0.3940                                              \n",
      "Epoch 002 | Train Loss: 14.4006 Acc: 0.3888 | Val Loss: 4.7841 Acc: 0.4693                                              \n",
      "Epoch 003 | Train Loss: 9.5193 Acc: 0.3991 | Val Loss: 4.2370 Acc: 0.4600                                               \n",
      "Epoch 004 | Train Loss: 7.0743 Acc: 0.4209 | Val Loss: 2.0802 Acc: 0.5093                                               \n",
      "Epoch 005 | Train Loss: 5.3099 Acc: 0.4329 | Val Loss: 3.1745 Acc: 0.4146                                               \n",
      "Epoch 006 | Train Loss: 4.2945 Acc: 0.4517 | Val Loss: 1.9260 Acc: 0.4970                                               \n",
      "Epoch 007 | Train Loss: 3.3681 Acc: 0.4613 | Val Loss: 1.2026 Acc: 0.5281                                               \n",
      "Epoch 008 | Train Loss: 2.8847 Acc: 0.4833 | Val Loss: 1.0804 Acc: 0.5564                                               \n",
      "Epoch 009 | Train Loss: 2.5371 Acc: 0.4831 | Val Loss: 1.7145 Acc: 0.5060                                               \n",
      "Epoch 010 | Train Loss: 2.2779 Acc: 0.5003 | Val Loss: 1.5124 Acc: 0.4609                                               \n",
      "Epoch 011 | Train Loss: 2.0622 Acc: 0.4981 | Val Loss: 1.0583 Acc: 0.5248                                               \n",
      "Epoch 012 | Train Loss: 1.9094 Acc: 0.5041 | Val Loss: 1.5155 Acc: 0.4522                                               \n",
      "Epoch 013 | Train Loss: 1.7673 Acc: 0.5136 | Val Loss: 1.1386 Acc: 0.5499                                               \n",
      "Epoch 014 | Train Loss: 1.6227 Acc: 0.5254 | Val Loss: 1.0110 Acc: 0.5451                                               \n",
      "Epoch 015 | Train Loss: 1.5245 Acc: 0.5345 | Val Loss: 0.8713 Acc: 0.5779                                               \n",
      "Epoch 016 | Train Loss: 1.4434 Acc: 0.5394 | Val Loss: 0.7134 Acc: 0.6800                                               \n",
      "Epoch 017 | Train Loss: 1.3890 Acc: 0.5471 | Val Loss: 0.9201 Acc: 0.5564                                               \n",
      "Epoch 018 | Train Loss: 1.2892 Acc: 0.5518 | Val Loss: 1.3256 Acc: 0.4958                                               \n",
      "Epoch 019 | Train Loss: 1.2731 Acc: 0.5559 | Val Loss: 0.8959 Acc: 0.5490                                               \n",
      "Epoch 020 | Train Loss: 1.2058 Acc: 0.5707 | Val Loss: 0.8341 Acc: 0.6143                                               \n",
      "Epoch 021 | Train Loss: 1.1183 Acc: 0.5797 | Val Loss: 0.6908 Acc: 0.6427                                               \n",
      "Epoch 022 | Train Loss: 1.1264 Acc: 0.5795 | Val Loss: 0.9096 Acc: 0.6176                                               \n",
      "Epoch 023 | Train Loss: 1.0829 Acc: 0.5848 | Val Loss: 0.7483 Acc: 0.5869                                               \n",
      "Epoch 024 | Train Loss: 1.0713 Acc: 0.5896 | Val Loss: 0.7954 Acc: 0.5236                                               \n",
      "Epoch 025 | Train Loss: 1.0178 Acc: 0.6050 | Val Loss: 0.7541 Acc: 0.6125                                               \n",
      "Epoch 026 | Train Loss: 0.9974 Acc: 0.6090 | Val Loss: 0.7518 Acc: 0.6110                                               \n",
      "Epoch 027 | Train Loss: 0.9764 Acc: 0.6144 | Val Loss: 0.8687 Acc: 0.6433                                               \n",
      "Epoch 028 | Train Loss: 0.9552 Acc: 0.6239 | Val Loss: 0.7306 Acc: 0.6513                                               \n",
      "Epoch 029 | Train Loss: 0.9376 Acc: 0.6267 | Val Loss: 0.9568 Acc: 0.5072                                               \n",
      "Epoch 030 | Train Loss: 0.9174 Acc: 0.6277 | Val Loss: 0.9493 Acc: 0.5663                                               \n",
      "Epoch 031 | Train Loss: 0.9037 Acc: 0.6296 | Val Loss: 0.6395 Acc: 0.7645                                               \n",
      "Epoch 032 | Train Loss: 0.8788 Acc: 0.6427 | Val Loss: 1.5922 Acc: 0.5594                                               \n",
      "Epoch 033 | Train Loss: 0.8511 Acc: 0.6556 | Val Loss: 0.8580 Acc: 0.6230                                               \n",
      "Epoch 034 | Train Loss: 0.8367 Acc: 0.6576 | Val Loss: 0.5889 Acc: 0.7803                                               \n",
      "Epoch 035 | Train Loss: 0.8135 Acc: 0.6699 | Val Loss: 0.5899 Acc: 0.7704                                               \n",
      "Epoch 036 | Train Loss: 0.7925 Acc: 0.6766 | Val Loss: 0.7449 Acc: 0.6307                                               \n",
      "Epoch 037 | Train Loss: 0.7782 Acc: 0.6912 | Val Loss: 0.6257 Acc: 0.7110                                               \n",
      "Epoch 038 | Train Loss: 0.7430 Acc: 0.7038 | Val Loss: 0.9716 Acc: 0.6119                                               \n",
      "Epoch 039 | Train Loss: 0.7220 Acc: 0.7143 | Val Loss: 0.8232 Acc: 0.6272                                               \n",
      "Epoch 040 | Train Loss: 0.6974 Acc: 0.7224 | Val Loss: 0.5456 Acc: 0.7710                                               \n",
      "Epoch 041 | Train Loss: 0.6844 Acc: 0.7354 | Val Loss: 0.5968 Acc: 0.7475                                               \n",
      "Epoch 042 | Train Loss: 0.6500 Acc: 0.7421 | Val Loss: 0.9289 Acc: 0.6149                                               \n",
      "Epoch 043 | Train Loss: 0.6198 Acc: 0.7585 | Val Loss: 0.5621 Acc: 0.7490                                               \n",
      "Epoch 044 | Train Loss: 0.6007 Acc: 0.7672 | Val Loss: 0.5128 Acc: 0.7773                                               \n",
      "Epoch 045 | Train Loss: 0.5677 Acc: 0.7760 | Val Loss: 0.4758 Acc: 0.8030                                               \n",
      "Epoch 046 | Train Loss: 0.5358 Acc: 0.7944 | Val Loss: 0.5920 Acc: 0.7251                                               \n",
      "Epoch 047 | Train Loss: 0.5068 Acc: 0.8036 | Val Loss: 0.4022 Acc: 0.8403                                               \n",
      "Epoch 048 | Train Loss: 0.4732 Acc: 0.8157 | Val Loss: 0.3440 Acc: 0.8669                                               \n",
      "Epoch 049 | Train Loss: 0.4439 Acc: 0.8299 | Val Loss: 0.9243 Acc: 0.6893                                               \n",
      "Epoch 050 | Train Loss: 0.4225 Acc: 0.8369 | Val Loss: 0.4660 Acc: 0.8281                                               \n",
      "Epoch 051 | Train Loss: 0.3888 Acc: 0.8547 | Val Loss: 0.3248 Acc: 0.8687                                               \n",
      "Epoch 052 | Train Loss: 0.3763 Acc: 0.8614 | Val Loss: 0.3707 Acc: 0.8504                                               \n",
      "Epoch 053 | Train Loss: 0.3508 Acc: 0.8721 | Val Loss: 0.2793 Acc: 0.8991                                               \n",
      "Epoch 054 | Train Loss: 0.3189 Acc: 0.8828 | Val Loss: 0.8375 Acc: 0.7242                                               \n",
      "Epoch 055 | Train Loss: 0.3078 Acc: 0.8922 | Val Loss: 0.3434 Acc: 0.8573                                               \n",
      "Epoch 056 | Train Loss: 0.2815 Acc: 0.8993 | Val Loss: 0.4925 Acc: 0.8134                                               \n",
      "Epoch 057 | Train Loss: 0.2624 Acc: 0.9075 | Val Loss: 0.2004 Acc: 0.9346                                               \n",
      "Epoch 058 | Train Loss: 0.2421 Acc: 0.9128 | Val Loss: 0.2022 Acc: 0.9301                                               \n",
      "Epoch 059 | Train Loss: 0.2249 Acc: 0.9186 | Val Loss: 0.1764 Acc: 0.9391                                               \n",
      "Epoch 060 | Train Loss: 0.2039 Acc: 0.9255 | Val Loss: 0.1920 Acc: 0.9313                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 128, 'cnn_dropout': 0.2274705954337717, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 16, 'learning_rate': 0.000374801909023915, 'lstm_dense': 128, 'lstm_hidden_size': 128, 'lstm_layers': 1, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 14.9519 Acc: 0.3835 | Val Loss: 2.7368 Acc: 0.5134                                              \n",
      "Epoch 002 | Train Loss: 3.7978 Acc: 0.4441 | Val Loss: 1.5006 Acc: 0.5352                                               \n",
      "Epoch 003 | Train Loss: 2.1500 Acc: 0.4850 | Val Loss: 1.4434 Acc: 0.5251                                               \n",
      "Epoch 004 | Train Loss: 1.5607 Acc: 0.5135 | Val Loss: 0.9460 Acc: 0.6051                                               \n",
      "Epoch 005 | Train Loss: 1.2520 Acc: 0.5374 | Val Loss: 0.9355 Acc: 0.5693                                               \n",
      "Epoch 006 | Train Loss: 1.0891 Acc: 0.5618 | Val Loss: 0.6923 Acc: 0.6257                                               \n",
      "Epoch 007 | Train Loss: 0.8849 Acc: 0.6268 | Val Loss: 0.5955 Acc: 0.7054                                               \n",
      "Epoch 008 | Train Loss: 0.7490 Acc: 0.6904 | Val Loss: 0.4689 Acc: 0.8084                                               \n",
      "Epoch 009 | Train Loss: 0.6077 Acc: 0.7461 | Val Loss: 0.5390 Acc: 0.7800                                               \n",
      "Epoch 010 | Train Loss: 0.5146 Acc: 0.7898 | Val Loss: 0.3478 Acc: 0.8719                                               \n",
      "Epoch 011 | Train Loss: 0.4304 Acc: 0.8299 | Val Loss: 0.3169 Acc: 0.8731                                               \n",
      "Epoch 012 | Train Loss: 0.3443 Acc: 0.8669 | Val Loss: 0.2649 Acc: 0.8994                                               \n",
      "Epoch 013 | Train Loss: 0.2943 Acc: 0.8910 | Val Loss: 0.2025 Acc: 0.9194                                               \n",
      "Epoch 014 | Train Loss: 0.2561 Acc: 0.9057 | Val Loss: 0.1714 Acc: 0.9388                                               \n",
      "Epoch 015 | Train Loss: 0.2207 Acc: 0.9167 | Val Loss: 0.1819 Acc: 0.9376                                               \n",
      "Epoch 016 | Train Loss: 0.1833 Acc: 0.9316 | Val Loss: 0.1426 Acc: 0.9484                                               \n",
      "Epoch 017 | Train Loss: 0.1668 Acc: 0.9395 | Val Loss: 0.1597 Acc: 0.9427                                               \n",
      "Epoch 018 | Train Loss: 0.1470 Acc: 0.9470 | Val Loss: 0.1128 Acc: 0.9600                                               \n",
      "Epoch 019 | Train Loss: 0.1482 Acc: 0.9497 | Val Loss: 0.1106 Acc: 0.9621                                               \n",
      "Epoch 020 | Train Loss: 0.1181 Acc: 0.9594 | Val Loss: 0.1424 Acc: 0.9478                                               \n",
      "Epoch 021 | Train Loss: 0.1273 Acc: 0.9569 | Val Loss: 0.1315 Acc: 0.9549                                               \n",
      "Epoch 022 | Train Loss: 0.0991 Acc: 0.9643 | Val Loss: 0.0855 Acc: 0.9716                                               \n",
      "Epoch 023 | Train Loss: 0.0912 Acc: 0.9685 | Val Loss: 0.1021 Acc: 0.9654                                               \n",
      "Epoch 024 | Train Loss: 0.0890 Acc: 0.9718 | Val Loss: 0.0906 Acc: 0.9707                                               \n",
      "Epoch 025 | Train Loss: 0.0882 Acc: 0.9688 | Val Loss: 0.0783 Acc: 0.9713                                               \n",
      "Epoch 026 | Train Loss: 0.0745 Acc: 0.9748 | Val Loss: 0.1235 Acc: 0.9627                                               \n",
      "Epoch 027 | Train Loss: 0.0821 Acc: 0.9725 | Val Loss: 0.0861 Acc: 0.9707                                               \n",
      "Epoch 028 | Train Loss: 0.0630 Acc: 0.9792 | Val Loss: 0.1019 Acc: 0.9651                                               \n",
      "Epoch 029 | Train Loss: 0.0544 Acc: 0.9807 | Val Loss: 0.0643 Acc: 0.9800                                               \n",
      "Epoch 030 | Train Loss: 0.0683 Acc: 0.9760 | Val Loss: 0.0940 Acc: 0.9669                                               \n",
      "Epoch 031 | Train Loss: 0.0532 Acc: 0.9838 | Val Loss: 0.0770 Acc: 0.9752                                               \n",
      "Epoch 032 | Train Loss: 0.0564 Acc: 0.9804 | Val Loss: 0.0646 Acc: 0.9764                                               \n",
      "Epoch 033 | Train Loss: 0.0467 Acc: 0.9851 | Val Loss: 0.1297 Acc: 0.9531                                               \n",
      "Epoch 034 | Train Loss: 0.0454 Acc: 0.9848 | Val Loss: 0.0877 Acc: 0.9743                                               \n",
      "Epoch 035 | Train Loss: 0.0436 Acc: 0.9854 | Val Loss: 0.0529 Acc: 0.9848                                               \n",
      "Epoch 036 | Train Loss: 0.0450 Acc: 0.9852 | Val Loss: 0.0664 Acc: 0.9791                                               \n",
      "Epoch 037 | Train Loss: 0.0400 Acc: 0.9861 | Val Loss: 0.0549 Acc: 0.9818                                               \n",
      "Epoch 038 | Train Loss: 0.0408 Acc: 0.9859 | Val Loss: 0.0454 Acc: 0.9881                                               \n",
      "Epoch 039 | Train Loss: 0.0339 Acc: 0.9894 | Val Loss: 0.0622 Acc: 0.9842                                               \n",
      "Epoch 040 | Train Loss: 0.0485 Acc: 0.9844 | Val Loss: 0.0506 Acc: 0.9830                                               \n",
      "Epoch 041 | Train Loss: 0.0346 Acc: 0.9885 | Val Loss: 0.0597 Acc: 0.9797                                               \n",
      "Epoch 042 | Train Loss: 0.0340 Acc: 0.9894 | Val Loss: 0.0685 Acc: 0.9800                                               \n",
      "Epoch 043 | Train Loss: 0.0330 Acc: 0.9884 | Val Loss: 0.0442 Acc: 0.9860                                               \n",
      "Epoch 044 | Train Loss: 0.0304 Acc: 0.9906 | Val Loss: 0.0625 Acc: 0.9848                                               \n",
      "Epoch 045 | Train Loss: 0.0330 Acc: 0.9885 | Val Loss: 0.0438 Acc: 0.9878                                               \n",
      "Epoch 046 | Train Loss: 0.0333 Acc: 0.9890 | Val Loss: 0.0679 Acc: 0.9770                                               \n",
      "Epoch 047 | Train Loss: 0.0332 Acc: 0.9891 | Val Loss: 0.0459 Acc: 0.9842                                               \n",
      "Epoch 048 | Train Loss: 0.0178 Acc: 0.9941 | Val Loss: 0.0530 Acc: 0.9851                                               \n",
      "Epoch 049 | Train Loss: 0.0330 Acc: 0.9887 | Val Loss: 0.0602 Acc: 0.9806                                               \n",
      "Epoch 050 | Train Loss: 0.0306 Acc: 0.9903 | Val Loss: 0.0432 Acc: 0.9881                                               \n",
      "Epoch 051 | Train Loss: 0.0222 Acc: 0.9922 | Val Loss: 0.0564 Acc: 0.9824                                               \n",
      "Epoch 052 | Train Loss: 0.0320 Acc: 0.9898 | Val Loss: 0.0549 Acc: 0.9824                                               \n",
      "Epoch 053 | Train Loss: 0.0253 Acc: 0.9916 | Val Loss: 0.0838 Acc: 0.9731                                               \n",
      "Epoch 054 | Train Loss: 0.0242 Acc: 0.9927 | Val Loss: 0.0537 Acc: 0.9848                                               \n",
      "Epoch 055 | Train Loss: 0.0316 Acc: 0.9896 | Val Loss: 0.1083 Acc: 0.9681                                               \n",
      "Epoch 056 | Train Loss: 0.0291 Acc: 0.9893 | Val Loss: 0.0599 Acc: 0.9842                                               \n",
      "Epoch 057 | Train Loss: 0.0347 Acc: 0.9890 | Val Loss: 0.0584 Acc: 0.9809                                               \n",
      "Epoch 058 | Train Loss: 0.0244 Acc: 0.9923 | Val Loss: 0.0711 Acc: 0.9782                                               \n",
      "Epoch 059 | Train Loss: 0.0239 Acc: 0.9916 | Val Loss: 0.0584 Acc: 0.9827                                               \n",
      "Epoch 060 | Train Loss: 0.0208 Acc: 0.9932 | Val Loss: 0.1064 Acc: 0.9681                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 64, 'cnn_dropout': 0.545656529790168, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 64, 'cnn_kernels_2': 32, 'learning_rate': 0.006854624755063305, 'lstm_dense': 64, 'lstm_hidden_size': 64, 'lstm_layers': 4, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 26.5726 Acc: 0.4204 | Val Loss: 1.1659 Acc: 0.4421                                              \n",
      "Epoch 002 | Train Loss: 1.2075 Acc: 0.4386 | Val Loss: 1.1626 Acc: 0.4421                                               \n",
      "Epoch 003 | Train Loss: 1.2728 Acc: 0.4419 | Val Loss: 1.2421 Acc: 0.4421                                               \n",
      "Epoch 004 | Train Loss: 1.2474 Acc: 0.4416 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 005 | Train Loss: 1.2635 Acc: 0.4410 | Val Loss: 1.2071 Acc: 0.4421                                               \n",
      "Epoch 006 | Train Loss: 1.2432 Acc: 0.4419 | Val Loss: 1.2420 Acc: 0.4421                                               \n",
      "Epoch 007 | Train Loss: 1.2578 Acc: 0.4415 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 008 | Train Loss: 1.2428 Acc: 0.4420 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 009 | Train Loss: 1.2541 Acc: 0.4417 | Val Loss: 1.2421 Acc: 0.4421                                               \n",
      "Epoch 010 | Train Loss: 1.2433 Acc: 0.4421 | Val Loss: 1.2425 Acc: 0.4421                                               \n",
      "Epoch 011 | Train Loss: 1.2424 Acc: 0.4422 | Val Loss: 1.2418 Acc: 0.4421                                               \n",
      "Epoch 012 | Train Loss: 1.7418 Acc: 0.4402 | Val Loss: 1.2423 Acc: 0.4421                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 64, 'cnn_dropout': 0.07912125246650552, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 32, 'learning_rate': 0.0038997811518281613, 'lstm_dense': 128, 'lstm_hidden_size': 128, 'lstm_layers': 1, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 8.1342 Acc: 0.4595 | Val Loss: 1.0940 Acc: 0.5042                                               \n",
      "Epoch 002 | Train Loss: 0.9770 Acc: 0.5357 | Val Loss: 1.4236 Acc: 0.4860                                               \n",
      "Epoch 003 | Train Loss: 0.9041 Acc: 0.5440 | Val Loss: 0.8275 Acc: 0.6116                                               \n",
      "Epoch 004 | Train Loss: 0.8986 Acc: 0.5538 | Val Loss: 0.7846 Acc: 0.5887                                               \n",
      "Epoch 005 | Train Loss: 0.8367 Acc: 0.5776 | Val Loss: 0.9273 Acc: 0.5269                                               \n",
      "Epoch 006 | Train Loss: 0.8601 Acc: 0.5630 | Val Loss: 1.2766 Acc: 0.5030                                               \n",
      "Epoch 007 | Train Loss: 0.8537 Acc: 0.5653 | Val Loss: 0.9393 Acc: 0.5251                                               \n",
      "Epoch 008 | Train Loss: 0.8729 Acc: 0.5648 | Val Loss: 0.9116 Acc: 0.6287                                               \n",
      "Epoch 009 | Train Loss: 0.8780 Acc: 0.5644 | Val Loss: 0.8653 Acc: 0.5588                                               \n",
      "Epoch 010 | Train Loss: 0.8839 Acc: 0.5621 | Val Loss: 2.7981 Acc: 0.3875                                               \n",
      "Epoch 011 | Train Loss: 0.8768 Acc: 0.5699 | Val Loss: 1.0376 Acc: 0.4143                                               \n",
      "Epoch 012 | Train Loss: 0.8779 Acc: 0.5521 | Val Loss: 0.8189 Acc: 0.5794                                               \n",
      "Epoch 013 | Train Loss: 0.8696 Acc: 0.5683 | Val Loss: 0.8153 Acc: 0.5973                                               \n",
      "Epoch 014 | Train Loss: 0.8915 Acc: 0.5703 | Val Loss: 0.8782 Acc: 0.5221                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 256, 'cnn_dropout': 0.5245513016776852, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 32, 'cnn_kernels_2': 16, 'learning_rate': 0.0009979087998569288, 'lstm_dense': 256, 'lstm_hidden_size': 32, 'lstm_layers': 1, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 23.9148 Acc: 0.3942 | Val Loss: 3.5484 Acc: 0.4713                                              \n",
      "Epoch 002 | Train Loss: 6.0212 Acc: 0.4407 | Val Loss: 2.8494 Acc: 0.5272                                               \n",
      "Epoch 003 | Train Loss: 3.1169 Acc: 0.4643 | Val Loss: 1.9738 Acc: 0.4469                                               \n",
      "Epoch 004 | Train Loss: 1.6485 Acc: 0.5082 | Val Loss: 0.7873 Acc: 0.6424                                               \n",
      "Epoch 005 | Train Loss: 1.0237 Acc: 0.5753 | Val Loss: 0.8067 Acc: 0.6060                                               \n",
      "Epoch 006 | Train Loss: 0.8653 Acc: 0.6253 | Val Loss: 0.7666 Acc: 0.6815                                               \n",
      "Epoch 007 | Train Loss: 0.8151 Acc: 0.6406 | Val Loss: 0.6809 Acc: 0.6973                                               \n",
      "Epoch 008 | Train Loss: 0.7723 Acc: 0.6612 | Val Loss: 0.6540 Acc: 0.7119                                               \n",
      "Epoch 009 | Train Loss: 0.7237 Acc: 0.6727 | Val Loss: 0.5900 Acc: 0.7096                                               \n",
      "Epoch 010 | Train Loss: 0.7086 Acc: 0.6809 | Val Loss: 0.5542 Acc: 0.7555                                               \n",
      "Epoch 011 | Train Loss: 0.6367 Acc: 0.7096 | Val Loss: 0.8118 Acc: 0.6600                                               \n",
      "Epoch 012 | Train Loss: 0.6227 Acc: 0.7184 | Val Loss: 0.4859 Acc: 0.7970                                               \n",
      "Epoch 013 | Train Loss: 0.5994 Acc: 0.7296 | Val Loss: 0.4671 Acc: 0.8063                                               \n",
      "Epoch 014 | Train Loss: 0.5864 Acc: 0.7404 | Val Loss: 0.5739 Acc: 0.7490                                               \n",
      "Epoch 015 | Train Loss: 0.5675 Acc: 0.7505 | Val Loss: 0.4952 Acc: 0.7872                                               \n",
      "Epoch 016 | Train Loss: 0.5374 Acc: 0.7679 | Val Loss: 0.5428 Acc: 0.7690                                               \n",
      "Epoch 017 | Train Loss: 0.5215 Acc: 0.7746 | Val Loss: 0.5922 Acc: 0.7555                                               \n",
      "Epoch 018 | Train Loss: 0.5206 Acc: 0.7677 | Val Loss: 0.4830 Acc: 0.7997                                               \n",
      "Epoch 019 | Train Loss: 0.5023 Acc: 0.7811 | Val Loss: 0.4580 Acc: 0.7773                                               \n",
      "Epoch 020 | Train Loss: 0.4907 Acc: 0.7883 | Val Loss: 0.3827 Acc: 0.8155                                               \n",
      "Epoch 021 | Train Loss: 0.4712 Acc: 0.7932 | Val Loss: 0.5656 Acc: 0.7361                                               \n",
      "Epoch 022 | Train Loss: 0.4824 Acc: 0.7955 | Val Loss: 0.4402 Acc: 0.8093                                               \n",
      "Epoch 023 | Train Loss: 0.4574 Acc: 0.8046 | Val Loss: 0.4213 Acc: 0.8313                                               \n",
      "Epoch 024 | Train Loss: 0.4554 Acc: 0.8042 | Val Loss: 0.3766 Acc: 0.8570                                               \n",
      "Epoch 025 | Train Loss: 0.4427 Acc: 0.8074 | Val Loss: 0.5413 Acc: 0.7310                                               \n",
      "Epoch 026 | Train Loss: 0.4305 Acc: 0.8191 | Val Loss: 0.4071 Acc: 0.8373                                               \n",
      "Epoch 027 | Train Loss: 0.4290 Acc: 0.8174 | Val Loss: 0.4028 Acc: 0.7949                                               \n",
      "Epoch 028 | Train Loss: 0.4069 Acc: 0.8305 | Val Loss: 0.4994 Acc: 0.7182                                               \n",
      "Epoch 029 | Train Loss: 0.4139 Acc: 0.8254 | Val Loss: 0.4459 Acc: 0.8075                                               \n",
      "Epoch 030 | Train Loss: 0.4271 Acc: 0.8213 | Val Loss: 0.4178 Acc: 0.8018                                               \n",
      "Epoch 031 | Train Loss: 0.4152 Acc: 0.8271 | Val Loss: 0.3535 Acc: 0.8352                                               \n",
      "Epoch 032 | Train Loss: 0.4153 Acc: 0.8289 | Val Loss: 0.4008 Acc: 0.8534                                               \n",
      "Epoch 033 | Train Loss: 0.4022 Acc: 0.8327 | Val Loss: 0.5440 Acc: 0.7433                                               \n",
      "Epoch 034 | Train Loss: 0.3916 Acc: 0.8363 | Val Loss: 0.3981 Acc: 0.8337                                               \n",
      "Epoch 035 | Train Loss: 0.3895 Acc: 0.8427 | Val Loss: 0.3658 Acc: 0.8343                                               \n",
      "Epoch 036 | Train Loss: 0.4010 Acc: 0.8371 | Val Loss: 0.4088 Acc: 0.8209                                               \n",
      "Epoch 037 | Train Loss: 0.3814 Acc: 0.8436 | Val Loss: 0.3834 Acc: 0.8534                                               \n",
      "Epoch 038 | Train Loss: 0.3883 Acc: 0.8404 | Val Loss: 0.5317 Acc: 0.7687                                               \n",
      "Epoch 039 | Train Loss: 0.3630 Acc: 0.8520 | Val Loss: 0.3984 Acc: 0.8367                                               \n",
      "Epoch 040 | Train Loss: 0.3828 Acc: 0.8475 | Val Loss: 0.4457 Acc: 0.7797                                               \n",
      "Epoch 041 | Train Loss: 0.3744 Acc: 0.8465 | Val Loss: 0.4584 Acc: 0.8493                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 256, 'cnn_dropout': 0.5018093522416276, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 48, 'cnn_kernels_2': 96, 'learning_rate': 3.0925753194772075e-05, 'lstm_dense': 32, 'lstm_hidden_size': 96, 'lstm_layers': 1, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 61.5026 Acc: 0.2322 | Val Loss: 15.9378 Acc: 0.4421                                             \n",
      "Epoch 002 | Train Loss: 22.8400 Acc: 0.3361 | Val Loss: 5.9406 Acc: 0.4075                                              \n",
      "Epoch 003 | Train Loss: 14.6261 Acc: 0.3614 | Val Loss: 4.5406 Acc: 0.4704                                              \n",
      "Epoch 004 | Train Loss: 10.5591 Acc: 0.3930 | Val Loss: 3.1658 Acc: 0.5045                                              \n",
      "Epoch 005 | Train Loss: 8.0980 Acc: 0.4117 | Val Loss: 2.3682 Acc: 0.5421                                               \n",
      "Epoch 006 | Train Loss: 6.4858 Acc: 0.4187 | Val Loss: 1.7441 Acc: 0.5119                                               \n",
      "Epoch 007 | Train Loss: 5.3032 Acc: 0.4326 | Val Loss: 1.4112 Acc: 0.5851                                               \n",
      "Epoch 008 | Train Loss: 4.5191 Acc: 0.4415 | Val Loss: 1.3040 Acc: 0.5746                                               \n",
      "Epoch 009 | Train Loss: 3.9359 Acc: 0.4479 | Val Loss: 1.3991 Acc: 0.5839                                               \n",
      "Epoch 010 | Train Loss: 3.5881 Acc: 0.4552 | Val Loss: 1.2726 Acc: 0.6128                                               \n",
      "Epoch 011 | Train Loss: 3.2571 Acc: 0.4702 | Val Loss: 1.6492 Acc: 0.5325                                               \n",
      "Epoch 012 | Train Loss: 2.8663 Acc: 0.4824 | Val Loss: 1.1692 Acc: 0.5678                                               \n",
      "Epoch 013 | Train Loss: 2.6673 Acc: 0.4876 | Val Loss: 1.2765 Acc: 0.5690                                               \n",
      "Epoch 014 | Train Loss: 2.4354 Acc: 0.4929 | Val Loss: 1.1917 Acc: 0.5648                                               \n",
      "Epoch 015 | Train Loss: 2.3356 Acc: 0.4939 | Val Loss: 1.1628 Acc: 0.5642                                               \n",
      "Epoch 016 | Train Loss: 2.1531 Acc: 0.5000 | Val Loss: 1.0987 Acc: 0.5666                                               \n",
      "Epoch 017 | Train Loss: 2.0225 Acc: 0.5122 | Val Loss: 1.1032 Acc: 0.5490                                               \n",
      "Epoch 018 | Train Loss: 1.8930 Acc: 0.5086 | Val Loss: 0.9939 Acc: 0.6349                                               \n",
      "Epoch 019 | Train Loss: 1.7868 Acc: 0.5172 | Val Loss: 0.8791 Acc: 0.6588                                               \n",
      "Epoch 020 | Train Loss: 1.7099 Acc: 0.5262 | Val Loss: 1.0574 Acc: 0.5594                                               \n",
      "Epoch 021 | Train Loss: 1.6162 Acc: 0.5317 | Val Loss: 0.9252 Acc: 0.5707                                               \n",
      "Epoch 022 | Train Loss: 1.5887 Acc: 0.5326 | Val Loss: 0.9086 Acc: 0.6045                                               \n",
      "Epoch 023 | Train Loss: 1.5307 Acc: 0.5383 | Val Loss: 1.0603 Acc: 0.5627                                               \n",
      "Epoch 024 | Train Loss: 1.4412 Acc: 0.5477 | Val Loss: 0.8205 Acc: 0.6534                                               \n",
      "Epoch 025 | Train Loss: 1.3947 Acc: 0.5478 | Val Loss: 1.0347 Acc: 0.5719                                               \n",
      "Epoch 026 | Train Loss: 1.3415 Acc: 0.5560 | Val Loss: 0.7996 Acc: 0.6388                                               \n",
      "Epoch 027 | Train Loss: 1.3042 Acc: 0.5580 | Val Loss: 0.8688 Acc: 0.6304                                               \n",
      "Epoch 028 | Train Loss: 1.2903 Acc: 0.5538 | Val Loss: 0.7816 Acc: 0.6510                                               \n",
      "Epoch 029 | Train Loss: 1.2630 Acc: 0.5578 | Val Loss: 0.7578 Acc: 0.6603                                               \n",
      "Epoch 030 | Train Loss: 1.2064 Acc: 0.5633 | Val Loss: 0.7162 Acc: 0.6716                                               \n",
      "Epoch 031 | Train Loss: 1.1854 Acc: 0.5677 | Val Loss: 0.7827 Acc: 0.6534                                               \n",
      "Epoch 032 | Train Loss: 1.1331 Acc: 0.5787 | Val Loss: 0.8024 Acc: 0.6403                                               \n",
      "Epoch 033 | Train Loss: 1.1154 Acc: 0.5924 | Val Loss: 0.7511 Acc: 0.6337                                               \n",
      "Epoch 034 | Train Loss: 1.0864 Acc: 0.5955 | Val Loss: 0.7033 Acc: 0.6654                                               \n",
      "Epoch 035 | Train Loss: 1.0429 Acc: 0.6061 | Val Loss: 0.8834 Acc: 0.6197                                               \n",
      "Epoch 036 | Train Loss: 1.0187 Acc: 0.6160 | Val Loss: 0.7052 Acc: 0.6851                                               \n",
      "Epoch 037 | Train Loss: 0.9612 Acc: 0.6375 | Val Loss: 0.7200 Acc: 0.6845                                               \n",
      "Epoch 038 | Train Loss: 0.9364 Acc: 0.6354 | Val Loss: 0.7142 Acc: 0.6884                                               \n",
      "Epoch 039 | Train Loss: 0.9418 Acc: 0.6427 | Val Loss: 0.6380 Acc: 0.7349                                               \n",
      "Epoch 040 | Train Loss: 0.9086 Acc: 0.6513 | Val Loss: 0.6961 Acc: 0.6881                                               \n",
      "Epoch 041 | Train Loss: 0.8836 Acc: 0.6580 | Val Loss: 0.6632 Acc: 0.6779                                               \n",
      "Epoch 042 | Train Loss: 0.8560 Acc: 0.6777 | Val Loss: 0.6754 Acc: 0.6910                                               \n",
      "Epoch 043 | Train Loss: 0.8335 Acc: 0.6843 | Val Loss: 0.6357 Acc: 0.7394                                               \n",
      "Epoch 044 | Train Loss: 0.8002 Acc: 0.6906 | Val Loss: 0.6290 Acc: 0.7212                                               \n",
      "Epoch 045 | Train Loss: 0.7741 Acc: 0.7073 | Val Loss: 0.5590 Acc: 0.7809                                               \n",
      "Epoch 046 | Train Loss: 0.7385 Acc: 0.7218 | Val Loss: 0.6129 Acc: 0.7531                                               \n",
      "Epoch 047 | Train Loss: 0.7145 Acc: 0.7286 | Val Loss: 0.5963 Acc: 0.7731                                               \n",
      "Epoch 048 | Train Loss: 0.6816 Acc: 0.7465 | Val Loss: 0.5544 Acc: 0.7901                                               \n",
      "Epoch 049 | Train Loss: 0.6264 Acc: 0.7645 | Val Loss: 0.5743 Acc: 0.7719                                               \n",
      "Epoch 050 | Train Loss: 0.6150 Acc: 0.7734 | Val Loss: 0.4291 Acc: 0.8463                                               \n",
      "Epoch 051 | Train Loss: 0.5580 Acc: 0.7935 | Val Loss: 0.4907 Acc: 0.7955                                               \n",
      "Epoch 052 | Train Loss: 0.5342 Acc: 0.8018 | Val Loss: 0.4308 Acc: 0.8418                                               \n",
      "Epoch 053 | Train Loss: 0.5164 Acc: 0.8156 | Val Loss: 0.4410 Acc: 0.8322                                               \n",
      "Epoch 054 | Train Loss: 0.4706 Acc: 0.8309 | Val Loss: 0.3781 Acc: 0.8597                                               \n",
      "Epoch 055 | Train Loss: 0.4585 Acc: 0.8336 | Val Loss: 0.3570 Acc: 0.8701                                               \n",
      "Epoch 056 | Train Loss: 0.4243 Acc: 0.8507 | Val Loss: 0.3345 Acc: 0.8734                                               \n",
      "Epoch 057 | Train Loss: 0.4089 Acc: 0.8553 | Val Loss: 0.3634 Acc: 0.8651                                               \n",
      "Epoch 058 | Train Loss: 0.3871 Acc: 0.8641 | Val Loss: 0.3759 Acc: 0.8516                                               \n",
      "Epoch 059 | Train Loss: 0.3722 Acc: 0.8657 | Val Loss: 0.3550 Acc: 0.8633                                               \n",
      "Epoch 060 | Train Loss: 0.3578 Acc: 0.8784 | Val Loss: 0.3089 Acc: 0.8931                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 256, 'cnn_dropout': 0.3749561133071856, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 48, 'cnn_kernels_2': 16, 'learning_rate': 0.00011213685509954423, 'lstm_dense': 64, 'lstm_hidden_size': 96, 'lstm_layers': 3, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 20.0207 Acc: 0.3779 | Val Loss: 3.2482 Acc: 0.4737                                              \n",
      "Epoch 002 | Train Loss: 6.6695 Acc: 0.4280 | Val Loss: 2.3247 Acc: 0.5379                                               \n",
      "Epoch 003 | Train Loss: 4.5387 Acc: 0.4496 | Val Loss: 2.4071 Acc: 0.5451                                               \n",
      "Epoch 004 | Train Loss: 3.4807 Acc: 0.4834 | Val Loss: 1.5019 Acc: 0.6430                                               \n",
      "Epoch 005 | Train Loss: 2.7812 Acc: 0.4930 | Val Loss: 1.6681 Acc: 0.5696                                               \n",
      "Epoch 006 | Train Loss: 2.4138 Acc: 0.5172 | Val Loss: 1.1215 Acc: 0.6594                                               \n",
      "Epoch 007 | Train Loss: 1.9969 Acc: 0.5399 | Val Loss: 1.0909 Acc: 0.6839                                               \n",
      "Epoch 008 | Train Loss: 1.7400 Acc: 0.5721 | Val Loss: 1.6361 Acc: 0.5916                                               \n",
      "Epoch 009 | Train Loss: 1.4878 Acc: 0.6035 | Val Loss: 1.7048 Acc: 0.6293                                               \n",
      "Epoch 010 | Train Loss: 1.3044 Acc: 0.6236 | Val Loss: 1.1654 Acc: 0.6269                                               \n",
      "Epoch 011 | Train Loss: 1.1489 Acc: 0.6586 | Val Loss: 1.0463 Acc: 0.6830                                               \n",
      "Epoch 012 | Train Loss: 0.9953 Acc: 0.6936 | Val Loss: 1.9527 Acc: 0.5042                                               \n",
      "Epoch 013 | Train Loss: 0.8788 Acc: 0.7153 | Val Loss: 0.4995 Acc: 0.7982                                               \n",
      "Epoch 014 | Train Loss: 0.8041 Acc: 0.7377 | Val Loss: 1.3034 Acc: 0.6284                                               \n",
      "Epoch 015 | Train Loss: 0.7013 Acc: 0.7618 | Val Loss: 0.8362 Acc: 0.7257                                               \n",
      "Epoch 016 | Train Loss: 0.6292 Acc: 0.7867 | Val Loss: 0.4147 Acc: 0.8475                                               \n",
      "Epoch 017 | Train Loss: 0.5588 Acc: 0.8039 | Val Loss: 0.3779 Acc: 0.8636                                               \n",
      "Epoch 018 | Train Loss: 0.5169 Acc: 0.8201 | Val Loss: 0.5576 Acc: 0.8063                                               \n",
      "Epoch 019 | Train Loss: 0.4661 Acc: 0.8378 | Val Loss: 0.3103 Acc: 0.8901                                               \n",
      "Epoch 020 | Train Loss: 0.4154 Acc: 0.8531 | Val Loss: 0.9846 Acc: 0.7257                                               \n",
      "Epoch 021 | Train Loss: 0.3668 Acc: 0.8680 | Val Loss: 1.5784 Acc: 0.5940                                               \n",
      "Epoch 022 | Train Loss: 0.3481 Acc: 0.8760 | Val Loss: 0.4486 Acc: 0.8373                                               \n",
      "Epoch 023 | Train Loss: 0.3243 Acc: 0.8879 | Val Loss: 0.3813 Acc: 0.8624                                               \n",
      "Epoch 024 | Train Loss: 0.2912 Acc: 0.8984 | Val Loss: 0.3906 Acc: 0.8549                                               \n",
      "Epoch 025 | Train Loss: 0.2819 Acc: 0.9020 | Val Loss: 0.1978 Acc: 0.9322                                               \n",
      "Epoch 026 | Train Loss: 0.2531 Acc: 0.9142 | Val Loss: 0.3381 Acc: 0.8931                                               \n",
      "Epoch 027 | Train Loss: 0.2451 Acc: 0.9139 | Val Loss: 0.2587 Acc: 0.9066                                               \n",
      "Epoch 028 | Train Loss: 0.2244 Acc: 0.9219 | Val Loss: 0.3286 Acc: 0.8913                                               \n",
      "Epoch 029 | Train Loss: 0.2033 Acc: 0.9270 | Val Loss: 0.1695 Acc: 0.9394                                               \n",
      "Epoch 030 | Train Loss: 0.1947 Acc: 0.9336 | Val Loss: 0.1246 Acc: 0.9564                                               \n",
      "Epoch 031 | Train Loss: 0.1711 Acc: 0.9417 | Val Loss: 0.1523 Acc: 0.9499                                               \n",
      "Epoch 032 | Train Loss: 0.1628 Acc: 0.9454 | Val Loss: 0.2872 Acc: 0.9084                                               \n",
      "Epoch 033 | Train Loss: 0.1611 Acc: 0.9448 | Val Loss: 0.4330 Acc: 0.8824                                               \n",
      "Epoch 034 | Train Loss: 0.1397 Acc: 0.9496 | Val Loss: 0.1482 Acc: 0.9475                                               \n",
      "Epoch 035 | Train Loss: 0.1421 Acc: 0.9515 | Val Loss: 0.1149 Acc: 0.9588                                               \n",
      "Epoch 036 | Train Loss: 0.1342 Acc: 0.9549 | Val Loss: 0.1158 Acc: 0.9588                                               \n",
      "Epoch 037 | Train Loss: 0.1222 Acc: 0.9591 | Val Loss: 0.2595 Acc: 0.9110                                               \n",
      "Epoch 038 | Train Loss: 0.1129 Acc: 0.9615 | Val Loss: 0.7394 Acc: 0.8158                                               \n",
      "Epoch 039 | Train Loss: 0.1125 Acc: 0.9632 | Val Loss: 0.1019 Acc: 0.9660                                               \n",
      "Epoch 040 | Train Loss: 0.1062 Acc: 0.9656 | Val Loss: 0.1230 Acc: 0.9597                                               \n",
      "Epoch 041 | Train Loss: 0.0993 Acc: 0.9668 | Val Loss: 0.3234 Acc: 0.9051                                               \n",
      "Epoch 042 | Train Loss: 0.0831 Acc: 0.9715 | Val Loss: 0.1230 Acc: 0.9639                                               \n",
      "Epoch 043 | Train Loss: 0.0789 Acc: 0.9746 | Val Loss: 0.2421 Acc: 0.9266                                               \n",
      "Epoch 044 | Train Loss: 0.0766 Acc: 0.9737 | Val Loss: 0.1096 Acc: 0.9645                                               \n",
      "Epoch 045 | Train Loss: 0.0767 Acc: 0.9732 | Val Loss: 1.2764 Acc: 0.7066                                               \n",
      "Epoch 046 | Train Loss: 0.0802 Acc: 0.9734 | Val Loss: 0.0852 Acc: 0.9707                                               \n",
      "Epoch 047 | Train Loss: 0.0708 Acc: 0.9767 | Val Loss: 0.0904 Acc: 0.9699                                               \n",
      "Epoch 048 | Train Loss: 0.0709 Acc: 0.9775 | Val Loss: 0.0778 Acc: 0.9740                                               \n",
      "Epoch 049 | Train Loss: 0.0642 Acc: 0.9796 | Val Loss: 0.1897 Acc: 0.9496                                               \n",
      "Epoch 050 | Train Loss: 0.0616 Acc: 0.9794 | Val Loss: 0.6094 Acc: 0.8597                                               \n",
      "Epoch 051 | Train Loss: 0.0577 Acc: 0.9804 | Val Loss: 0.0859 Acc: 0.9731                                               \n",
      "Epoch 052 | Train Loss: 0.0566 Acc: 0.9811 | Val Loss: 0.0576 Acc: 0.9809                                               \n",
      "Epoch 053 | Train Loss: 0.0533 Acc: 0.9817 | Val Loss: 0.0773 Acc: 0.9767                                               \n",
      "Epoch 054 | Train Loss: 0.0546 Acc: 0.9827 | Val Loss: 0.0833 Acc: 0.9758                                               \n",
      "Epoch 055 | Train Loss: 0.0484 Acc: 0.9825 | Val Loss: 0.0781 Acc: 0.9773                                               \n",
      "Epoch 056 | Train Loss: 0.0487 Acc: 0.9846 | Val Loss: 0.0625 Acc: 0.9809                                               \n",
      "Epoch 057 | Train Loss: 0.0558 Acc: 0.9814 | Val Loss: 0.1123 Acc: 0.9675                                               \n",
      "Epoch 058 | Train Loss: 0.0437 Acc: 0.9847 | Val Loss: 0.1449 Acc: 0.9570                                               \n",
      "Epoch 059 | Train Loss: 0.0422 Acc: 0.9863 | Val Loss: 0.0742 Acc: 0.9788                                               \n",
      "Epoch 060 | Train Loss: 0.0411 Acc: 0.9875 | Val Loss: 0.0848 Acc: 0.9731                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 256, 'cnn_dropout': 0.12231486900921316, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 96, 'learning_rate': 5.771497209701074e-05, 'lstm_dense': 64, 'lstm_hidden_size': 96, 'lstm_layers': 1, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 64.2483 Acc: 0.2685 | Val Loss: 20.9264 Acc: 0.4140                                             \n",
      "Epoch 002 | Train Loss: 24.3651 Acc: 0.3405 | Val Loss: 8.3160 Acc: 0.4633                                              \n",
      "Epoch 003 | Train Loss: 13.5044 Acc: 0.3838 | Val Loss: 5.2185 Acc: 0.4272                                              \n",
      "Epoch 004 | Train Loss: 9.7161 Acc: 0.3926 | Val Loss: 3.0340 Acc: 0.4806                                               \n",
      "Epoch 005 | Train Loss: 7.3831 Acc: 0.4070 | Val Loss: 2.1918 Acc: 0.5137                                               \n",
      "Epoch 006 | Train Loss: 6.2100 Acc: 0.4137 | Val Loss: 1.9470 Acc: 0.5397                                               \n",
      "Epoch 007 | Train Loss: 5.2062 Acc: 0.4203 | Val Loss: 1.3907 Acc: 0.5827                                               \n",
      "Epoch 008 | Train Loss: 4.5076 Acc: 0.4252 | Val Loss: 1.4652 Acc: 0.5660                                               \n",
      "Epoch 009 | Train Loss: 3.9690 Acc: 0.4321 | Val Loss: 1.2840 Acc: 0.5364                                               \n",
      "Epoch 010 | Train Loss: 3.5324 Acc: 0.4372 | Val Loss: 1.2924 Acc: 0.5716                                               \n",
      "Epoch 011 | Train Loss: 3.2015 Acc: 0.4424 | Val Loss: 1.5040 Acc: 0.4967                                               \n",
      "Epoch 012 | Train Loss: 2.9027 Acc: 0.4519 | Val Loss: 1.2361 Acc: 0.5773                                               \n",
      "Epoch 013 | Train Loss: 2.6578 Acc: 0.4611 | Val Loss: 1.2703 Acc: 0.5227                                               \n",
      "Epoch 014 | Train Loss: 2.4706 Acc: 0.4594 | Val Loss: 1.3029 Acc: 0.5251                                               \n",
      "Epoch 015 | Train Loss: 2.2912 Acc: 0.4606 | Val Loss: 1.2551 Acc: 0.5167                                               \n",
      "Epoch 016 | Train Loss: 2.1395 Acc: 0.4672 | Val Loss: 1.2344 Acc: 0.5125                                               \n",
      "Epoch 017 | Train Loss: 2.0516 Acc: 0.4695 | Val Loss: 1.1815 Acc: 0.5388                                               \n",
      "Epoch 018 | Train Loss: 1.8899 Acc: 0.4738 | Val Loss: 0.9284 Acc: 0.5573                                               \n",
      "Epoch 019 | Train Loss: 1.7405 Acc: 0.4853 | Val Loss: 0.9559 Acc: 0.5699                                               \n",
      "Epoch 020 | Train Loss: 1.6582 Acc: 0.5147 | Val Loss: 0.9370 Acc: 0.6054                                               \n",
      "Epoch 021 | Train Loss: 1.4704 Acc: 0.5380 | Val Loss: 0.9043 Acc: 0.6343                                               \n",
      "Epoch 022 | Train Loss: 1.3628 Acc: 0.5738 | Val Loss: 0.7245 Acc: 0.6773                                               \n",
      "Epoch 023 | Train Loss: 1.2398 Acc: 0.6009 | Val Loss: 0.7128 Acc: 0.6982                                               \n",
      "Epoch 024 | Train Loss: 1.1553 Acc: 0.6315 | Val Loss: 0.6612 Acc: 0.7146                                               \n",
      "Epoch 025 | Train Loss: 1.0332 Acc: 0.6598 | Val Loss: 0.9167 Acc: 0.6466                                               \n",
      "Epoch 026 | Train Loss: 0.9051 Acc: 0.6987 | Val Loss: 0.6962 Acc: 0.7370                                               \n",
      "Epoch 027 | Train Loss: 0.8101 Acc: 0.7371 | Val Loss: 0.4962 Acc: 0.8233                                               \n",
      "Epoch 028 | Train Loss: 0.7425 Acc: 0.7670 | Val Loss: 0.6008 Acc: 0.7525                                               \n",
      "Epoch 029 | Train Loss: 0.6209 Acc: 0.8022 | Val Loss: 0.4215 Acc: 0.8439                                               \n",
      "Epoch 030 | Train Loss: 0.5364 Acc: 0.8347 | Val Loss: 0.2872 Acc: 0.8970                                               \n",
      "Epoch 031 | Train Loss: 0.4507 Acc: 0.8606 | Val Loss: 0.3206 Acc: 0.8928                                               \n",
      "Epoch 032 | Train Loss: 0.4067 Acc: 0.8732 | Val Loss: 0.2803 Acc: 0.9033                                               \n",
      "Epoch 033 | Train Loss: 0.3959 Acc: 0.8807 | Val Loss: 0.3404 Acc: 0.8806                                               \n",
      "Epoch 034 | Train Loss: 0.3292 Acc: 0.8969 | Val Loss: 0.2390 Acc: 0.9131                                               \n",
      "Epoch 035 | Train Loss: 0.3070 Acc: 0.9051 | Val Loss: 0.2526 Acc: 0.9110                                               \n",
      "Epoch 036 | Train Loss: 0.2842 Acc: 0.9135 | Val Loss: 0.2765 Acc: 0.8988                                               \n",
      "Epoch 037 | Train Loss: 0.2434 Acc: 0.9263 | Val Loss: 0.2794 Acc: 0.9009                                               \n",
      "Epoch 038 | Train Loss: 0.2499 Acc: 0.9204 | Val Loss: 0.1927 Acc: 0.9361                                               \n",
      "Epoch 039 | Train Loss: 0.2166 Acc: 0.9326 | Val Loss: 0.2162 Acc: 0.9245                                               \n",
      "Epoch 040 | Train Loss: 0.1899 Acc: 0.9437 | Val Loss: 0.2274 Acc: 0.9233                                               \n",
      "Epoch 041 | Train Loss: 0.1812 Acc: 0.9419 | Val Loss: 0.2030 Acc: 0.9394                                               \n",
      "Epoch 042 | Train Loss: 0.1703 Acc: 0.9456 | Val Loss: 0.1742 Acc: 0.9394                                               \n",
      "Epoch 043 | Train Loss: 0.1559 Acc: 0.9507 | Val Loss: 0.1974 Acc: 0.9358                                               \n",
      "Epoch 044 | Train Loss: 0.1495 Acc: 0.9557 | Val Loss: 0.1579 Acc: 0.9472                                               \n",
      "Epoch 045 | Train Loss: 0.1292 Acc: 0.9613 | Val Loss: 0.1608 Acc: 0.9516                                               \n",
      "Epoch 046 | Train Loss: 0.1316 Acc: 0.9619 | Val Loss: 0.2209 Acc: 0.9328                                               \n",
      "Epoch 047 | Train Loss: 0.1134 Acc: 0.9651 | Val Loss: 0.1670 Acc: 0.9463                                               \n",
      "Epoch 048 | Train Loss: 0.1196 Acc: 0.9620 | Val Loss: 0.1882 Acc: 0.9439                                               \n",
      "Epoch 049 | Train Loss: 0.1064 Acc: 0.9679 | Val Loss: 0.1927 Acc: 0.9373                                               \n",
      "Epoch 050 | Train Loss: 0.1046 Acc: 0.9687 | Val Loss: 0.1599 Acc: 0.9504                                               \n",
      "Epoch 051 | Train Loss: 0.0910 Acc: 0.9705 | Val Loss: 0.1873 Acc: 0.9454                                               \n",
      "Epoch 052 | Train Loss: 0.0931 Acc: 0.9701 | Val Loss: 0.1455 Acc: 0.9564                                               \n",
      "Epoch 053 | Train Loss: 0.0831 Acc: 0.9731 | Val Loss: 0.1170 Acc: 0.9618                                               \n",
      "Epoch 054 | Train Loss: 0.0745 Acc: 0.9762 | Val Loss: 0.1802 Acc: 0.9448                                               \n",
      "Epoch 055 | Train Loss: 0.0685 Acc: 0.9782 | Val Loss: 0.1503 Acc: 0.9561                                               \n",
      "Epoch 056 | Train Loss: 0.0765 Acc: 0.9757 | Val Loss: 0.1244 Acc: 0.9621                                               \n",
      "Epoch 057 | Train Loss: 0.0673 Acc: 0.9780 | Val Loss: 0.1485 Acc: 0.9546                                               \n",
      "Epoch 058 | Train Loss: 0.0755 Acc: 0.9759 | Val Loss: 0.1350 Acc: 0.9591                                               \n",
      "Epoch 059 | Train Loss: 0.0557 Acc: 0.9815 | Val Loss: 0.1719 Acc: 0.9457                                               \n",
      "Epoch 060 | Train Loss: 0.0633 Acc: 0.9813 | Val Loss: 0.1356 Acc: 0.9579                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 36, 'cnn_dense': 128, 'cnn_dropout': 0.3794107777799805, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 32, 'cnn_kernels_2': 16, 'learning_rate': 0.0001759299728331942, 'lstm_dense': 64, 'lstm_hidden_size': 96, 'lstm_layers': 5, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 23.6509 Acc: 0.3744 | Val Loss: 5.0967 Acc: 0.4949                                              \n",
      "Epoch 002 | Train Loss: 6.8145 Acc: 0.4345 | Val Loss: 2.7085 Acc: 0.5146                                               \n",
      "Epoch 003 | Train Loss: 4.0366 Acc: 0.4586 | Val Loss: 2.1722 Acc: 0.5570                                               \n",
      "Epoch 004 | Train Loss: 2.7896 Acc: 0.4965 | Val Loss: 0.9962 Acc: 0.5940                                               \n",
      "Epoch 005 | Train Loss: 2.0174 Acc: 0.5475 | Val Loss: 1.2790 Acc: 0.6358                                               \n",
      "Epoch 006 | Train Loss: 1.6044 Acc: 0.5824 | Val Loss: 0.7832 Acc: 0.6510                                               \n",
      "Epoch 007 | Train Loss: 1.3464 Acc: 0.6280 | Val Loss: 1.0358 Acc: 0.6478                                               \n",
      "Epoch 008 | Train Loss: 1.1515 Acc: 0.6646 | Val Loss: 1.0144 Acc: 0.6991                                               \n",
      "Epoch 009 | Train Loss: 0.9363 Acc: 0.7017 | Val Loss: 0.7118 Acc: 0.6979                                               \n",
      "Epoch 010 | Train Loss: 0.8576 Acc: 0.7208 | Val Loss: 0.4980 Acc: 0.8152                                               \n",
      "Epoch 011 | Train Loss: 0.7402 Acc: 0.7526 | Val Loss: 0.4971 Acc: 0.7994                                               \n",
      "Epoch 012 | Train Loss: 0.6668 Acc: 0.7745 | Val Loss: 0.4375 Acc: 0.8385                                               \n",
      "Epoch 013 | Train Loss: 0.6596 Acc: 0.7853 | Val Loss: 0.7246 Acc: 0.7928                                               \n",
      "Epoch 014 | Train Loss: 0.5933 Acc: 0.8101 | Val Loss: 0.3927 Acc: 0.8657                                               \n",
      "Epoch 015 | Train Loss: 0.5196 Acc: 0.8251 | Val Loss: 0.4893 Acc: 0.8346                                               \n",
      "Epoch 016 | Train Loss: 0.4523 Acc: 0.8473 | Val Loss: 0.2944 Acc: 0.9030                                               \n",
      "Epoch 017 | Train Loss: 0.3890 Acc: 0.8707 | Val Loss: 0.3966 Acc: 0.8704                                               \n",
      "Epoch 018 | Train Loss: 0.3674 Acc: 0.8771 | Val Loss: 0.2842 Acc: 0.9215                                               \n",
      "Epoch 019 | Train Loss: 0.3090 Acc: 0.8998 | Val Loss: 0.3197 Acc: 0.8860                                               \n",
      "Epoch 020 | Train Loss: 0.3028 Acc: 0.9029 | Val Loss: 0.2571 Acc: 0.9191                                               \n",
      "Epoch 021 | Train Loss: 0.2503 Acc: 0.9151 | Val Loss: 0.2556 Acc: 0.9137                                               \n",
      "Epoch 022 | Train Loss: 0.2369 Acc: 0.9230 | Val Loss: 0.2307 Acc: 0.9313                                               \n",
      "Epoch 023 | Train Loss: 0.2264 Acc: 0.9257 | Val Loss: 0.3268 Acc: 0.8949                                               \n",
      "Epoch 024 | Train Loss: 0.2066 Acc: 0.9327 | Val Loss: 0.2363 Acc: 0.9361                                               \n",
      "Epoch 025 | Train Loss: 0.2091 Acc: 0.9352 | Val Loss: 0.2311 Acc: 0.9301                                               \n",
      "Epoch 026 | Train Loss: 0.2241 Acc: 0.9304 | Val Loss: 0.2585 Acc: 0.9119                                               \n",
      "Epoch 027 | Train Loss: 0.1743 Acc: 0.9455 | Val Loss: 0.1668 Acc: 0.9424                                               \n",
      "Epoch 028 | Train Loss: 0.1465 Acc: 0.9531 | Val Loss: 0.1852 Acc: 0.9319                                               \n",
      "Epoch 029 | Train Loss: 0.1494 Acc: 0.9524 | Val Loss: 0.1424 Acc: 0.9499                                               \n",
      "Epoch 030 | Train Loss: 0.1300 Acc: 0.9595 | Val Loss: 0.2757 Acc: 0.9182                                               \n",
      "Epoch 031 | Train Loss: 0.1109 Acc: 0.9637 | Val Loss: 0.0944 Acc: 0.9693                                               \n",
      "Epoch 032 | Train Loss: 0.1201 Acc: 0.9628 | Val Loss: 0.1024 Acc: 0.9722                                               \n",
      "Epoch 033 | Train Loss: 0.0957 Acc: 0.9701 | Val Loss: 0.1106 Acc: 0.9660                                               \n",
      "Epoch 034 | Train Loss: 0.0989 Acc: 0.9701 | Val Loss: 0.1143 Acc: 0.9621                                               \n",
      "Epoch 035 | Train Loss: 0.1044 Acc: 0.9671 | Val Loss: 0.1011 Acc: 0.9752                                               \n",
      "Epoch 036 | Train Loss: 0.0981 Acc: 0.9690 | Val Loss: 0.1172 Acc: 0.9660                                               \n",
      "Epoch 037 | Train Loss: 0.0866 Acc: 0.9732 | Val Loss: 0.1177 Acc: 0.9666                                               \n",
      "Epoch 038 | Train Loss: 0.0899 Acc: 0.9705 | Val Loss: 0.0944 Acc: 0.9710                                               \n",
      "Epoch 039 | Train Loss: 0.0902 Acc: 0.9713 | Val Loss: 0.1992 Acc: 0.9448                                               \n",
      "Epoch 040 | Train Loss: 0.0738 Acc: 0.9790 | Val Loss: 0.0984 Acc: 0.9722                                               \n",
      "Epoch 041 | Train Loss: 0.0822 Acc: 0.9737 | Val Loss: 0.1389 Acc: 0.9588                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 64, 'cnn_dropout': 0.659283866422043, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 32, 'cnn_kernels_2': 64, 'learning_rate': 0.0012844629134290291, 'lstm_dense': 64, 'lstm_hidden_size': 96, 'lstm_layers': 2, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 13.1396 Acc: 0.4331 | Val Loss: 1.2610 Acc: 0.4421                                              \n",
      "Epoch 002 | Train Loss: 1.2409 Acc: 0.4429 | Val Loss: 1.2163 Acc: 0.4421                                               \n",
      "Epoch 003 | Train Loss: 1.2017 Acc: 0.4513 | Val Loss: 1.1809 Acc: 0.4794                                               \n",
      "Epoch 004 | Train Loss: 1.1645 Acc: 0.4866 | Val Loss: 1.1404 Acc: 0.5000                                               \n",
      "Epoch 005 | Train Loss: 1.1474 Acc: 0.4935 | Val Loss: 1.1386 Acc: 0.4878                                               \n",
      "Epoch 006 | Train Loss: 1.1294 Acc: 0.5018 | Val Loss: 1.1195 Acc: 0.4869                                               \n",
      "Epoch 007 | Train Loss: 1.1060 Acc: 0.5212 | Val Loss: 1.0990 Acc: 0.5173                                               \n",
      "Epoch 008 | Train Loss: 1.0917 Acc: 0.5265 | Val Loss: 1.0864 Acc: 0.5033                                               \n",
      "Epoch 009 | Train Loss: 1.0763 Acc: 0.5310 | Val Loss: 1.0613 Acc: 0.5337                                               \n",
      "Epoch 010 | Train Loss: 1.0600 Acc: 0.5397 | Val Loss: 1.0531 Acc: 0.5251                                               \n",
      "Epoch 011 | Train Loss: 1.0516 Acc: 0.5437 | Val Loss: 1.0310 Acc: 0.5284                                               \n",
      "Epoch 012 | Train Loss: 1.0226 Acc: 0.5605 | Val Loss: 1.0004 Acc: 0.5633                                               \n",
      "Epoch 013 | Train Loss: 1.0147 Acc: 0.5652 | Val Loss: 0.9748 Acc: 0.5833                                               \n",
      "Epoch 014 | Train Loss: 1.0098 Acc: 0.5680 | Val Loss: 1.0411 Acc: 0.5352                                               \n",
      "Epoch 015 | Train Loss: 0.9692 Acc: 0.5864 | Val Loss: 0.9199 Acc: 0.6036                                               \n",
      "Epoch 016 | Train Loss: 0.9218 Acc: 0.6062 | Val Loss: 0.9086 Acc: 0.6176                                               \n",
      "Epoch 017 | Train Loss: 0.8958 Acc: 0.6158 | Val Loss: 0.8685 Acc: 0.6424                                               \n",
      "Epoch 018 | Train Loss: 0.8512 Acc: 0.6514 | Val Loss: 0.7927 Acc: 0.6893                                               \n",
      "Epoch 019 | Train Loss: 0.8140 Acc: 0.6745 | Val Loss: 0.8399 Acc: 0.6666                                               \n",
      "Epoch 020 | Train Loss: 0.7693 Acc: 0.6999 | Val Loss: 0.7541 Acc: 0.7185                                               \n",
      "Epoch 021 | Train Loss: 0.7625 Acc: 0.7042 | Val Loss: 0.6883 Acc: 0.7460                                               \n",
      "Epoch 022 | Train Loss: 0.7035 Acc: 0.7299 | Val Loss: 0.7994 Acc: 0.7349                                               \n",
      "Epoch 023 | Train Loss: 0.6735 Acc: 0.7432 | Val Loss: 0.6463 Acc: 0.7654                                               \n",
      "Epoch 024 | Train Loss: 0.6501 Acc: 0.7564 | Val Loss: 0.5916 Acc: 0.7887                                               \n",
      "Epoch 025 | Train Loss: 0.5935 Acc: 0.7795 | Val Loss: 0.6311 Acc: 0.7893                                               \n",
      "Epoch 026 | Train Loss: 0.5687 Acc: 0.7906 | Val Loss: 0.5453 Acc: 0.8087                                               \n",
      "Epoch 027 | Train Loss: 0.5360 Acc: 0.7989 | Val Loss: 0.5174 Acc: 0.8170                                               \n",
      "Epoch 028 | Train Loss: 0.4937 Acc: 0.8112 | Val Loss: 0.4927 Acc: 0.8266                                               \n",
      "Epoch 029 | Train Loss: 0.4788 Acc: 0.8210 | Val Loss: 0.4232 Acc: 0.8418                                               \n",
      "Epoch 030 | Train Loss: 0.4544 Acc: 0.8310 | Val Loss: 0.4700 Acc: 0.8361                                               \n",
      "Epoch 031 | Train Loss: 0.4205 Acc: 0.8450 | Val Loss: 0.3929 Acc: 0.8567                                               \n",
      "Epoch 032 | Train Loss: 0.4067 Acc: 0.8512 | Val Loss: 0.4006 Acc: 0.8516                                               \n",
      "Epoch 033 | Train Loss: 0.3958 Acc: 0.8539 | Val Loss: 0.4076 Acc: 0.8636                                               \n",
      "Epoch 034 | Train Loss: 0.3812 Acc: 0.8583 | Val Loss: 0.5695 Acc: 0.8236                                               \n",
      "Epoch 035 | Train Loss: 0.3709 Acc: 0.8644 | Val Loss: 0.3339 Acc: 0.8815                                               \n",
      "Epoch 036 | Train Loss: 0.3510 Acc: 0.8708 | Val Loss: 0.3847 Acc: 0.8642                                               \n",
      "Epoch 037 | Train Loss: 0.3400 Acc: 0.8786 | Val Loss: 0.3653 Acc: 0.8767                                               \n",
      "Epoch 038 | Train Loss: 0.3251 Acc: 0.8827 | Val Loss: 0.4087 Acc: 0.8618                                               \n",
      "Epoch 039 | Train Loss: 0.3098 Acc: 0.8878 | Val Loss: 0.2787 Acc: 0.8979                                               \n",
      "Epoch 040 | Train Loss: 0.3108 Acc: 0.8914 | Val Loss: 0.3362 Acc: 0.8934                                               \n",
      "Epoch 041 | Train Loss: 0.2960 Acc: 0.8966 | Val Loss: 0.2935 Acc: 0.9075                                               \n",
      "Epoch 042 | Train Loss: 0.2976 Acc: 0.9007 | Val Loss: 0.3481 Acc: 0.8881                                               \n",
      "Epoch 043 | Train Loss: 0.2730 Acc: 0.9064 | Val Loss: 0.2979 Acc: 0.9110                                               \n",
      "Epoch 044 | Train Loss: 0.2708 Acc: 0.9097 | Val Loss: 0.3638 Acc: 0.8913                                               \n",
      "Epoch 045 | Train Loss: 0.2556 Acc: 0.9178 | Val Loss: 0.3655 Acc: 0.8973                                               \n",
      "Epoch 046 | Train Loss: 0.2545 Acc: 0.9164 | Val Loss: 0.3664 Acc: 0.9003                                               \n",
      "Epoch 047 | Train Loss: 0.2527 Acc: 0.9127 | Val Loss: 0.3558 Acc: 0.8988                                               \n",
      "Epoch 048 | Train Loss: 0.2446 Acc: 0.9182 | Val Loss: 0.2782 Acc: 0.9179                                               \n",
      "Epoch 049 | Train Loss: 0.2492 Acc: 0.9186 | Val Loss: 0.2853 Acc: 0.9119                                               \n",
      "Epoch 050 | Train Loss: 0.2441 Acc: 0.9193 | Val Loss: 0.2613 Acc: 0.9185                                               \n",
      "Epoch 051 | Train Loss: 0.2150 Acc: 0.9297 | Val Loss: 0.2302 Acc: 0.9263                                               \n",
      "Epoch 052 | Train Loss: 0.2371 Acc: 0.9221 | Val Loss: 0.2829 Acc: 0.9116                                               \n",
      "Epoch 053 | Train Loss: 0.2089 Acc: 0.9317 | Val Loss: 0.3127 Acc: 0.9099                                               \n",
      "Epoch 054 | Train Loss: 0.1957 Acc: 0.9350 | Val Loss: 0.1961 Acc: 0.9364                                               \n",
      "Epoch 055 | Train Loss: 0.2023 Acc: 0.9325 | Val Loss: 0.2617 Acc: 0.9272                                               \n",
      "Epoch 056 | Train Loss: 0.1870 Acc: 0.9364 | Val Loss: 0.2571 Acc: 0.9269                                               \n",
      "Epoch 057 | Train Loss: 0.2067 Acc: 0.9331 | Val Loss: 0.3963 Acc: 0.8946                                               \n",
      "Epoch 058 | Train Loss: 0.2029 Acc: 0.9322 | Val Loss: 0.2507 Acc: 0.9236                                               \n",
      "Epoch 059 | Train Loss: 0.1901 Acc: 0.9386 | Val Loss: 0.2596 Acc: 0.9281                                               \n",
      "Epoch 060 | Train Loss: 0.1716 Acc: 0.9438 | Val Loss: 0.2443 Acc: 0.9293                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 256, 'cnn_dropout': 0.620855870101093, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 64, 'cnn_kernels_2': 96, 'learning_rate': 0.004756203019495833, 'lstm_dense': 64, 'lstm_hidden_size': 32, 'lstm_layers': 3, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 1470098.8526 Acc: 0.4275 | Val Loss: 1.2460 Acc: 0.4421                                         \n",
      "Epoch 002 | Train Loss: 1.2475 Acc: 0.4400 | Val Loss: 1.2403 Acc: 0.4421                                               \n",
      "Epoch 003 | Train Loss: 1.2406 Acc: 0.4421 | Val Loss: 1.2319 Acc: 0.4421                                               \n",
      "Epoch 004 | Train Loss: 1.2331 Acc: 0.4391 | Val Loss: 1.2127 Acc: 0.4528                                               \n",
      "Epoch 005 | Train Loss: 1.1861 Acc: 0.4579 | Val Loss: 1.1597 Acc: 0.4657                                               \n",
      "Epoch 006 | Train Loss: 1.1253 Acc: 0.4909 | Val Loss: 1.0374 Acc: 0.5113                                               \n",
      "Epoch 007 | Train Loss: 1.0762 Acc: 0.5077 | Val Loss: 1.0091 Acc: 0.5269                                               \n",
      "Epoch 008 | Train Loss: 1.0203 Acc: 0.5359 | Val Loss: 1.1546 Acc: 0.4534                                               \n",
      "Epoch 009 | Train Loss: 0.9592 Acc: 0.5800 | Val Loss: 0.9379 Acc: 0.5654                                               \n",
      "Epoch 010 | Train Loss: 0.9030 Acc: 0.6212 | Val Loss: 0.8731 Acc: 0.6507                                               \n",
      "Epoch 011 | Train Loss: 0.8582 Acc: 0.6483 | Val Loss: 0.7595 Acc: 0.6952                                               \n",
      "Epoch 012 | Train Loss: 0.8338 Acc: 0.6596 | Val Loss: 0.7517 Acc: 0.7030                                               \n",
      "Epoch 013 | Train Loss: 0.8150 Acc: 0.6702 | Val Loss: 0.8516 Acc: 0.6552                                               \n",
      "Epoch 014 | Train Loss: 0.7832 Acc: 0.6833 | Val Loss: 0.7351 Acc: 0.7069                                               \n",
      "Epoch 015 | Train Loss: 0.8449 Acc: 0.6565 | Val Loss: 0.7659 Acc: 0.6913                                               \n",
      "Epoch 016 | Train Loss: 0.8266 Acc: 0.6585 | Val Loss: 0.7861 Acc: 0.6881                                               \n",
      "Epoch 017 | Train Loss: 0.7753 Acc: 0.6840 | Val Loss: 0.7262 Acc: 0.7164                                               \n",
      "Epoch 018 | Train Loss: 0.7704 Acc: 0.6867 | Val Loss: 0.7292 Acc: 0.7078                                               \n",
      "Epoch 019 | Train Loss: 0.8014 Acc: 0.6752 | Val Loss: 0.7436 Acc: 0.6970                                               \n",
      "Epoch 020 | Train Loss: 0.7786 Acc: 0.6867 | Val Loss: 0.7193 Acc: 0.7134                                               \n",
      "Epoch 021 | Train Loss: 0.7847 Acc: 0.6871 | Val Loss: 0.7870 Acc: 0.6758                                               \n",
      "Epoch 022 | Train Loss: 0.7838 Acc: 0.6794 | Val Loss: 0.7026 Acc: 0.7179                                               \n",
      "Epoch 023 | Train Loss: 0.7655 Acc: 0.6933 | Val Loss: 0.6509 Acc: 0.7442                                               \n",
      "Epoch 024 | Train Loss: 0.7541 Acc: 0.6977 | Val Loss: 0.6814 Acc: 0.7340                                               \n",
      "Epoch 025 | Train Loss: 0.7305 Acc: 0.7074 | Val Loss: 0.6629 Acc: 0.7290                                               \n",
      "Epoch 026 | Train Loss: 0.7864 Acc: 0.6848 | Val Loss: 0.6983 Acc: 0.7310                                               \n",
      "Epoch 027 | Train Loss: 0.9246 Acc: 0.6131 | Val Loss: 0.8955 Acc: 0.6487                                               \n",
      "Epoch 028 | Train Loss: 1.0451 Acc: 0.5568 | Val Loss: 0.9594 Acc: 0.6003                                               \n",
      "Epoch 029 | Train Loss: 0.9179 Acc: 0.6240 | Val Loss: 0.8913 Acc: 0.6275                                               \n",
      "Epoch 030 | Train Loss: 0.8552 Acc: 0.6598 | Val Loss: 0.7607 Acc: 0.7003                                               \n",
      "Epoch 031 | Train Loss: 0.9476 Acc: 0.6021 | Val Loss: 0.9047 Acc: 0.6230                                               \n",
      "Epoch 032 | Train Loss: 0.9049 Acc: 0.6279 | Val Loss: 1.1303 Acc: 0.5215                                               \n",
      "Epoch 033 | Train Loss: 1.1919 Acc: 0.4658 | Val Loss: 1.1507 Acc: 0.4922                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 32, 'cnn_dropout': 0.24276123633121643, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 16, 'cnn_kernels_2': 64, 'learning_rate': 1.5643588971011074e-05, 'lstm_dense': 128, 'lstm_hidden_size': 64, 'lstm_layers': 3, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 6.5430 Acc: 0.4302 | Val Loss: 0.9258 Acc: 0.5603                                               \n",
      "Epoch 002 | Train Loss: 1.2586 Acc: 0.4617 | Val Loss: 0.8841 Acc: 0.5994                                               \n",
      "Epoch 003 | Train Loss: 1.0662 Acc: 0.5013 | Val Loss: 0.9126 Acc: 0.5113                                               \n",
      "Epoch 004 | Train Loss: 1.0317 Acc: 0.5111 | Val Loss: 0.8846 Acc: 0.6284                                               \n",
      "Epoch 005 | Train Loss: 0.9816 Acc: 0.5221 | Val Loss: 0.8306 Acc: 0.5964                                               \n",
      "Epoch 006 | Train Loss: 0.9327 Acc: 0.5380 | Val Loss: 0.8203 Acc: 0.6131                                               \n",
      "Epoch 007 | Train Loss: 0.9337 Acc: 0.5368 | Val Loss: 0.8254 Acc: 0.6534                                               \n",
      "Epoch 008 | Train Loss: 0.9053 Acc: 0.5481 | Val Loss: 0.8022 Acc: 0.6134                                               \n",
      "Epoch 009 | Train Loss: 0.8867 Acc: 0.5645 | Val Loss: 0.7686 Acc: 0.6666                                               \n",
      "Epoch 010 | Train Loss: 0.8713 Acc: 0.5633 | Val Loss: 0.7322 Acc: 0.6197                                               \n",
      "Epoch 011 | Train Loss: 0.8595 Acc: 0.5735 | Val Loss: 0.7382 Acc: 0.6713                                               \n",
      "Epoch 012 | Train Loss: 0.8501 Acc: 0.5715 | Val Loss: 0.7274 Acc: 0.6400                                               \n",
      "Epoch 013 | Train Loss: 0.8293 Acc: 0.5917 | Val Loss: 0.7350 Acc: 0.6594                                               \n",
      "Epoch 014 | Train Loss: 0.8340 Acc: 0.5886 | Val Loss: 0.7369 Acc: 0.7119                                               \n",
      "Epoch 015 | Train Loss: 0.8166 Acc: 0.6017 | Val Loss: 0.7165 Acc: 0.6785                                               \n",
      "Epoch 016 | Train Loss: 0.8139 Acc: 0.6006 | Val Loss: 0.7166 Acc: 0.6570                                               \n",
      "Epoch 017 | Train Loss: 0.8034 Acc: 0.6080 | Val Loss: 0.6906 Acc: 0.6896                                               \n",
      "Epoch 018 | Train Loss: 0.7977 Acc: 0.6162 | Val Loss: 0.6658 Acc: 0.7466                                               \n",
      "Epoch 019 | Train Loss: 0.7945 Acc: 0.6206 | Val Loss: 0.6953 Acc: 0.6699                                               \n",
      "Epoch 020 | Train Loss: 0.7805 Acc: 0.6268 | Val Loss: 0.6613 Acc: 0.7003                                               \n",
      "Epoch 021 | Train Loss: 0.7724 Acc: 0.6259 | Val Loss: 0.6528 Acc: 0.7316                                               \n",
      "Epoch 022 | Train Loss: 0.7709 Acc: 0.6291 | Val Loss: 0.6408 Acc: 0.7245                                               \n",
      "Epoch 023 | Train Loss: 0.7618 Acc: 0.6329 | Val Loss: 0.6433 Acc: 0.7361                                               \n",
      "Epoch 024 | Train Loss: 0.7449 Acc: 0.6474 | Val Loss: 0.6376 Acc: 0.6991                                               \n",
      "Epoch 025 | Train Loss: 0.7363 Acc: 0.6497 | Val Loss: 0.6104 Acc: 0.7609                                               \n",
      "Epoch 026 | Train Loss: 0.7326 Acc: 0.6501 | Val Loss: 0.6108 Acc: 0.7430                                               \n",
      "Epoch 027 | Train Loss: 0.7280 Acc: 0.6600 | Val Loss: 0.6058 Acc: 0.7648                                               \n",
      "Epoch 028 | Train Loss: 0.7194 Acc: 0.6606 | Val Loss: 0.5956 Acc: 0.7743                                               \n",
      "Epoch 029 | Train Loss: 0.7196 Acc: 0.6624 | Val Loss: 0.6011 Acc: 0.7791                                               \n",
      "Epoch 030 | Train Loss: 0.7127 Acc: 0.6685 | Val Loss: 0.5939 Acc: 0.7478                                               \n",
      "Epoch 031 | Train Loss: 0.7044 Acc: 0.6642 | Val Loss: 0.5742 Acc: 0.7564                                               \n",
      "Epoch 032 | Train Loss: 0.7057 Acc: 0.6674 | Val Loss: 0.5820 Acc: 0.7454                                               \n",
      "Epoch 033 | Train Loss: 0.6974 Acc: 0.6727 | Val Loss: 0.5755 Acc: 0.7675                                               \n",
      "Epoch 034 | Train Loss: 0.6976 Acc: 0.6695 | Val Loss: 0.5589 Acc: 0.7666                                               \n",
      "Epoch 035 | Train Loss: 0.6966 Acc: 0.6734 | Val Loss: 0.5718 Acc: 0.7743                                               \n",
      "Epoch 036 | Train Loss: 0.6848 Acc: 0.6741 | Val Loss: 0.5744 Acc: 0.7785                                               \n",
      "Epoch 037 | Train Loss: 0.6884 Acc: 0.6793 | Val Loss: 0.5704 Acc: 0.7707                                               \n",
      "Epoch 038 | Train Loss: 0.6849 Acc: 0.6748 | Val Loss: 0.5613 Acc: 0.7639                                               \n",
      "Epoch 039 | Train Loss: 0.6829 Acc: 0.6790 | Val Loss: 0.5477 Acc: 0.7854                                               \n",
      "Epoch 040 | Train Loss: 0.6715 Acc: 0.6835 | Val Loss: 0.5337 Acc: 0.7952                                               \n",
      "Epoch 041 | Train Loss: 0.6742 Acc: 0.6849 | Val Loss: 0.5439 Acc: 0.7710                                               \n",
      "Epoch 042 | Train Loss: 0.6735 Acc: 0.6839 | Val Loss: 0.5428 Acc: 0.7669                                               \n",
      "Epoch 043 | Train Loss: 0.6699 Acc: 0.6806 | Val Loss: 0.5405 Acc: 0.7597                                               \n",
      "Epoch 044 | Train Loss: 0.6629 Acc: 0.6901 | Val Loss: 0.5212 Acc: 0.7985                                               \n",
      "Epoch 045 | Train Loss: 0.6679 Acc: 0.6855 | Val Loss: 0.5594 Acc: 0.7669                                               \n",
      "Epoch 046 | Train Loss: 0.6604 Acc: 0.6886 | Val Loss: 0.5267 Acc: 0.8003                                               \n",
      "Epoch 047 | Train Loss: 0.6673 Acc: 0.6857 | Val Loss: 0.5366 Acc: 0.7896                                               \n",
      "Epoch 048 | Train Loss: 0.6618 Acc: 0.6927 | Val Loss: 0.5501 Acc: 0.7704                                               \n",
      "Epoch 049 | Train Loss: 0.6531 Acc: 0.6960 | Val Loss: 0.5115 Acc: 0.8078                                               \n",
      "Epoch 050 | Train Loss: 0.6525 Acc: 0.6906 | Val Loss: 0.5300 Acc: 0.7815                                               \n",
      "Epoch 051 | Train Loss: 0.6545 Acc: 0.6922 | Val Loss: 0.5332 Acc: 0.7806                                               \n",
      "Epoch 052 | Train Loss: 0.6492 Acc: 0.6921 | Val Loss: 0.5170 Acc: 0.7770                                               \n",
      "Epoch 053 | Train Loss: 0.6507 Acc: 0.6929 | Val Loss: 0.5124 Acc: 0.8003                                               \n",
      "Epoch 054 | Train Loss: 0.6476 Acc: 0.6945 | Val Loss: 0.5242 Acc: 0.7815                                               \n",
      "Epoch 055 | Train Loss: 0.6401 Acc: 0.6984 | Val Loss: 0.5115 Acc: 0.7824                                               \n",
      "Epoch 056 | Train Loss: 0.6430 Acc: 0.6992 | Val Loss: 0.5281 Acc: 0.7755                                               \n",
      "Epoch 057 | Train Loss: 0.6443 Acc: 0.6968 | Val Loss: 0.5083 Acc: 0.7970                                               \n",
      "Epoch 058 | Train Loss: 0.6317 Acc: 0.7088 | Val Loss: 0.5118 Acc: 0.7988                                               \n",
      "Epoch 059 | Train Loss: 0.6395 Acc: 0.7025 | Val Loss: 0.5277 Acc: 0.7507                                               \n",
      "Epoch 060 | Train Loss: 0.6363 Acc: 0.7025 | Val Loss: 0.5039 Acc: 0.7639                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 128, 'cnn_dropout': 0.2389909511995048, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 16, 'cnn_kernels_2': 64, 'learning_rate': 0.000427139114617022, 'lstm_dense': 128, 'lstm_hidden_size': 64, 'lstm_layers': 3, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 17.3929 Acc: 0.3945 | Val Loss: 2.9640 Acc: 0.4699                                              \n",
      "Epoch 002 | Train Loss: 4.3044 Acc: 0.4497 | Val Loss: 1.5694 Acc: 0.5499                                               \n",
      "Epoch 003 | Train Loss: 2.5118 Acc: 0.4826 | Val Loss: 0.9750 Acc: 0.5839                                               \n",
      "Epoch 004 | Train Loss: 1.7339 Acc: 0.5085 | Val Loss: 0.7986 Acc: 0.6606                                               \n",
      "Epoch 005 | Train Loss: 1.3336 Acc: 0.5319 | Val Loss: 1.1073 Acc: 0.6054                                               \n",
      "Epoch 006 | Train Loss: 1.1758 Acc: 0.5474 | Val Loss: 0.8039 Acc: 0.6493                                               \n",
      "Epoch 007 | Train Loss: 0.9627 Acc: 0.5932 | Val Loss: 0.7884 Acc: 0.6334                                               \n",
      "Epoch 008 | Train Loss: 0.7586 Acc: 0.6751 | Val Loss: 0.7153 Acc: 0.6666                                               \n",
      "Epoch 009 | Train Loss: 0.6423 Acc: 0.7229 | Val Loss: 0.4638 Acc: 0.8131                                               \n",
      "Epoch 010 | Train Loss: 0.5157 Acc: 0.7927 | Val Loss: 0.3994 Acc: 0.8328                                               \n",
      "Epoch 011 | Train Loss: 0.4149 Acc: 0.8435 | Val Loss: 0.4326 Acc: 0.8364                                               \n",
      "Epoch 012 | Train Loss: 0.3407 Acc: 0.8733 | Val Loss: 0.2577 Acc: 0.9087                                               \n",
      "Epoch 013 | Train Loss: 0.2940 Acc: 0.8948 | Val Loss: 0.2866 Acc: 0.8958                                               \n",
      "Epoch 014 | Train Loss: 0.2353 Acc: 0.9130 | Val Loss: 0.1698 Acc: 0.9418                                               \n",
      "Epoch 015 | Train Loss: 0.1994 Acc: 0.9304 | Val Loss: 0.3153 Acc: 0.8982                                               \n",
      "Epoch 016 | Train Loss: 0.1693 Acc: 0.9422 | Val Loss: 0.1885 Acc: 0.9340                                               \n",
      "Epoch 017 | Train Loss: 0.1465 Acc: 0.9504 | Val Loss: 0.1311 Acc: 0.9543                                               \n",
      "Epoch 018 | Train Loss: 0.1304 Acc: 0.9558 | Val Loss: 0.1307 Acc: 0.9549                                               \n",
      "Epoch 019 | Train Loss: 0.1092 Acc: 0.9619 | Val Loss: 0.1420 Acc: 0.9537                                               \n",
      "Epoch 020 | Train Loss: 0.1049 Acc: 0.9663 | Val Loss: 0.1169 Acc: 0.9615                                               \n",
      "Epoch 021 | Train Loss: 0.1091 Acc: 0.9625 | Val Loss: 0.1069 Acc: 0.9660                                               \n",
      "Epoch 022 | Train Loss: 0.0817 Acc: 0.9740 | Val Loss: 0.0855 Acc: 0.9713                                               \n",
      "Epoch 023 | Train Loss: 0.0819 Acc: 0.9740 | Val Loss: 0.1008 Acc: 0.9648                                               \n",
      "Epoch 024 | Train Loss: 0.0826 Acc: 0.9709 | Val Loss: 0.1382 Acc: 0.9630                                               \n",
      "Epoch 025 | Train Loss: 0.0741 Acc: 0.9737 | Val Loss: 0.0919 Acc: 0.9663                                               \n",
      "Epoch 026 | Train Loss: 0.0668 Acc: 0.9781 | Val Loss: 0.1407 Acc: 0.9555                                               \n",
      "Epoch 027 | Train Loss: 0.0623 Acc: 0.9782 | Val Loss: 0.0624 Acc: 0.9782                                               \n",
      "Epoch 028 | Train Loss: 0.0611 Acc: 0.9809 | Val Loss: 0.0728 Acc: 0.9767                                               \n",
      "Epoch 029 | Train Loss: 0.0591 Acc: 0.9807 | Val Loss: 0.0849 Acc: 0.9716                                               \n",
      "Epoch 030 | Train Loss: 0.0633 Acc: 0.9800 | Val Loss: 0.0754 Acc: 0.9755                                               \n",
      "Epoch 031 | Train Loss: 0.0603 Acc: 0.9802 | Val Loss: 0.1451 Acc: 0.9570                                               \n",
      "Epoch 032 | Train Loss: 0.0467 Acc: 0.9833 | Val Loss: 0.0877 Acc: 0.9719                                               \n",
      "Epoch 033 | Train Loss: 0.0502 Acc: 0.9830 | Val Loss: 0.1121 Acc: 0.9672                                               \n",
      "Epoch 034 | Train Loss: 0.0556 Acc: 0.9826 | Val Loss: 0.0772 Acc: 0.9770                                               \n",
      "Epoch 035 | Train Loss: 0.0473 Acc: 0.9843 | Val Loss: 0.0821 Acc: 0.9743                                               \n",
      "Epoch 036 | Train Loss: 0.0373 Acc: 0.9882 | Val Loss: 0.0635 Acc: 0.9791                                               \n",
      "Epoch 037 | Train Loss: 0.0394 Acc: 0.9875 | Val Loss: 0.0806 Acc: 0.9710                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 32, 'cnn_dropout': 0.18784029119717421, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 64, 'learning_rate': 0.00044794550081610305, 'lstm_dense': 128, 'lstm_hidden_size': 128, 'lstm_layers': 1, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 8.4070 Acc: 0.4166 | Val Loss: 1.3259 Acc: 0.5397                                               \n",
      "Epoch 002 | Train Loss: 2.2694 Acc: 0.5054 | Val Loss: 1.5605 Acc: 0.5233                                               \n",
      "Epoch 003 | Train Loss: 1.2340 Acc: 0.6071 | Val Loss: 0.7049 Acc: 0.6967                                               \n",
      "Epoch 004 | Train Loss: 0.7977 Acc: 0.7068 | Val Loss: 0.5176 Acc: 0.8045                                               \n",
      "Epoch 005 | Train Loss: 0.5772 Acc: 0.7835 | Val Loss: 0.4746 Acc: 0.8125                                               \n",
      "Epoch 006 | Train Loss: 0.4241 Acc: 0.8408 | Val Loss: 0.3724 Acc: 0.8501                                               \n",
      "Epoch 007 | Train Loss: 0.3270 Acc: 0.8811 | Val Loss: 0.2489 Acc: 0.9027                                               \n",
      "Epoch 008 | Train Loss: 0.2503 Acc: 0.9075 | Val Loss: 0.1663 Acc: 0.9364                                               \n",
      "Epoch 009 | Train Loss: 0.2061 Acc: 0.9256 | Val Loss: 0.2069 Acc: 0.9179                                               \n",
      "Epoch 010 | Train Loss: 0.1676 Acc: 0.9406 | Val Loss: 0.1864 Acc: 0.9364                                               \n",
      "Epoch 011 | Train Loss: 0.1446 Acc: 0.9501 | Val Loss: 0.1462 Acc: 0.9436                                               \n",
      "Epoch 012 | Train Loss: 0.1295 Acc: 0.9545 | Val Loss: 0.1104 Acc: 0.9636                                               \n",
      "Epoch 013 | Train Loss: 0.1107 Acc: 0.9610 | Val Loss: 0.0984 Acc: 0.9684                                               \n",
      "Epoch 014 | Train Loss: 0.1017 Acc: 0.9651 | Val Loss: 0.0698 Acc: 0.9791                                               \n",
      "Epoch 015 | Train Loss: 0.0880 Acc: 0.9721 | Val Loss: 0.1129 Acc: 0.9615                                               \n",
      "Epoch 016 | Train Loss: 0.0848 Acc: 0.9726 | Val Loss: 0.0869 Acc: 0.9710                                               \n",
      "Epoch 017 | Train Loss: 0.0748 Acc: 0.9754 | Val Loss: 0.0704 Acc: 0.9779                                               \n",
      "Epoch 018 | Train Loss: 0.0727 Acc: 0.9760 | Val Loss: 0.1033 Acc: 0.9663                                               \n",
      "Epoch 019 | Train Loss: 0.0645 Acc: 0.9791 | Val Loss: 0.0809 Acc: 0.9734                                               \n",
      "Epoch 020 | Train Loss: 0.0628 Acc: 0.9805 | Val Loss: 0.3170 Acc: 0.9146                                               \n",
      "Epoch 021 | Train Loss: 0.0616 Acc: 0.9795 | Val Loss: 0.0938 Acc: 0.9713                                               \n",
      "Epoch 022 | Train Loss: 0.0462 Acc: 0.9848 | Val Loss: 0.4785 Acc: 0.8734                                               \n",
      "Epoch 023 | Train Loss: 0.0554 Acc: 0.9824 | Val Loss: 0.0699 Acc: 0.9818                                               \n",
      "Epoch 024 | Train Loss: 0.0447 Acc: 0.9858 | Val Loss: 0.0599 Acc: 0.9785                                               \n",
      "Epoch 025 | Train Loss: 0.0451 Acc: 0.9867 | Val Loss: 0.1155 Acc: 0.9672                                               \n",
      "Epoch 026 | Train Loss: 0.0472 Acc: 0.9855 | Val Loss: 0.0566 Acc: 0.9824                                               \n",
      "Epoch 027 | Train Loss: 0.0422 Acc: 0.9862 | Val Loss: 0.0980 Acc: 0.9737                                               \n",
      "Epoch 028 | Train Loss: 0.0411 Acc: 0.9872 | Val Loss: 0.0567 Acc: 0.9818                                               \n",
      "Epoch 029 | Train Loss: 0.0399 Acc: 0.9881 | Val Loss: 0.0668 Acc: 0.9785                                               \n",
      "Epoch 030 | Train Loss: 0.0325 Acc: 0.9880 | Val Loss: 0.1267 Acc: 0.9687                                               \n",
      "Epoch 031 | Train Loss: 0.0325 Acc: 0.9884 | Val Loss: 0.0605 Acc: 0.9836                                               \n",
      "Epoch 032 | Train Loss: 0.0325 Acc: 0.9896 | Val Loss: 0.0482 Acc: 0.9866                                               \n",
      "Epoch 033 | Train Loss: 0.0331 Acc: 0.9897 | Val Loss: 0.0588 Acc: 0.9836                                               \n",
      "Epoch 034 | Train Loss: 0.0320 Acc: 0.9891 | Val Loss: 0.0778 Acc: 0.9770                                               \n",
      "Epoch 035 | Train Loss: 0.0338 Acc: 0.9896 | Val Loss: 0.0501 Acc: 0.9881                                               \n",
      "Epoch 036 | Train Loss: 0.0243 Acc: 0.9917 | Val Loss: 0.0578 Acc: 0.9824                                               \n",
      "Epoch 037 | Train Loss: 0.0233 Acc: 0.9922 | Val Loss: 0.1454 Acc: 0.9618                                               \n",
      "Epoch 038 | Train Loss: 0.0241 Acc: 0.9921 | Val Loss: 0.0968 Acc: 0.9704                                               \n",
      "Epoch 039 | Train Loss: 0.0356 Acc: 0.9912 | Val Loss: 0.0943 Acc: 0.9740                                               \n",
      "Epoch 040 | Train Loss: 0.0303 Acc: 0.9895 | Val Loss: 0.0323 Acc: 0.9919                                               \n",
      "Epoch 041 | Train Loss: 0.0280 Acc: 0.9908 | Val Loss: 0.0783 Acc: 0.9779                                               \n",
      "Epoch 042 | Train Loss: 0.0208 Acc: 0.9937 | Val Loss: 0.0600 Acc: 0.9830                                               \n",
      "Epoch 043 | Train Loss: 0.0256 Acc: 0.9916 | Val Loss: 0.0907 Acc: 0.9725                                               \n",
      "Epoch 044 | Train Loss: 0.0252 Acc: 0.9931 | Val Loss: 0.0473 Acc: 0.9857                                               \n",
      "Epoch 045 | Train Loss: 0.0252 Acc: 0.9915 | Val Loss: 0.1956 Acc: 0.9490                                               \n",
      "Epoch 046 | Train Loss: 0.0308 Acc: 0.9915 | Val Loss: 0.0609 Acc: 0.9854                                               \n",
      "Epoch 047 | Train Loss: 0.0227 Acc: 0.9928 | Val Loss: 0.0291 Acc: 0.9913                                               \n",
      "Epoch 048 | Train Loss: 0.0251 Acc: 0.9923 | Val Loss: 0.0499 Acc: 0.9857                                               \n",
      "Epoch 049 | Train Loss: 0.0259 Acc: 0.9917 | Val Loss: 0.1088 Acc: 0.9672                                               \n",
      "Epoch 050 | Train Loss: 0.0226 Acc: 0.9929 | Val Loss: 0.0560 Acc: 0.9854                                               \n",
      "Epoch 051 | Train Loss: 0.0209 Acc: 0.9932 | Val Loss: 0.0623 Acc: 0.9827                                               \n",
      "Epoch 052 | Train Loss: 0.0181 Acc: 0.9946 | Val Loss: 0.0441 Acc: 0.9881                                               \n",
      "Epoch 053 | Train Loss: 0.0228 Acc: 0.9928 | Val Loss: 0.0478 Acc: 0.9875                                               \n",
      "Epoch 054 | Train Loss: 0.0224 Acc: 0.9927 | Val Loss: 0.0542 Acc: 0.9863                                               \n",
      "Epoch 055 | Train Loss: 0.0178 Acc: 0.9941 | Val Loss: 0.0464 Acc: 0.9875                                               \n",
      "Epoch 056 | Train Loss: 0.0146 Acc: 0.9951 | Val Loss: 0.0490 Acc: 0.9860                                               \n",
      "Epoch 057 | Train Loss: 0.0237 Acc: 0.9932 | Val Loss: 0.0304 Acc: 0.9925                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 32, 'cnn_dropout': 0.18554655257626512, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 16, 'learning_rate': 0.00045395188498538084, 'lstm_dense': 128, 'lstm_hidden_size': 128, 'lstm_layers': 1, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 17.2269 Acc: 0.4045 | Val Loss: 1.8947 Acc: 0.4800                                              \n",
      "Epoch 002 | Train Loss: 4.0967 Acc: 0.4462 | Val Loss: 1.8329 Acc: 0.5579                                               \n",
      "Epoch 003 | Train Loss: 2.5376 Acc: 0.4775 | Val Loss: 2.4717 Acc: 0.5224                                               \n",
      "Epoch 004 | Train Loss: 1.8800 Acc: 0.5020 | Val Loss: 1.5288 Acc: 0.5373                                               \n",
      "Epoch 005 | Train Loss: 1.5130 Acc: 0.5171 | Val Loss: 1.1585 Acc: 0.5573                                               \n",
      "Epoch 006 | Train Loss: 1.2946 Acc: 0.5315 | Val Loss: 0.9715 Acc: 0.5385                                               \n",
      "Epoch 007 | Train Loss: 1.2216 Acc: 0.5330 | Val Loss: 0.7455 Acc: 0.6212                                               \n",
      "Epoch 008 | Train Loss: 0.9865 Acc: 0.5544 | Val Loss: 0.8089 Acc: 0.5672                                               \n",
      "Epoch 009 | Train Loss: 0.8846 Acc: 0.5736 | Val Loss: 0.7112 Acc: 0.6734                                               \n",
      "Epoch 010 | Train Loss: 0.8266 Acc: 0.6041 | Val Loss: 0.6330 Acc: 0.7191                                               \n",
      "Epoch 011 | Train Loss: 0.7642 Acc: 0.6329 | Val Loss: 0.5723 Acc: 0.7588                                               \n",
      "Epoch 012 | Train Loss: 0.6446 Acc: 0.7137 | Val Loss: 0.4863 Acc: 0.7836                                               \n",
      "Epoch 013 | Train Loss: 0.5336 Acc: 0.7711 | Val Loss: 0.3982 Acc: 0.8304                                               \n",
      "Epoch 014 | Train Loss: 0.4243 Acc: 0.8294 | Val Loss: 0.2965 Acc: 0.8890                                               \n",
      "Epoch 015 | Train Loss: 0.3541 Acc: 0.8631 | Val Loss: 0.2530 Acc: 0.9042                                               \n",
      "Epoch 016 | Train Loss: 0.2681 Acc: 0.9018 | Val Loss: 0.1902 Acc: 0.9334                                               \n",
      "Epoch 017 | Train Loss: 0.2268 Acc: 0.9163 | Val Loss: 0.1478 Acc: 0.9507                                               \n",
      "Epoch 018 | Train Loss: 0.1985 Acc: 0.9302 | Val Loss: 0.1399 Acc: 0.9528                                               \n",
      "Epoch 019 | Train Loss: 0.1799 Acc: 0.9363 | Val Loss: 0.1360 Acc: 0.9537                                               \n",
      "Epoch 020 | Train Loss: 0.1558 Acc: 0.9455 | Val Loss: 0.1154 Acc: 0.9624                                               \n",
      "Epoch 021 | Train Loss: 0.1410 Acc: 0.9503 | Val Loss: 0.1054 Acc: 0.9669                                               \n",
      "Epoch 022 | Train Loss: 0.1159 Acc: 0.9597 | Val Loss: 0.0881 Acc: 0.9728                                               \n",
      "Epoch 023 | Train Loss: 0.1118 Acc: 0.9613 | Val Loss: 0.0948 Acc: 0.9704                                               \n",
      "Epoch 024 | Train Loss: 0.1081 Acc: 0.9634 | Val Loss: 0.1181 Acc: 0.9615                                               \n",
      "Epoch 025 | Train Loss: 0.0998 Acc: 0.9645 | Val Loss: 0.1287 Acc: 0.9555                                               \n",
      "Epoch 026 | Train Loss: 0.0884 Acc: 0.9711 | Val Loss: 0.0832 Acc: 0.9734                                               \n",
      "Epoch 027 | Train Loss: 0.0845 Acc: 0.9704 | Val Loss: 0.0955 Acc: 0.9672                                               \n",
      "Epoch 028 | Train Loss: 0.0880 Acc: 0.9693 | Val Loss: 0.0974 Acc: 0.9666                                               \n",
      "Epoch 029 | Train Loss: 0.0822 Acc: 0.9725 | Val Loss: 0.0697 Acc: 0.9770                                               \n",
      "Epoch 030 | Train Loss: 0.0657 Acc: 0.9782 | Val Loss: 0.0931 Acc: 0.9678                                               \n",
      "Epoch 031 | Train Loss: 0.0687 Acc: 0.9762 | Val Loss: 0.0642 Acc: 0.9785                                               \n",
      "Epoch 032 | Train Loss: 0.0685 Acc: 0.9760 | Val Loss: 0.0766 Acc: 0.9743                                               \n",
      "Epoch 033 | Train Loss: 0.0643 Acc: 0.9785 | Val Loss: 0.0811 Acc: 0.9764                                               \n",
      "Epoch 034 | Train Loss: 0.0606 Acc: 0.9776 | Val Loss: 0.1059 Acc: 0.9657                                               \n",
      "Epoch 035 | Train Loss: 0.0548 Acc: 0.9816 | Val Loss: 0.0754 Acc: 0.9776                                               \n",
      "Epoch 036 | Train Loss: 0.0532 Acc: 0.9813 | Val Loss: 0.0634 Acc: 0.9779                                               \n",
      "Epoch 037 | Train Loss: 0.0535 Acc: 0.9810 | Val Loss: 0.0559 Acc: 0.9821                                               \n",
      "Epoch 038 | Train Loss: 0.0480 Acc: 0.9828 | Val Loss: 0.0486 Acc: 0.9842                                               \n",
      "Epoch 039 | Train Loss: 0.0484 Acc: 0.9840 | Val Loss: 0.0584 Acc: 0.9848                                               \n",
      "Epoch 040 | Train Loss: 0.0412 Acc: 0.9855 | Val Loss: 0.0514 Acc: 0.9833                                               \n",
      "Epoch 041 | Train Loss: 0.0442 Acc: 0.9847 | Val Loss: 0.0440 Acc: 0.9845                                               \n",
      "Epoch 042 | Train Loss: 0.0426 Acc: 0.9852 | Val Loss: 0.0739 Acc: 0.9773                                               \n",
      "Epoch 043 | Train Loss: 0.0468 Acc: 0.9824 | Val Loss: 0.0592 Acc: 0.9836                                               \n",
      "Epoch 044 | Train Loss: 0.0379 Acc: 0.9866 | Val Loss: 0.0537 Acc: 0.9830                                               \n",
      "Epoch 045 | Train Loss: 0.0348 Acc: 0.9889 | Val Loss: 0.0664 Acc: 0.9791                                               \n",
      "Epoch 046 | Train Loss: 0.0438 Acc: 0.9840 | Val Loss: 0.0523 Acc: 0.9842                                               \n",
      "Epoch 047 | Train Loss: 0.0301 Acc: 0.9893 | Val Loss: 0.0574 Acc: 0.9845                                               \n",
      "Epoch 048 | Train Loss: 0.0385 Acc: 0.9871 | Val Loss: 0.0470 Acc: 0.9854                                               \n",
      "Epoch 049 | Train Loss: 0.0386 Acc: 0.9864 | Val Loss: 0.0421 Acc: 0.9875                                               \n",
      "Epoch 050 | Train Loss: 0.0394 Acc: 0.9865 | Val Loss: 0.0466 Acc: 0.9854                                               \n",
      "Epoch 051 | Train Loss: 0.0279 Acc: 0.9902 | Val Loss: 0.1001 Acc: 0.9704                                               \n",
      "Epoch 052 | Train Loss: 0.0361 Acc: 0.9872 | Val Loss: 0.0458 Acc: 0.9860                                               \n",
      "Epoch 053 | Train Loss: 0.0343 Acc: 0.9880 | Val Loss: 0.0449 Acc: 0.9857                                               \n",
      "Epoch 054 | Train Loss: 0.0317 Acc: 0.9910 | Val Loss: 0.0693 Acc: 0.9788                                               \n",
      "Epoch 055 | Train Loss: 0.0435 Acc: 0.9843 | Val Loss: 0.0541 Acc: 0.9851                                               \n",
      "Epoch 056 | Train Loss: 0.0284 Acc: 0.9906 | Val Loss: 0.0462 Acc: 0.9836                                               \n",
      "Epoch 057 | Train Loss: 0.0265 Acc: 0.9904 | Val Loss: 0.0496 Acc: 0.9821                                               \n",
      "Epoch 058 | Train Loss: 0.0437 Acc: 0.9860 | Val Loss: 0.0527 Acc: 0.9851                                               \n",
      "Epoch 059 | Train Loss: 0.0312 Acc: 0.9910 | Val Loss: 0.0409 Acc: 0.9851                                               \n",
      "Epoch 060 | Train Loss: 0.0262 Acc: 0.9910 | Val Loss: 0.0580 Acc: 0.9815                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 32, 'cnn_dropout': 0.1755443328847421, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 64, 'learning_rate': 0.0023780888704141856, 'lstm_dense': 128, 'lstm_hidden_size': 128, 'lstm_layers': 1, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 16.0389 Acc: 0.4035 | Val Loss: 1.9454 Acc: 0.4821                                              \n",
      "Epoch 002 | Train Loss: 2.1952 Acc: 0.4735 | Val Loss: 0.9820 Acc: 0.5833                                               \n",
      "Epoch 003 | Train Loss: 1.2939 Acc: 0.5205 | Val Loss: 0.7705 Acc: 0.6293                                               \n",
      "Epoch 004 | Train Loss: 1.0739 Acc: 0.5517 | Val Loss: 1.0494 Acc: 0.5851                                               \n",
      "Epoch 005 | Train Loss: 0.8980 Acc: 0.6033 | Val Loss: 0.6906 Acc: 0.6776                                               \n",
      "Epoch 006 | Train Loss: 0.8431 Acc: 0.6187 | Val Loss: 0.7486 Acc: 0.6430                                               \n",
      "Epoch 007 | Train Loss: 0.7786 Acc: 0.6420 | Val Loss: 0.8381 Acc: 0.5985                                               \n",
      "Epoch 008 | Train Loss: 0.7546 Acc: 0.6509 | Val Loss: 0.6891 Acc: 0.6827                                               \n",
      "Epoch 009 | Train Loss: 0.7329 Acc: 0.6625 | Val Loss: 0.7415 Acc: 0.6293                                               \n",
      "Epoch 010 | Train Loss: 0.6749 Acc: 0.6958 | Val Loss: 0.6270 Acc: 0.7170                                               \n",
      "Epoch 011 | Train Loss: 0.6484 Acc: 0.7031 | Val Loss: 0.5950 Acc: 0.7328                                               \n",
      "Epoch 012 | Train Loss: 0.6363 Acc: 0.7189 | Val Loss: 0.5568 Acc: 0.7499                                               \n",
      "Epoch 013 | Train Loss: 0.5979 Acc: 0.7355 | Val Loss: 0.5941 Acc: 0.7651                                               \n",
      "Epoch 014 | Train Loss: 0.5924 Acc: 0.7378 | Val Loss: 0.5424 Acc: 0.7890                                               \n",
      "Epoch 015 | Train Loss: 0.5709 Acc: 0.7468 | Val Loss: 0.6130 Acc: 0.7564                                               \n",
      "Epoch 016 | Train Loss: 0.5640 Acc: 0.7553 | Val Loss: 0.4982 Acc: 0.7985                                               \n",
      "Epoch 017 | Train Loss: 0.5286 Acc: 0.7711 | Val Loss: 0.5158 Acc: 0.8051                                               \n",
      "Epoch 018 | Train Loss: 0.5241 Acc: 0.7755 | Val Loss: 0.5247 Acc: 0.7743                                               \n",
      "Epoch 019 | Train Loss: 0.5249 Acc: 0.7651 | Val Loss: 0.5330 Acc: 0.7669                                               \n",
      "Epoch 020 | Train Loss: 0.5021 Acc: 0.7836 | Val Loss: 0.5823 Acc: 0.7358                                               \n",
      "Epoch 021 | Train Loss: 0.4939 Acc: 0.7818 | Val Loss: 0.4934 Acc: 0.7833                                               \n",
      "Epoch 022 | Train Loss: 0.4809 Acc: 0.7957 | Val Loss: 0.5760 Acc: 0.7466                                               \n",
      "Epoch 023 | Train Loss: 0.4736 Acc: 0.7948 | Val Loss: 0.4697 Acc: 0.7597                                               \n",
      "Epoch 024 | Train Loss: 0.4914 Acc: 0.7860 | Val Loss: 0.5090 Acc: 0.7878                                               \n",
      "Epoch 025 | Train Loss: 0.4795 Acc: 0.7960 | Val Loss: 0.4550 Acc: 0.8012                                               \n",
      "Epoch 026 | Train Loss: 0.4621 Acc: 0.7977 | Val Loss: 0.5271 Acc: 0.7782                                               \n",
      "Epoch 027 | Train Loss: 0.4636 Acc: 0.7995 | Val Loss: 0.5846 Acc: 0.7382                                               \n",
      "Epoch 028 | Train Loss: 0.4531 Acc: 0.8040 | Val Loss: 0.4420 Acc: 0.8224                                               \n",
      "Epoch 029 | Train Loss: 0.4422 Acc: 0.8055 | Val Loss: 0.5023 Acc: 0.7824                                               \n",
      "Epoch 030 | Train Loss: 0.4439 Acc: 0.8065 | Val Loss: 0.6636 Acc: 0.7060                                               \n",
      "Epoch 031 | Train Loss: 0.4460 Acc: 0.8069 | Val Loss: 0.4738 Acc: 0.7866                                               \n",
      "Epoch 032 | Train Loss: 0.4298 Acc: 0.8179 | Val Loss: 0.4481 Acc: 0.8119                                               \n",
      "Epoch 033 | Train Loss: 0.4550 Acc: 0.7997 | Val Loss: 0.5451 Acc: 0.7833                                               \n",
      "Epoch 034 | Train Loss: 0.4475 Acc: 0.8091 | Val Loss: 0.4991 Acc: 0.7907                                               \n",
      "Epoch 035 | Train Loss: 0.4503 Acc: 0.8069 | Val Loss: 0.4856 Acc: 0.7997                                               \n",
      "Epoch 036 | Train Loss: 0.4212 Acc: 0.8151 | Val Loss: 0.5931 Acc: 0.7597                                               \n",
      "Epoch 037 | Train Loss: 0.4388 Acc: 0.8111 | Val Loss: 0.4923 Acc: 0.8113                                               \n",
      "Epoch 038 | Train Loss: 0.4539 Acc: 0.8089 | Val Loss: 0.5328 Acc: 0.7955                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 32, 'cnn_dropout': 0.0008881287958885653, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 64, 'learning_rate': 0.0005431260118290265, 'lstm_dense': 128, 'lstm_hidden_size': 128, 'lstm_layers': 1, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 8.0935 Acc: 0.4576 | Val Loss: 2.3812 Acc: 0.4221                                               \n",
      "Epoch 002 | Train Loss: 2.1661 Acc: 0.5303 | Val Loss: 1.2052 Acc: 0.5284                                               \n",
      "Epoch 003 | Train Loss: 1.2253 Acc: 0.5874 | Val Loss: 0.7559 Acc: 0.6684                                               \n",
      "Epoch 004 | Train Loss: 0.8628 Acc: 0.6374 | Val Loss: 0.6640 Acc: 0.7143                                               \n",
      "Epoch 005 | Train Loss: 0.7242 Acc: 0.6801 | Val Loss: 0.6923 Acc: 0.7140                                               \n",
      "Epoch 006 | Train Loss: 0.6423 Acc: 0.7165 | Val Loss: 0.5277 Acc: 0.7875                                               \n",
      "Epoch 007 | Train Loss: 0.5776 Acc: 0.7465 | Val Loss: 0.4518 Acc: 0.8137                                               \n",
      "Epoch 008 | Train Loss: 0.5160 Acc: 0.7766 | Val Loss: 0.4647 Acc: 0.8084                                               \n",
      "Epoch 009 | Train Loss: 0.4601 Acc: 0.8024 | Val Loss: 0.5147 Acc: 0.7794                                               \n",
      "Epoch 010 | Train Loss: 0.4282 Acc: 0.8196 | Val Loss: 0.4028 Acc: 0.8242                                               \n",
      "Epoch 011 | Train Loss: 0.4161 Acc: 0.8308 | Val Loss: 0.3823 Acc: 0.8719                                               \n",
      "Epoch 012 | Train Loss: 0.3938 Acc: 0.8404 | Val Loss: 0.4580 Acc: 0.8057                                               \n",
      "Epoch 013 | Train Loss: 0.3833 Acc: 0.8468 | Val Loss: 0.4541 Acc: 0.8182                                               \n",
      "Epoch 014 | Train Loss: 0.3661 Acc: 0.8575 | Val Loss: 0.3973 Acc: 0.8537                                               \n",
      "Epoch 015 | Train Loss: 0.3650 Acc: 0.8569 | Val Loss: 0.4346 Acc: 0.8552                                               \n",
      "Epoch 016 | Train Loss: 0.3497 Acc: 0.8657 | Val Loss: 0.5552 Acc: 0.7764                                               \n",
      "Epoch 017 | Train Loss: 0.3365 Acc: 0.8730 | Val Loss: 0.4401 Acc: 0.7878                                               \n",
      "Epoch 018 | Train Loss: 0.3271 Acc: 0.8740 | Val Loss: 0.4071 Acc: 0.8331                                               \n",
      "Epoch 019 | Train Loss: 0.3355 Acc: 0.8701 | Val Loss: 0.3842 Acc: 0.8236                                               \n",
      "Epoch 020 | Train Loss: 0.3241 Acc: 0.8766 | Val Loss: 0.3773 Acc: 0.8785                                               \n",
      "Epoch 021 | Train Loss: 0.3068 Acc: 0.8784 | Val Loss: 0.5299 Acc: 0.7254                                               \n",
      "Epoch 022 | Train Loss: 0.3104 Acc: 0.8831 | Val Loss: 0.4690 Acc: 0.7806                                               \n",
      "Epoch 023 | Train Loss: 0.3072 Acc: 0.8785 | Val Loss: 0.5109 Acc: 0.6994                                               \n",
      "Epoch 024 | Train Loss: 0.3078 Acc: 0.8859 | Val Loss: 0.4401 Acc: 0.7776                                               \n",
      "Epoch 025 | Train Loss: 0.3001 Acc: 0.8893 | Val Loss: 0.4765 Acc: 0.8385                                               \n",
      "Epoch 026 | Train Loss: 0.3053 Acc: 0.8854 | Val Loss: 0.4422 Acc: 0.7982                                               \n",
      "Epoch 027 | Train Loss: 0.2954 Acc: 0.8886 | Val Loss: 0.4873 Acc: 0.7567                                               \n",
      "Epoch 028 | Train Loss: 0.2817 Acc: 0.8933 | Val Loss: 0.4550 Acc: 0.7901                                               \n",
      "Epoch 029 | Train Loss: 0.2834 Acc: 0.8937 | Val Loss: 0.4385 Acc: 0.7782                                               \n",
      "Epoch 030 | Train Loss: 0.2954 Acc: 0.8904 | Val Loss: 0.5017 Acc: 0.8003                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 32, 'cnn_dropout': 0.027366844449794092, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 16, 'learning_rate': 0.0018574126458462147, 'lstm_dense': 128, 'lstm_hidden_size': 128, 'lstm_layers': 1, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 10.4072 Acc: 0.4348 | Val Loss: 1.2468 Acc: 0.4421                                              \n",
      "Epoch 002 | Train Loss: 1.2414 Acc: 0.4422 | Val Loss: 1.2368 Acc: 0.4421                                               \n",
      "Epoch 003 | Train Loss: 1.2227 Acc: 0.4421 | Val Loss: 1.2032 Acc: 0.4427                                               \n",
      "Epoch 004 | Train Loss: 1.1465 Acc: 0.4762 | Val Loss: 1.0610 Acc: 0.5304                                               \n",
      "Epoch 005 | Train Loss: 1.0163 Acc: 0.5497 | Val Loss: 1.0002 Acc: 0.5346                                               \n",
      "Epoch 006 | Train Loss: 0.9207 Acc: 0.6050 | Val Loss: 0.8711 Acc: 0.6325                                               \n",
      "Epoch 007 | Train Loss: 0.8079 Acc: 0.6695 | Val Loss: 0.7669 Acc: 0.6821                                               \n",
      "Epoch 008 | Train Loss: 0.7220 Acc: 0.7089 | Val Loss: 0.6635 Acc: 0.7373                                               \n",
      "Epoch 009 | Train Loss: 0.6442 Acc: 0.7415 | Val Loss: 0.6554 Acc: 0.7322                                               \n",
      "Epoch 010 | Train Loss: 0.5561 Acc: 0.7826 | Val Loss: 0.6134 Acc: 0.7678                                               \n",
      "Epoch 011 | Train Loss: 0.4838 Acc: 0.8131 | Val Loss: 0.4217 Acc: 0.8412                                               \n",
      "Epoch 012 | Train Loss: 0.4217 Acc: 0.8389 | Val Loss: 0.3898 Acc: 0.8606                                               \n",
      "Epoch 013 | Train Loss: 0.4767 Acc: 0.8202 | Val Loss: 0.5935 Acc: 0.7758                                               \n",
      "Epoch 014 | Train Loss: 0.4021 Acc: 0.8525 | Val Loss: 0.4087 Acc: 0.8522                                               \n",
      "Epoch 015 | Train Loss: 0.3281 Acc: 0.8846 | Val Loss: 0.3464 Acc: 0.8788                                               \n",
      "Epoch 016 | Train Loss: 0.3084 Acc: 0.8884 | Val Loss: 0.3413 Acc: 0.8809                                               \n",
      "Epoch 017 | Train Loss: 0.2865 Acc: 0.8985 | Val Loss: 0.3252 Acc: 0.8782                                               \n",
      "Epoch 018 | Train Loss: 0.2683 Acc: 0.9045 | Val Loss: 0.3089 Acc: 0.8904                                               \n",
      "Epoch 019 | Train Loss: 0.2386 Acc: 0.9163 | Val Loss: 0.3539 Acc: 0.8791                                               \n",
      "Epoch 020 | Train Loss: 0.2189 Acc: 0.9218 | Val Loss: 0.2666 Acc: 0.9081                                               \n",
      "Epoch 021 | Train Loss: 0.2310 Acc: 0.9182 | Val Loss: 0.2619 Acc: 0.9063                                               \n",
      "Epoch 022 | Train Loss: 0.1985 Acc: 0.9260 | Val Loss: 0.2780 Acc: 0.9012                                               \n",
      "Epoch 023 | Train Loss: 0.1984 Acc: 0.9301 | Val Loss: 0.2850 Acc: 0.8985                                               \n",
      "Epoch 024 | Train Loss: 0.1855 Acc: 0.9335 | Val Loss: 0.2207 Acc: 0.9242                                               \n",
      "Epoch 025 | Train Loss: 0.1575 Acc: 0.9445 | Val Loss: 0.2463 Acc: 0.9155                                               \n",
      "Epoch 026 | Train Loss: 0.1692 Acc: 0.9399 | Val Loss: 0.2285 Acc: 0.9224                                               \n",
      "Epoch 027 | Train Loss: 0.1543 Acc: 0.9437 | Val Loss: 0.2689 Acc: 0.9084                                               \n",
      "Epoch 028 | Train Loss: 0.1518 Acc: 0.9458 | Val Loss: 0.2579 Acc: 0.9155                                               \n",
      "Epoch 029 | Train Loss: 0.1575 Acc: 0.9428 | Val Loss: 0.2218 Acc: 0.9230                                               \n",
      "Epoch 030 | Train Loss: 0.1894 Acc: 0.9333 | Val Loss: 0.3597 Acc: 0.8815                                               \n",
      "Epoch 031 | Train Loss: 0.1565 Acc: 0.9435 | Val Loss: 0.2419 Acc: 0.9164                                               \n",
      "Epoch 032 | Train Loss: 0.1551 Acc: 0.9436 | Val Loss: 0.2310 Acc: 0.9272                                               \n",
      "Epoch 033 | Train Loss: 0.1414 Acc: 0.9479 | Val Loss: 0.2113 Acc: 0.9316                                               \n",
      "Epoch 034 | Train Loss: 0.1302 Acc: 0.9553 | Val Loss: 0.2990 Acc: 0.9048                                               \n",
      "Epoch 035 | Train Loss: 0.1531 Acc: 0.9426 | Val Loss: 0.2651 Acc: 0.9155                                               \n",
      "Epoch 036 | Train Loss: 0.1567 Acc: 0.9455 | Val Loss: 0.2248 Acc: 0.9260                                               \n",
      "Epoch 037 | Train Loss: 0.1398 Acc: 0.9498 | Val Loss: 0.2639 Acc: 0.9081                                               \n",
      "Epoch 038 | Train Loss: 0.1216 Acc: 0.9561 | Val Loss: 0.2265 Acc: 0.9215                                               \n",
      "Epoch 039 | Train Loss: 0.1243 Acc: 0.9551 | Val Loss: 0.2305 Acc: 0.9200                                               \n",
      "Epoch 040 | Train Loss: 0.1597 Acc: 0.9413 | Val Loss: 0.3137 Acc: 0.8907                                               \n",
      "Epoch 041 | Train Loss: 0.1575 Acc: 0.9408 | Val Loss: 0.2542 Acc: 0.9107                                               \n",
      "Epoch 042 | Train Loss: 0.1188 Acc: 0.9587 | Val Loss: 0.2086 Acc: 0.9281                                               \n",
      "Epoch 043 | Train Loss: 0.0998 Acc: 0.9666 | Val Loss: 0.2165 Acc: 0.9272                                               \n",
      "Epoch 044 | Train Loss: 0.1561 Acc: 0.9450 | Val Loss: 0.2475 Acc: 0.9173                                               \n",
      "Epoch 045 | Train Loss: 0.1281 Acc: 0.9543 | Val Loss: 0.2405 Acc: 0.9200                                               \n",
      "Epoch 046 | Train Loss: 0.1600 Acc: 0.9437 | Val Loss: 0.3042 Acc: 0.8887                                               \n",
      "Epoch 047 | Train Loss: 0.1410 Acc: 0.9495 | Val Loss: 0.2251 Acc: 0.9242                                               \n",
      "Epoch 048 | Train Loss: 0.1156 Acc: 0.9584 | Val Loss: 0.2055 Acc: 0.9355                                               \n",
      "Epoch 049 | Train Loss: 0.1431 Acc: 0.9490 | Val Loss: 0.2056 Acc: 0.9304                                               \n",
      "Epoch 050 | Train Loss: 0.1062 Acc: 0.9633 | Val Loss: 0.2199 Acc: 0.9230                                               \n",
      "Epoch 051 | Train Loss: 0.1099 Acc: 0.9625 | Val Loss: 0.2015 Acc: 0.9358                                               \n",
      "Epoch 052 | Train Loss: 0.1085 Acc: 0.9610 | Val Loss: 0.2090 Acc: 0.9364                                               \n",
      "Epoch 053 | Train Loss: 0.0965 Acc: 0.9659 | Val Loss: 0.2247 Acc: 0.9227                                               \n",
      "Epoch 054 | Train Loss: 0.0790 Acc: 0.9719 | Val Loss: 0.1661 Acc: 0.9478                                               \n",
      "Epoch 055 | Train Loss: 0.0873 Acc: 0.9683 | Val Loss: 0.1870 Acc: 0.9355                                               \n",
      "Epoch 056 | Train Loss: 0.1157 Acc: 0.9582 | Val Loss: 0.2520 Acc: 0.9179                                               \n",
      "Epoch 057 | Train Loss: 0.1716 Acc: 0.9410 | Val Loss: 0.2524 Acc: 0.9125                                               \n",
      "Epoch 058 | Train Loss: 0.1237 Acc: 0.9553 | Val Loss: 0.2256 Acc: 0.9236                                               \n",
      "Epoch 059 | Train Loss: 0.1535 Acc: 0.9448 | Val Loss: 0.2216 Acc: 0.9245                                               \n",
      "Epoch 060 | Train Loss: 0.1153 Acc: 0.9581 | Val Loss: 0.2493 Acc: 0.9173                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 32, 'cnn_dropout': 0.30992202679510605, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 96, 'learning_rate': 0.0005871081063821214, 'lstm_dense': 128, 'lstm_hidden_size': 128, 'lstm_layers': 2, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 7.3058 Acc: 0.4399 | Val Loss: 1.7243 Acc: 0.5916                                               \n",
      "Epoch 002 | Train Loss: 1.8748 Acc: 0.5124 | Val Loss: 0.8308 Acc: 0.6278                                               \n",
      "Epoch 003 | Train Loss: 1.2848 Acc: 0.5416 | Val Loss: 0.8150 Acc: 0.6406                                               \n",
      "Epoch 004 | Train Loss: 0.9585 Acc: 0.6024 | Val Loss: 0.8154 Acc: 0.6257                                               \n",
      "Epoch 005 | Train Loss: 0.7085 Acc: 0.7057 | Val Loss: 0.5189 Acc: 0.7878                                               \n",
      "Epoch 006 | Train Loss: 0.5930 Acc: 0.7563 | Val Loss: 0.5536 Acc: 0.7675                                               \n",
      "Epoch 007 | Train Loss: 0.5046 Acc: 0.7980 | Val Loss: 0.4171 Acc: 0.8284                                               \n",
      "Epoch 008 | Train Loss: 0.4096 Acc: 0.8429 | Val Loss: 0.3598 Acc: 0.8606                                               \n",
      "Epoch 009 | Train Loss: 0.3435 Acc: 0.8720 | Val Loss: 0.3102 Acc: 0.8860                                               \n",
      "Epoch 010 | Train Loss: 0.2694 Acc: 0.9024 | Val Loss: 0.2722 Acc: 0.8857                                               \n",
      "Epoch 011 | Train Loss: 0.2228 Acc: 0.9204 | Val Loss: 0.1865 Acc: 0.9322                                               \n",
      "Epoch 012 | Train Loss: 0.1830 Acc: 0.9343 | Val Loss: 0.1708 Acc: 0.9343                                               \n",
      "Epoch 013 | Train Loss: 0.1653 Acc: 0.9413 | Val Loss: 0.1634 Acc: 0.9448                                               \n",
      "Epoch 014 | Train Loss: 0.1689 Acc: 0.9422 | Val Loss: 0.1261 Acc: 0.9561                                               \n",
      "Epoch 015 | Train Loss: 0.1246 Acc: 0.9569 | Val Loss: 0.1486 Acc: 0.9513                                               \n",
      "Epoch 016 | Train Loss: 0.1279 Acc: 0.9584 | Val Loss: 0.1860 Acc: 0.9382                                               \n",
      "Epoch 017 | Train Loss: 0.1227 Acc: 0.9590 | Val Loss: 0.2145 Acc: 0.9203                                               \n",
      "Epoch 018 | Train Loss: 0.0992 Acc: 0.9671 | Val Loss: 0.1008 Acc: 0.9701                                               \n",
      "Epoch 019 | Train Loss: 0.1003 Acc: 0.9659 | Val Loss: 0.1393 Acc: 0.9525                                               \n",
      "Epoch 020 | Train Loss: 0.0851 Acc: 0.9712 | Val Loss: 0.1268 Acc: 0.9603                                               \n",
      "Epoch 021 | Train Loss: 0.1022 Acc: 0.9674 | Val Loss: 0.1167 Acc: 0.9648                                               \n",
      "Epoch 022 | Train Loss: 0.1133 Acc: 0.9622 | Val Loss: 0.1932 Acc: 0.9472                                               \n",
      "Epoch 023 | Train Loss: 0.0856 Acc: 0.9716 | Val Loss: 0.0874 Acc: 0.9746                                               \n",
      "Epoch 024 | Train Loss: 0.0781 Acc: 0.9730 | Val Loss: 0.1700 Acc: 0.9466                                               \n",
      "Epoch 025 | Train Loss: 0.0829 Acc: 0.9741 | Val Loss: 0.0847 Acc: 0.9734                                               \n",
      "Epoch 026 | Train Loss: 0.0779 Acc: 0.9757 | Val Loss: 0.0693 Acc: 0.9794                                               \n",
      "Epoch 027 | Train Loss: 0.0699 Acc: 0.9775 | Val Loss: 0.1384 Acc: 0.9603                                               \n",
      "Epoch 028 | Train Loss: 0.0645 Acc: 0.9790 | Val Loss: 0.0880 Acc: 0.9719                                               \n",
      "Epoch 029 | Train Loss: 0.0639 Acc: 0.9794 | Val Loss: 0.0863 Acc: 0.9746                                               \n",
      "Epoch 030 | Train Loss: 0.0669 Acc: 0.9773 | Val Loss: 0.1733 Acc: 0.9525                                               \n",
      "Epoch 031 | Train Loss: 0.0707 Acc: 0.9763 | Val Loss: 0.0962 Acc: 0.9699                                               \n",
      "Epoch 032 | Train Loss: 0.0571 Acc: 0.9813 | Val Loss: 0.0617 Acc: 0.9839                                               \n",
      "Epoch 033 | Train Loss: 0.0473 Acc: 0.9846 | Val Loss: 0.0818 Acc: 0.9770                                               \n",
      "Epoch 034 | Train Loss: 0.0650 Acc: 0.9790 | Val Loss: 0.0710 Acc: 0.9782                                               \n",
      "Epoch 035 | Train Loss: 0.0750 Acc: 0.9748 | Val Loss: 0.1233 Acc: 0.9624                                               \n",
      "Epoch 036 | Train Loss: 0.0614 Acc: 0.9805 | Val Loss: 0.0627 Acc: 0.9809                                               \n",
      "Epoch 037 | Train Loss: 0.0488 Acc: 0.9845 | Val Loss: 0.1213 Acc: 0.9701                                               \n",
      "Epoch 038 | Train Loss: 0.0618 Acc: 0.9784 | Val Loss: 0.0966 Acc: 0.9743                                               \n",
      "Epoch 039 | Train Loss: 0.0520 Acc: 0.9830 | Val Loss: 0.0555 Acc: 0.9827                                               \n",
      "Epoch 040 | Train Loss: 0.0411 Acc: 0.9863 | Val Loss: 0.0888 Acc: 0.9758                                               \n",
      "Epoch 041 | Train Loss: 0.0628 Acc: 0.9802 | Val Loss: 0.0705 Acc: 0.9815                                               \n",
      "Epoch 042 | Train Loss: 0.0481 Acc: 0.9840 | Val Loss: 0.1025 Acc: 0.9734                                               \n",
      "Epoch 043 | Train Loss: 0.0553 Acc: 0.9827 | Val Loss: 0.0646 Acc: 0.9836                                               \n",
      "Epoch 044 | Train Loss: 0.0294 Acc: 0.9903 | Val Loss: 0.0863 Acc: 0.9752                                               \n",
      "Epoch 045 | Train Loss: 0.0500 Acc: 0.9825 | Val Loss: 0.0742 Acc: 0.9821                                               \n",
      "Epoch 046 | Train Loss: 0.0412 Acc: 0.9864 | Val Loss: 0.0475 Acc: 0.9884                                               \n",
      "Epoch 047 | Train Loss: 0.0474 Acc: 0.9849 | Val Loss: 0.0710 Acc: 0.9800                                               \n",
      "Epoch 048 | Train Loss: 0.0401 Acc: 0.9882 | Val Loss: 0.1285 Acc: 0.9675                                               \n",
      "Epoch 049 | Train Loss: 0.0602 Acc: 0.9805 | Val Loss: 0.0649 Acc: 0.9800                                               \n",
      "Epoch 050 | Train Loss: 0.0529 Acc: 0.9837 | Val Loss: 0.0707 Acc: 0.9806                                               \n",
      "Epoch 051 | Train Loss: 0.0405 Acc: 0.9865 | Val Loss: 0.0776 Acc: 0.9809                                               \n",
      "Epoch 052 | Train Loss: 0.0499 Acc: 0.9835 | Val Loss: 0.0540 Acc: 0.9839                                               \n",
      "Epoch 053 | Train Loss: 0.0360 Acc: 0.9883 | Val Loss: 0.0543 Acc: 0.9869                                               \n",
      "Epoch 054 | Train Loss: 0.0408 Acc: 0.9869 | Val Loss: 0.0834 Acc: 0.9812                                               \n",
      "Epoch 055 | Train Loss: 0.0431 Acc: 0.9870 | Val Loss: 0.0744 Acc: 0.9794                                               \n",
      "Epoch 056 | Train Loss: 0.0426 Acc: 0.9870 | Val Loss: 0.1131 Acc: 0.9710                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 32, 'cnn_dropout': 0.30397378710739464, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 64, 'learning_rate': 0.0022965043907551877, 'lstm_dense': 32, 'lstm_hidden_size': 128, 'lstm_layers': 6, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 5.7404 Acc: 0.4856 | Val Loss: 0.8296 Acc: 0.5415                                               \n",
      "Epoch 002 | Train Loss: 0.8775 Acc: 0.5940 | Val Loss: 0.6968 Acc: 0.6970                                               \n",
      "Epoch 003 | Train Loss: 0.7818 Acc: 0.6259 | Val Loss: 0.6529 Acc: 0.6624                                               \n",
      "Epoch 004 | Train Loss: 0.7390 Acc: 0.6595 | Val Loss: 0.6512 Acc: 0.7182                                               \n",
      "Epoch 005 | Train Loss: 0.7244 Acc: 0.6565 | Val Loss: 0.6090 Acc: 0.7200                                               \n",
      "Epoch 006 | Train Loss: 0.7346 Acc: 0.6637 | Val Loss: 0.6484 Acc: 0.7087                                               \n",
      "Epoch 007 | Train Loss: 0.7120 Acc: 0.6671 | Val Loss: 0.5851 Acc: 0.7051                                               \n",
      "Epoch 008 | Train Loss: 0.6997 Acc: 0.6721 | Val Loss: 0.9179 Acc: 0.5513                                               \n",
      "Epoch 009 | Train Loss: 0.6882 Acc: 0.6794 | Val Loss: 0.7826 Acc: 0.6200                                               \n",
      "Epoch 010 | Train Loss: 0.6927 Acc: 0.6794 | Val Loss: 0.7739 Acc: 0.6716                                               \n",
      "Epoch 011 | Train Loss: 0.6895 Acc: 0.6744 | Val Loss: 0.7772 Acc: 0.5415                                               \n",
      "Epoch 012 | Train Loss: 0.6811 Acc: 0.6829 | Val Loss: 0.8829 Acc: 0.5878                                               \n",
      "Epoch 013 | Train Loss: 0.6611 Acc: 0.6969 | Val Loss: 0.8049 Acc: 0.5842                                               \n",
      "Epoch 014 | Train Loss: 0.6730 Acc: 0.6865 | Val Loss: 0.7426 Acc: 0.6000                                               \n",
      "Epoch 015 | Train Loss: 0.6531 Acc: 0.6983 | Val Loss: 0.8292 Acc: 0.5860                                               \n",
      "Epoch 016 | Train Loss: 0.7297 Acc: 0.6671 | Val Loss: 0.8793 Acc: 0.5734                                               \n",
      "Epoch 017 | Train Loss: 0.7218 Acc: 0.6515 | Val Loss: 0.8773 Acc: 0.5660                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 32, 'cnn_dropout': 0.04826627411326703, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 16, 'learning_rate': 1.659641232547607e-05, 'lstm_dense': 128, 'lstm_hidden_size': 128, 'lstm_layers': 5, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 86.4591 Acc: 0.3932 | Val Loss: 31.4611 Acc: 0.4373                                             \n",
      "Epoch 002 | Train Loss: 51.3371 Acc: 0.3628 | Val Loss: 14.5533 Acc: 0.3854                                             \n",
      "Epoch 003 | Train Loss: 39.7449 Acc: 0.3531 | Val Loss: 8.8729 Acc: 0.3355                                              \n",
      "Epoch 004 | Train Loss: 31.6943 Acc: 0.3517 | Val Loss: 6.4784 Acc: 0.3304                                              \n",
      "Epoch 005 | Train Loss: 25.0404 Acc: 0.3576 | Val Loss: 5.9602 Acc: 0.4149                                              \n",
      "Epoch 006 | Train Loss: 20.7598 Acc: 0.3723 | Val Loss: 4.8694 Acc: 0.4448                                              \n",
      "Epoch 007 | Train Loss: 15.9332 Acc: 0.3873 | Val Loss: 4.0555 Acc: 0.4561                                              \n",
      "Epoch 008 | Train Loss: 13.8552 Acc: 0.4022 | Val Loss: 3.0977 Acc: 0.4582                                              \n",
      "Epoch 009 | Train Loss: 11.8482 Acc: 0.3958 | Val Loss: 2.7729 Acc: 0.4567                                              \n",
      "Epoch 010 | Train Loss: 10.4142 Acc: 0.4047 | Val Loss: 2.5077 Acc: 0.5439                                              \n",
      "Epoch 011 | Train Loss: 8.9084 Acc: 0.4201 | Val Loss: 2.1925 Acc: 0.5806                                               \n",
      "Epoch 012 | Train Loss: 8.3392 Acc: 0.4232 | Val Loss: 1.9794 Acc: 0.6063                                               \n",
      "Epoch 013 | Train Loss: 7.6422 Acc: 0.4234 | Val Loss: 1.8295 Acc: 0.6203                                               \n",
      "Epoch 014 | Train Loss: 6.9471 Acc: 0.4399 | Val Loss: 1.8183 Acc: 0.5493                                               \n",
      "Epoch 015 | Train Loss: 6.3790 Acc: 0.4400 | Val Loss: 1.9589 Acc: 0.6296                                               \n",
      "Epoch 016 | Train Loss: 5.9812 Acc: 0.4506 | Val Loss: 1.7419 Acc: 0.6382                                               \n",
      "Epoch 017 | Train Loss: 5.6068 Acc: 0.4542 | Val Loss: 1.5906 Acc: 0.6469                                               \n",
      "Epoch 018 | Train Loss: 5.2766 Acc: 0.4685 | Val Loss: 1.5448 Acc: 0.6284                                               \n",
      "Epoch 019 | Train Loss: 4.9387 Acc: 0.4726 | Val Loss: 1.3693 Acc: 0.6904                                               \n",
      "Epoch 020 | Train Loss: 4.6406 Acc: 0.4809 | Val Loss: 1.5433 Acc: 0.6349                                               \n",
      "Epoch 021 | Train Loss: 4.4309 Acc: 0.4906 | Val Loss: 1.3691 Acc: 0.6451                                               \n",
      "Epoch 022 | Train Loss: 4.0806 Acc: 0.5013 | Val Loss: 1.1699 Acc: 0.7197                                               \n",
      "Epoch 023 | Train Loss: 3.9669 Acc: 0.5105 | Val Loss: 1.2362 Acc: 0.6713                                               \n",
      "Epoch 024 | Train Loss: 3.7409 Acc: 0.5214 | Val Loss: 1.1617 Acc: 0.7003                                               \n",
      "Epoch 025 | Train Loss: 3.6623 Acc: 0.5243 | Val Loss: 1.1931 Acc: 0.7257                                               \n",
      "Epoch 026 | Train Loss: 3.4298 Acc: 0.5371 | Val Loss: 1.1175 Acc: 0.7206                                               \n",
      "Epoch 027 | Train Loss: 3.2449 Acc: 0.5401 | Val Loss: 1.0862 Acc: 0.7430                                               \n",
      "Epoch 028 | Train Loss: 3.0941 Acc: 0.5532 | Val Loss: 1.0216 Acc: 0.7349                                               \n",
      "Epoch 029 | Train Loss: 3.0687 Acc: 0.5640 | Val Loss: 1.0063 Acc: 0.7579                                               \n",
      "Epoch 030 | Train Loss: 2.8479 Acc: 0.5695 | Val Loss: 1.0682 Acc: 0.6913                                               \n",
      "Epoch 031 | Train Loss: 2.7151 Acc: 0.5804 | Val Loss: 0.8792 Acc: 0.7469                                               \n",
      "Epoch 032 | Train Loss: 2.6194 Acc: 0.5886 | Val Loss: 0.9763 Acc: 0.7090                                               \n",
      "Epoch 033 | Train Loss: 2.5194 Acc: 0.5969 | Val Loss: 0.7913 Acc: 0.7812                                               \n",
      "Epoch 034 | Train Loss: 2.4375 Acc: 0.6030 | Val Loss: 0.7789 Acc: 0.7740                                               \n",
      "Epoch 035 | Train Loss: 2.3846 Acc: 0.6038 | Val Loss: 0.8629 Acc: 0.7504                                               \n",
      "Epoch 036 | Train Loss: 2.2915 Acc: 0.6234 | Val Loss: 0.8366 Acc: 0.7764                                               \n",
      "Epoch 037 | Train Loss: 2.2464 Acc: 0.6246 | Val Loss: 0.7329 Acc: 0.7722                                               \n",
      "Epoch 038 | Train Loss: 2.1148 Acc: 0.6297 | Val Loss: 0.7496 Acc: 0.7740                                               \n",
      "Epoch 039 | Train Loss: 2.0320 Acc: 0.6376 | Val Loss: 0.7648 Acc: 0.7358                                               \n",
      "Epoch 040 | Train Loss: 1.9385 Acc: 0.6472 | Val Loss: 0.6382 Acc: 0.7997                                               \n",
      "Epoch 041 | Train Loss: 1.8619 Acc: 0.6571 | Val Loss: 0.7919 Acc: 0.7513                                               \n",
      "Epoch 042 | Train Loss: 1.8080 Acc: 0.6643 | Val Loss: 0.5907 Acc: 0.8215                                               \n",
      "Epoch 043 | Train Loss: 1.7297 Acc: 0.6627 | Val Loss: 0.6353 Acc: 0.8036                                               \n",
      "Epoch 044 | Train Loss: 1.6762 Acc: 0.6685 | Val Loss: 0.6219 Acc: 0.8093                                               \n",
      "Epoch 045 | Train Loss: 1.6492 Acc: 0.6759 | Val Loss: 0.5442 Acc: 0.8331                                               \n",
      "Epoch 046 | Train Loss: 1.5920 Acc: 0.6842 | Val Loss: 0.6193 Acc: 0.8173                                               \n",
      "Epoch 047 | Train Loss: 1.5292 Acc: 0.6921 | Val Loss: 0.6485 Acc: 0.7848                                               \n",
      "Epoch 048 | Train Loss: 1.4747 Acc: 0.6965 | Val Loss: 0.5674 Acc: 0.7922                                               \n",
      "Epoch 049 | Train Loss: 1.4159 Acc: 0.7023 | Val Loss: 0.6537 Acc: 0.8107                                               \n",
      "Epoch 050 | Train Loss: 1.3697 Acc: 0.7092 | Val Loss: 0.6772 Acc: 0.7666                                               \n",
      "Epoch 051 | Train Loss: 1.3135 Acc: 0.7201 | Val Loss: 0.5241 Acc: 0.8003                                               \n",
      "Epoch 052 | Train Loss: 1.2695 Acc: 0.7192 | Val Loss: 0.4828 Acc: 0.8463                                               \n",
      "Epoch 053 | Train Loss: 1.2286 Acc: 0.7304 | Val Loss: 0.4932 Acc: 0.8242                                               \n",
      "Epoch 054 | Train Loss: 1.1360 Acc: 0.7401 | Val Loss: 0.5146 Acc: 0.8048                                               \n",
      "Epoch 055 | Train Loss: 1.1170 Acc: 0.7373 | Val Loss: 0.5202 Acc: 0.8104                                               \n",
      "Epoch 056 | Train Loss: 1.0944 Acc: 0.7422 | Val Loss: 0.5279 Acc: 0.7967                                               \n",
      "Epoch 057 | Train Loss: 1.0891 Acc: 0.7454 | Val Loss: 0.4794 Acc: 0.8442                                               \n",
      "Epoch 058 | Train Loss: 1.0586 Acc: 0.7480 | Val Loss: 0.4946 Acc: 0.8188                                               \n",
      "Epoch 059 | Train Loss: 1.0153 Acc: 0.7583 | Val Loss: 0.6218 Acc: 0.8051                                               \n",
      "Epoch 060 | Train Loss: 0.9962 Acc: 0.7613 | Val Loss: 0.5128 Acc: 0.8224                                               \n",
      "100%|███████████████████████████████████████████████| 30/30 [30:02<00:00, 60.08s/trial, best loss: 0.029098299862677927]\n",
      "Best hyperparameters: {'batch_size': np.int64(0), 'cnn_dense': np.int64(0), 'cnn_dropout': np.float64(0.18784029119717421), 'cnn_kernel_size_1': np.int64(1), 'cnn_kernel_size_2': np.int64(1), 'cnn_kernels_1': np.int64(0), 'cnn_kernels_2': np.int64(2), 'learning_rate': np.float64(0.00044794550081610305), 'lstm_dense': np.int64(2), 'lstm_hidden_size': np.int64(3), 'lstm_layers': np.int64(0), 'optimizer': np.int64(1)}\n",
      "TPE search finished in 1802.28 seconds\n",
      "Best (raw indices): {'batch_size': np.int64(0), 'cnn_dense': np.int64(0), 'cnn_dropout': np.float64(0.18784029119717421), 'cnn_kernel_size_1': np.int64(1), 'cnn_kernel_size_2': np.int64(1), 'cnn_kernels_1': np.int64(0), 'cnn_kernels_2': np.int64(2), 'learning_rate': np.float64(0.00044794550081610305), 'lstm_dense': np.int64(2), 'lstm_hidden_size': np.int64(3), 'lstm_layers': np.int64(0), 'optimizer': np.int64(1)}\n",
      "Best (interpreted): {'batch_size': 32, 'cnn_dense': 32, 'cnn_dropout': np.float64(0.18784029119717421), 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 64, 'learning_rate': np.float64(0.00044794550081610305), 'lstm_dense': 128, 'lstm_hidden_size': 128, 'lstm_layers': 1, 'optimizer': 'rmsprop'}\n",
      "Starting TPE search...\n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 64, 'cnn_dropout': 0.2720252354232569, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 16, 'learning_rate': 3.0241608652440237e-05, 'lstm_dense': 256, 'lstm_hidden_size': 96, 'lstm_layers': 4, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 46.0115 Acc: 0.3242 | Val Loss: 12.1409 Acc: 0.4269                                             \n",
      "Epoch 002 | Train Loss: 24.2132 Acc: 0.3423 | Val Loss: 6.9410 Acc: 0.3922                                              \n",
      "Epoch 003 | Train Loss: 15.0025 Acc: 0.3640 | Val Loss: 5.0233 Acc: 0.3967                                              \n",
      "Epoch 004 | Train Loss: 10.7558 Acc: 0.3878 | Val Loss: 2.8199 Acc: 0.4731                                              \n",
      "Epoch 005 | Train Loss: 8.6397 Acc: 0.4006 | Val Loss: 2.9048 Acc: 0.4797                                               \n",
      "Epoch 006 | Train Loss: 6.6275 Acc: 0.4248 | Val Loss: 1.9221 Acc: 0.5872                                               \n",
      "Epoch 007 | Train Loss: 5.5567 Acc: 0.4448 | Val Loss: 1.9579 Acc: 0.5337                                               \n",
      "Epoch 008 | Train Loss: 4.6261 Acc: 0.4716 | Val Loss: 1.5456 Acc: 0.5934                                               \n",
      "Epoch 009 | Train Loss: 3.9793 Acc: 0.4788 | Val Loss: 1.4134 Acc: 0.5845                                               \n",
      "Epoch 010 | Train Loss: 3.5127 Acc: 0.4982 | Val Loss: 1.7682 Acc: 0.5516                                               \n",
      "Epoch 011 | Train Loss: 3.1424 Acc: 0.5012 | Val Loss: 1.2039 Acc: 0.5982                                               \n",
      "Epoch 012 | Train Loss: 2.8210 Acc: 0.5146 | Val Loss: 0.9144 Acc: 0.6158                                               \n",
      "Epoch 013 | Train Loss: 2.5142 Acc: 0.5253 | Val Loss: 1.0822 Acc: 0.6364                                               \n",
      "Epoch 014 | Train Loss: 2.2369 Acc: 0.5427 | Val Loss: 1.0904 Acc: 0.6203                                               \n",
      "Epoch 015 | Train Loss: 2.1103 Acc: 0.5550 | Val Loss: 0.9261 Acc: 0.6391                                               \n",
      "Epoch 016 | Train Loss: 1.8935 Acc: 0.5636 | Val Loss: 1.0983 Acc: 0.6030                                               \n",
      "Epoch 017 | Train Loss: 1.7845 Acc: 0.5727 | Val Loss: 0.8897 Acc: 0.6576                                               \n",
      "Epoch 018 | Train Loss: 1.6291 Acc: 0.5800 | Val Loss: 0.7749 Acc: 0.6788                                               \n",
      "Epoch 019 | Train Loss: 1.5330 Acc: 0.5960 | Val Loss: 0.9463 Acc: 0.6373                                               \n",
      "Epoch 020 | Train Loss: 1.4568 Acc: 0.5952 | Val Loss: 1.1853 Acc: 0.6421                                               \n",
      "Epoch 021 | Train Loss: 1.3851 Acc: 0.6068 | Val Loss: 0.8338 Acc: 0.6639                                               \n",
      "Epoch 022 | Train Loss: 1.3050 Acc: 0.6120 | Val Loss: 0.6446 Acc: 0.7281                                               \n",
      "Epoch 023 | Train Loss: 1.1924 Acc: 0.6284 | Val Loss: 0.6749 Acc: 0.7304                                               \n",
      "Epoch 024 | Train Loss: 1.1503 Acc: 0.6353 | Val Loss: 0.6352 Acc: 0.7284                                               \n",
      "Epoch 025 | Train Loss: 1.0842 Acc: 0.6374 | Val Loss: 0.6838 Acc: 0.6913                                               \n",
      "Epoch 026 | Train Loss: 1.0006 Acc: 0.6616 | Val Loss: 0.8694 Acc: 0.6340                                               \n",
      "Epoch 027 | Train Loss: 0.9395 Acc: 0.6662 | Val Loss: 0.6942 Acc: 0.7293                                               \n",
      "Epoch 028 | Train Loss: 0.9098 Acc: 0.6810 | Val Loss: 0.6358 Acc: 0.7499                                               \n",
      "Epoch 029 | Train Loss: 0.8763 Acc: 0.6931 | Val Loss: 0.5752 Acc: 0.7701                                               \n",
      "Epoch 030 | Train Loss: 0.8204 Acc: 0.6982 | Val Loss: 0.5496 Acc: 0.7869                                               \n",
      "Epoch 031 | Train Loss: 0.7843 Acc: 0.7163 | Val Loss: 0.6593 Acc: 0.7194                                               \n",
      "Epoch 032 | Train Loss: 0.7810 Acc: 0.7153 | Val Loss: 0.5240 Acc: 0.7907                                               \n",
      "Epoch 033 | Train Loss: 0.7270 Acc: 0.7318 | Val Loss: 0.5403 Acc: 0.7788                                               \n",
      "Epoch 034 | Train Loss: 0.6893 Acc: 0.7445 | Val Loss: 0.5505 Acc: 0.7770                                               \n",
      "Epoch 035 | Train Loss: 0.6949 Acc: 0.7411 | Val Loss: 0.4967 Acc: 0.7890                                               \n",
      "Epoch 036 | Train Loss: 0.6301 Acc: 0.7659 | Val Loss: 0.5015 Acc: 0.7964                                               \n",
      "Epoch 037 | Train Loss: 0.6274 Acc: 0.7665 | Val Loss: 0.4595 Acc: 0.8254                                               \n",
      "Epoch 038 | Train Loss: 0.6013 Acc: 0.7723 | Val Loss: 0.5016 Acc: 0.8218                                               \n",
      "Epoch 039 | Train Loss: 0.5921 Acc: 0.7794 | Val Loss: 0.7414 Acc: 0.6910                                               \n",
      "Epoch 040 | Train Loss: 0.5858 Acc: 0.7825 | Val Loss: 0.4220 Acc: 0.8379                                               \n",
      "Epoch 041 | Train Loss: 0.5437 Acc: 0.7918 | Val Loss: 0.4321 Acc: 0.8278                                               \n",
      "Epoch 042 | Train Loss: 0.5483 Acc: 0.7976 | Val Loss: 0.6410 Acc: 0.7451                                               \n",
      "Epoch 043 | Train Loss: 0.5150 Acc: 0.8057 | Val Loss: 0.4058 Acc: 0.8534                                               \n",
      "Epoch 044 | Train Loss: 0.4969 Acc: 0.8103 | Val Loss: 0.4643 Acc: 0.8122                                               \n",
      "Epoch 045 | Train Loss: 0.4892 Acc: 0.8220 | Val Loss: 0.4586 Acc: 0.8224                                               \n",
      "Epoch 046 | Train Loss: 0.4805 Acc: 0.8204 | Val Loss: 0.4031 Acc: 0.8549                                               \n",
      "Epoch 047 | Train Loss: 0.4610 Acc: 0.8307 | Val Loss: 0.5352 Acc: 0.8018                                               \n",
      "Epoch 048 | Train Loss: 0.4463 Acc: 0.8375 | Val Loss: 0.3926 Acc: 0.8427                                               \n",
      "Epoch 049 | Train Loss: 0.4361 Acc: 0.8383 | Val Loss: 0.4613 Acc: 0.8066                                               \n",
      "Epoch 050 | Train Loss: 0.4100 Acc: 0.8484 | Val Loss: 0.3727 Acc: 0.8678                                               \n",
      "Epoch 051 | Train Loss: 0.4015 Acc: 0.8521 | Val Loss: 0.5009 Acc: 0.8006                                               \n",
      "Epoch 052 | Train Loss: 0.3917 Acc: 0.8573 | Val Loss: 0.3671 Acc: 0.8540                                               \n",
      "Epoch 053 | Train Loss: 0.3785 Acc: 0.8649 | Val Loss: 0.4142 Acc: 0.8319                                               \n",
      "Epoch 054 | Train Loss: 0.3649 Acc: 0.8683 | Val Loss: 0.3214 Acc: 0.8863                                               \n",
      "Epoch 055 | Train Loss: 0.3544 Acc: 0.8755 | Val Loss: 0.6105 Acc: 0.7576                                               \n",
      "Epoch 056 | Train Loss: 0.3441 Acc: 0.8796 | Val Loss: 0.3691 Acc: 0.8663                                               \n",
      "Epoch 057 | Train Loss: 0.3335 Acc: 0.8827 | Val Loss: 0.3713 Acc: 0.8564                                               \n",
      "Epoch 058 | Train Loss: 0.3152 Acc: 0.8890 | Val Loss: 0.3196 Acc: 0.8836                                               \n",
      "Epoch 059 | Train Loss: 0.3249 Acc: 0.8852 | Val Loss: 0.2581 Acc: 0.9075                                               \n",
      "Epoch 060 | Train Loss: 0.2975 Acc: 0.8925 | Val Loss: 0.2880 Acc: 0.8964                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 36, 'cnn_dense': 32, 'cnn_dropout': 0.2006907036151267, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 64, 'cnn_kernels_2': 16, 'learning_rate': 2.1495138667247518e-05, 'lstm_dense': 32, 'lstm_hidden_size': 64, 'lstm_layers': 6, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 86.2907 Acc: 0.2929 | Val Loss: 21.2659 Acc: 0.4519                                             \n",
      "Epoch 002 | Train Loss: 47.7692 Acc: 0.3447 | Val Loss: 12.2013 Acc: 0.4776                                             \n",
      "Epoch 003 | Train Loss: 35.6685 Acc: 0.3605 | Val Loss: 9.4819 Acc: 0.4337                                              \n",
      "Epoch 004 | Train Loss: 27.2423 Acc: 0.3770 | Val Loss: 8.3434 Acc: 0.4725                                              \n",
      "Epoch 005 | Train Loss: 22.2670 Acc: 0.3770 | Val Loss: 6.1145 Acc: 0.4910                                              \n",
      "Epoch 006 | Train Loss: 18.0717 Acc: 0.3901 | Val Loss: 6.5950 Acc: 0.5176                                              \n",
      "Epoch 007 | Train Loss: 14.9769 Acc: 0.4011 | Val Loss: 4.3512 Acc: 0.5654                                              \n",
      "Epoch 008 | Train Loss: 12.7447 Acc: 0.4101 | Val Loss: 3.7004 Acc: 0.5716                                              \n",
      "Epoch 009 | Train Loss: 10.9292 Acc: 0.4151 | Val Loss: 3.8531 Acc: 0.5746                                              \n",
      "Epoch 010 | Train Loss: 9.5993 Acc: 0.4303 | Val Loss: 2.8497 Acc: 0.6066                                               \n",
      "Epoch 011 | Train Loss: 8.4721 Acc: 0.4432 | Val Loss: 3.0054 Acc: 0.5863                                               \n",
      "Epoch 012 | Train Loss: 7.2065 Acc: 0.4539 | Val Loss: 2.1130 Acc: 0.6343                                               \n",
      "Epoch 013 | Train Loss: 6.6367 Acc: 0.4613 | Val Loss: 2.1475 Acc: 0.6087                                               \n",
      "Epoch 014 | Train Loss: 5.8489 Acc: 0.4721 | Val Loss: 2.1439 Acc: 0.6090                                               \n",
      "Epoch 015 | Train Loss: 5.4786 Acc: 0.4739 | Val Loss: 1.8183 Acc: 0.6301                                               \n",
      "Epoch 016 | Train Loss: 4.9069 Acc: 0.4879 | Val Loss: 1.7104 Acc: 0.6209                                               \n",
      "Epoch 017 | Train Loss: 4.5823 Acc: 0.4968 | Val Loss: 1.9270 Acc: 0.6325                                               \n",
      "Epoch 018 | Train Loss: 4.1990 Acc: 0.5039 | Val Loss: 1.6188 Acc: 0.6119                                               \n",
      "Epoch 019 | Train Loss: 3.9546 Acc: 0.5141 | Val Loss: 1.5573 Acc: 0.6266                                               \n",
      "Epoch 020 | Train Loss: 3.5613 Acc: 0.5256 | Val Loss: 1.7761 Acc: 0.6331                                               \n",
      "Epoch 021 | Train Loss: 3.4355 Acc: 0.5195 | Val Loss: 1.4757 Acc: 0.6543                                               \n",
      "Epoch 022 | Train Loss: 3.0613 Acc: 0.5424 | Val Loss: 1.3722 Acc: 0.6588                                               \n",
      "Epoch 023 | Train Loss: 2.9421 Acc: 0.5403 | Val Loss: 1.2037 Acc: 0.6728                                               \n",
      "Epoch 024 | Train Loss: 2.8331 Acc: 0.5463 | Val Loss: 1.3910 Acc: 0.6424                                               \n",
      "Epoch 025 | Train Loss: 2.5884 Acc: 0.5585 | Val Loss: 1.2845 Acc: 0.6531                                               \n",
      "Epoch 026 | Train Loss: 2.4028 Acc: 0.5734 | Val Loss: 1.2029 Acc: 0.6543                                               \n",
      "Epoch 027 | Train Loss: 2.2926 Acc: 0.5780 | Val Loss: 1.2799 Acc: 0.6758                                               \n",
      "Epoch 028 | Train Loss: 2.1815 Acc: 0.5812 | Val Loss: 1.2278 Acc: 0.6890                                               \n",
      "Epoch 029 | Train Loss: 2.0822 Acc: 0.5893 | Val Loss: 1.0702 Acc: 0.7015                                               \n",
      "Epoch 030 | Train Loss: 1.9155 Acc: 0.5983 | Val Loss: 1.0112 Acc: 0.6881                                               \n",
      "Epoch 031 | Train Loss: 1.9110 Acc: 0.6054 | Val Loss: 0.8842 Acc: 0.7364                                               \n",
      "Epoch 032 | Train Loss: 1.7153 Acc: 0.6157 | Val Loss: 0.9878 Acc: 0.7218                                               \n",
      "Epoch 033 | Train Loss: 1.6691 Acc: 0.6217 | Val Loss: 1.0133 Acc: 0.7370                                               \n",
      "Epoch 034 | Train Loss: 1.5890 Acc: 0.6262 | Val Loss: 0.9167 Acc: 0.7454                                               \n",
      "Epoch 035 | Train Loss: 1.5160 Acc: 0.6351 | Val Loss: 0.9050 Acc: 0.7430                                               \n",
      "Epoch 036 | Train Loss: 1.4665 Acc: 0.6427 | Val Loss: 0.8334 Acc: 0.7531                                               \n",
      "Epoch 037 | Train Loss: 1.3774 Acc: 0.6527 | Val Loss: 0.8928 Acc: 0.7382                                               \n",
      "Epoch 038 | Train Loss: 1.3389 Acc: 0.6569 | Val Loss: 0.8007 Acc: 0.7690                                               \n",
      "Epoch 039 | Train Loss: 1.2676 Acc: 0.6627 | Val Loss: 0.8332 Acc: 0.7522                                               \n",
      "Epoch 040 | Train Loss: 1.2084 Acc: 0.6702 | Val Loss: 0.7333 Acc: 0.7606                                               \n",
      "Epoch 041 | Train Loss: 1.1904 Acc: 0.6762 | Val Loss: 0.8501 Acc: 0.7475                                               \n",
      "Epoch 042 | Train Loss: 1.1062 Acc: 0.6854 | Val Loss: 0.7533 Acc: 0.7666                                               \n",
      "Epoch 043 | Train Loss: 1.0674 Acc: 0.6936 | Val Loss: 0.6750 Acc: 0.7842                                               \n",
      "Epoch 044 | Train Loss: 1.0274 Acc: 0.7018 | Val Loss: 0.6799 Acc: 0.7552                                               \n",
      "Epoch 045 | Train Loss: 0.9772 Acc: 0.7096 | Val Loss: 0.6869 Acc: 0.7734                                               \n",
      "Epoch 046 | Train Loss: 0.9516 Acc: 0.7142 | Val Loss: 0.7035 Acc: 0.7809                                               \n",
      "Epoch 047 | Train Loss: 0.9029 Acc: 0.7248 | Val Loss: 0.6064 Acc: 0.8018                                               \n",
      "Epoch 048 | Train Loss: 0.8564 Acc: 0.7367 | Val Loss: 0.5753 Acc: 0.8125                                               \n",
      "Epoch 049 | Train Loss: 0.8332 Acc: 0.7355 | Val Loss: 0.6394 Acc: 0.7970                                               \n",
      "Epoch 050 | Train Loss: 0.7901 Acc: 0.7440 | Val Loss: 0.5923 Acc: 0.8104                                               \n",
      "Epoch 051 | Train Loss: 0.7669 Acc: 0.7507 | Val Loss: 0.5453 Acc: 0.8290                                               \n",
      "Epoch 052 | Train Loss: 0.7343 Acc: 0.7639 | Val Loss: 0.7478 Acc: 0.7699                                               \n",
      "Epoch 053 | Train Loss: 0.7262 Acc: 0.7600 | Val Loss: 0.5875 Acc: 0.8176                                               \n",
      "Epoch 054 | Train Loss: 0.6879 Acc: 0.7707 | Val Loss: 0.5562 Acc: 0.8170                                               \n",
      "Epoch 055 | Train Loss: 0.6647 Acc: 0.7768 | Val Loss: 0.5258 Acc: 0.8287                                               \n",
      "Epoch 056 | Train Loss: 0.6616 Acc: 0.7826 | Val Loss: 0.5135 Acc: 0.8316                                               \n",
      "Epoch 057 | Train Loss: 0.6409 Acc: 0.7892 | Val Loss: 0.4514 Acc: 0.8540                                               \n",
      "Epoch 058 | Train Loss: 0.6054 Acc: 0.7919 | Val Loss: 0.5095 Acc: 0.8373                                               \n",
      "Epoch 059 | Train Loss: 0.5897 Acc: 0.7998 | Val Loss: 0.5110 Acc: 0.8448                                               \n",
      "Epoch 060 | Train Loss: 0.5608 Acc: 0.8068 | Val Loss: 0.5371 Acc: 0.8382                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 64, 'cnn_dropout': 0.32647921558797083, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 16, 'cnn_kernels_2': 16, 'learning_rate': 1.2972571570669482e-05, 'lstm_dense': 64, 'lstm_hidden_size': 128, 'lstm_layers': 6, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 189.6276 Acc: 0.2167 | Val Loss: 100.8666 Acc: 0.2591                                           \n",
      "Epoch 002 | Train Loss: 124.7197 Acc: 0.2176 | Val Loss: 37.6663 Acc: 0.2654                                            \n",
      "Epoch 003 | Train Loss: 72.2700 Acc: 0.2555 | Val Loss: 31.5494 Acc: 0.4421                                             \n",
      "Epoch 004 | Train Loss: 57.0102 Acc: 0.2968 | Val Loss: 34.4316 Acc: 0.4421                                             \n",
      "Epoch 005 | Train Loss: 51.3548 Acc: 0.3193 | Val Loss: 30.8980 Acc: 0.4421                                             \n",
      "Epoch 006 | Train Loss: 46.5717 Acc: 0.3302 | Val Loss: 29.2488 Acc: 0.4421                                             \n",
      "Epoch 007 | Train Loss: 42.4053 Acc: 0.3253 | Val Loss: 24.5054 Acc: 0.4421                                             \n",
      "Epoch 008 | Train Loss: 38.2356 Acc: 0.3456 | Val Loss: 22.6026 Acc: 0.4421                                             \n",
      "Epoch 009 | Train Loss: 34.5999 Acc: 0.3489 | Val Loss: 19.7762 Acc: 0.4421                                             \n",
      "Epoch 010 | Train Loss: 31.3863 Acc: 0.3529 | Val Loss: 17.4859 Acc: 0.4490                                             \n",
      "Epoch 011 | Train Loss: 27.8034 Acc: 0.3596 | Val Loss: 15.6941 Acc: 0.4442                                             \n",
      "Epoch 012 | Train Loss: 26.0928 Acc: 0.3608 | Val Loss: 13.5339 Acc: 0.4442                                             \n",
      "Epoch 013 | Train Loss: 24.0565 Acc: 0.3605 | Val Loss: 11.9422 Acc: 0.4442                                             \n",
      "Epoch 014 | Train Loss: 20.8927 Acc: 0.3775 | Val Loss: 10.2489 Acc: 0.4442                                             \n",
      "Epoch 015 | Train Loss: 19.4723 Acc: 0.3735 | Val Loss: 8.5995 Acc: 0.4379                                              \n",
      "Epoch 016 | Train Loss: 18.1384 Acc: 0.3913 | Val Loss: 7.8932 Acc: 0.4433                                              \n",
      "Epoch 017 | Train Loss: 16.7416 Acc: 0.3900 | Val Loss: 6.7521 Acc: 0.4445                                              \n",
      "Epoch 018 | Train Loss: 15.4921 Acc: 0.3974 | Val Loss: 5.8336 Acc: 0.4018                                              \n",
      "Epoch 019 | Train Loss: 14.3046 Acc: 0.3982 | Val Loss: 5.1370 Acc: 0.4349                                              \n",
      "Epoch 020 | Train Loss: 13.6993 Acc: 0.4022 | Val Loss: 4.5898 Acc: 0.4693                                              \n",
      "Epoch 021 | Train Loss: 12.5541 Acc: 0.4092 | Val Loss: 4.4662 Acc: 0.4182                                              \n",
      "Epoch 022 | Train Loss: 11.7495 Acc: 0.4082 | Val Loss: 4.2188 Acc: 0.4579                                              \n",
      "Epoch 023 | Train Loss: 11.3911 Acc: 0.4165 | Val Loss: 3.6143 Acc: 0.4394                                              \n",
      "Epoch 024 | Train Loss: 10.7180 Acc: 0.4176 | Val Loss: 3.2820 Acc: 0.4439                                              \n",
      "Epoch 025 | Train Loss: 9.9538 Acc: 0.4221 | Val Loss: 2.9499 Acc: 0.5024                                               \n",
      "Epoch 026 | Train Loss: 9.6356 Acc: 0.4290 | Val Loss: 2.6836 Acc: 0.5155                                               \n",
      "Epoch 027 | Train Loss: 9.2055 Acc: 0.4285 | Val Loss: 2.7316 Acc: 0.5669                                               \n",
      "Epoch 028 | Train Loss: 8.6269 Acc: 0.4363 | Val Loss: 2.4572 Acc: 0.5173                                               \n",
      "Epoch 029 | Train Loss: 8.1895 Acc: 0.4402 | Val Loss: 2.5933 Acc: 0.5540                                               \n",
      "Epoch 030 | Train Loss: 7.8064 Acc: 0.4387 | Val Loss: 2.5055 Acc: 0.5418                                               \n",
      "Epoch 031 | Train Loss: 7.5696 Acc: 0.4451 | Val Loss: 2.4967 Acc: 0.5609                                               \n",
      "Epoch 032 | Train Loss: 7.3641 Acc: 0.4470 | Val Loss: 2.3490 Acc: 0.5654                                               \n",
      "Epoch 033 | Train Loss: 7.0776 Acc: 0.4479 | Val Loss: 2.3768 Acc: 0.5540                                               \n",
      "Epoch 034 | Train Loss: 6.6830 Acc: 0.4562 | Val Loss: 2.1117 Acc: 0.5573                                               \n",
      "Epoch 035 | Train Loss: 6.6495 Acc: 0.4541 | Val Loss: 2.2017 Acc: 0.5627                                               \n",
      "Epoch 036 | Train Loss: 6.4065 Acc: 0.4556 | Val Loss: 2.1914 Acc: 0.5639                                               \n",
      "Epoch 037 | Train Loss: 6.2663 Acc: 0.4579 | Val Loss: 2.1494 Acc: 0.5719                                               \n",
      "Epoch 038 | Train Loss: 6.0700 Acc: 0.4600 | Val Loss: 1.9674 Acc: 0.5896                                               \n",
      "Epoch 039 | Train Loss: 5.7231 Acc: 0.4604 | Val Loss: 2.1258 Acc: 0.5812                                               \n",
      "Epoch 040 | Train Loss: 5.8489 Acc: 0.4612 | Val Loss: 2.1327 Acc: 0.5699                                               \n",
      "Epoch 041 | Train Loss: 5.3859 Acc: 0.4727 | Val Loss: 2.1605 Acc: 0.5687                                               \n",
      "Epoch 042 | Train Loss: 5.3380 Acc: 0.4809 | Val Loss: 2.2082 Acc: 0.5788                                               \n",
      "Epoch 043 | Train Loss: 5.1053 Acc: 0.4892 | Val Loss: 2.2667 Acc: 0.5860                                               \n",
      "Epoch 044 | Train Loss: 4.8894 Acc: 0.4959 | Val Loss: 2.0717 Acc: 0.5952                                               \n",
      "Epoch 045 | Train Loss: 4.7897 Acc: 0.4914 | Val Loss: 2.2315 Acc: 0.5949                                               \n",
      "Epoch 046 | Train Loss: 4.6682 Acc: 0.4976 | Val Loss: 2.1583 Acc: 0.5845                                               \n",
      "Epoch 047 | Train Loss: 4.5538 Acc: 0.5009 | Val Loss: 2.1102 Acc: 0.6110                                               \n",
      "Epoch 048 | Train Loss: 4.2144 Acc: 0.5147 | Val Loss: 2.2372 Acc: 0.5636                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 64, 'cnn_dropout': 0.28164616764901107, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 96, 'learning_rate': 0.0022527560619391832, 'lstm_dense': 32, 'lstm_hidden_size': 128, 'lstm_layers': 4, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 9.1428 Acc: 0.4218 | Val Loss: 0.9663 Acc: 0.4979                                               \n",
      "Epoch 002 | Train Loss: 1.1151 Acc: 0.4941 | Val Loss: 0.9426 Acc: 0.5104                                               \n",
      "Epoch 003 | Train Loss: 0.9669 Acc: 0.5356 | Val Loss: 0.8905 Acc: 0.5776                                               \n",
      "Epoch 004 | Train Loss: 0.9056 Acc: 0.5647 | Val Loss: 0.8698 Acc: 0.6006                                               \n",
      "Epoch 005 | Train Loss: 0.8351 Acc: 0.5895 | Val Loss: 0.6985 Acc: 0.6591                                               \n",
      "Epoch 006 | Train Loss: 0.7411 Acc: 0.6495 | Val Loss: 0.8684 Acc: 0.5170                                               \n",
      "Epoch 007 | Train Loss: 0.7158 Acc: 0.6654 | Val Loss: 0.6300 Acc: 0.7033                                               \n",
      "Epoch 008 | Train Loss: 0.7000 Acc: 0.6769 | Val Loss: 0.6383 Acc: 0.7236                                               \n",
      "Epoch 009 | Train Loss: 0.6628 Acc: 0.6912 | Val Loss: 0.5836 Acc: 0.7666                                               \n",
      "Epoch 010 | Train Loss: 0.6334 Acc: 0.7101 | Val Loss: 0.5995 Acc: 0.7725                                               \n",
      "Epoch 011 | Train Loss: 0.6055 Acc: 0.7271 | Val Loss: 0.8160 Acc: 0.6221                                               \n",
      "Epoch 012 | Train Loss: 0.5984 Acc: 0.7373 | Val Loss: 0.6774 Acc: 0.7325                                               \n",
      "Epoch 013 | Train Loss: 0.5800 Acc: 0.7460 | Val Loss: 0.5633 Acc: 0.7406                                               \n",
      "Epoch 014 | Train Loss: 0.5687 Acc: 0.7543 | Val Loss: 0.5272 Acc: 0.8137                                               \n",
      "Epoch 015 | Train Loss: 0.5588 Acc: 0.7575 | Val Loss: 0.5684 Acc: 0.6722                                               \n",
      "Epoch 016 | Train Loss: 0.5795 Acc: 0.7465 | Val Loss: 0.7295 Acc: 0.7137                                               \n",
      "Epoch 017 | Train Loss: 0.5692 Acc: 0.7526 | Val Loss: 0.6159 Acc: 0.6845                                               \n",
      "Epoch 018 | Train Loss: 0.5464 Acc: 0.7666 | Val Loss: 0.5840 Acc: 0.7513                                               \n",
      "Epoch 019 | Train Loss: 0.5348 Acc: 0.7691 | Val Loss: 0.6516 Acc: 0.7039                                               \n",
      "Epoch 020 | Train Loss: 0.5509 Acc: 0.7621 | Val Loss: 0.6235 Acc: 0.7382                                               \n",
      "Epoch 021 | Train Loss: 0.5478 Acc: 0.7670 | Val Loss: 0.5473 Acc: 0.7263                                               \n",
      "Epoch 022 | Train Loss: 0.5302 Acc: 0.7813 | Val Loss: 0.6475 Acc: 0.7057                                               \n",
      "Epoch 023 | Train Loss: 0.5276 Acc: 0.7807 | Val Loss: 0.5715 Acc: 0.7093                                               \n",
      "Epoch 024 | Train Loss: 0.5214 Acc: 0.7830 | Val Loss: 0.4567 Acc: 0.8096                                               \n",
      "Epoch 025 | Train Loss: 0.5116 Acc: 0.7897 | Val Loss: 0.5166 Acc: 0.7660                                               \n",
      "Epoch 026 | Train Loss: 0.5027 Acc: 0.7991 | Val Loss: 0.8255 Acc: 0.6415                                               \n",
      "Epoch 027 | Train Loss: 0.4946 Acc: 0.7935 | Val Loss: 0.4244 Acc: 0.8188                                               \n",
      "Epoch 028 | Train Loss: 0.6107 Acc: 0.7666 | Val Loss: 0.6701 Acc: 0.6537                                               \n",
      "Epoch 029 | Train Loss: 0.5626 Acc: 0.7624 | Val Loss: 0.7320 Acc: 0.6496                                               \n",
      "Epoch 030 | Train Loss: 0.5272 Acc: 0.7818 | Val Loss: 0.4208 Acc: 0.8054                                               \n",
      "Epoch 031 | Train Loss: 0.4997 Acc: 0.7964 | Val Loss: 0.7282 Acc: 0.7140                                               \n",
      "Epoch 032 | Train Loss: 0.5077 Acc: 0.7881 | Val Loss: 0.7566 Acc: 0.6287                                               \n",
      "Epoch 033 | Train Loss: 0.4954 Acc: 0.7940 | Val Loss: 0.4092 Acc: 0.8549                                               \n",
      "Epoch 034 | Train Loss: 0.5110 Acc: 0.7873 | Val Loss: 0.4987 Acc: 0.8030                                               \n",
      "Epoch 035 | Train Loss: 0.5184 Acc: 0.7857 | Val Loss: 0.6320 Acc: 0.7773                                               \n",
      "Epoch 036 | Train Loss: 0.5225 Acc: 0.7850 | Val Loss: 0.5954 Acc: 0.7376                                               \n",
      "Epoch 037 | Train Loss: 0.5277 Acc: 0.7857 | Val Loss: 0.7224 Acc: 0.6672                                               \n",
      "Epoch 038 | Train Loss: 0.5225 Acc: 0.7930 | Val Loss: 0.4970 Acc: 0.7370                                               \n",
      "Epoch 039 | Train Loss: 0.4978 Acc: 0.7953 | Val Loss: 0.9738 Acc: 0.7131                                               \n",
      "Epoch 040 | Train Loss: 0.5248 Acc: 0.7913 | Val Loss: 0.7774 Acc: 0.6893                                               \n",
      "Epoch 041 | Train Loss: 0.5466 Acc: 0.7880 | Val Loss: 0.4335 Acc: 0.8209                                               \n",
      "Epoch 042 | Train Loss: 0.4815 Acc: 0.8041 | Val Loss: 0.6701 Acc: 0.7149                                               \n",
      "Epoch 043 | Train Loss: 0.5282 Acc: 0.7874 | Val Loss: 0.4607 Acc: 0.8200                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 256, 'cnn_dropout': 0.6882594591687513, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 48, 'cnn_kernels_2': 64, 'learning_rate': 0.005079174621535214, 'lstm_dense': 256, 'lstm_hidden_size': 128, 'lstm_layers': 4, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 13.4542 Acc: 0.3838 | Val Loss: 1.1702 Acc: 0.4421                                              \n",
      "Epoch 002 | Train Loss: 1.1832 Acc: 0.4568 | Val Loss: 0.9557 Acc: 0.5251                                               \n",
      "Epoch 003 | Train Loss: 0.9800 Acc: 0.4935 | Val Loss: 0.8799 Acc: 0.5406                                               \n",
      "Epoch 004 | Train Loss: 0.9296 Acc: 0.5281 | Val Loss: 0.8486 Acc: 0.6000                                               \n",
      "Epoch 005 | Train Loss: 0.9048 Acc: 0.5493 | Val Loss: 0.9047 Acc: 0.4666                                               \n",
      "Epoch 006 | Train Loss: 0.8938 Acc: 0.5409 | Val Loss: 0.8287 Acc: 0.5770                                               \n",
      "Epoch 007 | Train Loss: 0.8745 Acc: 0.5527 | Val Loss: 0.7811 Acc: 0.6227                                               \n",
      "Epoch 008 | Train Loss: 0.8844 Acc: 0.5462 | Val Loss: 0.8779 Acc: 0.5266                                               \n",
      "Epoch 009 | Train Loss: 0.8773 Acc: 0.5584 | Val Loss: 0.8331 Acc: 0.4600                                               \n",
      "Epoch 010 | Train Loss: 0.8611 Acc: 0.5806 | Val Loss: 0.7810 Acc: 0.6516                                               \n",
      "Epoch 011 | Train Loss: 0.8749 Acc: 0.5726 | Val Loss: 0.8443 Acc: 0.5651                                               \n",
      "Epoch 012 | Train Loss: 0.8309 Acc: 0.5840 | Val Loss: 0.8283 Acc: 0.6260                                               \n",
      "Epoch 013 | Train Loss: 0.8436 Acc: 0.5888 | Val Loss: 0.7790 Acc: 0.5997                                               \n",
      "Epoch 014 | Train Loss: 0.9102 Acc: 0.5747 | Val Loss: 0.8479 Acc: 0.5021                                               \n",
      "Epoch 015 | Train Loss: 0.8783 Acc: 0.5854 | Val Loss: 0.7272 Acc: 0.6925                                               \n",
      "Epoch 016 | Train Loss: 0.8346 Acc: 0.5999 | Val Loss: 0.7443 Acc: 0.6666                                               \n",
      "Epoch 017 | Train Loss: 0.8875 Acc: 0.5732 | Val Loss: 1.5068 Acc: 0.4534                                               \n",
      "Epoch 018 | Train Loss: 0.9830 Acc: 0.4792 | Val Loss: 0.8599 Acc: 0.5752                                               \n",
      "Epoch 019 | Train Loss: 0.9382 Acc: 0.5109 | Val Loss: 0.9092 Acc: 0.4893                                               \n",
      "Epoch 020 | Train Loss: 0.9201 Acc: 0.5356 | Val Loss: 0.9309 Acc: 0.5504                                               \n",
      "Epoch 021 | Train Loss: 0.9057 Acc: 0.5496 | Val Loss: 0.8517 Acc: 0.5806                                               \n",
      "Epoch 022 | Train Loss: 0.9323 Acc: 0.5269 | Val Loss: 0.8487 Acc: 0.5752                                               \n",
      "Epoch 023 | Train Loss: 0.9296 Acc: 0.5330 | Val Loss: 0.8091 Acc: 0.5931                                               \n",
      "Epoch 024 | Train Loss: 0.9247 Acc: 0.5409 | Val Loss: 0.8649 Acc: 0.5734                                               \n",
      "Epoch 025 | Train Loss: 0.9570 Acc: 0.5355 | Val Loss: 0.8777 Acc: 0.5952                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 36, 'cnn_dense': 32, 'cnn_dropout': 0.46930979450606436, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 32, 'cnn_kernels_2': 64, 'learning_rate': 0.003857894055202184, 'lstm_dense': 128, 'lstm_hidden_size': 64, 'lstm_layers': 3, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 8.0592 Acc: 0.4601 | Val Loss: 0.8808 Acc: 0.5048                                               \n",
      "Epoch 002 | Train Loss: 0.9067 Acc: 0.5570 | Val Loss: 1.1050 Acc: 0.5239                                               \n",
      "Epoch 003 | Train Loss: 0.8522 Acc: 0.5812 | Val Loss: 1.1126 Acc: 0.4561                                               \n",
      "Epoch 004 | Train Loss: 0.8640 Acc: 0.5703 | Val Loss: 0.8376 Acc: 0.6155                                               \n",
      "Epoch 005 | Train Loss: 0.8974 Acc: 0.5714 | Val Loss: 0.9419 Acc: 0.5284                                               \n",
      "Epoch 006 | Train Loss: 0.8633 Acc: 0.5691 | Val Loss: 0.7770 Acc: 0.6516                                               \n",
      "Epoch 007 | Train Loss: 0.8639 Acc: 0.5815 | Val Loss: 0.9470 Acc: 0.4934                                               \n",
      "Epoch 008 | Train Loss: 0.8554 Acc: 0.5797 | Val Loss: 0.8863 Acc: 0.5328                                               \n",
      "Epoch 009 | Train Loss: 0.8668 Acc: 0.5686 | Val Loss: 0.9512 Acc: 0.4534                                               \n",
      "Epoch 010 | Train Loss: 0.9567 Acc: 0.5209 | Val Loss: 0.7632 Acc: 0.6006                                               \n",
      "Epoch 011 | Train Loss: 0.9234 Acc: 0.5540 | Val Loss: 0.9995 Acc: 0.4899                                               \n",
      "Epoch 012 | Train Loss: 0.9015 Acc: 0.5565 | Val Loss: 0.8024 Acc: 0.5845                                               \n",
      "Epoch 013 | Train Loss: 0.9211 Acc: 0.5533 | Val Loss: 0.9218 Acc: 0.4284                                               \n",
      "Epoch 014 | Train Loss: 0.9498 Acc: 0.5415 | Val Loss: 0.9910 Acc: 0.5269                                               \n",
      "Epoch 015 | Train Loss: 1.0021 Acc: 0.4982 | Val Loss: 1.0428 Acc: 0.4687                                               \n",
      "Epoch 016 | Train Loss: 0.9314 Acc: 0.5218 | Val Loss: 0.9180 Acc: 0.4376                                               \n",
      "Epoch 017 | Train Loss: 0.9280 Acc: 0.5170 | Val Loss: 0.9393 Acc: 0.5087                                               \n",
      "Epoch 018 | Train Loss: 0.9547 Acc: 0.5032 | Val Loss: 0.8424 Acc: 0.5340                                               \n",
      "Epoch 019 | Train Loss: 0.9679 Acc: 0.5200 | Val Loss: 0.9268 Acc: 0.4958                                               \n",
      "Epoch 020 | Train Loss: 0.9275 Acc: 0.5244 | Val Loss: 0.8536 Acc: 0.5743                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 256, 'cnn_dropout': 0.4306485570106769, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 32, 'cnn_kernels_2': 16, 'learning_rate': 0.0010090421955965996, 'lstm_dense': 32, 'lstm_hidden_size': 96, 'lstm_layers': 1, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 10.2846 Acc: 0.4575 | Val Loss: 0.8676 Acc: 0.6451                                              \n",
      "Epoch 002 | Train Loss: 1.8802 Acc: 0.5385 | Val Loss: 1.2995 Acc: 0.5884                                               \n",
      "Epoch 003 | Train Loss: 1.0808 Acc: 0.6018 | Val Loss: 0.6789 Acc: 0.7224                                               \n",
      "Epoch 004 | Train Loss: 0.8553 Acc: 0.6390 | Val Loss: 1.5295 Acc: 0.5794                                               \n",
      "Epoch 005 | Train Loss: 0.7754 Acc: 0.6655 | Val Loss: 0.5756 Acc: 0.7048                                               \n",
      "Epoch 006 | Train Loss: 0.6945 Acc: 0.6967 | Val Loss: 0.5617 Acc: 0.7734                                               \n",
      "Epoch 007 | Train Loss: 0.6632 Acc: 0.7114 | Val Loss: 0.8187 Acc: 0.7131                                               \n",
      "Epoch 008 | Train Loss: 0.6120 Acc: 0.7288 | Val Loss: 0.4913 Acc: 0.7531                                               \n",
      "Epoch 009 | Train Loss: 0.5685 Acc: 0.7528 | Val Loss: 0.6142 Acc: 0.7561                                               \n",
      "Epoch 010 | Train Loss: 0.5563 Acc: 0.7639 | Val Loss: 0.5217 Acc: 0.7534                                               \n",
      "Epoch 011 | Train Loss: 0.5329 Acc: 0.7725 | Val Loss: 0.4614 Acc: 0.8134                                               \n",
      "Epoch 012 | Train Loss: 0.5232 Acc: 0.7757 | Val Loss: 0.6218 Acc: 0.7087                                               \n",
      "Epoch 013 | Train Loss: 0.5025 Acc: 0.7861 | Val Loss: 0.3687 Acc: 0.8504                                               \n",
      "Epoch 014 | Train Loss: 0.4978 Acc: 0.7945 | Val Loss: 0.3747 Acc: 0.8734                                               \n",
      "Epoch 015 | Train Loss: 0.4673 Acc: 0.8057 | Val Loss: 0.3827 Acc: 0.8340                                               \n",
      "Epoch 016 | Train Loss: 0.4764 Acc: 0.8064 | Val Loss: 0.3199 Acc: 0.8934                                               \n",
      "Epoch 017 | Train Loss: 0.4650 Acc: 0.8107 | Val Loss: 0.3707 Acc: 0.8352                                               \n",
      "Epoch 018 | Train Loss: 0.4595 Acc: 0.8095 | Val Loss: 0.7754 Acc: 0.7403                                               \n",
      "Epoch 019 | Train Loss: 0.5116 Acc: 0.7951 | Val Loss: 0.3870 Acc: 0.8361                                               \n",
      "Epoch 020 | Train Loss: 0.4891 Acc: 0.8019 | Val Loss: 0.5654 Acc: 0.7451                                               \n",
      "Epoch 021 | Train Loss: 0.4673 Acc: 0.8132 | Val Loss: 0.5866 Acc: 0.7866                                               \n",
      "Epoch 022 | Train Loss: 0.4547 Acc: 0.8174 | Val Loss: 0.4963 Acc: 0.8122                                               \n",
      "Epoch 023 | Train Loss: 0.4418 Acc: 0.8270 | Val Loss: 0.4482 Acc: 0.8385                                               \n",
      "Epoch 024 | Train Loss: 0.4178 Acc: 0.8403 | Val Loss: 0.3143 Acc: 0.8812                                               \n",
      "Epoch 025 | Train Loss: 0.3895 Acc: 0.8495 | Val Loss: 0.6651 Acc: 0.7884                                               \n",
      "Epoch 026 | Train Loss: 0.3685 Acc: 0.8594 | Val Loss: 0.3261 Acc: 0.8597                                               \n",
      "Epoch 027 | Train Loss: 0.3519 Acc: 0.8651 | Val Loss: 0.2969 Acc: 0.8743                                               \n",
      "Epoch 028 | Train Loss: 0.3298 Acc: 0.8761 | Val Loss: 0.2523 Acc: 0.8893                                               \n",
      "Epoch 029 | Train Loss: 0.3096 Acc: 0.8820 | Val Loss: 0.2456 Acc: 0.9009                                               \n",
      "Epoch 030 | Train Loss: 0.2935 Acc: 0.8868 | Val Loss: 0.1753 Acc: 0.9325                                               \n",
      "Epoch 031 | Train Loss: 0.2900 Acc: 0.8921 | Val Loss: 0.2076 Acc: 0.9158                                               \n",
      "Epoch 032 | Train Loss: 0.2641 Acc: 0.8982 | Val Loss: 0.2264 Acc: 0.9042                                               \n",
      "Epoch 033 | Train Loss: 0.2468 Acc: 0.9098 | Val Loss: 0.1838 Acc: 0.9304                                               \n",
      "Epoch 034 | Train Loss: 0.2456 Acc: 0.9086 | Val Loss: 0.2306 Acc: 0.9066                                               \n",
      "Epoch 035 | Train Loss: 0.2228 Acc: 0.9178 | Val Loss: 0.1983 Acc: 0.9176                                               \n",
      "Epoch 036 | Train Loss: 0.2114 Acc: 0.9266 | Val Loss: 0.1787 Acc: 0.9296                                               \n",
      "Epoch 037 | Train Loss: 0.2156 Acc: 0.9216 | Val Loss: 0.1990 Acc: 0.9221                                               \n",
      "Epoch 038 | Train Loss: 0.2054 Acc: 0.9267 | Val Loss: 0.1762 Acc: 0.9275                                               \n",
      "Epoch 039 | Train Loss: 0.1909 Acc: 0.9310 | Val Loss: 0.2108 Acc: 0.9185                                               \n",
      "Epoch 040 | Train Loss: 0.2010 Acc: 0.9298 | Val Loss: 0.1736 Acc: 0.9373                                               \n",
      "Epoch 041 | Train Loss: 0.1782 Acc: 0.9366 | Val Loss: 0.1923 Acc: 0.9215                                               \n",
      "Epoch 042 | Train Loss: 0.1776 Acc: 0.9391 | Val Loss: 0.1228 Acc: 0.9531                                               \n",
      "Epoch 043 | Train Loss: 0.1772 Acc: 0.9374 | Val Loss: 0.1413 Acc: 0.9400                                               \n",
      "Epoch 044 | Train Loss: 0.1579 Acc: 0.9423 | Val Loss: 0.1757 Acc: 0.9439                                               \n",
      "Epoch 045 | Train Loss: 0.1581 Acc: 0.9435 | Val Loss: 0.1017 Acc: 0.9669                                               \n",
      "Epoch 046 | Train Loss: 0.1489 Acc: 0.9487 | Val Loss: 0.2403 Acc: 0.9218                                               \n",
      "Epoch 047 | Train Loss: 0.1494 Acc: 0.9510 | Val Loss: 0.0789 Acc: 0.9764                                               \n",
      "Epoch 048 | Train Loss: 0.1439 Acc: 0.9513 | Val Loss: 0.0842 Acc: 0.9707                                               \n",
      "Epoch 049 | Train Loss: 0.1429 Acc: 0.9506 | Val Loss: 0.2720 Acc: 0.9048                                               \n",
      "Epoch 050 | Train Loss: 0.1370 Acc: 0.9538 | Val Loss: 0.1455 Acc: 0.9472                                               \n",
      "Epoch 051 | Train Loss: 0.1327 Acc: 0.9564 | Val Loss: 0.0877 Acc: 0.9669                                               \n",
      "Epoch 052 | Train Loss: 0.1150 Acc: 0.9626 | Val Loss: 0.1598 Acc: 0.9487                                               \n",
      "Epoch 053 | Train Loss: 0.1226 Acc: 0.9585 | Val Loss: 0.1248 Acc: 0.9591                                               \n",
      "Epoch 054 | Train Loss: 0.1278 Acc: 0.9566 | Val Loss: 0.1016 Acc: 0.9627                                               \n",
      "Epoch 055 | Train Loss: 0.1161 Acc: 0.9599 | Val Loss: 0.0973 Acc: 0.9672                                               \n",
      "Epoch 056 | Train Loss: 0.1129 Acc: 0.9623 | Val Loss: 0.1252 Acc: 0.9594                                               \n",
      "Epoch 057 | Train Loss: 0.1099 Acc: 0.9632 | Val Loss: 0.1744 Acc: 0.9400                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 256, 'cnn_dropout': 0.2774446397100795, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 48, 'cnn_kernels_2': 16, 'learning_rate': 0.0017359711969226466, 'lstm_dense': 64, 'lstm_hidden_size': 32, 'lstm_layers': 1, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 11.6610 Acc: 0.4346 | Val Loss: 1.3368 Acc: 0.4866                                              \n",
      "Epoch 002 | Train Loss: 1.7938 Acc: 0.5161 | Val Loss: 0.8729 Acc: 0.5863                                               \n",
      "Epoch 003 | Train Loss: 1.0252 Acc: 0.6047 | Val Loss: 0.9367 Acc: 0.6328                                               \n",
      "Epoch 004 | Train Loss: 0.8631 Acc: 0.6431 | Val Loss: 0.8503 Acc: 0.6048                                               \n",
      "Epoch 005 | Train Loss: 0.7565 Acc: 0.6766 | Val Loss: 0.5797 Acc: 0.7319                                               \n",
      "Epoch 006 | Train Loss: 0.7099 Acc: 0.6880 | Val Loss: 0.7314 Acc: 0.6704                                               \n",
      "Epoch 007 | Train Loss: 0.6993 Acc: 0.6971 | Val Loss: 0.6479 Acc: 0.7218                                               \n",
      "Epoch 008 | Train Loss: 0.6600 Acc: 0.7174 | Val Loss: 0.5629 Acc: 0.7481                                               \n",
      "Epoch 009 | Train Loss: 0.6021 Acc: 0.7380 | Val Loss: 0.5723 Acc: 0.7385                                               \n",
      "Epoch 010 | Train Loss: 0.6073 Acc: 0.7313 | Val Loss: 0.6564 Acc: 0.6836                                               \n",
      "Epoch 011 | Train Loss: 0.6152 Acc: 0.7254 | Val Loss: 0.5119 Acc: 0.7585                                               \n",
      "Epoch 012 | Train Loss: 0.5794 Acc: 0.7421 | Val Loss: 0.4976 Acc: 0.7875                                               \n",
      "Epoch 013 | Train Loss: 0.5855 Acc: 0.7454 | Val Loss: 0.4689 Acc: 0.8316                                               \n",
      "Epoch 014 | Train Loss: 0.5552 Acc: 0.7528 | Val Loss: 0.5005 Acc: 0.7827                                               \n",
      "Epoch 015 | Train Loss: 0.5786 Acc: 0.7510 | Val Loss: 0.5600 Acc: 0.7609                                               \n",
      "Epoch 016 | Train Loss: 0.5844 Acc: 0.7430 | Val Loss: 0.4234 Acc: 0.8269                                               \n",
      "Epoch 017 | Train Loss: 0.5677 Acc: 0.7513 | Val Loss: 0.4339 Acc: 0.8310                                               \n",
      "Epoch 018 | Train Loss: 0.5697 Acc: 0.7496 | Val Loss: 0.4728 Acc: 0.8012                                               \n",
      "Epoch 019 | Train Loss: 0.5750 Acc: 0.7452 | Val Loss: 0.4071 Acc: 0.8236                                               \n",
      "Epoch 020 | Train Loss: 0.5336 Acc: 0.7697 | Val Loss: 0.4257 Acc: 0.8337                                               \n",
      "Epoch 021 | Train Loss: 0.5537 Acc: 0.7607 | Val Loss: 0.5509 Acc: 0.7322                                               \n",
      "Epoch 022 | Train Loss: 0.5679 Acc: 0.7548 | Val Loss: 0.4349 Acc: 0.8018                                               \n",
      "Epoch 023 | Train Loss: 0.5346 Acc: 0.7711 | Val Loss: 0.4469 Acc: 0.8039                                               \n",
      "Epoch 024 | Train Loss: 0.5220 Acc: 0.7693 | Val Loss: 0.4346 Acc: 0.7970                                               \n",
      "Epoch 025 | Train Loss: 0.5124 Acc: 0.7726 | Val Loss: 0.4279 Acc: 0.8164                                               \n",
      "Epoch 026 | Train Loss: 0.5567 Acc: 0.7574 | Val Loss: 0.4678 Acc: 0.7973                                               \n",
      "Epoch 027 | Train Loss: 0.5039 Acc: 0.7763 | Val Loss: 0.4247 Acc: 0.8185                                               \n",
      "Epoch 028 | Train Loss: 0.5284 Acc: 0.7645 | Val Loss: 0.4543 Acc: 0.7600                                               \n",
      "Epoch 029 | Train Loss: 0.5102 Acc: 0.7776 | Val Loss: 0.4037 Acc: 0.8239                                               \n",
      "Epoch 030 | Train Loss: 0.4808 Acc: 0.7895 | Val Loss: 0.4408 Acc: 0.7946                                               \n",
      "Epoch 031 | Train Loss: 0.5178 Acc: 0.7763 | Val Loss: 0.4203 Acc: 0.8293                                               \n",
      "Epoch 032 | Train Loss: 0.5276 Acc: 0.7722 | Val Loss: 0.3406 Acc: 0.8490                                               \n",
      "Epoch 033 | Train Loss: 0.5177 Acc: 0.7708 | Val Loss: 0.4918 Acc: 0.7528                                               \n",
      "Epoch 034 | Train Loss: 0.5004 Acc: 0.7806 | Val Loss: 0.3920 Acc: 0.7851                                               \n",
      "Epoch 035 | Train Loss: 0.5284 Acc: 0.7721 | Val Loss: 0.3727 Acc: 0.8230                                               \n",
      "Epoch 036 | Train Loss: 0.5242 Acc: 0.7668 | Val Loss: 0.3898 Acc: 0.8364                                               \n",
      "Epoch 037 | Train Loss: 0.4921 Acc: 0.7792 | Val Loss: 0.4058 Acc: 0.7955                                               \n",
      "Epoch 038 | Train Loss: 0.4871 Acc: 0.7836 | Val Loss: 0.4174 Acc: 0.8275                                               \n",
      "Epoch 039 | Train Loss: 0.5023 Acc: 0.7845 | Val Loss: 0.3990 Acc: 0.8388                                               \n",
      "Epoch 040 | Train Loss: 0.5042 Acc: 0.7740 | Val Loss: 0.5405 Acc: 0.7854                                               \n",
      "Epoch 041 | Train Loss: 0.5089 Acc: 0.7774 | Val Loss: 0.4833 Acc: 0.7958                                               \n",
      "Epoch 042 | Train Loss: 0.4814 Acc: 0.7858 | Val Loss: 0.4000 Acc: 0.7890                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 128, 'cnn_dropout': 0.12478153573360602, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 32, 'cnn_kernels_2': 32, 'learning_rate': 0.00043375596893201434, 'lstm_dense': 64, 'lstm_hidden_size': 128, 'lstm_layers': 2, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 20.4807 Acc: 0.4002 | Val Loss: 1.2095 Acc: 0.4421                                              \n",
      "Epoch 002 | Train Loss: 1.1803 Acc: 0.4372 | Val Loss: 1.3505 Acc: 0.4421                                               \n",
      "Epoch 003 | Train Loss: 1.3249 Acc: 0.4422 | Val Loss: 1.3075 Acc: 0.4421                                               \n",
      "Epoch 004 | Train Loss: 1.2978 Acc: 0.4422 | Val Loss: 1.2868 Acc: 0.4421                                               \n",
      "Epoch 005 | Train Loss: 1.2798 Acc: 0.4422 | Val Loss: 1.2715 Acc: 0.4421                                               \n",
      "Epoch 006 | Train Loss: 1.2665 Acc: 0.4422 | Val Loss: 1.2598 Acc: 0.4421                                               \n",
      "Epoch 007 | Train Loss: 1.2559 Acc: 0.4422 | Val Loss: 1.2504 Acc: 0.4421                                               \n",
      "Epoch 008 | Train Loss: 1.2471 Acc: 0.4422 | Val Loss: 1.2410 Acc: 0.4421                                               \n",
      "Epoch 009 | Train Loss: 1.2391 Acc: 0.4422 | Val Loss: 1.2328 Acc: 0.4421                                               \n",
      "Epoch 010 | Train Loss: 1.2311 Acc: 0.4422 | Val Loss: 1.2253 Acc: 0.4421                                               \n",
      "Epoch 011 | Train Loss: 1.2234 Acc: 0.4422 | Val Loss: 1.2169 Acc: 0.4421                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 128, 'cnn_dropout': 0.0967106519903759, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 64, 'learning_rate': 3.808414606729071e-05, 'lstm_dense': 64, 'lstm_hidden_size': 64, 'lstm_layers': 3, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 80.3112 Acc: 0.3175 | Val Loss: 44.4616 Acc: 0.4510                                             \n",
      "Epoch 002 | Train Loss: 51.7682 Acc: 0.3463 | Val Loss: 27.4105 Acc: 0.4457                                             \n",
      "Epoch 003 | Train Loss: 38.7108 Acc: 0.3608 | Val Loss: 15.8013 Acc: 0.4457                                             \n",
      "Epoch 004 | Train Loss: 29.5317 Acc: 0.3712 | Val Loss: 10.2413 Acc: 0.4128                                             \n",
      "Epoch 005 | Train Loss: 23.4877 Acc: 0.3790 | Val Loss: 8.6215 Acc: 0.3746                                              \n",
      "Epoch 006 | Train Loss: 19.3205 Acc: 0.3889 | Val Loss: 7.0633 Acc: 0.3764                                              \n",
      "Epoch 007 | Train Loss: 16.9865 Acc: 0.3981 | Val Loss: 6.5701 Acc: 0.3510                                              \n",
      "Epoch 008 | Train Loss: 15.0301 Acc: 0.3944 | Val Loss: 5.4645 Acc: 0.3436                                              \n",
      "Epoch 009 | Train Loss: 13.0761 Acc: 0.4064 | Val Loss: 4.0886 Acc: 0.3842                                              \n",
      "Epoch 010 | Train Loss: 11.9890 Acc: 0.4102 | Val Loss: 3.2243 Acc: 0.4313                                              \n",
      "Epoch 011 | Train Loss: 10.8045 Acc: 0.4179 | Val Loss: 2.8104 Acc: 0.4570                                              \n",
      "Epoch 012 | Train Loss: 9.8068 Acc: 0.4280 | Val Loss: 2.3038 Acc: 0.5278                                               \n",
      "Epoch 013 | Train Loss: 9.0816 Acc: 0.4288 | Val Loss: 3.3300 Acc: 0.4415                                               \n",
      "Epoch 014 | Train Loss: 8.1949 Acc: 0.4413 | Val Loss: 1.8757 Acc: 0.6194                                               \n",
      "Epoch 015 | Train Loss: 7.7497 Acc: 0.4473 | Val Loss: 1.9034 Acc: 0.5484                                               \n",
      "Epoch 016 | Train Loss: 7.1177 Acc: 0.4540 | Val Loss: 1.7085 Acc: 0.6558                                               \n",
      "Epoch 017 | Train Loss: 6.5945 Acc: 0.4556 | Val Loss: 1.9037 Acc: 0.6272                                               \n",
      "Epoch 018 | Train Loss: 6.1066 Acc: 0.4653 | Val Loss: 1.6092 Acc: 0.6436                                               \n",
      "Epoch 019 | Train Loss: 5.8726 Acc: 0.4728 | Val Loss: 1.7747 Acc: 0.6066                                               \n",
      "Epoch 020 | Train Loss: 5.3517 Acc: 0.4770 | Val Loss: 1.4519 Acc: 0.6818                                               \n",
      "Epoch 021 | Train Loss: 4.9973 Acc: 0.4958 | Val Loss: 1.6027 Acc: 0.6687                                               \n",
      "Epoch 022 | Train Loss: 4.7098 Acc: 0.4823 | Val Loss: 1.5490 Acc: 0.6648                                               \n",
      "Epoch 023 | Train Loss: 4.6339 Acc: 0.4979 | Val Loss: 1.4424 Acc: 0.6857                                               \n",
      "Epoch 024 | Train Loss: 4.3119 Acc: 0.4988 | Val Loss: 1.4848 Acc: 0.6734                                               \n",
      "Epoch 025 | Train Loss: 4.0131 Acc: 0.5082 | Val Loss: 1.4376 Acc: 0.6558                                               \n",
      "Epoch 026 | Train Loss: 3.6895 Acc: 0.5215 | Val Loss: 1.3875 Acc: 0.6785                                               \n",
      "Epoch 027 | Train Loss: 3.4781 Acc: 0.5234 | Val Loss: 1.4109 Acc: 0.6761                                               \n",
      "Epoch 028 | Train Loss: 3.2743 Acc: 0.5347 | Val Loss: 1.3659 Acc: 0.6878                                               \n",
      "Epoch 029 | Train Loss: 3.2349 Acc: 0.5387 | Val Loss: 1.2981 Acc: 0.7033                                               \n",
      "Epoch 030 | Train Loss: 3.0300 Acc: 0.5463 | Val Loss: 1.6419 Acc: 0.6672                                               \n",
      "Epoch 031 | Train Loss: 2.8748 Acc: 0.5520 | Val Loss: 1.4196 Acc: 0.6839                                               \n",
      "Epoch 032 | Train Loss: 2.6712 Acc: 0.5640 | Val Loss: 1.1982 Acc: 0.6937                                               \n",
      "Epoch 033 | Train Loss: 2.5216 Acc: 0.5717 | Val Loss: 1.3416 Acc: 0.6934                                               \n",
      "Epoch 034 | Train Loss: 2.3486 Acc: 0.5883 | Val Loss: 1.1063 Acc: 0.7349                                               \n",
      "Epoch 035 | Train Loss: 2.2575 Acc: 0.6012 | Val Loss: 1.0591 Acc: 0.7346                                               \n",
      "Epoch 036 | Train Loss: 2.1066 Acc: 0.6072 | Val Loss: 1.1913 Acc: 0.7113                                               \n",
      "Epoch 037 | Train Loss: 1.9989 Acc: 0.6178 | Val Loss: 0.8457 Acc: 0.7740                                               \n",
      "Epoch 038 | Train Loss: 1.8806 Acc: 0.6265 | Val Loss: 1.0295 Acc: 0.6982                                               \n",
      "Epoch 039 | Train Loss: 1.8326 Acc: 0.6315 | Val Loss: 0.8791 Acc: 0.7672                                               \n",
      "Epoch 040 | Train Loss: 1.7355 Acc: 0.6482 | Val Loss: 1.0023 Acc: 0.7281                                               \n",
      "Epoch 041 | Train Loss: 1.6316 Acc: 0.6575 | Val Loss: 0.7512 Acc: 0.7949                                               \n",
      "Epoch 042 | Train Loss: 1.5527 Acc: 0.6681 | Val Loss: 0.8325 Acc: 0.7785                                               \n",
      "Epoch 043 | Train Loss: 1.4775 Acc: 0.6755 | Val Loss: 0.8209 Acc: 0.7785                                               \n",
      "Epoch 044 | Train Loss: 1.4049 Acc: 0.6806 | Val Loss: 0.8433 Acc: 0.7678                                               \n",
      "Epoch 045 | Train Loss: 1.3282 Acc: 0.6956 | Val Loss: 0.6513 Acc: 0.8143                                               \n",
      "Epoch 046 | Train Loss: 1.2641 Acc: 0.7059 | Val Loss: 0.7478 Acc: 0.7487                                               \n",
      "Epoch 047 | Train Loss: 1.2223 Acc: 0.7178 | Val Loss: 0.6937 Acc: 0.8101                                               \n",
      "Epoch 048 | Train Loss: 1.1800 Acc: 0.7195 | Val Loss: 0.6801 Acc: 0.8093                                               \n",
      "Epoch 049 | Train Loss: 1.0969 Acc: 0.7348 | Val Loss: 0.6522 Acc: 0.7970                                               \n",
      "Epoch 050 | Train Loss: 1.0714 Acc: 0.7374 | Val Loss: 0.6781 Acc: 0.7872                                               \n",
      "Epoch 051 | Train Loss: 1.0003 Acc: 0.7461 | Val Loss: 0.6099 Acc: 0.8042                                               \n",
      "Epoch 052 | Train Loss: 0.9784 Acc: 0.7507 | Val Loss: 0.5227 Acc: 0.8507                                               \n",
      "Epoch 053 | Train Loss: 0.9623 Acc: 0.7573 | Val Loss: 0.6064 Acc: 0.8221                                               \n",
      "Epoch 054 | Train Loss: 0.9085 Acc: 0.7698 | Val Loss: 0.5823 Acc: 0.8215                                               \n",
      "Epoch 055 | Train Loss: 0.8635 Acc: 0.7767 | Val Loss: 0.6011 Acc: 0.8260                                               \n",
      "Epoch 056 | Train Loss: 0.8150 Acc: 0.7796 | Val Loss: 0.5057 Acc: 0.8543                                               \n",
      "Epoch 057 | Train Loss: 0.7820 Acc: 0.7895 | Val Loss: 0.6787 Acc: 0.8158                                               \n",
      "Epoch 058 | Train Loss: 0.7395 Acc: 0.8028 | Val Loss: 0.5878 Acc: 0.8391                                               \n",
      "Epoch 059 | Train Loss: 0.6964 Acc: 0.8095 | Val Loss: 0.4464 Acc: 0.8707                                               \n",
      "Epoch 060 | Train Loss: 0.6740 Acc: 0.8122 | Val Loss: 0.6618 Acc: 0.8107                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 64, 'cnn_dropout': 0.06893664528241467, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 48, 'cnn_kernels_2': 16, 'learning_rate': 0.0021905826907246194, 'lstm_dense': 32, 'lstm_hidden_size': 128, 'lstm_layers': 3, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 7.5016 Acc: 0.4611 | Val Loss: 0.8902 Acc: 0.6421                                               \n",
      "Epoch 002 | Train Loss: 1.2068 Acc: 0.5611 | Val Loss: 1.0972 Acc: 0.5922                                               \n",
      "Epoch 003 | Train Loss: 0.8580 Acc: 0.6298 | Val Loss: 0.6951 Acc: 0.6704                                               \n",
      "Epoch 004 | Train Loss: 0.7526 Acc: 0.6672 | Val Loss: 0.6149 Acc: 0.7346                                               \n",
      "Epoch 005 | Train Loss: 0.7027 Acc: 0.6889 | Val Loss: 0.6369 Acc: 0.7337                                               \n",
      "Epoch 006 | Train Loss: 0.6525 Acc: 0.7068 | Val Loss: 0.5711 Acc: 0.7564                                               \n",
      "Epoch 007 | Train Loss: 0.6419 Acc: 0.7174 | Val Loss: 0.6533 Acc: 0.6869                                               \n",
      "Epoch 008 | Train Loss: 0.5957 Acc: 0.7378 | Val Loss: 0.6005 Acc: 0.7475                                               \n",
      "Epoch 009 | Train Loss: 0.5793 Acc: 0.7465 | Val Loss: 0.5431 Acc: 0.7510                                               \n",
      "Epoch 010 | Train Loss: 0.5671 Acc: 0.7486 | Val Loss: 0.6080 Acc: 0.7113                                               \n",
      "Epoch 011 | Train Loss: 0.5858 Acc: 0.7456 | Val Loss: 0.5872 Acc: 0.7430                                               \n",
      "Epoch 012 | Train Loss: 0.6120 Acc: 0.7259 | Val Loss: 0.6250 Acc: 0.7322                                               \n",
      "Epoch 013 | Train Loss: 0.6076 Acc: 0.7333 | Val Loss: 0.5890 Acc: 0.7293                                               \n",
      "Epoch 014 | Train Loss: 0.5750 Acc: 0.7459 | Val Loss: 0.6735 Acc: 0.7540                                               \n",
      "Epoch 015 | Train Loss: 0.5521 Acc: 0.7609 | Val Loss: 0.6551 Acc: 0.7316                                               \n",
      "Epoch 016 | Train Loss: 0.5907 Acc: 0.7419 | Val Loss: 0.8922 Acc: 0.6737                                               \n",
      "Epoch 017 | Train Loss: 0.6032 Acc: 0.7336 | Val Loss: 0.6907 Acc: 0.7075                                               \n",
      "Epoch 018 | Train Loss: 0.5855 Acc: 0.7417 | Val Loss: 0.8128 Acc: 0.7122                                               \n",
      "Epoch 019 | Train Loss: 0.5526 Acc: 0.7558 | Val Loss: 0.7502 Acc: 0.6931                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 128, 'cnn_dropout': 0.6288082989323227, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 16, 'cnn_kernels_2': 96, 'learning_rate': 0.008049012411894292, 'lstm_dense': 256, 'lstm_hidden_size': 128, 'lstm_layers': 3, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 19.4085 Acc: 0.4392 | Val Loss: 1.0512 Acc: 0.5078                                              \n",
      "Epoch 002 | Train Loss: 1.0271 Acc: 0.4902 | Val Loss: 0.8755 Acc: 0.5830                                               \n",
      "Epoch 003 | Train Loss: 0.9601 Acc: 0.5274 | Val Loss: 0.8732 Acc: 0.5042                                               \n",
      "Epoch 004 | Train Loss: 0.9413 Acc: 0.5277 | Val Loss: 0.9231 Acc: 0.5301                                               \n",
      "Epoch 005 | Train Loss: 0.9564 Acc: 0.5035 | Val Loss: 1.0187 Acc: 0.4675                                               \n",
      "Epoch 006 | Train Loss: 1.0025 Acc: 0.4803 | Val Loss: 0.8718 Acc: 0.5287                                               \n",
      "Epoch 007 | Train Loss: 1.0233 Acc: 0.4786 | Val Loss: 0.9402 Acc: 0.5197                                               \n",
      "Epoch 008 | Train Loss: 1.1148 Acc: 0.4556 | Val Loss: 1.0407 Acc: 0.5125                                               \n",
      "Epoch 009 | Train Loss: 1.1488 Acc: 0.4525 | Val Loss: 1.0710 Acc: 0.4985                                               \n",
      "Epoch 010 | Train Loss: 1.0518 Acc: 0.4657 | Val Loss: 0.9241 Acc: 0.5451                                               \n",
      "Epoch 011 | Train Loss: 1.1762 Acc: 0.4553 | Val Loss: 0.9745 Acc: 0.4872                                               \n",
      "Epoch 012 | Train Loss: 1.1519 Acc: 0.4424 | Val Loss: 0.9618 Acc: 0.3893                                               \n",
      "Epoch 013 | Train Loss: 1.0862 Acc: 0.4410 | Val Loss: 1.1695 Acc: 0.3388                                               \n",
      "Epoch 014 | Train Loss: 1.7173 Acc: 0.4362 | Val Loss: 0.9780 Acc: 0.4421                                               \n",
      "Epoch 015 | Train Loss: 1.0051 Acc: 0.4597 | Val Loss: 0.9973 Acc: 0.4728                                               \n",
      "Epoch 016 | Train Loss: 1.2737 Acc: 0.4353 | Val Loss: 1.2410 Acc: 0.4421                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 64, 'cnn_dropout': 0.2886368103536889, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 32, 'learning_rate': 0.0035260496106655926, 'lstm_dense': 32, 'lstm_hidden_size': 32, 'lstm_layers': 2, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 16.3524 Acc: 0.4478 | Val Loss: 0.9442 Acc: 0.5087                                              \n",
      "Epoch 002 | Train Loss: 1.2834 Acc: 0.5290 | Val Loss: 0.9790 Acc: 0.5430                                               \n",
      "Epoch 003 | Train Loss: 0.9027 Acc: 0.5997 | Val Loss: 0.8459 Acc: 0.5878                                               \n",
      "Epoch 004 | Train Loss: 0.8297 Acc: 0.6198 | Val Loss: 0.8994 Acc: 0.5913                                               \n",
      "Epoch 005 | Train Loss: 0.8211 Acc: 0.6288 | Val Loss: 0.7290 Acc: 0.6242                                               \n",
      "Epoch 006 | Train Loss: 0.8004 Acc: 0.6324 | Val Loss: 1.0361 Acc: 0.5510                                               \n",
      "Epoch 007 | Train Loss: 0.8017 Acc: 0.6271 | Val Loss: 0.6591 Acc: 0.6991                                               \n",
      "Epoch 008 | Train Loss: 0.7704 Acc: 0.6466 | Val Loss: 0.7754 Acc: 0.6191                                               \n",
      "Epoch 009 | Train Loss: 0.7943 Acc: 0.6318 | Val Loss: 0.7675 Acc: 0.6558                                               \n",
      "Epoch 010 | Train Loss: 0.8063 Acc: 0.6110 | Val Loss: 0.8286 Acc: 0.5355                                               \n",
      "Epoch 011 | Train Loss: 0.7999 Acc: 0.6059 | Val Loss: 0.7446 Acc: 0.6678                                               \n",
      "Epoch 012 | Train Loss: 0.8073 Acc: 0.6085 | Val Loss: 0.7960 Acc: 0.5719                                               \n",
      "Epoch 013 | Train Loss: 0.8399 Acc: 0.6091 | Val Loss: 0.7367 Acc: 0.6466                                               \n",
      "Epoch 014 | Train Loss: 0.7759 Acc: 0.6274 | Val Loss: 0.7076 Acc: 0.6734                                               \n",
      "Epoch 015 | Train Loss: 0.7566 Acc: 0.6338 | Val Loss: 0.7385 Acc: 0.6116                                               \n",
      "Epoch 016 | Train Loss: 0.7345 Acc: 0.6483 | Val Loss: 0.6493 Acc: 0.7299                                               \n",
      "Epoch 017 | Train Loss: 0.7352 Acc: 0.6409 | Val Loss: 0.6310 Acc: 0.7379                                               \n",
      "Epoch 018 | Train Loss: 0.7042 Acc: 0.6601 | Val Loss: 0.6878 Acc: 0.6773                                               \n",
      "Epoch 019 | Train Loss: 0.7197 Acc: 0.6553 | Val Loss: 0.7614 Acc: 0.6087                                               \n",
      "Epoch 020 | Train Loss: 0.7006 Acc: 0.6628 | Val Loss: 0.7066 Acc: 0.7090                                               \n",
      "Epoch 021 | Train Loss: 0.8116 Acc: 0.6284 | Val Loss: 0.8024 Acc: 0.6209                                               \n",
      "Epoch 022 | Train Loss: 0.8245 Acc: 0.5973 | Val Loss: 0.6780 Acc: 0.6591                                               \n",
      "Epoch 023 | Train Loss: 0.7481 Acc: 0.6375 | Val Loss: 0.8404 Acc: 0.5215                                               \n",
      "Epoch 024 | Train Loss: 0.7712 Acc: 0.6312 | Val Loss: 0.7709 Acc: 0.6143                                               \n",
      "Epoch 025 | Train Loss: 0.7478 Acc: 0.6339 | Val Loss: 0.9827 Acc: 0.5346                                               \n",
      "Epoch 026 | Train Loss: 0.7253 Acc: 0.6422 | Val Loss: 0.6419 Acc: 0.6919                                               \n",
      "Epoch 027 | Train Loss: 0.7752 Acc: 0.6395 | Val Loss: 0.7095 Acc: 0.6869                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 128, 'cnn_dropout': 0.2526326887841496, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 16, 'cnn_kernels_2': 96, 'learning_rate': 0.00014482562654632688, 'lstm_dense': 64, 'lstm_hidden_size': 64, 'lstm_layers': 1, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 23.9168 Acc: 0.3647 | Val Loss: 5.2234 Acc: 0.4522                                              \n",
      "Epoch 002 | Train Loss: 7.9598 Acc: 0.4061 | Val Loss: 2.2968 Acc: 0.4848                                               \n",
      "Epoch 003 | Train Loss: 4.6452 Acc: 0.4254 | Val Loss: 1.7171 Acc: 0.4740                                               \n",
      "Epoch 004 | Train Loss: 3.1513 Acc: 0.4406 | Val Loss: 1.8873 Acc: 0.5006                                               \n",
      "Epoch 005 | Train Loss: 2.3320 Acc: 0.4638 | Val Loss: 1.1297 Acc: 0.5579                                               \n",
      "Epoch 006 | Train Loss: 1.8110 Acc: 0.4831 | Val Loss: 1.0738 Acc: 0.5510                                               \n",
      "Epoch 007 | Train Loss: 1.4562 Acc: 0.5053 | Val Loss: 1.1090 Acc: 0.6033                                               \n",
      "Epoch 008 | Train Loss: 1.2443 Acc: 0.5283 | Val Loss: 0.9946 Acc: 0.6158                                               \n",
      "Epoch 009 | Train Loss: 1.1158 Acc: 0.5398 | Val Loss: 0.8162 Acc: 0.6427                                               \n",
      "Epoch 010 | Train Loss: 1.0254 Acc: 0.5580 | Val Loss: 0.7634 Acc: 0.6713                                               \n",
      "Epoch 011 | Train Loss: 0.9502 Acc: 0.5750 | Val Loss: 0.8063 Acc: 0.6227                                               \n",
      "Epoch 012 | Train Loss: 0.8844 Acc: 0.5941 | Val Loss: 0.7440 Acc: 0.6281                                               \n",
      "Epoch 013 | Train Loss: 0.8222 Acc: 0.6247 | Val Loss: 0.7791 Acc: 0.6104                                               \n",
      "Epoch 014 | Train Loss: 0.7485 Acc: 0.6508 | Val Loss: 0.6360 Acc: 0.6973                                               \n",
      "Epoch 015 | Train Loss: 0.7149 Acc: 0.6715 | Val Loss: 0.6262 Acc: 0.6707                                               \n",
      "Epoch 016 | Train Loss: 0.6723 Acc: 0.6974 | Val Loss: 0.5851 Acc: 0.7143                                               \n",
      "Epoch 017 | Train Loss: 0.6238 Acc: 0.7263 | Val Loss: 0.5737 Acc: 0.7606                                               \n",
      "Epoch 018 | Train Loss: 0.5813 Acc: 0.7542 | Val Loss: 0.6125 Acc: 0.6654                                               \n",
      "Epoch 019 | Train Loss: 0.5356 Acc: 0.7764 | Val Loss: 0.4890 Acc: 0.7979                                               \n",
      "Epoch 020 | Train Loss: 0.4888 Acc: 0.7976 | Val Loss: 0.3905 Acc: 0.8367                                               \n",
      "Epoch 021 | Train Loss: 0.4489 Acc: 0.8249 | Val Loss: 0.3787 Acc: 0.8597                                               \n",
      "Epoch 022 | Train Loss: 0.3895 Acc: 0.8547 | Val Loss: 0.2960 Acc: 0.8890                                               \n",
      "Epoch 023 | Train Loss: 0.3457 Acc: 0.8722 | Val Loss: 0.3112 Acc: 0.8734                                               \n",
      "Epoch 024 | Train Loss: 0.3157 Acc: 0.8872 | Val Loss: 0.4314 Acc: 0.8257                                               \n",
      "Epoch 025 | Train Loss: 0.2808 Acc: 0.9028 | Val Loss: 0.2160 Acc: 0.9158                                               \n",
      "Epoch 026 | Train Loss: 0.2487 Acc: 0.9158 | Val Loss: 0.2176 Acc: 0.9218                                               \n",
      "Epoch 027 | Train Loss: 0.2255 Acc: 0.9223 | Val Loss: 0.2022 Acc: 0.9272                                               \n",
      "Epoch 028 | Train Loss: 0.2089 Acc: 0.9272 | Val Loss: 0.1518 Acc: 0.9481                                               \n",
      "Epoch 029 | Train Loss: 0.1893 Acc: 0.9374 | Val Loss: 0.1769 Acc: 0.9364                                               \n",
      "Epoch 030 | Train Loss: 0.1727 Acc: 0.9414 | Val Loss: 0.1736 Acc: 0.9367                                               \n",
      "Epoch 031 | Train Loss: 0.1562 Acc: 0.9467 | Val Loss: 0.1341 Acc: 0.9504                                               \n",
      "Epoch 032 | Train Loss: 0.1442 Acc: 0.9516 | Val Loss: 0.1512 Acc: 0.9439                                               \n",
      "Epoch 033 | Train Loss: 0.1429 Acc: 0.9512 | Val Loss: 0.1757 Acc: 0.9403                                               \n",
      "Epoch 034 | Train Loss: 0.1227 Acc: 0.9583 | Val Loss: 0.1590 Acc: 0.9463                                               \n",
      "Epoch 035 | Train Loss: 0.1140 Acc: 0.9613 | Val Loss: 0.1155 Acc: 0.9588                                               \n",
      "Epoch 036 | Train Loss: 0.1055 Acc: 0.9623 | Val Loss: 0.1253 Acc: 0.9579                                               \n",
      "Epoch 037 | Train Loss: 0.0972 Acc: 0.9676 | Val Loss: 0.1260 Acc: 0.9513                                               \n",
      "Epoch 038 | Train Loss: 0.0902 Acc: 0.9689 | Val Loss: 0.1745 Acc: 0.9400                                               \n",
      "Epoch 039 | Train Loss: 0.0808 Acc: 0.9715 | Val Loss: 0.1874 Acc: 0.9385                                               \n",
      "Epoch 040 | Train Loss: 0.0821 Acc: 0.9725 | Val Loss: 0.0976 Acc: 0.9684                                               \n",
      "Epoch 041 | Train Loss: 0.0770 Acc: 0.9741 | Val Loss: 0.1131 Acc: 0.9582                                               \n",
      "Epoch 042 | Train Loss: 0.0712 Acc: 0.9770 | Val Loss: 0.0955 Acc: 0.9651                                               \n",
      "Epoch 043 | Train Loss: 0.0614 Acc: 0.9787 | Val Loss: 0.0764 Acc: 0.9740                                               \n",
      "Epoch 044 | Train Loss: 0.0590 Acc: 0.9797 | Val Loss: 0.1325 Acc: 0.9540                                               \n",
      "Epoch 045 | Train Loss: 0.0558 Acc: 0.9798 | Val Loss: 0.0775 Acc: 0.9752                                               \n",
      "Epoch 046 | Train Loss: 0.0555 Acc: 0.9806 | Val Loss: 0.0740 Acc: 0.9752                                               \n",
      "Epoch 047 | Train Loss: 0.0512 Acc: 0.9826 | Val Loss: 0.0650 Acc: 0.9806                                               \n",
      "Epoch 048 | Train Loss: 0.0522 Acc: 0.9842 | Val Loss: 0.1263 Acc: 0.9597                                               \n",
      "Epoch 049 | Train Loss: 0.0465 Acc: 0.9849 | Val Loss: 0.2181 Acc: 0.9304                                               \n",
      "Epoch 050 | Train Loss: 0.0439 Acc: 0.9861 | Val Loss: 0.0779 Acc: 0.9707                                               \n",
      "Epoch 051 | Train Loss: 0.0420 Acc: 0.9867 | Val Loss: 0.0796 Acc: 0.9749                                               \n",
      "Epoch 052 | Train Loss: 0.0437 Acc: 0.9851 | Val Loss: 0.0740 Acc: 0.9758                                               \n",
      "Epoch 053 | Train Loss: 0.0347 Acc: 0.9880 | Val Loss: 0.0760 Acc: 0.9758                                               \n",
      "Epoch 054 | Train Loss: 0.0368 Acc: 0.9880 | Val Loss: 0.0811 Acc: 0.9725                                               \n",
      "Epoch 055 | Train Loss: 0.0408 Acc: 0.9870 | Val Loss: 0.0634 Acc: 0.9791                                               \n",
      "Epoch 056 | Train Loss: 0.0355 Acc: 0.9889 | Val Loss: 0.0626 Acc: 0.9794                                               \n",
      "Epoch 057 | Train Loss: 0.0317 Acc: 0.9896 | Val Loss: 0.1275 Acc: 0.9594                                               \n",
      "Epoch 058 | Train Loss: 0.0316 Acc: 0.9899 | Val Loss: 0.0833 Acc: 0.9758                                               \n",
      "Epoch 059 | Train Loss: 0.0344 Acc: 0.9885 | Val Loss: 0.0835 Acc: 0.9752                                               \n",
      "Epoch 060 | Train Loss: 0.0332 Acc: 0.9891 | Val Loss: 0.0811 Acc: 0.9767                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 64, 'cnn_dropout': 0.6065855193169686, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 32, 'cnn_kernels_2': 16, 'learning_rate': 0.0066074251333805975, 'lstm_dense': 32, 'lstm_hidden_size': 32, 'lstm_layers': 2, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 16.5916 Acc: 0.4362 | Val Loss: 1.0792 Acc: 0.4415                                              \n",
      "Epoch 002 | Train Loss: 1.0444 Acc: 0.5067 | Val Loss: 0.8948 Acc: 0.5919                                               \n",
      "Epoch 003 | Train Loss: 0.9864 Acc: 0.5247 | Val Loss: 1.1438 Acc: 0.3352                                               \n",
      "Epoch 004 | Train Loss: 1.0228 Acc: 0.5059 | Val Loss: 0.9360 Acc: 0.4896                                               \n",
      "Epoch 005 | Train Loss: 0.9965 Acc: 0.5127 | Val Loss: 0.9428 Acc: 0.5104                                               \n",
      "Epoch 006 | Train Loss: 1.0176 Acc: 0.5057 | Val Loss: 1.1159 Acc: 0.5036                                               \n",
      "Epoch 007 | Train Loss: 1.0672 Acc: 0.4797 | Val Loss: 1.0510 Acc: 0.4158                                               \n",
      "Epoch 008 | Train Loss: 1.0789 Acc: 0.4800 | Val Loss: 0.9898 Acc: 0.5125                                               \n",
      "Epoch 009 | Train Loss: 1.1127 Acc: 0.4870 | Val Loss: 0.9552 Acc: 0.4543                                               \n",
      "Epoch 010 | Train Loss: 1.1682 Acc: 0.4691 | Val Loss: 0.9727 Acc: 0.4791                                               \n",
      "Epoch 011 | Train Loss: 1.2493 Acc: 0.4853 | Val Loss: 0.9474 Acc: 0.5051                                               \n",
      "Epoch 012 | Train Loss: 1.0694 Acc: 0.4920 | Val Loss: 0.9406 Acc: 0.5176                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 64, 'cnn_dropout': 0.1653403654725192, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 32, 'cnn_kernels_2': 64, 'learning_rate': 0.00149632721027429, 'lstm_dense': 64, 'lstm_hidden_size': 32, 'lstm_layers': 6, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 10.3471 Acc: 0.4328 | Val Loss: 2.0183 Acc: 0.5678                                              \n",
      "Epoch 002 | Train Loss: 1.6219 Acc: 0.5300 | Val Loss: 0.8760 Acc: 0.6039                                               \n",
      "Epoch 003 | Train Loss: 1.0416 Acc: 0.5908 | Val Loss: 0.6452 Acc: 0.7293                                               \n",
      "Epoch 004 | Train Loss: 0.9713 Acc: 0.5978 | Val Loss: 0.6661 Acc: 0.7242                                               \n",
      "Epoch 005 | Train Loss: 0.8109 Acc: 0.6386 | Val Loss: 0.6313 Acc: 0.7110                                               \n",
      "Epoch 006 | Train Loss: 0.7554 Acc: 0.6520 | Val Loss: 0.7482 Acc: 0.6466                                               \n",
      "Epoch 007 | Train Loss: 0.7465 Acc: 0.6595 | Val Loss: 0.6102 Acc: 0.7376                                               \n",
      "Epoch 008 | Train Loss: 0.6897 Acc: 0.6806 | Val Loss: 0.5538 Acc: 0.7863                                               \n",
      "Epoch 009 | Train Loss: 0.6898 Acc: 0.6774 | Val Loss: 0.6326 Acc: 0.7340                                               \n",
      "Epoch 010 | Train Loss: 0.6499 Acc: 0.7004 | Val Loss: 0.5106 Acc: 0.7797                                               \n",
      "Epoch 011 | Train Loss: 0.6315 Acc: 0.7014 | Val Loss: 0.4718 Acc: 0.8119                                               \n",
      "Epoch 012 | Train Loss: 0.6254 Acc: 0.7086 | Val Loss: 0.5766 Acc: 0.7675                                               \n",
      "Epoch 013 | Train Loss: 0.6456 Acc: 0.6999 | Val Loss: 0.5703 Acc: 0.7170                                               \n",
      "Epoch 014 | Train Loss: 0.6226 Acc: 0.7055 | Val Loss: 0.5948 Acc: 0.7513                                               \n",
      "Epoch 015 | Train Loss: 0.6104 Acc: 0.7148 | Val Loss: 0.4733 Acc: 0.8230                                               \n",
      "Epoch 016 | Train Loss: 0.6128 Acc: 0.7135 | Val Loss: 0.4893 Acc: 0.8060                                               \n",
      "Epoch 017 | Train Loss: 0.6095 Acc: 0.7148 | Val Loss: 0.5407 Acc: 0.7460                                               \n",
      "Epoch 018 | Train Loss: 0.5988 Acc: 0.7219 | Val Loss: 0.5060 Acc: 0.7591                                               \n",
      "Epoch 019 | Train Loss: 0.5812 Acc: 0.7318 | Val Loss: 0.5098 Acc: 0.7719                                               \n",
      "Epoch 020 | Train Loss: 0.5796 Acc: 0.7367 | Val Loss: 0.4625 Acc: 0.8078                                               \n",
      "Epoch 021 | Train Loss: 0.5835 Acc: 0.7289 | Val Loss: 0.6136 Acc: 0.7069                                               \n",
      "Epoch 022 | Train Loss: 0.5854 Acc: 0.7310 | Val Loss: 0.5711 Acc: 0.6907                                               \n",
      "Epoch 023 | Train Loss: 0.5538 Acc: 0.7462 | Val Loss: 0.4544 Acc: 0.7713                                               \n",
      "Epoch 024 | Train Loss: 0.5303 Acc: 0.7650 | Val Loss: 0.4604 Acc: 0.7916                                               \n",
      "Epoch 025 | Train Loss: 0.5207 Acc: 0.7680 | Val Loss: 0.5072 Acc: 0.7907                                               \n",
      "Epoch 026 | Train Loss: 0.5222 Acc: 0.7651 | Val Loss: 0.4602 Acc: 0.7648                                               \n",
      "Epoch 027 | Train Loss: 0.5343 Acc: 0.7639 | Val Loss: 0.4792 Acc: 0.8012                                               \n",
      "Epoch 028 | Train Loss: 0.5196 Acc: 0.7754 | Val Loss: 0.5504 Acc: 0.7316                                               \n",
      "Epoch 029 | Train Loss: 0.5050 Acc: 0.7736 | Val Loss: 0.4504 Acc: 0.8072                                               \n",
      "Epoch 030 | Train Loss: 0.4933 Acc: 0.7833 | Val Loss: 0.5802 Acc: 0.7669                                               \n",
      "Epoch 031 | Train Loss: 0.5775 Acc: 0.7536 | Val Loss: 0.5908 Acc: 0.7403                                               \n",
      "Epoch 032 | Train Loss: 0.5605 Acc: 0.7596 | Val Loss: 0.5291 Acc: 0.7531                                               \n",
      "Epoch 033 | Train Loss: 0.5356 Acc: 0.7723 | Val Loss: 0.4965 Acc: 0.7884                                               \n",
      "Epoch 034 | Train Loss: 0.5345 Acc: 0.7776 | Val Loss: 0.4533 Acc: 0.7764                                               \n",
      "Epoch 035 | Train Loss: 0.5286 Acc: 0.7785 | Val Loss: 0.5215 Acc: 0.7881                                               \n",
      "Epoch 036 | Train Loss: 0.4856 Acc: 0.7916 | Val Loss: 0.5033 Acc: 0.7890                                               \n",
      "Epoch 037 | Train Loss: 0.4842 Acc: 0.7971 | Val Loss: 0.4857 Acc: 0.7639                                               \n",
      "Epoch 038 | Train Loss: 0.4872 Acc: 0.7945 | Val Loss: 0.5017 Acc: 0.7973                                               \n",
      "Epoch 039 | Train Loss: 0.4944 Acc: 0.7981 | Val Loss: 0.6041 Acc: 0.7343                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 32, 'cnn_dropout': 0.529902423611025, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 64, 'learning_rate': 3.4650546513622465e-05, 'lstm_dense': 64, 'lstm_hidden_size': 96, 'lstm_layers': 3, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 50.0790 Acc: 0.3330 | Val Loss: 14.7101 Acc: 0.3681                                             \n",
      "Epoch 002 | Train Loss: 28.5221 Acc: 0.3495 | Val Loss: 7.7031 Acc: 0.3510                                              \n",
      "Epoch 003 | Train Loss: 18.7641 Acc: 0.3637 | Val Loss: 6.4441 Acc: 0.3104                                              \n",
      "Epoch 004 | Train Loss: 14.3882 Acc: 0.3856 | Val Loss: 5.4946 Acc: 0.4388                                              \n",
      "Epoch 005 | Train Loss: 11.9115 Acc: 0.3920 | Val Loss: 4.6779 Acc: 0.3991                                              \n",
      "Epoch 006 | Train Loss: 9.9573 Acc: 0.4047 | Val Loss: 3.5875 Acc: 0.4304                                               \n",
      "Epoch 007 | Train Loss: 8.4658 Acc: 0.4153 | Val Loss: 3.0781 Acc: 0.4678                                               \n",
      "Epoch 008 | Train Loss: 7.5735 Acc: 0.4250 | Val Loss: 2.9839 Acc: 0.5627                                               \n",
      "Epoch 009 | Train Loss: 6.4939 Acc: 0.4374 | Val Loss: 2.5650 Acc: 0.5299                                               \n",
      "Epoch 010 | Train Loss: 5.6523 Acc: 0.4550 | Val Loss: 2.0185 Acc: 0.6290                                               \n",
      "Epoch 011 | Train Loss: 5.2074 Acc: 0.4651 | Val Loss: 1.9908 Acc: 0.5567                                               \n",
      "Epoch 012 | Train Loss: 4.7476 Acc: 0.4831 | Val Loss: 2.0949 Acc: 0.5899                                               \n",
      "Epoch 013 | Train Loss: 4.2514 Acc: 0.4914 | Val Loss: 2.2326 Acc: 0.5552                                               \n",
      "Epoch 014 | Train Loss: 3.7024 Acc: 0.5024 | Val Loss: 1.4909 Acc: 0.6394                                               \n",
      "Epoch 015 | Train Loss: 3.4631 Acc: 0.5217 | Val Loss: 1.5711 Acc: 0.5937                                               \n",
      "Epoch 016 | Train Loss: 3.1931 Acc: 0.5260 | Val Loss: 1.4987 Acc: 0.5916                                               \n",
      "Epoch 017 | Train Loss: 2.8996 Acc: 0.5428 | Val Loss: 1.7041 Acc: 0.5382                                               \n",
      "Epoch 018 | Train Loss: 2.7279 Acc: 0.5479 | Val Loss: 1.5612 Acc: 0.5815                                               \n",
      "Epoch 019 | Train Loss: 2.5545 Acc: 0.5525 | Val Loss: 1.5618 Acc: 0.5549                                               \n",
      "Epoch 020 | Train Loss: 2.3215 Acc: 0.5724 | Val Loss: 1.4925 Acc: 0.5642                                               \n",
      "Epoch 021 | Train Loss: 2.1589 Acc: 0.5857 | Val Loss: 1.4324 Acc: 0.5922                                               \n",
      "Epoch 022 | Train Loss: 2.0452 Acc: 0.5928 | Val Loss: 1.3826 Acc: 0.5576                                               \n",
      "Epoch 023 | Train Loss: 1.9282 Acc: 0.5974 | Val Loss: 1.0035 Acc: 0.6564                                               \n",
      "Epoch 024 | Train Loss: 1.7975 Acc: 0.6109 | Val Loss: 1.4464 Acc: 0.6245                                               \n",
      "Epoch 025 | Train Loss: 1.6439 Acc: 0.6271 | Val Loss: 1.9204 Acc: 0.5200                                               \n",
      "Epoch 026 | Train Loss: 1.5499 Acc: 0.6365 | Val Loss: 1.7186 Acc: 0.5445                                               \n",
      "Epoch 027 | Train Loss: 1.5285 Acc: 0.6424 | Val Loss: 1.0812 Acc: 0.6200                                               \n",
      "Epoch 028 | Train Loss: 1.3981 Acc: 0.6504 | Val Loss: 1.5121 Acc: 0.5534                                               \n",
      "Epoch 029 | Train Loss: 1.3294 Acc: 0.6628 | Val Loss: 0.9030 Acc: 0.6949                                               \n",
      "Epoch 030 | Train Loss: 1.2916 Acc: 0.6679 | Val Loss: 0.7737 Acc: 0.7269                                               \n",
      "Epoch 031 | Train Loss: 1.2275 Acc: 0.6868 | Val Loss: 0.9129 Acc: 0.6564                                               \n",
      "Epoch 032 | Train Loss: 1.1544 Acc: 0.6965 | Val Loss: 0.9283 Acc: 0.6266                                               \n",
      "Epoch 033 | Train Loss: 1.0773 Acc: 0.7148 | Val Loss: 1.0430 Acc: 0.6433                                               \n",
      "Epoch 034 | Train Loss: 1.0373 Acc: 0.7047 | Val Loss: 0.6447 Acc: 0.7699                                               \n",
      "Epoch 035 | Train Loss: 1.0174 Acc: 0.7233 | Val Loss: 1.2952 Acc: 0.5949                                               \n",
      "Epoch 036 | Train Loss: 0.9666 Acc: 0.7276 | Val Loss: 0.7511 Acc: 0.7239                                               \n",
      "Epoch 037 | Train Loss: 0.8795 Acc: 0.7444 | Val Loss: 0.7067 Acc: 0.7767                                               \n",
      "Epoch 038 | Train Loss: 0.8861 Acc: 0.7449 | Val Loss: 0.8060 Acc: 0.6779                                               \n",
      "Epoch 039 | Train Loss: 0.8157 Acc: 0.7590 | Val Loss: 0.8880 Acc: 0.6600                                               \n",
      "Epoch 040 | Train Loss: 0.8017 Acc: 0.7601 | Val Loss: 0.6018 Acc: 0.7561                                               \n",
      "Epoch 041 | Train Loss: 0.7820 Acc: 0.7670 | Val Loss: 0.7116 Acc: 0.7764                                               \n",
      "Epoch 042 | Train Loss: 0.7509 Acc: 0.7792 | Val Loss: 0.6524 Acc: 0.7316                                               \n",
      "Epoch 043 | Train Loss: 0.7241 Acc: 0.7773 | Val Loss: 0.5486 Acc: 0.8054                                               \n",
      "Epoch 044 | Train Loss: 0.6892 Acc: 0.7884 | Val Loss: 0.4096 Acc: 0.8615                                               \n",
      "Epoch 045 | Train Loss: 0.6631 Acc: 0.7898 | Val Loss: 0.6250 Acc: 0.7797                                               \n",
      "Epoch 046 | Train Loss: 0.6336 Acc: 0.7969 | Val Loss: 0.7950 Acc: 0.7352                                               \n",
      "Epoch 047 | Train Loss: 0.6195 Acc: 0.8054 | Val Loss: 0.5317 Acc: 0.7967                                               \n",
      "Epoch 048 | Train Loss: 0.6008 Acc: 0.8075 | Val Loss: 0.5052 Acc: 0.7925                                               \n",
      "Epoch 049 | Train Loss: 0.5662 Acc: 0.8147 | Val Loss: 0.4921 Acc: 0.7973                                               \n",
      "Epoch 050 | Train Loss: 0.5725 Acc: 0.8112 | Val Loss: 0.3585 Acc: 0.8594                                               \n",
      "Epoch 051 | Train Loss: 0.5427 Acc: 0.8255 | Val Loss: 0.3886 Acc: 0.8519                                               \n",
      "Epoch 052 | Train Loss: 0.5338 Acc: 0.8241 | Val Loss: 0.5822 Acc: 0.7901                                               \n",
      "Epoch 053 | Train Loss: 0.5178 Acc: 0.8345 | Val Loss: 0.4511 Acc: 0.8316                                               \n",
      "Epoch 054 | Train Loss: 0.4870 Acc: 0.8434 | Val Loss: 0.5428 Acc: 0.8137                                               \n",
      "Epoch 055 | Train Loss: 0.4776 Acc: 0.8439 | Val Loss: 0.3477 Acc: 0.8743                                               \n",
      "Epoch 056 | Train Loss: 0.4725 Acc: 0.8457 | Val Loss: 0.4855 Acc: 0.8394                                               \n",
      "Epoch 057 | Train Loss: 0.4628 Acc: 0.8513 | Val Loss: 0.4034 Acc: 0.8531                                               \n",
      "Epoch 058 | Train Loss: 0.4568 Acc: 0.8518 | Val Loss: 0.3789 Acc: 0.8660                                               \n",
      "Epoch 059 | Train Loss: 0.4307 Acc: 0.8548 | Val Loss: 0.4108 Acc: 0.8388                                               \n",
      "Epoch 060 | Train Loss: 0.4384 Acc: 0.8554 | Val Loss: 0.4633 Acc: 0.8191                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 128, 'cnn_dropout': 0.4099345718415792, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 64, 'cnn_kernels_2': 32, 'learning_rate': 0.009001735433555938, 'lstm_dense': 256, 'lstm_hidden_size': 32, 'lstm_layers': 2, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 811.4349 Acc: 0.4347 | Val Loss: 1.2466 Acc: 0.4421                                             \n",
      "Epoch 002 | Train Loss: 1.2413 Acc: 0.4382 | Val Loss: 1.2317 Acc: 0.4421                                               \n",
      "Epoch 003 | Train Loss: 1.5022 Acc: 0.4382 | Val Loss: 1.2479 Acc: 0.4421                                               \n",
      "Epoch 004 | Train Loss: 1.2445 Acc: 0.4422 | Val Loss: 1.2421 Acc: 0.4421                                               \n",
      "Epoch 005 | Train Loss: 1.2445 Acc: 0.4422 | Val Loss: 1.2423 Acc: 0.4421                                               \n",
      "Epoch 006 | Train Loss: 1.2438 Acc: 0.4422 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 007 | Train Loss: 1.2453 Acc: 0.4422 | Val Loss: 1.2433 Acc: 0.4421                                               \n",
      "Epoch 008 | Train Loss: 1.2442 Acc: 0.4422 | Val Loss: 1.2422 Acc: 0.4421                                               \n",
      "Epoch 009 | Train Loss: 1.2438 Acc: 0.4422 | Val Loss: 1.2438 Acc: 0.4421                                               \n",
      "Epoch 010 | Train Loss: 1.2439 Acc: 0.4422 | Val Loss: 1.2444 Acc: 0.4421                                               \n",
      "Epoch 011 | Train Loss: 1.2442 Acc: 0.4422 | Val Loss: 1.2415 Acc: 0.4421                                               \n",
      "Epoch 012 | Train Loss: 1.2435 Acc: 0.4422 | Val Loss: 1.2472 Acc: 0.4421                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 36, 'cnn_dense': 256, 'cnn_dropout': 0.018232560925496855, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 32, 'cnn_kernels_2': 64, 'learning_rate': 0.0007052666628001733, 'lstm_dense': 64, 'lstm_hidden_size': 96, 'lstm_layers': 1, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 8.3742 Acc: 0.4362 | Val Loss: 1.7056 Acc: 0.5815                                               \n",
      "Epoch 002 | Train Loss: 1.7554 Acc: 0.5188 | Val Loss: 1.1092 Acc: 0.6179                                               \n",
      "Epoch 003 | Train Loss: 1.1771 Acc: 0.5459 | Val Loss: 0.8223 Acc: 0.6030                                               \n",
      "Epoch 004 | Train Loss: 0.9129 Acc: 0.5980 | Val Loss: 0.8282 Acc: 0.6045                                               \n",
      "Epoch 005 | Train Loss: 0.8375 Acc: 0.6221 | Val Loss: 0.8399 Acc: 0.6015                                               \n",
      "Epoch 006 | Train Loss: 0.7855 Acc: 0.6412 | Val Loss: 0.6529 Acc: 0.7346                                               \n",
      "Epoch 007 | Train Loss: 0.7342 Acc: 0.6622 | Val Loss: 0.6386 Acc: 0.7107                                               \n",
      "Epoch 008 | Train Loss: 0.6992 Acc: 0.6779 | Val Loss: 0.5878 Acc: 0.7272                                               \n",
      "Epoch 009 | Train Loss: 0.6803 Acc: 0.6961 | Val Loss: 0.5785 Acc: 0.7373                                               \n",
      "Epoch 010 | Train Loss: 0.6359 Acc: 0.7148 | Val Loss: 0.6715 Acc: 0.6424                                               \n",
      "Epoch 011 | Train Loss: 0.6282 Acc: 0.7201 | Val Loss: 0.5299 Acc: 0.7749                                               \n",
      "Epoch 012 | Train Loss: 0.5900 Acc: 0.7410 | Val Loss: 0.4868 Acc: 0.7872                                               \n",
      "Epoch 013 | Train Loss: 0.5717 Acc: 0.7529 | Val Loss: 0.5152 Acc: 0.7678                                               \n",
      "Epoch 014 | Train Loss: 0.5407 Acc: 0.7680 | Val Loss: 0.5069 Acc: 0.7361                                               \n",
      "Epoch 015 | Train Loss: 0.5412 Acc: 0.7653 | Val Loss: 0.4496 Acc: 0.8266                                               \n",
      "Epoch 016 | Train Loss: 0.5224 Acc: 0.7777 | Val Loss: 0.4499 Acc: 0.8125                                               \n",
      "Epoch 017 | Train Loss: 0.4953 Acc: 0.7881 | Val Loss: 0.3908 Acc: 0.8621                                               \n",
      "Epoch 018 | Train Loss: 0.4954 Acc: 0.7950 | Val Loss: 0.5994 Acc: 0.7597                                               \n",
      "Epoch 019 | Train Loss: 0.4667 Acc: 0.8001 | Val Loss: 0.4576 Acc: 0.8057                                               \n",
      "Epoch 020 | Train Loss: 0.4720 Acc: 0.8025 | Val Loss: 0.4193 Acc: 0.8433                                               \n",
      "Epoch 021 | Train Loss: 0.4559 Acc: 0.8094 | Val Loss: 0.5817 Acc: 0.7630                                               \n",
      "Epoch 022 | Train Loss: 0.4509 Acc: 0.8139 | Val Loss: 0.3763 Acc: 0.8364                                               \n",
      "Epoch 023 | Train Loss: 0.4228 Acc: 0.8248 | Val Loss: 0.3656 Acc: 0.8490                                               \n",
      "Epoch 024 | Train Loss: 0.4324 Acc: 0.8253 | Val Loss: 0.3683 Acc: 0.8454                                               \n",
      "Epoch 025 | Train Loss: 0.4218 Acc: 0.8241 | Val Loss: 0.4391 Acc: 0.8203                                               \n",
      "Epoch 026 | Train Loss: 0.3896 Acc: 0.8360 | Val Loss: 0.4427 Acc: 0.8036                                               \n",
      "Epoch 027 | Train Loss: 0.4087 Acc: 0.8269 | Val Loss: 0.3942 Acc: 0.8024                                               \n",
      "Epoch 028 | Train Loss: 0.3891 Acc: 0.8401 | Val Loss: 0.3346 Acc: 0.8633                                               \n",
      "Epoch 029 | Train Loss: 0.3885 Acc: 0.8422 | Val Loss: 0.4191 Acc: 0.8081                                               \n",
      "Epoch 030 | Train Loss: 0.3817 Acc: 0.8385 | Val Loss: 0.3458 Acc: 0.8687                                               \n",
      "Epoch 031 | Train Loss: 0.3762 Acc: 0.8460 | Val Loss: 0.3413 Acc: 0.8657                                               \n",
      "Epoch 032 | Train Loss: 0.3844 Acc: 0.8427 | Val Loss: 0.3815 Acc: 0.8654                                               \n",
      "Epoch 033 | Train Loss: 0.3527 Acc: 0.8551 | Val Loss: 0.3864 Acc: 0.8487                                               \n",
      "Epoch 034 | Train Loss: 0.3519 Acc: 0.8530 | Val Loss: 0.3531 Acc: 0.8487                                               \n",
      "Epoch 035 | Train Loss: 0.3541 Acc: 0.8510 | Val Loss: 0.3443 Acc: 0.8699                                               \n",
      "Epoch 036 | Train Loss: 0.3456 Acc: 0.8582 | Val Loss: 0.4187 Acc: 0.8212                                               \n",
      "Epoch 037 | Train Loss: 0.3575 Acc: 0.8530 | Val Loss: 0.4177 Acc: 0.7991                                               \n",
      "Epoch 038 | Train Loss: 0.3617 Acc: 0.8511 | Val Loss: 0.3491 Acc: 0.8263                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 128, 'cnn_dropout': 0.04456157211958034, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 32, 'cnn_kernels_2': 32, 'learning_rate': 1.3996714013418097e-05, 'lstm_dense': 32, 'lstm_hidden_size': 96, 'lstm_layers': 3, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 74.2798 Acc: 0.2970 | Val Loss: 19.6387 Acc: 0.3794                                             \n",
      "Epoch 002 | Train Loss: 47.7599 Acc: 0.3344 | Val Loss: 17.0991 Acc: 0.4066                                             \n",
      "Epoch 003 | Train Loss: 38.9823 Acc: 0.3485 | Val Loss: 11.3997 Acc: 0.3761                                             \n",
      "Epoch 004 | Train Loss: 32.4158 Acc: 0.3425 | Val Loss: 7.5266 Acc: 0.3952                                              \n",
      "Epoch 005 | Train Loss: 26.4310 Acc: 0.3558 | Val Loss: 6.5713 Acc: 0.3678                                              \n",
      "Epoch 006 | Train Loss: 23.0815 Acc: 0.3690 | Val Loss: 5.4829 Acc: 0.3722                                              \n",
      "Epoch 007 | Train Loss: 20.6992 Acc: 0.3756 | Val Loss: 5.1399 Acc: 0.4099                                              \n",
      "Epoch 008 | Train Loss: 19.0189 Acc: 0.3701 | Val Loss: 4.3958 Acc: 0.4713                                              \n",
      "Epoch 009 | Train Loss: 17.1189 Acc: 0.3723 | Val Loss: 3.4335 Acc: 0.4672                                              \n",
      "Epoch 010 | Train Loss: 15.9658 Acc: 0.3782 | Val Loss: 2.7151 Acc: 0.5400                                              \n",
      "Epoch 011 | Train Loss: 14.6814 Acc: 0.3865 | Val Loss: 2.8800 Acc: 0.5657                                              \n",
      "Epoch 012 | Train Loss: 13.8713 Acc: 0.3883 | Val Loss: 3.4740 Acc: 0.5030                                              \n",
      "Epoch 013 | Train Loss: 12.8638 Acc: 0.3927 | Val Loss: 2.4348 Acc: 0.5773                                              \n",
      "Epoch 014 | Train Loss: 12.2627 Acc: 0.3981 | Val Loss: 2.1921 Acc: 0.5943                                              \n",
      "Epoch 015 | Train Loss: 11.2896 Acc: 0.4047 | Val Loss: 2.7144 Acc: 0.5322                                              \n",
      "Epoch 016 | Train Loss: 10.7860 Acc: 0.4020 | Val Loss: 2.2024 Acc: 0.5797                                              \n",
      "Epoch 017 | Train Loss: 9.8869 Acc: 0.4073 | Val Loss: 2.1156 Acc: 0.6018                                               \n",
      "Epoch 018 | Train Loss: 9.4671 Acc: 0.4127 | Val Loss: 2.0103 Acc: 0.6009                                               \n",
      "Epoch 019 | Train Loss: 8.8478 Acc: 0.4198 | Val Loss: 1.9013 Acc: 0.5854                                               \n",
      "Epoch 020 | Train Loss: 8.6299 Acc: 0.4284 | Val Loss: 1.8766 Acc: 0.5922                                               \n",
      "Epoch 021 | Train Loss: 7.9021 Acc: 0.4355 | Val Loss: 2.2556 Acc: 0.5609                                               \n",
      "Epoch 022 | Train Loss: 7.5031 Acc: 0.4427 | Val Loss: 2.0039 Acc: 0.5818                                               \n",
      "Epoch 023 | Train Loss: 7.2976 Acc: 0.4375 | Val Loss: 2.2911 Acc: 0.5645                                               \n",
      "Epoch 024 | Train Loss: 6.9518 Acc: 0.4432 | Val Loss: 2.0279 Acc: 0.5388                                               \n",
      "Epoch 025 | Train Loss: 6.5727 Acc: 0.4477 | Val Loss: 2.0114 Acc: 0.5633                                               \n",
      "Epoch 026 | Train Loss: 6.2287 Acc: 0.4559 | Val Loss: 2.0785 Acc: 0.5534                                               \n",
      "Epoch 027 | Train Loss: 5.8766 Acc: 0.4668 | Val Loss: 1.6183 Acc: 0.5973                                               \n",
      "Epoch 028 | Train Loss: 5.7177 Acc: 0.4702 | Val Loss: 2.3171 Acc: 0.5540                                               \n",
      "Epoch 029 | Train Loss: 5.5577 Acc: 0.4704 | Val Loss: 1.6789 Acc: 0.6337                                               \n",
      "Epoch 030 | Train Loss: 5.1725 Acc: 0.4867 | Val Loss: 1.5570 Acc: 0.6155                                               \n",
      "Epoch 031 | Train Loss: 5.0715 Acc: 0.4895 | Val Loss: 1.6289 Acc: 0.6251                                               \n",
      "Epoch 032 | Train Loss: 4.7781 Acc: 0.4944 | Val Loss: 1.5346 Acc: 0.6540                                               \n",
      "Epoch 033 | Train Loss: 4.5042 Acc: 0.5099 | Val Loss: 1.5434 Acc: 0.6078                                               \n",
      "Epoch 034 | Train Loss: 4.3321 Acc: 0.5127 | Val Loss: 1.5589 Acc: 0.6678                                               \n",
      "Epoch 035 | Train Loss: 4.1711 Acc: 0.5153 | Val Loss: 1.5816 Acc: 0.6639                                               \n",
      "Epoch 036 | Train Loss: 4.1609 Acc: 0.5129 | Val Loss: 1.3637 Acc: 0.6776                                               \n",
      "Epoch 037 | Train Loss: 4.1030 Acc: 0.5185 | Val Loss: 1.5395 Acc: 0.6639                                               \n",
      "Epoch 038 | Train Loss: 3.8190 Acc: 0.5295 | Val Loss: 1.3739 Acc: 0.7140                                               \n",
      "Epoch 039 | Train Loss: 3.6645 Acc: 0.5350 | Val Loss: 1.3180 Acc: 0.7036                                               \n",
      "Epoch 040 | Train Loss: 3.4997 Acc: 0.5367 | Val Loss: 1.4309 Acc: 0.6863                                               \n",
      "Epoch 041 | Train Loss: 3.3784 Acc: 0.5441 | Val Loss: 1.9113 Acc: 0.6218                                               \n",
      "Epoch 042 | Train Loss: 3.3036 Acc: 0.5427 | Val Loss: 1.3930 Acc: 0.6669                                               \n",
      "Epoch 043 | Train Loss: 3.1524 Acc: 0.5551 | Val Loss: 1.1633 Acc: 0.7116                                               \n",
      "Epoch 044 | Train Loss: 3.0497 Acc: 0.5535 | Val Loss: 1.5402 Acc: 0.6630                                               \n",
      "Epoch 045 | Train Loss: 2.9182 Acc: 0.5585 | Val Loss: 1.3411 Acc: 0.6943                                               \n",
      "Epoch 046 | Train Loss: 2.8755 Acc: 0.5608 | Val Loss: 1.2347 Acc: 0.6809                                               \n",
      "Epoch 047 | Train Loss: 2.7777 Acc: 0.5613 | Val Loss: 1.2659 Acc: 0.7101                                               \n",
      "Epoch 048 | Train Loss: 2.7048 Acc: 0.5709 | Val Loss: 1.0648 Acc: 0.6973                                               \n",
      "Epoch 049 | Train Loss: 2.6157 Acc: 0.5736 | Val Loss: 1.0643 Acc: 0.7301                                               \n",
      "Epoch 050 | Train Loss: 2.5097 Acc: 0.5785 | Val Loss: 1.1498 Acc: 0.6890                                               \n",
      "Epoch 051 | Train Loss: 2.4203 Acc: 0.5783 | Val Loss: 1.1894 Acc: 0.7015                                               \n",
      "Epoch 052 | Train Loss: 2.3824 Acc: 0.5837 | Val Loss: 1.1180 Acc: 0.7110                                               \n",
      "Epoch 053 | Train Loss: 2.2907 Acc: 0.5842 | Val Loss: 1.1853 Acc: 0.6988                                               \n",
      "Epoch 054 | Train Loss: 2.2358 Acc: 0.5930 | Val Loss: 0.9409 Acc: 0.7430                                               \n",
      "Epoch 055 | Train Loss: 2.1188 Acc: 0.5966 | Val Loss: 0.9701 Acc: 0.7424                                               \n",
      "Epoch 056 | Train Loss: 2.0713 Acc: 0.6111 | Val Loss: 1.1186 Acc: 0.6734                                               \n",
      "Epoch 057 | Train Loss: 2.0283 Acc: 0.6058 | Val Loss: 0.8554 Acc: 0.7469                                               \n",
      "Epoch 058 | Train Loss: 1.9416 Acc: 0.6138 | Val Loss: 0.9412 Acc: 0.7230                                               \n",
      "Epoch 059 | Train Loss: 1.9486 Acc: 0.6135 | Val Loss: 1.0320 Acc: 0.7266                                               \n",
      "Epoch 060 | Train Loss: 1.8526 Acc: 0.6212 | Val Loss: 0.7889 Acc: 0.7633                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 256, 'cnn_dropout': 0.39611697009447155, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 96, 'learning_rate': 9.483640063562956e-05, 'lstm_dense': 128, 'lstm_hidden_size': 64, 'lstm_layers': 5, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 3.0295 Acc: 0.4447 | Val Loss: 1.2995 Acc: 0.4537                                               \n",
      "Epoch 002 | Train Loss: 1.2931 Acc: 0.4524 | Val Loss: 1.2762 Acc: 0.4600                                               \n",
      "Epoch 003 | Train Loss: 1.2755 Acc: 0.4502 | Val Loss: 1.2586 Acc: 0.4600                                               \n",
      "Epoch 004 | Train Loss: 1.2594 Acc: 0.4519 | Val Loss: 1.2443 Acc: 0.4600                                               \n",
      "Epoch 005 | Train Loss: 1.2453 Acc: 0.4542 | Val Loss: 1.2323 Acc: 0.4600                                               \n",
      "Epoch 006 | Train Loss: 1.2376 Acc: 0.4543 | Val Loss: 1.2240 Acc: 0.4600                                               \n",
      "Epoch 007 | Train Loss: 1.2288 Acc: 0.4551 | Val Loss: 1.2165 Acc: 0.4600                                               \n",
      "Epoch 008 | Train Loss: 1.2242 Acc: 0.4544 | Val Loss: 1.2110 Acc: 0.4600                                               \n",
      "Epoch 009 | Train Loss: 1.2193 Acc: 0.4545 | Val Loss: 1.2072 Acc: 0.4600                                               \n",
      "Epoch 010 | Train Loss: 1.2167 Acc: 0.4541 | Val Loss: 1.2033 Acc: 0.4600                                               \n",
      "Epoch 011 | Train Loss: 1.2157 Acc: 0.4535 | Val Loss: 1.2005 Acc: 0.4600                                               \n",
      "Epoch 012 | Train Loss: 1.2100 Acc: 0.4551 | Val Loss: 1.1986 Acc: 0.4600                                               \n",
      "Epoch 013 | Train Loss: 1.2092 Acc: 0.4541 | Val Loss: 1.1963 Acc: 0.4600                                               \n",
      "Epoch 014 | Train Loss: 1.2071 Acc: 0.4547 | Val Loss: 1.1945 Acc: 0.4600                                               \n",
      "Epoch 015 | Train Loss: 1.2039 Acc: 0.4550 | Val Loss: 1.1928 Acc: 0.4600                                               \n",
      "Epoch 016 | Train Loss: 1.2052 Acc: 0.4541 | Val Loss: 1.1915 Acc: 0.4600                                               \n",
      "Epoch 017 | Train Loss: 1.2031 Acc: 0.4548 | Val Loss: 1.1906 Acc: 0.4600                                               \n",
      "Epoch 018 | Train Loss: 1.2001 Acc: 0.4558 | Val Loss: 1.1898 Acc: 0.4600                                               \n",
      "Epoch 019 | Train Loss: 1.2008 Acc: 0.4552 | Val Loss: 1.1884 Acc: 0.4600                                               \n",
      "Epoch 020 | Train Loss: 1.1996 Acc: 0.4554 | Val Loss: 1.1879 Acc: 0.4600                                               \n",
      "Epoch 021 | Train Loss: 1.1968 Acc: 0.4560 | Val Loss: 1.1877 Acc: 0.4600                                               \n",
      "Epoch 022 | Train Loss: 1.1987 Acc: 0.4556 | Val Loss: 1.1867 Acc: 0.4600                                               \n",
      "Epoch 023 | Train Loss: 1.2001 Acc: 0.4546 | Val Loss: 1.1863 Acc: 0.4600                                               \n",
      "Epoch 024 | Train Loss: 1.1977 Acc: 0.4556 | Val Loss: 1.1863 Acc: 0.4600                                               \n",
      "Epoch 025 | Train Loss: 1.1992 Acc: 0.4550 | Val Loss: 1.1858 Acc: 0.4600                                               \n",
      "Epoch 026 | Train Loss: 1.1975 Acc: 0.4553 | Val Loss: 1.1858 Acc: 0.4600                                               \n",
      "Epoch 027 | Train Loss: 1.2006 Acc: 0.4541 | Val Loss: 1.1854 Acc: 0.4600                                               \n",
      "Epoch 028 | Train Loss: 1.1965 Acc: 0.4554 | Val Loss: 1.1853 Acc: 0.4600                                               \n",
      "Epoch 029 | Train Loss: 1.1970 Acc: 0.4553 | Val Loss: 1.1846 Acc: 0.4600                                               \n",
      "Epoch 030 | Train Loss: 1.1955 Acc: 0.4556 | Val Loss: 1.1851 Acc: 0.4600                                               \n",
      "Epoch 031 | Train Loss: 1.1995 Acc: 0.4544 | Val Loss: 1.1865 Acc: 0.4600                                               \n",
      "Epoch 032 | Train Loss: 1.2006 Acc: 0.4546 | Val Loss: 1.1861 Acc: 0.4600                                               \n",
      "Epoch 033 | Train Loss: 1.1984 Acc: 0.4553 | Val Loss: 1.1859 Acc: 0.4600                                               \n",
      "Epoch 034 | Train Loss: 1.1973 Acc: 0.4556 | Val Loss: 1.1857 Acc: 0.4600                                               \n",
      "Epoch 035 | Train Loss: 1.1982 Acc: 0.4550 | Val Loss: 1.1859 Acc: 0.4600                                               \n",
      "Epoch 036 | Train Loss: 1.1993 Acc: 0.4547 | Val Loss: 1.1857 Acc: 0.4600                                               \n",
      "Epoch 037 | Train Loss: 1.1973 Acc: 0.4554 | Val Loss: 1.1857 Acc: 0.4600                                               \n",
      "Epoch 038 | Train Loss: 1.1955 Acc: 0.4563 | Val Loss: 1.1854 Acc: 0.4600                                               \n",
      "Epoch 039 | Train Loss: 1.2003 Acc: 0.4545 | Val Loss: 1.1856 Acc: 0.4600                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 256, 'cnn_dropout': 0.4909723589694506, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 96, 'learning_rate': 0.0001377051350860004, 'lstm_dense': 128, 'lstm_hidden_size': 96, 'lstm_layers': 1, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 10.6438 Acc: 0.4159 | Val Loss: 2.3720 Acc: 0.4537                                              \n",
      "Epoch 002 | Train Loss: 4.1654 Acc: 0.4586 | Val Loss: 2.0700 Acc: 0.5287                                               \n",
      "Epoch 003 | Train Loss: 2.7088 Acc: 0.4800 | Val Loss: 1.0293 Acc: 0.6012                                               \n",
      "Epoch 004 | Train Loss: 1.8760 Acc: 0.5121 | Val Loss: 0.8833 Acc: 0.6749                                               \n",
      "Epoch 005 | Train Loss: 1.3981 Acc: 0.5382 | Val Loss: 0.7672 Acc: 0.6890                                               \n",
      "Epoch 006 | Train Loss: 1.1516 Acc: 0.5761 | Val Loss: 0.7748 Acc: 0.6899                                               \n",
      "Epoch 007 | Train Loss: 0.9854 Acc: 0.6122 | Val Loss: 0.6117 Acc: 0.7382                                               \n",
      "Epoch 008 | Train Loss: 0.8744 Acc: 0.6401 | Val Loss: 0.5889 Acc: 0.7242                                               \n",
      "Epoch 009 | Train Loss: 0.8052 Acc: 0.6640 | Val Loss: 0.6466 Acc: 0.7343                                               \n",
      "Epoch 010 | Train Loss: 0.7276 Acc: 0.6939 | Val Loss: 0.6296 Acc: 0.7463                                               \n",
      "Epoch 011 | Train Loss: 0.6570 Acc: 0.7242 | Val Loss: 0.5611 Acc: 0.7582                                               \n",
      "Epoch 012 | Train Loss: 0.5994 Acc: 0.7478 | Val Loss: 0.4448 Acc: 0.8030                                               \n",
      "Epoch 013 | Train Loss: 0.5539 Acc: 0.7713 | Val Loss: 0.4587 Acc: 0.7958                                               \n",
      "Epoch 014 | Train Loss: 0.5046 Acc: 0.7957 | Val Loss: 0.4478 Acc: 0.8269                                               \n",
      "Epoch 015 | Train Loss: 0.4385 Acc: 0.8251 | Val Loss: 0.3357 Acc: 0.8669                                               \n",
      "Epoch 016 | Train Loss: 0.3926 Acc: 0.8483 | Val Loss: 0.2521 Acc: 0.9030                                               \n",
      "Epoch 017 | Train Loss: 0.3450 Acc: 0.8680 | Val Loss: 0.2051 Acc: 0.9263                                               \n",
      "Epoch 018 | Train Loss: 0.2971 Acc: 0.8893 | Val Loss: 0.2522 Acc: 0.8979                                               \n",
      "Epoch 019 | Train Loss: 0.2712 Acc: 0.8987 | Val Loss: 0.1853 Acc: 0.9287                                               \n",
      "Epoch 020 | Train Loss: 0.2404 Acc: 0.9107 | Val Loss: 0.1749 Acc: 0.9328                                               \n",
      "Epoch 021 | Train Loss: 0.1995 Acc: 0.9303 | Val Loss: 0.2855 Acc: 0.8899                                               \n",
      "Epoch 022 | Train Loss: 0.1897 Acc: 0.9333 | Val Loss: 0.3344 Acc: 0.8752                                               \n",
      "Epoch 023 | Train Loss: 0.1671 Acc: 0.9395 | Val Loss: 0.1005 Acc: 0.9621                                               \n",
      "Epoch 024 | Train Loss: 0.1574 Acc: 0.9463 | Val Loss: 0.1233 Acc: 0.9546                                               \n",
      "Epoch 025 | Train Loss: 0.1349 Acc: 0.9513 | Val Loss: 0.1541 Acc: 0.9448                                               \n",
      "Epoch 026 | Train Loss: 0.1284 Acc: 0.9566 | Val Loss: 0.0966 Acc: 0.9648                                               \n",
      "Epoch 027 | Train Loss: 0.1119 Acc: 0.9622 | Val Loss: 0.1231 Acc: 0.9570                                               \n",
      "Epoch 028 | Train Loss: 0.1041 Acc: 0.9646 | Val Loss: 0.1759 Acc: 0.9340                                               \n",
      "Epoch 029 | Train Loss: 0.0983 Acc: 0.9660 | Val Loss: 0.0729 Acc: 0.9740                                               \n",
      "Epoch 030 | Train Loss: 0.0877 Acc: 0.9719 | Val Loss: 0.0711 Acc: 0.9761                                               \n",
      "Epoch 031 | Train Loss: 0.0876 Acc: 0.9709 | Val Loss: 0.0824 Acc: 0.9731                                               \n",
      "Epoch 032 | Train Loss: 0.0855 Acc: 0.9724 | Val Loss: 0.0788 Acc: 0.9719                                               \n",
      "Epoch 033 | Train Loss: 0.0712 Acc: 0.9761 | Val Loss: 0.0802 Acc: 0.9719                                               \n",
      "Epoch 034 | Train Loss: 0.0708 Acc: 0.9754 | Val Loss: 0.0684 Acc: 0.9770                                               \n",
      "Epoch 035 | Train Loss: 0.0626 Acc: 0.9790 | Val Loss: 0.0739 Acc: 0.9704                                               \n",
      "Epoch 036 | Train Loss: 0.0620 Acc: 0.9800 | Val Loss: 0.0424 Acc: 0.9869                                               \n",
      "Epoch 037 | Train Loss: 0.0572 Acc: 0.9808 | Val Loss: 0.0584 Acc: 0.9800                                               \n",
      "Epoch 038 | Train Loss: 0.0551 Acc: 0.9822 | Val Loss: 0.0551 Acc: 0.9815                                               \n",
      "Epoch 039 | Train Loss: 0.0441 Acc: 0.9862 | Val Loss: 0.0542 Acc: 0.9800                                               \n",
      "Epoch 040 | Train Loss: 0.0458 Acc: 0.9854 | Val Loss: 0.0767 Acc: 0.9719                                               \n",
      "Epoch 041 | Train Loss: 0.0404 Acc: 0.9864 | Val Loss: 0.0388 Acc: 0.9857                                               \n",
      "Epoch 042 | Train Loss: 0.0421 Acc: 0.9857 | Val Loss: 0.0405 Acc: 0.9824                                               \n",
      "Epoch 043 | Train Loss: 0.0414 Acc: 0.9867 | Val Loss: 0.0551 Acc: 0.9821                                               \n",
      "Epoch 044 | Train Loss: 0.0370 Acc: 0.9880 | Val Loss: 0.0587 Acc: 0.9779                                               \n",
      "Epoch 045 | Train Loss: 0.0336 Acc: 0.9878 | Val Loss: 0.0490 Acc: 0.9809                                               \n",
      "Epoch 046 | Train Loss: 0.0378 Acc: 0.9885 | Val Loss: 0.0412 Acc: 0.9857                                               \n",
      "Epoch 047 | Train Loss: 0.0369 Acc: 0.9881 | Val Loss: 0.0323 Acc: 0.9890                                               \n",
      "Epoch 048 | Train Loss: 0.0350 Acc: 0.9903 | Val Loss: 0.0504 Acc: 0.9809                                               \n",
      "Epoch 049 | Train Loss: 0.0277 Acc: 0.9906 | Val Loss: 0.0427 Acc: 0.9857                                               \n",
      "Epoch 050 | Train Loss: 0.0291 Acc: 0.9904 | Val Loss: 0.0415 Acc: 0.9866                                               \n",
      "Epoch 051 | Train Loss: 0.0333 Acc: 0.9904 | Val Loss: 0.0400 Acc: 0.9863                                               \n",
      "Epoch 052 | Train Loss: 0.0283 Acc: 0.9913 | Val Loss: 0.0508 Acc: 0.9809                                               \n",
      "Epoch 053 | Train Loss: 0.0267 Acc: 0.9918 | Val Loss: 0.0346 Acc: 0.9884                                               \n",
      "Epoch 054 | Train Loss: 0.0237 Acc: 0.9918 | Val Loss: 0.0306 Acc: 0.9913                                               \n",
      "Epoch 055 | Train Loss: 0.0246 Acc: 0.9931 | Val Loss: 0.0373 Acc: 0.9881                                               \n",
      "Epoch 056 | Train Loss: 0.0242 Acc: 0.9922 | Val Loss: 0.0326 Acc: 0.9901                                               \n",
      "Epoch 057 | Train Loss: 0.0211 Acc: 0.9933 | Val Loss: 0.0830 Acc: 0.9743                                               \n",
      "Epoch 058 | Train Loss: 0.0234 Acc: 0.9922 | Val Loss: 0.0427 Acc: 0.9869                                               \n",
      "Epoch 059 | Train Loss: 0.0234 Acc: 0.9925 | Val Loss: 0.0401 Acc: 0.9881                                               \n",
      "Epoch 060 | Train Loss: 0.0205 Acc: 0.9938 | Val Loss: 0.0299 Acc: 0.9913                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 256, 'cnn_dropout': 0.5018318387329893, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 96, 'learning_rate': 0.00014521979753259078, 'lstm_dense': 128, 'lstm_hidden_size': 64, 'lstm_layers': 1, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 23.5528 Acc: 0.3596 | Val Loss: 4.3417 Acc: 0.4988                                              \n",
      "Epoch 002 | Train Loss: 6.8403 Acc: 0.4269 | Val Loss: 1.4963 Acc: 0.5710                                               \n",
      "Epoch 003 | Train Loss: 3.8825 Acc: 0.4623 | Val Loss: 2.0961 Acc: 0.5758                                               \n",
      "Epoch 004 | Train Loss: 2.6292 Acc: 0.4955 | Val Loss: 1.6752 Acc: 0.5737                                               \n",
      "Epoch 005 | Train Loss: 1.9355 Acc: 0.5262 | Val Loss: 1.6433 Acc: 0.6087                                               \n",
      "Epoch 006 | Train Loss: 1.6310 Acc: 0.5341 | Val Loss: 0.9206 Acc: 0.6284                                               \n",
      "Epoch 007 | Train Loss: 1.3204 Acc: 0.5601 | Val Loss: 0.8942 Acc: 0.6251                                               \n",
      "Epoch 008 | Train Loss: 1.1875 Acc: 0.5755 | Val Loss: 0.7514 Acc: 0.6731                                               \n",
      "Epoch 009 | Train Loss: 1.0509 Acc: 0.6016 | Val Loss: 0.7727 Acc: 0.7331                                               \n",
      "Epoch 010 | Train Loss: 0.8998 Acc: 0.6446 | Val Loss: 0.5832 Acc: 0.8021                                               \n",
      "Epoch 011 | Train Loss: 0.8154 Acc: 0.6727 | Val Loss: 0.7591 Acc: 0.6412                                               \n",
      "Epoch 012 | Train Loss: 0.7531 Acc: 0.6919 | Val Loss: 0.6630 Acc: 0.7400                                               \n",
      "Epoch 013 | Train Loss: 0.7144 Acc: 0.7063 | Val Loss: 0.5499 Acc: 0.7722                                               \n",
      "Epoch 014 | Train Loss: 0.6846 Acc: 0.7186 | Val Loss: 0.5752 Acc: 0.7469                                               \n",
      "Epoch 015 | Train Loss: 0.6312 Acc: 0.7389 | Val Loss: 0.4737 Acc: 0.7970                                               \n",
      "Epoch 016 | Train Loss: 0.5796 Acc: 0.7665 | Val Loss: 0.6100 Acc: 0.6991                                               \n",
      "Epoch 017 | Train Loss: 0.5238 Acc: 0.7910 | Val Loss: 0.3725 Acc: 0.8463                                               \n",
      "Epoch 018 | Train Loss: 0.4836 Acc: 0.8079 | Val Loss: 0.4047 Acc: 0.8248                                               \n",
      "Epoch 019 | Train Loss: 0.4336 Acc: 0.8320 | Val Loss: 0.3534 Acc: 0.8549                                               \n",
      "Epoch 020 | Train Loss: 0.4018 Acc: 0.8439 | Val Loss: 0.3791 Acc: 0.8507                                               \n",
      "Epoch 021 | Train Loss: 0.3464 Acc: 0.8680 | Val Loss: 0.2899 Acc: 0.8869                                               \n",
      "Epoch 022 | Train Loss: 0.3303 Acc: 0.8768 | Val Loss: 0.2085 Acc: 0.9275                                               \n",
      "Epoch 023 | Train Loss: 0.2893 Acc: 0.8942 | Val Loss: 0.1812 Acc: 0.9364                                               \n",
      "Epoch 024 | Train Loss: 0.2655 Acc: 0.9040 | Val Loss: 0.2000 Acc: 0.9215                                               \n",
      "Epoch 025 | Train Loss: 0.2486 Acc: 0.9098 | Val Loss: 0.2653 Acc: 0.8899                                               \n",
      "Epoch 026 | Train Loss: 0.2255 Acc: 0.9216 | Val Loss: 0.1478 Acc: 0.9454                                               \n",
      "Epoch 027 | Train Loss: 0.1972 Acc: 0.9287 | Val Loss: 0.1670 Acc: 0.9382                                               \n",
      "Epoch 028 | Train Loss: 0.1869 Acc: 0.9328 | Val Loss: 0.1095 Acc: 0.9609                                               \n",
      "Epoch 029 | Train Loss: 0.1738 Acc: 0.9395 | Val Loss: 0.2289 Acc: 0.9146                                               \n",
      "Epoch 030 | Train Loss: 0.1613 Acc: 0.9434 | Val Loss: 0.1394 Acc: 0.9507                                               \n",
      "Epoch 031 | Train Loss: 0.1505 Acc: 0.9470 | Val Loss: 0.1340 Acc: 0.9499                                               \n",
      "Epoch 032 | Train Loss: 0.1430 Acc: 0.9495 | Val Loss: 0.1404 Acc: 0.9519                                               \n",
      "Epoch 033 | Train Loss: 0.1357 Acc: 0.9553 | Val Loss: 0.1061 Acc: 0.9615                                               \n",
      "Epoch 034 | Train Loss: 0.1218 Acc: 0.9584 | Val Loss: 0.1325 Acc: 0.9510                                               \n",
      "Epoch 035 | Train Loss: 0.1150 Acc: 0.9606 | Val Loss: 0.1818 Acc: 0.9301                                               \n",
      "Epoch 036 | Train Loss: 0.1098 Acc: 0.9631 | Val Loss: 0.0717 Acc: 0.9731                                               \n",
      "Epoch 037 | Train Loss: 0.0935 Acc: 0.9691 | Val Loss: 0.0915 Acc: 0.9678                                               \n",
      "Epoch 038 | Train Loss: 0.0980 Acc: 0.9671 | Val Loss: 0.1134 Acc: 0.9624                                               \n",
      "Epoch 039 | Train Loss: 0.0895 Acc: 0.9684 | Val Loss: 0.1071 Acc: 0.9606                                               \n",
      "Epoch 040 | Train Loss: 0.0856 Acc: 0.9695 | Val Loss: 0.0803 Acc: 0.9693                                               \n",
      "Epoch 041 | Train Loss: 0.0829 Acc: 0.9727 | Val Loss: 0.0879 Acc: 0.9693                                               \n",
      "Epoch 042 | Train Loss: 0.0792 Acc: 0.9734 | Val Loss: 0.0577 Acc: 0.9815                                               \n",
      "Epoch 043 | Train Loss: 0.0701 Acc: 0.9768 | Val Loss: 0.0681 Acc: 0.9740                                               \n",
      "Epoch 044 | Train Loss: 0.0668 Acc: 0.9769 | Val Loss: 0.0787 Acc: 0.9713                                               \n",
      "Epoch 045 | Train Loss: 0.0665 Acc: 0.9765 | Val Loss: 0.0630 Acc: 0.9785                                               \n",
      "Epoch 046 | Train Loss: 0.0613 Acc: 0.9805 | Val Loss: 0.0435 Acc: 0.9839                                               \n",
      "Epoch 047 | Train Loss: 0.0573 Acc: 0.9808 | Val Loss: 0.0840 Acc: 0.9710                                               \n",
      "Epoch 048 | Train Loss: 0.0589 Acc: 0.9799 | Val Loss: 0.0761 Acc: 0.9699                                               \n",
      "Epoch 049 | Train Loss: 0.0550 Acc: 0.9837 | Val Loss: 0.0424 Acc: 0.9839                                               \n",
      "Epoch 050 | Train Loss: 0.0510 Acc: 0.9838 | Val Loss: 0.0459 Acc: 0.9833                                               \n",
      "Epoch 051 | Train Loss: 0.0470 Acc: 0.9851 | Val Loss: 0.0399 Acc: 0.9848                                               \n",
      "Epoch 052 | Train Loss: 0.0464 Acc: 0.9851 | Val Loss: 0.0542 Acc: 0.9800                                               \n",
      "Epoch 053 | Train Loss: 0.0472 Acc: 0.9838 | Val Loss: 0.1874 Acc: 0.9400                                               \n",
      "Epoch 054 | Train Loss: 0.0479 Acc: 0.9837 | Val Loss: 0.0891 Acc: 0.9690                                               \n",
      "Epoch 055 | Train Loss: 0.0380 Acc: 0.9869 | Val Loss: 0.0574 Acc: 0.9824                                               \n",
      "Epoch 056 | Train Loss: 0.0400 Acc: 0.9864 | Val Loss: 0.0507 Acc: 0.9833                                               \n",
      "Epoch 057 | Train Loss: 0.0384 Acc: 0.9860 | Val Loss: 0.0417 Acc: 0.9839                                               \n",
      "Epoch 058 | Train Loss: 0.0330 Acc: 0.9901 | Val Loss: 0.0347 Acc: 0.9863                                               \n",
      "Epoch 059 | Train Loss: 0.0347 Acc: 0.9887 | Val Loss: 0.0519 Acc: 0.9815                                               \n",
      "Epoch 060 | Train Loss: 0.0329 Acc: 0.9896 | Val Loss: 0.0566 Acc: 0.9818                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 256, 'cnn_dropout': 0.5278490388051149, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 96, 'learning_rate': 0.00017536911050779934, 'lstm_dense': 128, 'lstm_hidden_size': 64, 'lstm_layers': 1, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 3.3544 Acc: 0.3046 | Val Loss: 1.3380 Acc: 0.2743                                               \n",
      "Epoch 002 | Train Loss: 1.3063 Acc: 0.4138 | Val Loss: 1.3131 Acc: 0.4421                                               \n",
      "Epoch 003 | Train Loss: 1.2623 Acc: 0.4399 | Val Loss: 1.2978 Acc: 0.4421                                               \n",
      "Epoch 004 | Train Loss: 1.2387 Acc: 0.4405 | Val Loss: 1.2866 Acc: 0.4421                                               \n",
      "Epoch 005 | Train Loss: 1.1830 Acc: 0.4379 | Val Loss: 1.1519 Acc: 0.4421                                               \n",
      "Epoch 006 | Train Loss: 1.1212 Acc: 0.4352 | Val Loss: 1.0519 Acc: 0.4421                                               \n",
      "Epoch 007 | Train Loss: 1.0837 Acc: 0.4345 | Val Loss: 1.0402 Acc: 0.4421                                               \n",
      "Epoch 008 | Train Loss: 1.0669 Acc: 0.4401 | Val Loss: 0.9991 Acc: 0.4594                                               \n",
      "Epoch 009 | Train Loss: 1.0484 Acc: 0.4603 | Val Loss: 0.9850 Acc: 0.5167                                               \n",
      "Epoch 010 | Train Loss: 1.0357 Acc: 0.4706 | Val Loss: 0.9886 Acc: 0.5367                                               \n",
      "Epoch 011 | Train Loss: 1.0182 Acc: 0.4895 | Val Loss: 0.9718 Acc: 0.5090                                               \n",
      "Epoch 012 | Train Loss: 1.0154 Acc: 0.4853 | Val Loss: 0.9543 Acc: 0.5254                                               \n",
      "Epoch 013 | Train Loss: 1.0120 Acc: 0.4862 | Val Loss: 0.9924 Acc: 0.4961                                               \n",
      "Epoch 014 | Train Loss: 1.0006 Acc: 0.4893 | Val Loss: 0.9171 Acc: 0.5510                                               \n",
      "Epoch 015 | Train Loss: 0.9990 Acc: 0.4937 | Val Loss: 0.9205 Acc: 0.5122                                               \n",
      "Epoch 016 | Train Loss: 0.9901 Acc: 0.4932 | Val Loss: 0.9334 Acc: 0.5251                                               \n",
      "Epoch 017 | Train Loss: 0.9884 Acc: 0.5009 | Val Loss: 0.9746 Acc: 0.4821                                               \n",
      "Epoch 018 | Train Loss: 0.9904 Acc: 0.4970 | Val Loss: 0.9090 Acc: 0.5301                                               \n",
      "Epoch 019 | Train Loss: 0.9901 Acc: 0.4902 | Val Loss: 0.9022 Acc: 0.5645                                               \n",
      "Epoch 020 | Train Loss: 0.9939 Acc: 0.4977 | Val Loss: 1.0069 Acc: 0.4773                                               \n",
      "Epoch 021 | Train Loss: 0.9892 Acc: 0.4942 | Val Loss: 0.9333 Acc: 0.5382                                               \n",
      "Epoch 022 | Train Loss: 0.9843 Acc: 0.4992 | Val Loss: 0.9179 Acc: 0.5316                                               \n",
      "Epoch 023 | Train Loss: 0.9801 Acc: 0.4996 | Val Loss: 0.9422 Acc: 0.4946                                               \n",
      "Epoch 024 | Train Loss: 0.9842 Acc: 0.4962 | Val Loss: 0.9249 Acc: 0.5030                                               \n",
      "Epoch 025 | Train Loss: 0.9829 Acc: 0.4944 | Val Loss: 0.9134 Acc: 0.5484                                               \n",
      "Epoch 026 | Train Loss: 0.9770 Acc: 0.5006 | Val Loss: 1.0070 Acc: 0.4869                                               \n",
      "Epoch 027 | Train Loss: 0.9769 Acc: 0.4942 | Val Loss: 0.9591 Acc: 0.4887                                               \n",
      "Epoch 028 | Train Loss: 0.9855 Acc: 0.4914 | Val Loss: 0.8863 Acc: 0.5510                                               \n",
      "Epoch 029 | Train Loss: 0.9809 Acc: 0.4974 | Val Loss: 0.9028 Acc: 0.5301                                               \n",
      "Epoch 030 | Train Loss: 0.9665 Acc: 0.5052 | Val Loss: 0.8685 Acc: 0.5854                                               \n",
      "Epoch 031 | Train Loss: 0.9751 Acc: 0.5010 | Val Loss: 0.9111 Acc: 0.5391                                               \n",
      "Epoch 032 | Train Loss: 1.0191 Acc: 0.4823 | Val Loss: 0.9529 Acc: 0.5024                                               \n",
      "Epoch 033 | Train Loss: 0.9834 Acc: 0.4951 | Val Loss: 0.8979 Acc: 0.5263                                               \n",
      "Epoch 034 | Train Loss: 0.9718 Acc: 0.5028 | Val Loss: 0.8780 Acc: 0.5510                                               \n",
      "Epoch 035 | Train Loss: 0.9692 Acc: 0.5004 | Val Loss: 0.8798 Acc: 0.5940                                               \n",
      "Epoch 036 | Train Loss: 0.9842 Acc: 0.5015 | Val Loss: 0.9142 Acc: 0.5107                                               \n",
      "Epoch 037 | Train Loss: 0.9756 Acc: 0.5097 | Val Loss: 0.8962 Acc: 0.5621                                               \n",
      "Epoch 038 | Train Loss: 0.9838 Acc: 0.4986 | Val Loss: 0.9827 Acc: 0.5158                                               \n",
      "Epoch 039 | Train Loss: 0.9721 Acc: 0.5108 | Val Loss: 0.9193 Acc: 0.5146                                               \n",
      "Epoch 040 | Train Loss: 0.9920 Acc: 0.5029 | Val Loss: 0.8541 Acc: 0.5725                                               \n",
      "Epoch 041 | Train Loss: 0.9745 Acc: 0.5140 | Val Loss: 0.9248 Acc: 0.5681                                               \n",
      "Epoch 042 | Train Loss: 1.0581 Acc: 0.4763 | Val Loss: 0.9169 Acc: 0.5224                                               \n",
      "Epoch 043 | Train Loss: 1.1266 Acc: 0.4606 | Val Loss: 0.9807 Acc: 0.5090                                               \n",
      "Epoch 044 | Train Loss: 1.0453 Acc: 0.4821 | Val Loss: 1.0065 Acc: 0.4881                                               \n",
      "Epoch 045 | Train Loss: 1.0247 Acc: 0.4772 | Val Loss: 0.9838 Acc: 0.4794                                               \n",
      "Epoch 046 | Train Loss: 1.0181 Acc: 0.4847 | Val Loss: 0.9611 Acc: 0.4818                                               \n",
      "Epoch 047 | Train Loss: 1.0190 Acc: 0.5009 | Val Loss: 0.8948 Acc: 0.5475                                               \n",
      "Epoch 048 | Train Loss: 1.0676 Acc: 0.4868 | Val Loss: 0.9715 Acc: 0.5036                                               \n",
      "Epoch 049 | Train Loss: 1.0209 Acc: 0.4772 | Val Loss: 0.9756 Acc: 0.4907                                               \n",
      "Epoch 050 | Train Loss: 1.0508 Acc: 0.4827 | Val Loss: 0.9694 Acc: 0.4916                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 256, 'cnn_dropout': 0.5174133199701038, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 96, 'learning_rate': 6.9088635530152e-05, 'lstm_dense': 128, 'lstm_hidden_size': 96, 'lstm_layers': 5, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 26.4298 Acc: 0.3690 | Val Loss: 5.8593 Acc: 0.3510                                              \n",
      "Epoch 002 | Train Loss: 9.5913 Acc: 0.4226 | Val Loss: 1.7144 Acc: 0.5313                                               \n",
      "Epoch 003 | Train Loss: 6.2411 Acc: 0.4532 | Val Loss: 2.2065 Acc: 0.5272                                               \n",
      "Epoch 004 | Train Loss: 4.6438 Acc: 0.4705 | Val Loss: 1.2839 Acc: 0.6048                                               \n",
      "Epoch 005 | Train Loss: 3.6571 Acc: 0.4912 | Val Loss: 1.7512 Acc: 0.6182                                               \n",
      "Epoch 006 | Train Loss: 2.9601 Acc: 0.5067 | Val Loss: 1.2031 Acc: 0.5946                                               \n",
      "Epoch 007 | Train Loss: 2.3540 Acc: 0.5340 | Val Loss: 1.0437 Acc: 0.5934                                               \n",
      "Epoch 008 | Train Loss: 1.9503 Acc: 0.5715 | Val Loss: 0.8513 Acc: 0.6842                                               \n",
      "Epoch 009 | Train Loss: 1.7133 Acc: 0.5904 | Val Loss: 0.9827 Acc: 0.6445                                               \n",
      "Epoch 010 | Train Loss: 1.4392 Acc: 0.6182 | Val Loss: 0.9442 Acc: 0.7063                                               \n",
      "Epoch 011 | Train Loss: 1.2779 Acc: 0.6457 | Val Loss: 1.0065 Acc: 0.7042                                               \n",
      "Epoch 012 | Train Loss: 1.1218 Acc: 0.6706 | Val Loss: 0.8171 Acc: 0.7504                                               \n",
      "Epoch 013 | Train Loss: 1.0287 Acc: 0.6946 | Val Loss: 0.6161 Acc: 0.7564                                               \n",
      "Epoch 014 | Train Loss: 0.9468 Acc: 0.7114 | Val Loss: 0.6839 Acc: 0.7454                                               \n",
      "Epoch 015 | Train Loss: 0.8414 Acc: 0.7347 | Val Loss: 0.5290 Acc: 0.8197                                               \n",
      "Epoch 016 | Train Loss: 0.7572 Acc: 0.7613 | Val Loss: 0.4720 Acc: 0.8454                                               \n",
      "Epoch 017 | Train Loss: 0.6543 Acc: 0.7824 | Val Loss: 0.6547 Acc: 0.7875                                               \n",
      "Epoch 018 | Train Loss: 0.5919 Acc: 0.8019 | Val Loss: 0.3926 Acc: 0.8582                                               \n",
      "Epoch 019 | Train Loss: 0.5342 Acc: 0.8236 | Val Loss: 0.3945 Acc: 0.8672                                               \n",
      "Epoch 020 | Train Loss: 0.4848 Acc: 0.8370 | Val Loss: 0.4631 Acc: 0.8618                                               \n",
      "Epoch 021 | Train Loss: 0.4351 Acc: 0.8507 | Val Loss: 0.3045 Acc: 0.8940                                               \n",
      "Epoch 022 | Train Loss: 0.3897 Acc: 0.8689 | Val Loss: 0.6166 Acc: 0.8128                                               \n",
      "Epoch 023 | Train Loss: 0.3648 Acc: 0.8798 | Val Loss: 0.2605 Acc: 0.9063                                               \n",
      "Epoch 024 | Train Loss: 0.3247 Acc: 0.8925 | Val Loss: 0.2592 Acc: 0.9128                                               \n",
      "Epoch 025 | Train Loss: 0.3018 Acc: 0.9020 | Val Loss: 0.2554 Acc: 0.9096                                               \n",
      "Epoch 026 | Train Loss: 0.2614 Acc: 0.9131 | Val Loss: 0.2005 Acc: 0.9373                                               \n",
      "Epoch 027 | Train Loss: 0.2430 Acc: 0.9181 | Val Loss: 0.1816 Acc: 0.9445                                               \n",
      "Epoch 028 | Train Loss: 0.2084 Acc: 0.9329 | Val Loss: 0.2324 Acc: 0.9227                                               \n",
      "Epoch 029 | Train Loss: 0.1936 Acc: 0.9380 | Val Loss: 0.1521 Acc: 0.9481                                               \n",
      "Epoch 030 | Train Loss: 0.1821 Acc: 0.9427 | Val Loss: 0.1493 Acc: 0.9507                                               \n",
      "Epoch 031 | Train Loss: 0.1485 Acc: 0.9507 | Val Loss: 0.1369 Acc: 0.9546                                               \n",
      "Epoch 032 | Train Loss: 0.1480 Acc: 0.9517 | Val Loss: 0.1316 Acc: 0.9552                                               \n",
      "Epoch 033 | Train Loss: 0.1301 Acc: 0.9578 | Val Loss: 0.1250 Acc: 0.9591                                               \n",
      "Epoch 034 | Train Loss: 0.1231 Acc: 0.9623 | Val Loss: 0.1272 Acc: 0.9555                                               \n",
      "Epoch 035 | Train Loss: 0.1129 Acc: 0.9648 | Val Loss: 0.1085 Acc: 0.9612                                               \n",
      "Epoch 036 | Train Loss: 0.1054 Acc: 0.9664 | Val Loss: 0.0988 Acc: 0.9645                                               \n",
      "Epoch 037 | Train Loss: 0.0910 Acc: 0.9728 | Val Loss: 0.1273 Acc: 0.9564                                               \n",
      "Epoch 038 | Train Loss: 0.0873 Acc: 0.9706 | Val Loss: 0.1288 Acc: 0.9558                                               \n",
      "Epoch 039 | Train Loss: 0.0819 Acc: 0.9748 | Val Loss: 0.0899 Acc: 0.9675                                               \n",
      "Epoch 040 | Train Loss: 0.0799 Acc: 0.9746 | Val Loss: 0.1069 Acc: 0.9615                                               \n",
      "Epoch 041 | Train Loss: 0.0701 Acc: 0.9780 | Val Loss: 0.1263 Acc: 0.9624                                               \n",
      "Epoch 042 | Train Loss: 0.0637 Acc: 0.9790 | Val Loss: 0.0870 Acc: 0.9684                                               \n",
      "Epoch 043 | Train Loss: 0.0642 Acc: 0.9810 | Val Loss: 0.0952 Acc: 0.9654                                               \n",
      "Epoch 044 | Train Loss: 0.0603 Acc: 0.9813 | Val Loss: 0.0908 Acc: 0.9678                                               \n",
      "Epoch 045 | Train Loss: 0.0537 Acc: 0.9838 | Val Loss: 0.0952 Acc: 0.9701                                               \n",
      "Epoch 046 | Train Loss: 0.0503 Acc: 0.9846 | Val Loss: 0.0893 Acc: 0.9734                                               \n",
      "Epoch 047 | Train Loss: 0.0452 Acc: 0.9859 | Val Loss: 0.1252 Acc: 0.9710                                               \n",
      "Epoch 048 | Train Loss: 0.0481 Acc: 0.9854 | Val Loss: 0.0873 Acc: 0.9707                                               \n",
      "Epoch 049 | Train Loss: 0.0482 Acc: 0.9858 | Val Loss: 0.1108 Acc: 0.9666                                               \n",
      "Epoch 050 | Train Loss: 0.0425 Acc: 0.9875 | Val Loss: 0.2052 Acc: 0.9472                                               \n",
      "Epoch 051 | Train Loss: 0.0449 Acc: 0.9872 | Val Loss: 0.1073 Acc: 0.9642                                               \n",
      "Epoch 052 | Train Loss: 0.0476 Acc: 0.9854 | Val Loss: 0.0801 Acc: 0.9710                                               \n",
      "Epoch 053 | Train Loss: 0.0354 Acc: 0.9894 | Val Loss: 0.1099 Acc: 0.9672                                               \n",
      "Epoch 054 | Train Loss: 0.0414 Acc: 0.9863 | Val Loss: 0.1133 Acc: 0.9645                                               \n",
      "Epoch 055 | Train Loss: 0.0334 Acc: 0.9892 | Val Loss: 0.1042 Acc: 0.9687                                               \n",
      "Epoch 056 | Train Loss: 0.0351 Acc: 0.9894 | Val Loss: 0.0780 Acc: 0.9806                                               \n",
      "Epoch 057 | Train Loss: 0.0371 Acc: 0.9892 | Val Loss: 0.1018 Acc: 0.9684                                               \n",
      "Epoch 058 | Train Loss: 0.0315 Acc: 0.9916 | Val Loss: 0.0815 Acc: 0.9719                                               \n",
      "Epoch 059 | Train Loss: 0.0288 Acc: 0.9918 | Val Loss: 0.1329 Acc: 0.9624                                               \n",
      "Epoch 060 | Train Loss: 0.0306 Acc: 0.9915 | Val Loss: 0.1238 Acc: 0.9591                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 256, 'cnn_dropout': 0.5995800461258646, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 96, 'learning_rate': 0.00031308682316262993, 'lstm_dense': 128, 'lstm_hidden_size': 96, 'lstm_layers': 1, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 11.8398 Acc: 0.3981 | Val Loss: 7.4619 Acc: 0.4487                                              \n",
      "Epoch 002 | Train Loss: 3.8399 Acc: 0.4555 | Val Loss: 2.0311 Acc: 0.4949                                               \n",
      "Epoch 003 | Train Loss: 2.5319 Acc: 0.4784 | Val Loss: 1.8594 Acc: 0.4696                                               \n",
      "Epoch 004 | Train Loss: 1.8462 Acc: 0.4983 | Val Loss: 2.1650 Acc: 0.4573                                               \n",
      "Epoch 005 | Train Loss: 1.4847 Acc: 0.5234 | Val Loss: 2.8768 Acc: 0.4555                                               \n",
      "Epoch 006 | Train Loss: 1.2651 Acc: 0.5511 | Val Loss: 1.3765 Acc: 0.4976                                               \n",
      "Epoch 007 | Train Loss: 1.1421 Acc: 0.5713 | Val Loss: 0.7069 Acc: 0.6549                                               \n",
      "Epoch 008 | Train Loss: 1.0562 Acc: 0.5773 | Val Loss: 1.0053 Acc: 0.5976                                               \n",
      "Epoch 009 | Train Loss: 0.9501 Acc: 0.6034 | Val Loss: 0.6852 Acc: 0.6376                                               \n",
      "Epoch 010 | Train Loss: 0.8848 Acc: 0.6147 | Val Loss: 0.6366 Acc: 0.7307                                               \n",
      "Epoch 011 | Train Loss: 0.8132 Acc: 0.6386 | Val Loss: 1.9266 Acc: 0.5403                                               \n",
      "Epoch 012 | Train Loss: 0.7623 Acc: 0.6652 | Val Loss: 0.6132 Acc: 0.7060                                               \n",
      "Epoch 013 | Train Loss: 0.7275 Acc: 0.6773 | Val Loss: 0.5445 Acc: 0.7415                                               \n",
      "Epoch 014 | Train Loss: 0.6739 Acc: 0.6979 | Val Loss: 0.6122 Acc: 0.7451                                               \n",
      "Epoch 015 | Train Loss: 0.6622 Acc: 0.7062 | Val Loss: 0.6201 Acc: 0.7845                                               \n",
      "Epoch 016 | Train Loss: 0.6327 Acc: 0.7230 | Val Loss: 1.1327 Acc: 0.5776                                               \n",
      "Epoch 017 | Train Loss: 0.6055 Acc: 0.7295 | Val Loss: 0.7543 Acc: 0.6624                                               \n",
      "Epoch 018 | Train Loss: 0.5840 Acc: 0.7428 | Val Loss: 1.1497 Acc: 0.6406                                               \n",
      "Epoch 019 | Train Loss: 0.5719 Acc: 0.7495 | Val Loss: 0.6709 Acc: 0.6922                                               \n",
      "Epoch 020 | Train Loss: 0.5520 Acc: 0.7580 | Val Loss: 0.3776 Acc: 0.8699                                               \n",
      "Epoch 021 | Train Loss: 0.5406 Acc: 0.7658 | Val Loss: 0.5630 Acc: 0.7266                                               \n",
      "Epoch 022 | Train Loss: 0.5287 Acc: 0.7730 | Val Loss: 0.4963 Acc: 0.7681                                               \n",
      "Epoch 023 | Train Loss: 0.5142 Acc: 0.7832 | Val Loss: 0.6217 Acc: 0.7358                                               \n",
      "Epoch 024 | Train Loss: 0.5051 Acc: 0.7859 | Val Loss: 0.4882 Acc: 0.7937                                               \n",
      "Epoch 025 | Train Loss: 0.4870 Acc: 0.7921 | Val Loss: 0.5700 Acc: 0.7830                                               \n",
      "Epoch 026 | Train Loss: 0.4812 Acc: 0.7971 | Val Loss: 0.3135 Acc: 0.8719                                               \n",
      "Epoch 027 | Train Loss: 0.4685 Acc: 0.8044 | Val Loss: 0.4029 Acc: 0.8540                                               \n",
      "Epoch 028 | Train Loss: 0.4568 Acc: 0.8121 | Val Loss: 0.3042 Acc: 0.8913                                               \n",
      "Epoch 029 | Train Loss: 0.4561 Acc: 0.8162 | Val Loss: 0.5117 Acc: 0.7863                                               \n",
      "Epoch 030 | Train Loss: 0.4439 Acc: 0.8165 | Val Loss: 0.3962 Acc: 0.8045                                               \n",
      "Epoch 031 | Train Loss: 0.4240 Acc: 0.8298 | Val Loss: 0.3243 Acc: 0.8403                                               \n",
      "Epoch 032 | Train Loss: 0.4278 Acc: 0.8315 | Val Loss: 0.3697 Acc: 0.8597                                               \n",
      "Epoch 033 | Train Loss: 0.4154 Acc: 0.8366 | Val Loss: 0.2943 Acc: 0.8875                                               \n",
      "Epoch 034 | Train Loss: 0.3905 Acc: 0.8433 | Val Loss: 0.3813 Acc: 0.8313                                               \n",
      "Epoch 035 | Train Loss: 0.3813 Acc: 0.8544 | Val Loss: 0.2993 Acc: 0.8896                                               \n",
      "Epoch 036 | Train Loss: 0.3631 Acc: 0.8586 | Val Loss: 0.3268 Acc: 0.8588                                               \n",
      "Epoch 037 | Train Loss: 0.3526 Acc: 0.8658 | Val Loss: 0.2348 Acc: 0.9263                                               \n",
      "Epoch 038 | Train Loss: 0.3409 Acc: 0.8708 | Val Loss: 0.2528 Acc: 0.8907                                               \n",
      "Epoch 039 | Train Loss: 0.3071 Acc: 0.8862 | Val Loss: 0.2881 Acc: 0.8806                                               \n",
      "Epoch 040 | Train Loss: 0.2984 Acc: 0.8907 | Val Loss: 0.3534 Acc: 0.8669                                               \n",
      "Epoch 041 | Train Loss: 0.2653 Acc: 0.9029 | Val Loss: 0.6168 Acc: 0.8146                                               \n",
      "Epoch 042 | Train Loss: 0.2635 Acc: 0.9061 | Val Loss: 0.2215 Acc: 0.9322                                               \n",
      "Epoch 043 | Train Loss: 0.2311 Acc: 0.9197 | Val Loss: 0.5301 Acc: 0.8107                                               \n",
      "Epoch 044 | Train Loss: 0.2321 Acc: 0.9227 | Val Loss: 0.2289 Acc: 0.9125                                               \n",
      "Epoch 045 | Train Loss: 0.2046 Acc: 0.9287 | Val Loss: 0.3591 Acc: 0.8967                                               \n",
      "Epoch 046 | Train Loss: 0.1895 Acc: 0.9373 | Val Loss: 0.1442 Acc: 0.9499                                               \n",
      "Epoch 047 | Train Loss: 0.1689 Acc: 0.9428 | Val Loss: 0.1264 Acc: 0.9657                                               \n",
      "Epoch 048 | Train Loss: 0.1579 Acc: 0.9463 | Val Loss: 0.1112 Acc: 0.9704                                               \n",
      "Epoch 049 | Train Loss: 0.1503 Acc: 0.9502 | Val Loss: 0.1221 Acc: 0.9606                                               \n",
      "Epoch 050 | Train Loss: 0.1425 Acc: 0.9516 | Val Loss: 0.0901 Acc: 0.9743                                               \n",
      "Epoch 051 | Train Loss: 0.1240 Acc: 0.9572 | Val Loss: 0.0909 Acc: 0.9710                                               \n",
      "Epoch 052 | Train Loss: 0.1171 Acc: 0.9617 | Val Loss: 0.3659 Acc: 0.8878                                               \n",
      "Epoch 053 | Train Loss: 0.1214 Acc: 0.9601 | Val Loss: 0.1362 Acc: 0.9487                                               \n",
      "Epoch 054 | Train Loss: 0.1060 Acc: 0.9648 | Val Loss: 0.0649 Acc: 0.9824                                               \n",
      "Epoch 055 | Train Loss: 0.1040 Acc: 0.9677 | Val Loss: 0.0790 Acc: 0.9761                                               \n",
      "Epoch 056 | Train Loss: 0.0944 Acc: 0.9698 | Val Loss: 0.0941 Acc: 0.9716                                               \n",
      "Epoch 057 | Train Loss: 0.0945 Acc: 0.9684 | Val Loss: 0.0725 Acc: 0.9773                                               \n",
      "Epoch 058 | Train Loss: 0.0888 Acc: 0.9699 | Val Loss: 0.0980 Acc: 0.9684                                               \n",
      "Epoch 059 | Train Loss: 0.0833 Acc: 0.9727 | Val Loss: 0.0440 Acc: 0.9899                                               \n",
      "Epoch 060 | Train Loss: 0.0733 Acc: 0.9762 | Val Loss: 0.0338 Acc: 0.9899                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 256, 'cnn_dropout': 0.5960083777069589, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 96, 'learning_rate': 0.0003380745856036139, 'lstm_dense': 128, 'lstm_hidden_size': 96, 'lstm_layers': 1, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 8.8432 Acc: 0.4115 | Val Loss: 2.0885 Acc: 0.5430                                               \n",
      "Epoch 002 | Train Loss: 3.0555 Acc: 0.4576 | Val Loss: 3.1954 Acc: 0.4660                                               \n",
      "Epoch 003 | Train Loss: 2.1973 Acc: 0.4897 | Val Loss: 4.1045 Acc: 0.4209                                               \n",
      "Epoch 004 | Train Loss: 1.7797 Acc: 0.4999 | Val Loss: 1.6033 Acc: 0.4967                                               \n",
      "Epoch 005 | Train Loss: 1.5570 Acc: 0.5182 | Val Loss: 1.0150 Acc: 0.6003                                               \n",
      "Epoch 006 | Train Loss: 1.3945 Acc: 0.5382 | Val Loss: 1.2182 Acc: 0.5958                                               \n",
      "Epoch 007 | Train Loss: 1.2983 Acc: 0.5505 | Val Loss: 0.6870 Acc: 0.6815                                               \n",
      "Epoch 008 | Train Loss: 1.2048 Acc: 0.5644 | Val Loss: 0.7472 Acc: 0.5803                                               \n",
      "Epoch 009 | Train Loss: 1.1113 Acc: 0.5806 | Val Loss: 1.9714 Acc: 0.4663                                               \n",
      "Epoch 010 | Train Loss: 1.0357 Acc: 0.5967 | Val Loss: 0.6782 Acc: 0.7304                                               \n",
      "Epoch 011 | Train Loss: 0.9475 Acc: 0.6099 | Val Loss: 1.0919 Acc: 0.5746                                               \n",
      "Epoch 012 | Train Loss: 0.8770 Acc: 0.6355 | Val Loss: 0.8243 Acc: 0.6460                                               \n",
      "Epoch 013 | Train Loss: 0.8184 Acc: 0.6489 | Val Loss: 1.2795 Acc: 0.6212                                               \n",
      "Epoch 014 | Train Loss: 0.7637 Acc: 0.6664 | Val Loss: 0.5017 Acc: 0.7439                                               \n",
      "Epoch 015 | Train Loss: 0.7154 Acc: 0.6868 | Val Loss: 1.4963 Acc: 0.5352                                               \n",
      "Epoch 016 | Train Loss: 0.6735 Acc: 0.6997 | Val Loss: 0.6506 Acc: 0.7188                                               \n",
      "Epoch 017 | Train Loss: 0.6327 Acc: 0.7222 | Val Loss: 0.5966 Acc: 0.7212                                               \n",
      "Epoch 018 | Train Loss: 0.6062 Acc: 0.7395 | Val Loss: 0.5209 Acc: 0.7684                                               \n",
      "Epoch 019 | Train Loss: 0.5850 Acc: 0.7418 | Val Loss: 0.4269 Acc: 0.8337                                               \n",
      "Epoch 020 | Train Loss: 0.5517 Acc: 0.7651 | Val Loss: 0.4515 Acc: 0.8057                                               \n",
      "Epoch 021 | Train Loss: 0.5115 Acc: 0.7885 | Val Loss: 0.6221 Acc: 0.7328                                               \n",
      "Epoch 022 | Train Loss: 0.4744 Acc: 0.8065 | Val Loss: 0.5033 Acc: 0.7991                                               \n",
      "Epoch 023 | Train Loss: 0.4420 Acc: 0.8217 | Val Loss: 0.5028 Acc: 0.7800                                               \n",
      "Epoch 024 | Train Loss: 0.4028 Acc: 0.8469 | Val Loss: 0.3973 Acc: 0.8504                                               \n",
      "Epoch 025 | Train Loss: 0.3614 Acc: 0.8639 | Val Loss: 0.7837 Acc: 0.7773                                               \n",
      "Epoch 026 | Train Loss: 0.3317 Acc: 0.8755 | Val Loss: 0.4003 Acc: 0.8406                                               \n",
      "Epoch 027 | Train Loss: 0.2850 Acc: 0.8907 | Val Loss: 0.3720 Acc: 0.8606                                               \n",
      "Epoch 028 | Train Loss: 0.2681 Acc: 0.9032 | Val Loss: 0.1852 Acc: 0.9301                                               \n",
      "Epoch 029 | Train Loss: 0.2216 Acc: 0.9185 | Val Loss: 0.1181 Acc: 0.9657                                               \n",
      "Epoch 030 | Train Loss: 0.1923 Acc: 0.9331 | Val Loss: 0.1353 Acc: 0.9573                                               \n",
      "Epoch 031 | Train Loss: 0.1679 Acc: 0.9418 | Val Loss: 0.1444 Acc: 0.9451                                               \n",
      "Epoch 032 | Train Loss: 0.1572 Acc: 0.9442 | Val Loss: 0.3179 Acc: 0.8887                                               \n",
      "Epoch 033 | Train Loss: 0.1402 Acc: 0.9543 | Val Loss: 0.1006 Acc: 0.9642                                               \n",
      "Epoch 034 | Train Loss: 0.1277 Acc: 0.9539 | Val Loss: 0.1150 Acc: 0.9600                                               \n",
      "Epoch 035 | Train Loss: 0.1171 Acc: 0.9585 | Val Loss: 0.0763 Acc: 0.9749                                               \n",
      "Epoch 036 | Train Loss: 0.1017 Acc: 0.9647 | Val Loss: 0.0621 Acc: 0.9788                                               \n",
      "Epoch 037 | Train Loss: 0.1028 Acc: 0.9643 | Val Loss: 0.1765 Acc: 0.9406                                               \n",
      "Epoch 038 | Train Loss: 0.0880 Acc: 0.9695 | Val Loss: 0.1242 Acc: 0.9528                                               \n",
      "Epoch 039 | Train Loss: 0.0817 Acc: 0.9718 | Val Loss: 0.0643 Acc: 0.9785                                               \n",
      "Epoch 040 | Train Loss: 0.0764 Acc: 0.9754 | Val Loss: 0.1929 Acc: 0.9346                                               \n",
      "Epoch 041 | Train Loss: 0.0780 Acc: 0.9748 | Val Loss: 0.0669 Acc: 0.9782                                               \n",
      "Epoch 042 | Train Loss: 0.0636 Acc: 0.9798 | Val Loss: 0.0632 Acc: 0.9773                                               \n",
      "Epoch 043 | Train Loss: 0.0637 Acc: 0.9784 | Val Loss: 0.0635 Acc: 0.9812                                               \n",
      "Epoch 044 | Train Loss: 0.0544 Acc: 0.9806 | Val Loss: 0.0633 Acc: 0.9782                                               \n",
      "Epoch 045 | Train Loss: 0.0532 Acc: 0.9818 | Val Loss: 0.0570 Acc: 0.9818                                               \n",
      "Epoch 046 | Train Loss: 0.0527 Acc: 0.9829 | Val Loss: 0.0434 Acc: 0.9851                                               \n",
      "Epoch 047 | Train Loss: 0.0496 Acc: 0.9828 | Val Loss: 0.0580 Acc: 0.9776                                               \n",
      "Epoch 048 | Train Loss: 0.0429 Acc: 0.9863 | Val Loss: 0.0562 Acc: 0.9806                                               \n",
      "Epoch 049 | Train Loss: 0.0449 Acc: 0.9854 | Val Loss: 0.0487 Acc: 0.9845                                               \n",
      "Epoch 050 | Train Loss: 0.0479 Acc: 0.9846 | Val Loss: 0.0467 Acc: 0.9851                                               \n",
      "Epoch 051 | Train Loss: 0.0353 Acc: 0.9887 | Val Loss: 0.0487 Acc: 0.9857                                               \n",
      "Epoch 052 | Train Loss: 0.0440 Acc: 0.9869 | Val Loss: 0.0569 Acc: 0.9824                                               \n",
      "Epoch 053 | Train Loss: 0.0362 Acc: 0.9872 | Val Loss: 0.0836 Acc: 0.9731                                               \n",
      "Epoch 054 | Train Loss: 0.0408 Acc: 0.9867 | Val Loss: 0.0503 Acc: 0.9839                                               \n",
      "Epoch 055 | Train Loss: 0.0333 Acc: 0.9885 | Val Loss: 0.0370 Acc: 0.9869                                               \n",
      "Epoch 056 | Train Loss: 0.0363 Acc: 0.9881 | Val Loss: 0.0412 Acc: 0.9839                                               \n",
      "Epoch 057 | Train Loss: 0.0329 Acc: 0.9895 | Val Loss: 0.0517 Acc: 0.9809                                               \n",
      "Epoch 058 | Train Loss: 0.0358 Acc: 0.9878 | Val Loss: 0.1668 Acc: 0.9475                                               \n",
      "Epoch 059 | Train Loss: 0.0323 Acc: 0.9894 | Val Loss: 0.0433 Acc: 0.9866                                               \n",
      "Epoch 060 | Train Loss: 0.0290 Acc: 0.9901 | Val Loss: 0.0366 Acc: 0.9896                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 256, 'cnn_dropout': 0.6870449996194248, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 96, 'learning_rate': 0.0005272824290771867, 'lstm_dense': 128, 'lstm_hidden_size': 96, 'lstm_layers': 1, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 3.5331 Acc: 0.4188 | Val Loss: 1.2769 Acc: 0.4421                                               \n",
      "Epoch 002 | Train Loss: 1.2670 Acc: 0.4422 | Val Loss: 1.2553 Acc: 0.4421                                               \n",
      "Epoch 003 | Train Loss: 1.2512 Acc: 0.4422 | Val Loss: 1.2442 Acc: 0.4421                                               \n",
      "Epoch 004 | Train Loss: 1.2426 Acc: 0.4422 | Val Loss: 1.2376 Acc: 0.4421                                               \n",
      "Epoch 005 | Train Loss: 1.2379 Acc: 0.4420 | Val Loss: 1.2330 Acc: 0.4427                                               \n",
      "Epoch 006 | Train Loss: 1.2344 Acc: 0.4426 | Val Loss: 1.2286 Acc: 0.4424                                               \n",
      "Epoch 007 | Train Loss: 1.2295 Acc: 0.4432 | Val Loss: 1.2229 Acc: 0.4430                                               \n",
      "Epoch 008 | Train Loss: 1.2219 Acc: 0.4435 | Val Loss: 1.2199 Acc: 0.4475                                               \n",
      "Epoch 009 | Train Loss: 1.2113 Acc: 0.4455 | Val Loss: 1.2076 Acc: 0.4591                                               \n",
      "Epoch 010 | Train Loss: 1.1996 Acc: 0.4547 | Val Loss: 1.2028 Acc: 0.4549                                               \n",
      "Epoch 011 | Train Loss: 1.1843 Acc: 0.4623 | Val Loss: 1.1916 Acc: 0.4484                                               \n",
      "Epoch 012 | Train Loss: 1.1740 Acc: 0.4696 | Val Loss: 1.1605 Acc: 0.4857                                               \n",
      "Epoch 013 | Train Loss: 1.1597 Acc: 0.4864 | Val Loss: 1.1536 Acc: 0.4734                                               \n",
      "Epoch 014 | Train Loss: 1.1512 Acc: 0.4896 | Val Loss: 1.1410 Acc: 0.4851                                               \n",
      "Epoch 015 | Train Loss: 1.1387 Acc: 0.5024 | Val Loss: 1.1275 Acc: 0.4878                                               \n",
      "Epoch 016 | Train Loss: 1.1224 Acc: 0.5044 | Val Loss: 1.1155 Acc: 0.4848                                               \n",
      "Epoch 017 | Train Loss: 1.1069 Acc: 0.5121 | Val Loss: 1.1005 Acc: 0.5048                                               \n",
      "Epoch 018 | Train Loss: 1.0823 Acc: 0.5133 | Val Loss: 1.1334 Acc: 0.5057                                               \n",
      "Epoch 019 | Train Loss: 1.0540 Acc: 0.5303 | Val Loss: 1.1212 Acc: 0.5128                                               \n",
      "Epoch 020 | Train Loss: 1.0147 Acc: 0.5618 | Val Loss: 0.9738 Acc: 0.5701                                               \n",
      "Epoch 021 | Train Loss: 0.9853 Acc: 0.5856 | Val Loss: 0.9618 Acc: 0.5896                                               \n",
      "Epoch 022 | Train Loss: 0.9590 Acc: 0.6046 | Val Loss: 0.9324 Acc: 0.5991                                               \n",
      "Epoch 023 | Train Loss: 0.9284 Acc: 0.6153 | Val Loss: 0.9254 Acc: 0.6161                                               \n",
      "Epoch 024 | Train Loss: 0.9039 Acc: 0.6285 | Val Loss: 0.9679 Acc: 0.5860                                               \n",
      "Epoch 025 | Train Loss: 0.8815 Acc: 0.6417 | Val Loss: 0.8490 Acc: 0.6543                                               \n",
      "Epoch 026 | Train Loss: 0.8581 Acc: 0.6583 | Val Loss: 0.8900 Acc: 0.6448                                               \n",
      "Epoch 027 | Train Loss: 0.8301 Acc: 0.6668 | Val Loss: 0.8055 Acc: 0.6809                                               \n",
      "Epoch 028 | Train Loss: 0.7994 Acc: 0.6831 | Val Loss: 0.7381 Acc: 0.7116                                               \n",
      "Epoch 029 | Train Loss: 0.7735 Acc: 0.6912 | Val Loss: 0.8205 Acc: 0.6845                                               \n",
      "Epoch 030 | Train Loss: 0.7430 Acc: 0.7099 | Val Loss: 0.7697 Acc: 0.6812                                               \n",
      "Epoch 031 | Train Loss: 0.7316 Acc: 0.7108 | Val Loss: 0.6519 Acc: 0.7543                                               \n",
      "Epoch 032 | Train Loss: 0.7019 Acc: 0.7239 | Val Loss: 0.6755 Acc: 0.7361                                               \n",
      "Epoch 033 | Train Loss: 0.6810 Acc: 0.7344 | Val Loss: 0.7865 Acc: 0.7104                                               \n",
      "Epoch 034 | Train Loss: 0.6780 Acc: 0.7346 | Val Loss: 0.5785 Acc: 0.7785                                               \n",
      "Epoch 035 | Train Loss: 0.6263 Acc: 0.7539 | Val Loss: 0.5686 Acc: 0.7707                                               \n",
      "Epoch 036 | Train Loss: 0.6024 Acc: 0.7640 | Val Loss: 0.5700 Acc: 0.7809                                               \n",
      "Epoch 037 | Train Loss: 0.5765 Acc: 0.7736 | Val Loss: 0.6557 Acc: 0.7501                                               \n",
      "Epoch 038 | Train Loss: 0.5631 Acc: 0.7774 | Val Loss: 0.5074 Acc: 0.7997                                               \n",
      "Epoch 039 | Train Loss: 0.5247 Acc: 0.7908 | Val Loss: 0.5247 Acc: 0.7946                                               \n",
      "Epoch 040 | Train Loss: 0.4990 Acc: 0.8030 | Val Loss: 0.5649 Acc: 0.7881                                               \n",
      "Epoch 041 | Train Loss: 0.4911 Acc: 0.8076 | Val Loss: 0.4750 Acc: 0.8131                                               \n",
      "Epoch 042 | Train Loss: 0.4683 Acc: 0.8189 | Val Loss: 0.5537 Acc: 0.8018                                               \n",
      "Epoch 043 | Train Loss: 0.4506 Acc: 0.8283 | Val Loss: 0.5171 Acc: 0.8099                                               \n",
      "Epoch 044 | Train Loss: 0.4443 Acc: 0.8277 | Val Loss: 0.4631 Acc: 0.8227                                               \n",
      "Epoch 045 | Train Loss: 0.4080 Acc: 0.8470 | Val Loss: 0.3739 Acc: 0.8588                                               \n",
      "Epoch 046 | Train Loss: 0.3995 Acc: 0.8560 | Val Loss: 0.4427 Acc: 0.8376                                               \n",
      "Epoch 047 | Train Loss: 0.3911 Acc: 0.8557 | Val Loss: 0.4305 Acc: 0.8439                                               \n",
      "Epoch 048 | Train Loss: 0.3674 Acc: 0.8710 | Val Loss: 0.4299 Acc: 0.8472                                               \n",
      "Epoch 049 | Train Loss: 0.3601 Acc: 0.8713 | Val Loss: 0.3601 Acc: 0.8749                                               \n",
      "Epoch 050 | Train Loss: 0.3479 Acc: 0.8752 | Val Loss: 0.4606 Acc: 0.8400                                               \n",
      "Epoch 051 | Train Loss: 0.3203 Acc: 0.8875 | Val Loss: 0.3613 Acc: 0.8731                                               \n",
      "Epoch 052 | Train Loss: 0.3155 Acc: 0.8889 | Val Loss: 0.3789 Acc: 0.8701                                               \n",
      "Epoch 053 | Train Loss: 0.3097 Acc: 0.8915 | Val Loss: 0.3207 Acc: 0.8884                                               \n",
      "Epoch 054 | Train Loss: 0.2786 Acc: 0.9019 | Val Loss: 0.3753 Acc: 0.8746                                               \n",
      "Epoch 055 | Train Loss: 0.2719 Acc: 0.9051 | Val Loss: 0.4335 Acc: 0.8561                                               \n",
      "Epoch 056 | Train Loss: 0.2604 Acc: 0.9086 | Val Loss: 0.4098 Acc: 0.8719                                               \n",
      "Epoch 057 | Train Loss: 0.2671 Acc: 0.9059 | Val Loss: 0.4007 Acc: 0.8678                                               \n",
      "Epoch 058 | Train Loss: 0.2261 Acc: 0.9225 | Val Loss: 0.2898 Acc: 0.9042                                               \n",
      "Epoch 059 | Train Loss: 0.2241 Acc: 0.9208 | Val Loss: 0.3021 Acc: 0.8970                                               \n",
      "Epoch 060 | Train Loss: 0.2170 Acc: 0.9260 | Val Loss: 0.2660 Acc: 0.9078                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 256, 'cnn_dropout': 0.577704751398205, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 96, 'learning_rate': 0.00021659080078565443, 'lstm_dense': 128, 'lstm_hidden_size': 96, 'lstm_layers': 1, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 16.8157 Acc: 0.4002 | Val Loss: 3.8247 Acc: 0.4955                                              \n",
      "Epoch 002 | Train Loss: 6.6558 Acc: 0.4453 | Val Loss: 2.7449 Acc: 0.4609                                               \n",
      "Epoch 003 | Train Loss: 4.1815 Acc: 0.4578 | Val Loss: 1.7886 Acc: 0.5570                                               \n",
      "Epoch 004 | Train Loss: 2.9098 Acc: 0.4727 | Val Loss: 1.2625 Acc: 0.5287                                               \n",
      "Epoch 005 | Train Loss: 2.2003 Acc: 0.4976 | Val Loss: 2.3998 Acc: 0.5170                                               \n",
      "Epoch 006 | Train Loss: 1.7489 Acc: 0.5183 | Val Loss: 0.7820 Acc: 0.6358                                               \n",
      "Epoch 007 | Train Loss: 1.4720 Acc: 0.5345 | Val Loss: 1.6876 Acc: 0.5436                                               \n",
      "Epoch 008 | Train Loss: 1.2139 Acc: 0.5491 | Val Loss: 1.9331 Acc: 0.5260                                               \n",
      "Epoch 009 | Train Loss: 1.0722 Acc: 0.5658 | Val Loss: 0.7375 Acc: 0.5961                                               \n",
      "Epoch 010 | Train Loss: 0.9319 Acc: 0.5976 | Val Loss: 0.6853 Acc: 0.6836                                               \n",
      "Epoch 011 | Train Loss: 0.8378 Acc: 0.6233 | Val Loss: 0.9059 Acc: 0.6469                                               \n",
      "Epoch 012 | Train Loss: 0.7833 Acc: 0.6456 | Val Loss: 0.9389 Acc: 0.5239                                               \n",
      "Epoch 013 | Train Loss: 0.7197 Acc: 0.6736 | Val Loss: 0.6057 Acc: 0.6887                                               \n",
      "Epoch 014 | Train Loss: 0.6525 Acc: 0.7115 | Val Loss: 0.7627 Acc: 0.6681                                               \n",
      "Epoch 015 | Train Loss: 0.6214 Acc: 0.7274 | Val Loss: 0.5157 Acc: 0.7812                                               \n",
      "Epoch 016 | Train Loss: 0.5820 Acc: 0.7460 | Val Loss: 0.7604 Acc: 0.6442                                               \n",
      "Epoch 017 | Train Loss: 0.5482 Acc: 0.7611 | Val Loss: 0.5408 Acc: 0.7699                                               \n",
      "Epoch 018 | Train Loss: 0.5113 Acc: 0.7863 | Val Loss: 0.4253 Acc: 0.8143                                               \n",
      "Epoch 019 | Train Loss: 0.4512 Acc: 0.8157 | Val Loss: 0.5215 Acc: 0.7612                                               \n",
      "Epoch 020 | Train Loss: 0.3994 Acc: 0.8399 | Val Loss: 0.3876 Acc: 0.8331                                               \n",
      "Epoch 021 | Train Loss: 0.3570 Acc: 0.8610 | Val Loss: 0.2726 Acc: 0.8904                                               \n",
      "Epoch 022 | Train Loss: 0.3284 Acc: 0.8724 | Val Loss: 0.2449 Acc: 0.9018                                               \n",
      "Epoch 023 | Train Loss: 0.2906 Acc: 0.8919 | Val Loss: 0.2529 Acc: 0.8961                                               \n",
      "Epoch 024 | Train Loss: 0.2679 Acc: 0.8990 | Val Loss: 0.8993 Acc: 0.7284                                               \n",
      "Epoch 025 | Train Loss: 0.2316 Acc: 0.9182 | Val Loss: 0.2601 Acc: 0.8961                                               \n",
      "Epoch 026 | Train Loss: 0.2175 Acc: 0.9225 | Val Loss: 0.1810 Acc: 0.9266                                               \n",
      "Epoch 027 | Train Loss: 0.1939 Acc: 0.9297 | Val Loss: 0.1651 Acc: 0.9400                                               \n",
      "Epoch 028 | Train Loss: 0.1790 Acc: 0.9373 | Val Loss: 0.0969 Acc: 0.9696                                               \n",
      "Epoch 029 | Train Loss: 0.1549 Acc: 0.9460 | Val Loss: 0.3091 Acc: 0.8884                                               \n",
      "Epoch 030 | Train Loss: 0.1468 Acc: 0.9497 | Val Loss: 0.1297 Acc: 0.9522                                               \n",
      "Epoch 031 | Train Loss: 0.1240 Acc: 0.9582 | Val Loss: 0.0918 Acc: 0.9693                                               \n",
      "Epoch 032 | Train Loss: 0.1192 Acc: 0.9581 | Val Loss: 0.0690 Acc: 0.9794                                               \n",
      "Epoch 033 | Train Loss: 0.1070 Acc: 0.9640 | Val Loss: 0.0849 Acc: 0.9725                                               \n",
      "Epoch 034 | Train Loss: 0.0953 Acc: 0.9676 | Val Loss: 0.0981 Acc: 0.9681                                               \n",
      "Epoch 035 | Train Loss: 0.0909 Acc: 0.9691 | Val Loss: 0.1157 Acc: 0.9627                                               \n",
      "Epoch 036 | Train Loss: 0.0871 Acc: 0.9698 | Val Loss: 0.0761 Acc: 0.9734                                               \n",
      "Epoch 037 | Train Loss: 0.0789 Acc: 0.9709 | Val Loss: 0.0691 Acc: 0.9743                                               \n",
      "Epoch 038 | Train Loss: 0.0726 Acc: 0.9763 | Val Loss: 0.0846 Acc: 0.9725                                               \n",
      "Epoch 039 | Train Loss: 0.0690 Acc: 0.9777 | Val Loss: 0.0958 Acc: 0.9639                                               \n",
      "Epoch 040 | Train Loss: 0.0635 Acc: 0.9785 | Val Loss: 0.0463 Acc: 0.9848                                               \n",
      "Epoch 041 | Train Loss: 0.0550 Acc: 0.9816 | Val Loss: 0.1019 Acc: 0.9636                                               \n",
      "Epoch 042 | Train Loss: 0.0613 Acc: 0.9784 | Val Loss: 0.0367 Acc: 0.9872                                               \n",
      "Epoch 043 | Train Loss: 0.0525 Acc: 0.9815 | Val Loss: 0.0511 Acc: 0.9830                                               \n",
      "Epoch 044 | Train Loss: 0.0526 Acc: 0.9819 | Val Loss: 0.0421 Acc: 0.9830                                               \n",
      "Epoch 045 | Train Loss: 0.0519 Acc: 0.9838 | Val Loss: 0.0898 Acc: 0.9704                                               \n",
      "Epoch 046 | Train Loss: 0.0456 Acc: 0.9842 | Val Loss: 0.0344 Acc: 0.9887                                               \n",
      "Epoch 047 | Train Loss: 0.0423 Acc: 0.9854 | Val Loss: 0.0334 Acc: 0.9881                                               \n",
      "Epoch 048 | Train Loss: 0.0417 Acc: 0.9853 | Val Loss: 0.0799 Acc: 0.9740                                               \n",
      "Epoch 049 | Train Loss: 0.0400 Acc: 0.9865 | Val Loss: 0.0305 Acc: 0.9899                                               \n",
      "Epoch 050 | Train Loss: 0.0383 Acc: 0.9876 | Val Loss: 0.0870 Acc: 0.9693                                               \n",
      "Epoch 051 | Train Loss: 0.0355 Acc: 0.9889 | Val Loss: 0.0422 Acc: 0.9872                                               \n",
      "Epoch 052 | Train Loss: 0.0336 Acc: 0.9895 | Val Loss: 0.3256 Acc: 0.8970                                               \n",
      "Epoch 053 | Train Loss: 0.0316 Acc: 0.9885 | Val Loss: 0.0209 Acc: 0.9940                                               \n",
      "Epoch 054 | Train Loss: 0.0304 Acc: 0.9903 | Val Loss: 0.0485 Acc: 0.9848                                               \n",
      "Epoch 055 | Train Loss: 0.0354 Acc: 0.9875 | Val Loss: 0.0318 Acc: 0.9904                                               \n",
      "Epoch 056 | Train Loss: 0.0344 Acc: 0.9897 | Val Loss: 0.0284 Acc: 0.9893                                               \n",
      "Epoch 057 | Train Loss: 0.0293 Acc: 0.9913 | Val Loss: 0.0383 Acc: 0.9881                                               \n",
      "Epoch 058 | Train Loss: 0.0289 Acc: 0.9908 | Val Loss: 0.0414 Acc: 0.9875                                               \n",
      "Epoch 059 | Train Loss: 0.0279 Acc: 0.9909 | Val Loss: 0.0233 Acc: 0.9928                                               \n",
      "Epoch 060 | Train Loss: 0.0290 Acc: 0.9907 | Val Loss: 0.0361 Acc: 0.9901                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 256, 'cnn_dropout': 0.3571503000785696, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 96, 'learning_rate': 5.7237300879481257e-05, 'lstm_dense': 128, 'lstm_hidden_size': 96, 'lstm_layers': 5, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 28.6257 Acc: 0.3458 | Val Loss: 2.8068 Acc: 0.4164                                              \n",
      "Epoch 002 | Train Loss: 9.7069 Acc: 0.3990 | Val Loss: 2.6936 Acc: 0.4451                                               \n",
      "Epoch 003 | Train Loss: 6.3946 Acc: 0.4209 | Val Loss: 2.1960 Acc: 0.4716                                               \n",
      "Epoch 004 | Train Loss: 4.6750 Acc: 0.4379 | Val Loss: 2.1045 Acc: 0.4982                                               \n",
      "Epoch 005 | Train Loss: 3.6719 Acc: 0.4559 | Val Loss: 1.4643 Acc: 0.5800                                               \n",
      "Epoch 006 | Train Loss: 2.8510 Acc: 0.4853 | Val Loss: 1.4060 Acc: 0.5803                                               \n",
      "Epoch 007 | Train Loss: 2.3329 Acc: 0.5174 | Val Loss: 1.6163 Acc: 0.6194                                               \n",
      "Epoch 008 | Train Loss: 2.0007 Acc: 0.5406 | Val Loss: 1.2816 Acc: 0.5645                                               \n",
      "Epoch 009 | Train Loss: 1.7200 Acc: 0.5576 | Val Loss: 1.1070 Acc: 0.6663                                               \n",
      "Epoch 010 | Train Loss: 1.5263 Acc: 0.5897 | Val Loss: 0.7158 Acc: 0.7301                                               \n",
      "Epoch 011 | Train Loss: 1.3210 Acc: 0.6218 | Val Loss: 1.1834 Acc: 0.6448                                               \n",
      "Epoch 012 | Train Loss: 1.2038 Acc: 0.6462 | Val Loss: 0.6869 Acc: 0.6788                                               \n",
      "Epoch 013 | Train Loss: 1.0501 Acc: 0.6771 | Val Loss: 0.8651 Acc: 0.7218                                               \n",
      "Epoch 014 | Train Loss: 0.9507 Acc: 0.7034 | Val Loss: 0.5209 Acc: 0.8101                                               \n",
      "Epoch 015 | Train Loss: 0.8787 Acc: 0.7215 | Val Loss: 0.5729 Acc: 0.7866                                               \n",
      "Epoch 016 | Train Loss: 0.7885 Acc: 0.7380 | Val Loss: 0.6081 Acc: 0.7770                                               \n",
      "Epoch 017 | Train Loss: 0.7074 Acc: 0.7638 | Val Loss: 0.9005 Acc: 0.7725                                               \n",
      "Epoch 018 | Train Loss: 0.6562 Acc: 0.7864 | Val Loss: 0.7497 Acc: 0.7104                                               \n",
      "Epoch 019 | Train Loss: 0.5889 Acc: 0.8035 | Val Loss: 0.4036 Acc: 0.8475                                               \n",
      "Epoch 020 | Train Loss: 0.5382 Acc: 0.8224 | Val Loss: 0.7184 Acc: 0.7722                                               \n",
      "Epoch 021 | Train Loss: 0.5017 Acc: 0.8283 | Val Loss: 0.3429 Acc: 0.8899                                               \n",
      "Epoch 022 | Train Loss: 0.4578 Acc: 0.8461 | Val Loss: 1.3173 Acc: 0.6669                                               \n",
      "Epoch 023 | Train Loss: 0.4485 Acc: 0.8489 | Val Loss: 0.3866 Acc: 0.8433                                               \n",
      "Epoch 024 | Train Loss: 0.3978 Acc: 0.8642 | Val Loss: 0.4914 Acc: 0.8406                                               \n",
      "Epoch 025 | Train Loss: 0.3808 Acc: 0.8739 | Val Loss: 0.3758 Acc: 0.8866                                               \n",
      "Epoch 026 | Train Loss: 0.3437 Acc: 0.8856 | Val Loss: 0.5520 Acc: 0.8293                                               \n",
      "Epoch 027 | Train Loss: 0.3233 Acc: 0.8892 | Val Loss: 0.4031 Acc: 0.8690                                               \n",
      "Epoch 028 | Train Loss: 0.2952 Acc: 0.9034 | Val Loss: 0.2472 Acc: 0.9224                                               \n",
      "Epoch 029 | Train Loss: 0.2845 Acc: 0.9066 | Val Loss: 0.4237 Acc: 0.8788                                               \n",
      "Epoch 030 | Train Loss: 0.2544 Acc: 0.9148 | Val Loss: 0.6404 Acc: 0.7696                                               \n",
      "Epoch 031 | Train Loss: 0.2445 Acc: 0.9169 | Val Loss: 0.2628 Acc: 0.9075                                               \n",
      "Epoch 032 | Train Loss: 0.2240 Acc: 0.9231 | Val Loss: 0.2329 Acc: 0.9140                                               \n",
      "Epoch 033 | Train Loss: 0.2094 Acc: 0.9304 | Val Loss: 0.2528 Acc: 0.9087                                               \n",
      "Epoch 034 | Train Loss: 0.2050 Acc: 0.9306 | Val Loss: 0.1716 Acc: 0.9379                                               \n",
      "Epoch 035 | Train Loss: 0.1904 Acc: 0.9346 | Val Loss: 0.2907 Acc: 0.9030                                               \n",
      "Epoch 036 | Train Loss: 0.1839 Acc: 0.9381 | Val Loss: 0.1923 Acc: 0.9310                                               \n",
      "Epoch 037 | Train Loss: 0.1652 Acc: 0.9463 | Val Loss: 0.3881 Acc: 0.8716                                               \n",
      "Epoch 038 | Train Loss: 0.1604 Acc: 0.9460 | Val Loss: 0.1516 Acc: 0.9454                                               \n",
      "Epoch 039 | Train Loss: 0.1407 Acc: 0.9518 | Val Loss: 0.2577 Acc: 0.9084                                               \n",
      "Epoch 040 | Train Loss: 0.1400 Acc: 0.9545 | Val Loss: 0.1765 Acc: 0.9364                                               \n",
      "Epoch 041 | Train Loss: 0.1285 Acc: 0.9578 | Val Loss: 0.1339 Acc: 0.9519                                               \n",
      "Epoch 042 | Train Loss: 0.1192 Acc: 0.9613 | Val Loss: 0.1300 Acc: 0.9528                                               \n",
      "Epoch 043 | Train Loss: 0.1026 Acc: 0.9669 | Val Loss: 0.1509 Acc: 0.9507                                               \n",
      "Epoch 044 | Train Loss: 0.0920 Acc: 0.9716 | Val Loss: 0.1618 Acc: 0.9445                                               \n",
      "Epoch 045 | Train Loss: 0.0916 Acc: 0.9710 | Val Loss: 0.1724 Acc: 0.9418                                               \n",
      "Epoch 046 | Train Loss: 0.0794 Acc: 0.9757 | Val Loss: 0.1223 Acc: 0.9624                                               \n",
      "Epoch 047 | Train Loss: 0.0777 Acc: 0.9773 | Val Loss: 0.1447 Acc: 0.9537                                               \n",
      "Epoch 048 | Train Loss: 0.0716 Acc: 0.9776 | Val Loss: 0.1540 Acc: 0.9579                                               \n",
      "Epoch 049 | Train Loss: 0.0654 Acc: 0.9788 | Val Loss: 0.1067 Acc: 0.9699                                               \n",
      "Epoch 050 | Train Loss: 0.0645 Acc: 0.9812 | Val Loss: 0.1136 Acc: 0.9701                                               \n",
      "Epoch 051 | Train Loss: 0.0647 Acc: 0.9804 | Val Loss: 0.1476 Acc: 0.9570                                               \n",
      "Epoch 052 | Train Loss: 0.0561 Acc: 0.9826 | Val Loss: 0.1398 Acc: 0.9570                                               \n",
      "Epoch 053 | Train Loss: 0.0521 Acc: 0.9843 | Val Loss: 0.1109 Acc: 0.9672                                               \n",
      "Epoch 054 | Train Loss: 0.0583 Acc: 0.9825 | Val Loss: 0.1020 Acc: 0.9701                                               \n",
      "Epoch 055 | Train Loss: 0.0458 Acc: 0.9861 | Val Loss: 0.0871 Acc: 0.9719                                               \n",
      "Epoch 056 | Train Loss: 0.0450 Acc: 0.9872 | Val Loss: 0.0986 Acc: 0.9725                                               \n",
      "Epoch 057 | Train Loss: 0.0497 Acc: 0.9842 | Val Loss: 0.2717 Acc: 0.9337                                               \n",
      "Epoch 058 | Train Loss: 0.0423 Acc: 0.9881 | Val Loss: 0.1422 Acc: 0.9624                                               \n",
      "Epoch 059 | Train Loss: 0.0426 Acc: 0.9862 | Val Loss: 0.1044 Acc: 0.9693                                               \n",
      "Epoch 060 | Train Loss: 0.0400 Acc: 0.9880 | Val Loss: 0.0830 Acc: 0.9764                                               \n",
      "100%|████████████████████████████████████████████████| 30/30 [25:24<00:00, 50.82s/trial, best loss: 0.02091915539479745]\n",
      "Best hyperparameters: {'batch_size': np.int64(2), 'cnn_dense': np.int64(3), 'cnn_dropout': np.float64(0.577704751398205), 'cnn_kernel_size_1': np.int64(0), 'cnn_kernel_size_2': np.int64(1), 'cnn_kernels_1': np.int64(3), 'cnn_kernels_2': np.int64(3), 'learning_rate': np.float64(0.00021659080078565443), 'lstm_dense': np.int64(2), 'lstm_hidden_size': np.int64(2), 'lstm_layers': np.int64(0), 'optimizer': np.int64(1)}\n",
      "TPE search finished in 1524.58 seconds\n",
      "Best (raw indices): {'batch_size': np.int64(2), 'cnn_dense': np.int64(3), 'cnn_dropout': np.float64(0.577704751398205), 'cnn_kernel_size_1': np.int64(0), 'cnn_kernel_size_2': np.int64(1), 'cnn_kernels_1': np.int64(3), 'cnn_kernels_2': np.int64(3), 'learning_rate': np.float64(0.00021659080078565443), 'lstm_dense': np.int64(2), 'lstm_hidden_size': np.int64(2), 'lstm_layers': np.int64(0), 'optimizer': np.int64(1)}\n",
      "Best (interpreted): {'batch_size': 48, 'cnn_dense': 256, 'cnn_dropout': np.float64(0.577704751398205), 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 96, 'learning_rate': np.float64(0.00021659080078565443), 'lstm_dense': 128, 'lstm_hidden_size': 96, 'lstm_layers': 1, 'optimizer': 'rmsprop'}\n",
      "Starting TPE search...\n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 256, 'cnn_dropout': 0.6401000978281256, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 32, 'learning_rate': 0.0012697364366207515, 'lstm_dense': 128, 'lstm_hidden_size': 32, 'lstm_layers': 5, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 34.3050 Acc: 0.3918 | Val Loss: 2.9909 Acc: 0.4690                                              \n",
      "Epoch 002 | Train Loss: 5.0032 Acc: 0.4559 | Val Loss: 1.8688 Acc: 0.4872                                               \n",
      "Epoch 003 | Train Loss: 2.7605 Acc: 0.4773 | Val Loss: 1.1313 Acc: 0.5284                                               \n",
      "Epoch 004 | Train Loss: 1.7837 Acc: 0.5242 | Val Loss: 0.7067 Acc: 0.6755                                               \n",
      "Epoch 005 | Train Loss: 1.4395 Acc: 0.5383 | Val Loss: 0.7300 Acc: 0.7218                                               \n",
      "Epoch 006 | Train Loss: 1.2044 Acc: 0.5645 | Val Loss: 0.7445 Acc: 0.6328                                               \n",
      "Epoch 007 | Train Loss: 1.0868 Acc: 0.5700 | Val Loss: 0.7159 Acc: 0.6979                                               \n",
      "Epoch 008 | Train Loss: 0.9598 Acc: 0.5853 | Val Loss: 0.8202 Acc: 0.6988                                               \n",
      "Epoch 009 | Train Loss: 0.8871 Acc: 0.6074 | Val Loss: 0.6931 Acc: 0.7107                                               \n",
      "Epoch 010 | Train Loss: 0.8415 Acc: 0.6173 | Val Loss: 0.8300 Acc: 0.5946                                               \n",
      "Epoch 011 | Train Loss: 0.8188 Acc: 0.6313 | Val Loss: 0.7332 Acc: 0.6731                                               \n",
      "Epoch 012 | Train Loss: 0.7437 Acc: 0.6623 | Val Loss: 0.7848 Acc: 0.6113                                               \n",
      "Epoch 013 | Train Loss: 0.7229 Acc: 0.6712 | Val Loss: 0.7035 Acc: 0.6657                                               \n",
      "Epoch 014 | Train Loss: 0.7022 Acc: 0.6783 | Val Loss: 0.6623 Acc: 0.7448                                               \n",
      "Epoch 015 | Train Loss: 0.6903 Acc: 0.6838 | Val Loss: 0.6611 Acc: 0.7469                                               \n",
      "Epoch 016 | Train Loss: 0.6810 Acc: 0.6921 | Val Loss: 0.6138 Acc: 0.7030                                               \n",
      "Epoch 017 | Train Loss: 0.6610 Acc: 0.6993 | Val Loss: 0.6363 Acc: 0.6854                                               \n",
      "Epoch 018 | Train Loss: 0.6572 Acc: 0.7060 | Val Loss: 0.6321 Acc: 0.6970                                               \n",
      "Epoch 019 | Train Loss: 0.6568 Acc: 0.7071 | Val Loss: 0.6115 Acc: 0.7134                                               \n",
      "Epoch 020 | Train Loss: 0.6317 Acc: 0.7166 | Val Loss: 0.6017 Acc: 0.7513                                               \n",
      "Epoch 021 | Train Loss: 0.6148 Acc: 0.7268 | Val Loss: 0.5591 Acc: 0.7588                                               \n",
      "Epoch 022 | Train Loss: 0.6031 Acc: 0.7304 | Val Loss: 0.6126 Acc: 0.6836                                               \n",
      "Epoch 023 | Train Loss: 0.5770 Acc: 0.7434 | Val Loss: 0.5625 Acc: 0.7699                                               \n",
      "Epoch 024 | Train Loss: 0.5944 Acc: 0.7447 | Val Loss: 0.6166 Acc: 0.7370                                               \n",
      "Epoch 025 | Train Loss: 0.5844 Acc: 0.7540 | Val Loss: 0.5349 Acc: 0.7618                                               \n",
      "Epoch 026 | Train Loss: 0.5449 Acc: 0.7721 | Val Loss: 0.5381 Acc: 0.7513                                               \n",
      "Epoch 027 | Train Loss: 0.5414 Acc: 0.7772 | Val Loss: 0.5199 Acc: 0.7293                                               \n",
      "Epoch 028 | Train Loss: 0.5006 Acc: 0.7874 | Val Loss: 0.5732 Acc: 0.7328                                               \n",
      "Epoch 029 | Train Loss: 0.5142 Acc: 0.7843 | Val Loss: 0.4600 Acc: 0.8397                                               \n",
      "Epoch 030 | Train Loss: 0.4828 Acc: 0.7946 | Val Loss: 0.4414 Acc: 0.8496                                               \n",
      "Epoch 031 | Train Loss: 0.4644 Acc: 0.8048 | Val Loss: 0.4686 Acc: 0.8182                                               \n",
      "Epoch 032 | Train Loss: 0.4667 Acc: 0.8039 | Val Loss: 0.4726 Acc: 0.7854                                               \n",
      "Epoch 033 | Train Loss: 0.4522 Acc: 0.8094 | Val Loss: 0.3676 Acc: 0.8287                                               \n",
      "Epoch 034 | Train Loss: 0.4216 Acc: 0.8240 | Val Loss: 0.4399 Acc: 0.8101                                               \n",
      "Epoch 035 | Train Loss: 0.4423 Acc: 0.8147 | Val Loss: 0.3690 Acc: 0.8519                                               \n",
      "Epoch 036 | Train Loss: 0.4183 Acc: 0.8277 | Val Loss: 0.4093 Acc: 0.8236                                               \n",
      "Epoch 037 | Train Loss: 0.4063 Acc: 0.8318 | Val Loss: 0.3777 Acc: 0.8627                                               \n",
      "Epoch 038 | Train Loss: 0.3994 Acc: 0.8365 | Val Loss: 0.4249 Acc: 0.8078                                               \n",
      "Epoch 039 | Train Loss: 0.4363 Acc: 0.8211 | Val Loss: 0.4003 Acc: 0.8287                                               \n",
      "Epoch 040 | Train Loss: 0.4186 Acc: 0.8293 | Val Loss: 0.4278 Acc: 0.8146                                               \n",
      "Epoch 041 | Train Loss: 0.3849 Acc: 0.8453 | Val Loss: 0.5230 Acc: 0.8078                                               \n",
      "Epoch 042 | Train Loss: 0.4171 Acc: 0.8252 | Val Loss: 0.4724 Acc: 0.8206                                               \n",
      "Epoch 043 | Train Loss: 0.4117 Acc: 0.8322 | Val Loss: 0.4768 Acc: 0.8060                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 256, 'cnn_dropout': 0.4444003160269533, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 32, 'cnn_kernels_2': 64, 'learning_rate': 4.1509222884390874e-05, 'lstm_dense': 32, 'lstm_hidden_size': 32, 'lstm_layers': 4, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 75.0980 Acc: 0.3026 | Val Loss: 42.1304 Acc: 0.2093                                             \n",
      "Epoch 002 | Train Loss: 33.4103 Acc: 0.3420 | Val Loss: 15.9284 Acc: 0.2946                                             \n",
      "Epoch 003 | Train Loss: 21.5095 Acc: 0.3535 | Val Loss: 8.6839 Acc: 0.4060                                              \n",
      "Epoch 004 | Train Loss: 15.4875 Acc: 0.3620 | Val Loss: 3.6081 Acc: 0.4881                                              \n",
      "Epoch 005 | Train Loss: 12.4088 Acc: 0.3797 | Val Loss: 2.9442 Acc: 0.4203                                              \n",
      "Epoch 006 | Train Loss: 10.2522 Acc: 0.3929 | Val Loss: 2.7095 Acc: 0.4534                                              \n",
      "Epoch 007 | Train Loss: 8.8620 Acc: 0.3959 | Val Loss: 3.3300 Acc: 0.3842                                               \n",
      "Epoch 008 | Train Loss: 7.7900 Acc: 0.3991 | Val Loss: 2.2505 Acc: 0.4134                                               \n",
      "Epoch 009 | Train Loss: 6.6935 Acc: 0.4120 | Val Loss: 2.8812 Acc: 0.3301                                               \n",
      "Epoch 010 | Train Loss: 6.1073 Acc: 0.4181 | Val Loss: 1.9218 Acc: 0.4481                                               \n",
      "Epoch 011 | Train Loss: 5.4127 Acc: 0.4211 | Val Loss: 2.4151 Acc: 0.4525                                               \n",
      "Epoch 012 | Train Loss: 4.8668 Acc: 0.4343 | Val Loss: 3.3032 Acc: 0.4943                                               \n",
      "Epoch 013 | Train Loss: 4.4983 Acc: 0.4362 | Val Loss: 1.7078 Acc: 0.5066                                               \n",
      "Epoch 014 | Train Loss: 4.0696 Acc: 0.4504 | Val Loss: 2.2108 Acc: 0.4355                                               \n",
      "Epoch 015 | Train Loss: 3.6621 Acc: 0.4614 | Val Loss: 1.6999 Acc: 0.5475                                               \n",
      "Epoch 016 | Train Loss: 3.3169 Acc: 0.4714 | Val Loss: 1.3564 Acc: 0.5510                                               \n",
      "Epoch 017 | Train Loss: 3.0464 Acc: 0.4775 | Val Loss: 1.7893 Acc: 0.5030                                               \n",
      "Epoch 018 | Train Loss: 2.8356 Acc: 0.4936 | Val Loss: 1.6116 Acc: 0.5316                                               \n",
      "Epoch 019 | Train Loss: 2.5564 Acc: 0.5100 | Val Loss: 1.3421 Acc: 0.5836                                               \n",
      "Epoch 020 | Train Loss: 2.3788 Acc: 0.5241 | Val Loss: 1.1377 Acc: 0.5943                                               \n",
      "Epoch 021 | Train Loss: 2.2701 Acc: 0.5343 | Val Loss: 1.4548 Acc: 0.5943                                               \n",
      "Epoch 022 | Train Loss: 2.0935 Acc: 0.5333 | Val Loss: 0.9326 Acc: 0.6457                                               \n",
      "Epoch 023 | Train Loss: 1.9230 Acc: 0.5439 | Val Loss: 1.2407 Acc: 0.6355                                               \n",
      "Epoch 024 | Train Loss: 1.9041 Acc: 0.5538 | Val Loss: 0.9048 Acc: 0.6287                                               \n",
      "Epoch 025 | Train Loss: 1.7781 Acc: 0.5706 | Val Loss: 1.4729 Acc: 0.5418                                               \n",
      "Epoch 026 | Train Loss: 1.6450 Acc: 0.5813 | Val Loss: 1.2137 Acc: 0.6191                                               \n",
      "Epoch 027 | Train Loss: 1.6096 Acc: 0.5924 | Val Loss: 0.8377 Acc: 0.6890                                               \n",
      "Epoch 028 | Train Loss: 1.5404 Acc: 0.5947 | Val Loss: 0.9439 Acc: 0.6925                                               \n",
      "Epoch 029 | Train Loss: 1.4530 Acc: 0.6178 | Val Loss: 0.8520 Acc: 0.7054                                               \n",
      "Epoch 030 | Train Loss: 1.3652 Acc: 0.6259 | Val Loss: 2.3113 Acc: 0.5030                                               \n",
      "Epoch 031 | Train Loss: 1.3268 Acc: 0.6351 | Val Loss: 1.2728 Acc: 0.6307                                               \n",
      "Epoch 032 | Train Loss: 1.2510 Acc: 0.6563 | Val Loss: 0.6312 Acc: 0.7779                                               \n",
      "Epoch 033 | Train Loss: 1.1786 Acc: 0.6659 | Val Loss: 1.0040 Acc: 0.6866                                               \n",
      "Epoch 034 | Train Loss: 1.1769 Acc: 0.6786 | Val Loss: 0.7924 Acc: 0.7501                                               \n",
      "Epoch 035 | Train Loss: 1.0846 Acc: 0.6987 | Val Loss: 0.7059 Acc: 0.7066                                               \n",
      "Epoch 036 | Train Loss: 1.0319 Acc: 0.6983 | Val Loss: 0.5519 Acc: 0.8021                                               \n",
      "Epoch 037 | Train Loss: 1.0277 Acc: 0.7154 | Val Loss: 0.6190 Acc: 0.7773                                               \n",
      "Epoch 038 | Train Loss: 0.9273 Acc: 0.7316 | Val Loss: 1.0421 Acc: 0.6451                                               \n",
      "Epoch 039 | Train Loss: 0.9251 Acc: 0.7309 | Val Loss: 0.7664 Acc: 0.7543                                               \n",
      "Epoch 040 | Train Loss: 0.8632 Acc: 0.7485 | Val Loss: 0.5664 Acc: 0.8033                                               \n",
      "Epoch 041 | Train Loss: 0.8815 Acc: 0.7560 | Val Loss: 0.4959 Acc: 0.8284                                               \n",
      "Epoch 042 | Train Loss: 0.8183 Acc: 0.7649 | Val Loss: 0.9123 Acc: 0.6791                                               \n",
      "Epoch 043 | Train Loss: 0.7793 Acc: 0.7704 | Val Loss: 0.9260 Acc: 0.7054                                               \n",
      "Epoch 044 | Train Loss: 0.7516 Acc: 0.7807 | Val Loss: 0.6553 Acc: 0.8075                                               \n",
      "Epoch 045 | Train Loss: 0.7420 Acc: 0.7816 | Val Loss: 0.8129 Acc: 0.7251                                               \n",
      "Epoch 046 | Train Loss: 0.7083 Acc: 0.7884 | Val Loss: 0.4664 Acc: 0.8257                                               \n",
      "Epoch 047 | Train Loss: 0.6937 Acc: 0.7966 | Val Loss: 0.4117 Acc: 0.8558                                               \n",
      "Epoch 048 | Train Loss: 0.6738 Acc: 0.8011 | Val Loss: 0.9215 Acc: 0.7003                                               \n",
      "Epoch 049 | Train Loss: 0.6663 Acc: 0.8065 | Val Loss: 0.4538 Acc: 0.8490                                               \n",
      "Epoch 050 | Train Loss: 0.6330 Acc: 0.8157 | Val Loss: 0.3867 Acc: 0.8749                                               \n",
      "Epoch 051 | Train Loss: 0.6106 Acc: 0.8213 | Val Loss: 0.4253 Acc: 0.8713                                               \n",
      "Epoch 052 | Train Loss: 0.6024 Acc: 0.8227 | Val Loss: 0.8189 Acc: 0.7707                                               \n",
      "Epoch 053 | Train Loss: 0.5531 Acc: 0.8345 | Val Loss: 0.4418 Acc: 0.8612                                               \n",
      "Epoch 054 | Train Loss: 0.5495 Acc: 0.8422 | Val Loss: 0.4688 Acc: 0.8558                                               \n",
      "Epoch 055 | Train Loss: 0.5139 Acc: 0.8494 | Val Loss: 0.4990 Acc: 0.8519                                               \n",
      "Epoch 056 | Train Loss: 0.5057 Acc: 0.8525 | Val Loss: 0.4655 Acc: 0.8576                                               \n",
      "Epoch 057 | Train Loss: 0.4925 Acc: 0.8535 | Val Loss: 0.3573 Acc: 0.8821                                               \n",
      "Epoch 058 | Train Loss: 0.4564 Acc: 0.8618 | Val Loss: 0.4786 Acc: 0.8612                                               \n",
      "Epoch 059 | Train Loss: 0.4606 Acc: 0.8633 | Val Loss: 0.3995 Acc: 0.8866                                               \n",
      "Epoch 060 | Train Loss: 0.4495 Acc: 0.8663 | Val Loss: 0.3332 Acc: 0.8952                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 36, 'cnn_dense': 256, 'cnn_dropout': 0.12039269562026479, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 32, 'learning_rate': 0.0008512129672850844, 'lstm_dense': 64, 'lstm_hidden_size': 96, 'lstm_layers': 5, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 4.7243 Acc: 0.4151 | Val Loss: 1.2502 Acc: 0.4421                                               \n",
      "Epoch 002 | Train Loss: 1.2455 Acc: 0.4422 | Val Loss: 1.2428 Acc: 0.4421                                               \n",
      "Epoch 003 | Train Loss: 1.2423 Acc: 0.4422 | Val Loss: 1.2416 Acc: 0.4421                                               \n",
      "Epoch 004 | Train Loss: 1.2417 Acc: 0.4422 | Val Loss: 1.2434 Acc: 0.4421                                               \n",
      "Epoch 005 | Train Loss: 1.2418 Acc: 0.4422 | Val Loss: 1.2412 Acc: 0.4421                                               \n",
      "Epoch 006 | Train Loss: 1.2415 Acc: 0.4422 | Val Loss: 1.2411 Acc: 0.4421                                               \n",
      "Epoch 007 | Train Loss: 1.2415 Acc: 0.4422 | Val Loss: 1.2409 Acc: 0.4421                                               \n",
      "Epoch 008 | Train Loss: 1.2412 Acc: 0.4422 | Val Loss: 1.2408 Acc: 0.4421                                               \n",
      "Epoch 009 | Train Loss: 1.2407 Acc: 0.4422 | Val Loss: 1.2402 Acc: 0.4421                                               \n",
      "Epoch 010 | Train Loss: 1.2404 Acc: 0.4422 | Val Loss: 1.2396 Acc: 0.4421                                               \n",
      "Epoch 011 | Train Loss: 1.2399 Acc: 0.4422 | Val Loss: 1.2387 Acc: 0.4421                                               \n",
      "Epoch 012 | Train Loss: 1.2386 Acc: 0.4422 | Val Loss: 1.2378 Acc: 0.4421                                               \n",
      "Epoch 013 | Train Loss: 1.2343 Acc: 0.4422 | Val Loss: 1.2310 Acc: 0.4421                                               \n",
      "Epoch 014 | Train Loss: 1.2258 Acc: 0.4422 | Val Loss: 1.2214 Acc: 0.4421                                               \n",
      "Epoch 015 | Train Loss: 1.1968 Acc: 0.4485 | Val Loss: 1.1775 Acc: 0.4749                                               \n",
      "Epoch 016 | Train Loss: 1.1550 Acc: 0.4962 | Val Loss: 1.1510 Acc: 0.4946                                               \n",
      "Epoch 017 | Train Loss: 1.1293 Acc: 0.5153 | Val Loss: 1.1456 Acc: 0.4740                                               \n",
      "Epoch 018 | Train Loss: 1.1246 Acc: 0.5131 | Val Loss: 1.1111 Acc: 0.5125                                               \n",
      "Epoch 019 | Train Loss: 1.0806 Acc: 0.5362 | Val Loss: 1.0447 Acc: 0.5475                                               \n",
      "Epoch 020 | Train Loss: 1.0432 Acc: 0.5510 | Val Loss: 1.0532 Acc: 0.5373                                               \n",
      "Epoch 021 | Train Loss: 0.9899 Acc: 0.5756 | Val Loss: 0.9910 Acc: 0.5642                                               \n",
      "Epoch 022 | Train Loss: 0.9582 Acc: 0.5898 | Val Loss: 0.9627 Acc: 0.5812                                               \n",
      "Epoch 023 | Train Loss: 0.9201 Acc: 0.6056 | Val Loss: 0.9064 Acc: 0.6036                                               \n",
      "Epoch 024 | Train Loss: 0.8903 Acc: 0.6183 | Val Loss: 0.8787 Acc: 0.6254                                               \n",
      "Epoch 025 | Train Loss: 0.8547 Acc: 0.6336 | Val Loss: 0.8484 Acc: 0.6406                                               \n",
      "Epoch 026 | Train Loss: 0.8460 Acc: 0.6359 | Val Loss: 0.8156 Acc: 0.6528                                               \n",
      "Epoch 027 | Train Loss: 0.8185 Acc: 0.6518 | Val Loss: 0.7657 Acc: 0.6636                                               \n",
      "Epoch 028 | Train Loss: 0.7693 Acc: 0.6707 | Val Loss: 0.8038 Acc: 0.6475                                               \n",
      "Epoch 029 | Train Loss: 0.7393 Acc: 0.6947 | Val Loss: 0.7364 Acc: 0.6958                                               \n",
      "Epoch 030 | Train Loss: 0.6997 Acc: 0.7227 | Val Loss: 0.6779 Acc: 0.7388                                               \n",
      "Epoch 031 | Train Loss: 0.6842 Acc: 0.7417 | Val Loss: 0.8677 Acc: 0.6866                                               \n",
      "Epoch 032 | Train Loss: 0.6447 Acc: 0.7641 | Val Loss: 0.6481 Acc: 0.7585                                               \n",
      "Epoch 033 | Train Loss: 0.6006 Acc: 0.7816 | Val Loss: 0.6111 Acc: 0.7872                                               \n",
      "Epoch 034 | Train Loss: 0.5721 Acc: 0.7937 | Val Loss: 0.6979 Acc: 0.7621                                               \n",
      "Epoch 035 | Train Loss: 0.5347 Acc: 0.8092 | Val Loss: 0.5531 Acc: 0.7973                                               \n",
      "Epoch 036 | Train Loss: 0.5229 Acc: 0.8097 | Val Loss: 0.5234 Acc: 0.8134                                               \n",
      "Epoch 037 | Train Loss: 0.5053 Acc: 0.8174 | Val Loss: 0.5595 Acc: 0.7967                                               \n",
      "Epoch 038 | Train Loss: 0.5133 Acc: 0.8134 | Val Loss: 0.5093 Acc: 0.8176                                               \n",
      "Epoch 039 | Train Loss: 0.5053 Acc: 0.8166 | Val Loss: 0.4835 Acc: 0.8194                                               \n",
      "Epoch 040 | Train Loss: 0.4529 Acc: 0.8348 | Val Loss: 0.5145 Acc: 0.8212                                               \n",
      "Epoch 041 | Train Loss: 0.4391 Acc: 0.8385 | Val Loss: 0.4382 Acc: 0.8510                                               \n",
      "Epoch 042 | Train Loss: 0.4440 Acc: 0.8401 | Val Loss: 0.4583 Acc: 0.8391                                               \n",
      "Epoch 043 | Train Loss: 0.4325 Acc: 0.8433 | Val Loss: 0.4521 Acc: 0.8484                                               \n",
      "Epoch 044 | Train Loss: 0.4389 Acc: 0.8458 | Val Loss: 0.6658 Acc: 0.7952                                               \n",
      "Epoch 045 | Train Loss: 0.4550 Acc: 0.8409 | Val Loss: 0.4602 Acc: 0.8355                                               \n",
      "Epoch 046 | Train Loss: 0.4118 Acc: 0.8589 | Val Loss: 0.4630 Acc: 0.8340                                               \n",
      "Epoch 047 | Train Loss: 0.4150 Acc: 0.8582 | Val Loss: 0.6757 Acc: 0.7493                                               \n",
      "Epoch 048 | Train Loss: 0.5048 Acc: 0.8192 | Val Loss: 0.4894 Acc: 0.8301                                               \n",
      "Epoch 049 | Train Loss: 0.4168 Acc: 0.8575 | Val Loss: 0.4467 Acc: 0.8582                                               \n",
      "Epoch 050 | Train Loss: 0.4130 Acc: 0.8618 | Val Loss: 0.4326 Acc: 0.8504                                               \n",
      "Epoch 051 | Train Loss: 0.3821 Acc: 0.8690 | Val Loss: 0.4565 Acc: 0.8567                                               \n",
      "Epoch 052 | Train Loss: 0.3729 Acc: 0.8795 | Val Loss: 0.4366 Acc: 0.8627                                               \n",
      "Epoch 053 | Train Loss: 0.4130 Acc: 0.8630 | Val Loss: 0.4811 Acc: 0.8373                                               \n",
      "Epoch 054 | Train Loss: 0.3978 Acc: 0.8700 | Val Loss: 0.5244 Acc: 0.8325                                               \n",
      "Epoch 055 | Train Loss: 0.7344 Acc: 0.6998 | Val Loss: 0.5204 Acc: 0.8006                                               \n",
      "Epoch 056 | Train Loss: 0.4469 Acc: 0.8466 | Val Loss: 0.4877 Acc: 0.8197                                               \n",
      "Epoch 057 | Train Loss: 0.3831 Acc: 0.8724 | Val Loss: 0.4004 Acc: 0.8660                                               \n",
      "Epoch 058 | Train Loss: 0.3746 Acc: 0.8736 | Val Loss: 0.4172 Acc: 0.8636                                               \n",
      "Epoch 059 | Train Loss: 0.3502 Acc: 0.8820 | Val Loss: 0.4195 Acc: 0.8606                                               \n",
      "Epoch 060 | Train Loss: 0.6252 Acc: 0.7762 | Val Loss: 0.4256 Acc: 0.8519                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 256, 'cnn_dropout': 0.3478312623610203, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 64, 'cnn_kernels_2': 96, 'learning_rate': 3.577352678145031e-05, 'lstm_dense': 32, 'lstm_hidden_size': 64, 'lstm_layers': 5, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 56.2555 Acc: 0.2968 | Val Loss: 16.9602 Acc: 0.4284                                             \n",
      "Epoch 002 | Train Loss: 35.0646 Acc: 0.3362 | Val Loss: 10.2109 Acc: 0.4239                                             \n",
      "Epoch 003 | Train Loss: 25.9102 Acc: 0.3481 | Val Loss: 8.3457 Acc: 0.4433                                              \n",
      "Epoch 004 | Train Loss: 20.4938 Acc: 0.3688 | Val Loss: 6.8629 Acc: 0.4391                                              \n",
      "Epoch 005 | Train Loss: 16.5692 Acc: 0.3794 | Val Loss: 7.9674 Acc: 0.4433                                              \n",
      "Epoch 006 | Train Loss: 14.1471 Acc: 0.3832 | Val Loss: 5.9118 Acc: 0.4436                                              \n",
      "Epoch 007 | Train Loss: 12.3623 Acc: 0.4001 | Val Loss: 6.2309 Acc: 0.4612                                              \n",
      "Epoch 008 | Train Loss: 10.6890 Acc: 0.4117 | Val Loss: 4.9090 Acc: 0.4418                                              \n",
      "Epoch 009 | Train Loss: 9.8293 Acc: 0.4192 | Val Loss: 3.7897 Acc: 0.5125                                               \n",
      "Epoch 010 | Train Loss: 8.8867 Acc: 0.4291 | Val Loss: 3.0040 Acc: 0.5734                                               \n",
      "Epoch 011 | Train Loss: 8.1914 Acc: 0.4407 | Val Loss: 2.7996 Acc: 0.5931                                               \n",
      "Epoch 012 | Train Loss: 7.2635 Acc: 0.4541 | Val Loss: 2.4886 Acc: 0.6415                                               \n",
      "Epoch 013 | Train Loss: 6.8560 Acc: 0.4637 | Val Loss: 2.2857 Acc: 0.6567                                               \n",
      "Epoch 014 | Train Loss: 6.3214 Acc: 0.4750 | Val Loss: 2.4083 Acc: 0.4788                                               \n",
      "Epoch 015 | Train Loss: 5.7772 Acc: 0.4836 | Val Loss: 2.1219 Acc: 0.6725                                               \n",
      "Epoch 016 | Train Loss: 5.2920 Acc: 0.5021 | Val Loss: 1.9334 Acc: 0.6884                                               \n",
      "Epoch 017 | Train Loss: 4.9945 Acc: 0.5044 | Val Loss: 1.8936 Acc: 0.6672                                               \n",
      "Epoch 018 | Train Loss: 4.6959 Acc: 0.5091 | Val Loss: 1.8214 Acc: 0.6048                                               \n",
      "Epoch 019 | Train Loss: 4.3352 Acc: 0.5250 | Val Loss: 1.6280 Acc: 0.6394                                               \n",
      "Epoch 020 | Train Loss: 4.0437 Acc: 0.5335 | Val Loss: 1.8532 Acc: 0.6570                                               \n",
      "Epoch 021 | Train Loss: 3.8679 Acc: 0.5384 | Val Loss: 1.3256 Acc: 0.6663                                               \n",
      "Epoch 022 | Train Loss: 3.6086 Acc: 0.5514 | Val Loss: 1.3163 Acc: 0.6636                                               \n",
      "Epoch 023 | Train Loss: 3.4593 Acc: 0.5618 | Val Loss: 1.1956 Acc: 0.6910                                               \n",
      "Epoch 024 | Train Loss: 3.0671 Acc: 0.5713 | Val Loss: 1.3536 Acc: 0.6713                                               \n",
      "Epoch 025 | Train Loss: 2.9510 Acc: 0.5884 | Val Loss: 1.2934 Acc: 0.6436                                               \n",
      "Epoch 026 | Train Loss: 2.7453 Acc: 0.5989 | Val Loss: 1.1468 Acc: 0.6985                                               \n",
      "Epoch 027 | Train Loss: 2.4428 Acc: 0.6166 | Val Loss: 0.9304 Acc: 0.7361                                               \n",
      "Epoch 028 | Train Loss: 2.3863 Acc: 0.6207 | Val Loss: 0.7482 Acc: 0.7973                                               \n",
      "Epoch 029 | Train Loss: 2.2394 Acc: 0.6309 | Val Loss: 0.7868 Acc: 0.7937                                               \n",
      "Epoch 030 | Train Loss: 2.0016 Acc: 0.6562 | Val Loss: 0.9488 Acc: 0.7597                                               \n",
      "Epoch 031 | Train Loss: 1.9620 Acc: 0.6611 | Val Loss: 0.7146 Acc: 0.7976                                               \n",
      "Epoch 032 | Train Loss: 1.8024 Acc: 0.6781 | Val Loss: 0.7407 Acc: 0.7952                                               \n",
      "Epoch 033 | Train Loss: 1.6282 Acc: 0.6869 | Val Loss: 0.6374 Acc: 0.8418                                               \n",
      "Epoch 034 | Train Loss: 1.5423 Acc: 0.7033 | Val Loss: 0.7457 Acc: 0.8096                                               \n",
      "Epoch 035 | Train Loss: 1.4623 Acc: 0.7120 | Val Loss: 0.6072 Acc: 0.8439                                               \n",
      "Epoch 036 | Train Loss: 1.4235 Acc: 0.7198 | Val Loss: 0.5720 Acc: 0.8478                                               \n",
      "Epoch 037 | Train Loss: 1.2793 Acc: 0.7410 | Val Loss: 0.6111 Acc: 0.8430                                               \n",
      "Epoch 038 | Train Loss: 1.2282 Acc: 0.7469 | Val Loss: 0.9105 Acc: 0.7696                                               \n",
      "Epoch 039 | Train Loss: 1.1944 Acc: 0.7504 | Val Loss: 0.6010 Acc: 0.8499                                               \n",
      "Epoch 040 | Train Loss: 1.1157 Acc: 0.7646 | Val Loss: 1.0480 Acc: 0.7307                                               \n",
      "Epoch 041 | Train Loss: 1.0550 Acc: 0.7748 | Val Loss: 0.7906 Acc: 0.8024                                               \n",
      "Epoch 042 | Train Loss: 0.9851 Acc: 0.7816 | Val Loss: 0.7717 Acc: 0.7943                                               \n",
      "Epoch 043 | Train Loss: 0.9346 Acc: 0.7971 | Val Loss: 0.5743 Acc: 0.8367                                               \n",
      "Epoch 044 | Train Loss: 0.8996 Acc: 0.7976 | Val Loss: 0.7299 Acc: 0.8257                                               \n",
      "Epoch 045 | Train Loss: 0.8190 Acc: 0.8095 | Val Loss: 0.7864 Acc: 0.7919                                               \n",
      "Epoch 046 | Train Loss: 0.7727 Acc: 0.8184 | Val Loss: 0.4210 Acc: 0.8755                                               \n",
      "Epoch 047 | Train Loss: 0.7182 Acc: 0.8266 | Val Loss: 0.3804 Acc: 0.8955                                               \n",
      "Epoch 048 | Train Loss: 0.6678 Acc: 0.8354 | Val Loss: 0.5441 Acc: 0.8409                                               \n",
      "Epoch 049 | Train Loss: 0.6536 Acc: 0.8449 | Val Loss: 0.3826 Acc: 0.8899                                               \n",
      "Epoch 050 | Train Loss: 0.6273 Acc: 0.8475 | Val Loss: 0.4893 Acc: 0.8427                                               \n",
      "Epoch 051 | Train Loss: 0.5828 Acc: 0.8539 | Val Loss: 0.3705 Acc: 0.8916                                               \n",
      "Epoch 052 | Train Loss: 0.5679 Acc: 0.8596 | Val Loss: 0.4068 Acc: 0.8863                                               \n",
      "Epoch 053 | Train Loss: 0.5374 Acc: 0.8632 | Val Loss: 0.3722 Acc: 0.8860                                               \n",
      "Epoch 054 | Train Loss: 0.4961 Acc: 0.8750 | Val Loss: 0.4331 Acc: 0.8687                                               \n",
      "Epoch 055 | Train Loss: 0.5070 Acc: 0.8708 | Val Loss: 0.4250 Acc: 0.8743                                               \n",
      "Epoch 056 | Train Loss: 0.4630 Acc: 0.8805 | Val Loss: 0.4108 Acc: 0.8585                                               \n",
      "Epoch 057 | Train Loss: 0.4344 Acc: 0.8845 | Val Loss: 0.3274 Acc: 0.8967                                               \n",
      "Epoch 058 | Train Loss: 0.4067 Acc: 0.8915 | Val Loss: 0.6110 Acc: 0.8251                                               \n",
      "Epoch 059 | Train Loss: 0.3814 Acc: 0.8931 | Val Loss: 0.2942 Acc: 0.9078                                               \n",
      "Epoch 060 | Train Loss: 0.3633 Acc: 0.9018 | Val Loss: 0.3024 Acc: 0.8958                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 32, 'cnn_dropout': 0.154021203631077, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 16, 'cnn_kernels_2': 16, 'learning_rate': 0.0006241091897269032, 'lstm_dense': 128, 'lstm_hidden_size': 64, 'lstm_layers': 5, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 16.4177 Acc: 0.3871 | Val Loss: 3.3004 Acc: 0.4236                                              \n",
      "Epoch 002 | Train Loss: 3.8541 Acc: 0.4485 | Val Loss: 1.5567 Acc: 0.5215                                               \n",
      "Epoch 003 | Train Loss: 2.3404 Acc: 0.4777 | Val Loss: 1.5645 Acc: 0.5110                                               \n",
      "Epoch 004 | Train Loss: 1.6683 Acc: 0.5022 | Val Loss: 0.9611 Acc: 0.5764                                               \n",
      "Epoch 005 | Train Loss: 1.3739 Acc: 0.5365 | Val Loss: 1.1087 Acc: 0.5776                                               \n",
      "Epoch 006 | Train Loss: 1.2191 Acc: 0.5695 | Val Loss: 1.0506 Acc: 0.5937                                               \n",
      "Epoch 007 | Train Loss: 0.9796 Acc: 0.6329 | Val Loss: 0.6801 Acc: 0.6913                                               \n",
      "Epoch 008 | Train Loss: 0.9420 Acc: 0.6656 | Val Loss: 0.6758 Acc: 0.7212                                               \n",
      "Epoch 009 | Train Loss: 0.7683 Acc: 0.7075 | Val Loss: 0.5247 Acc: 0.7633                                               \n",
      "Epoch 010 | Train Loss: 0.7059 Acc: 0.7322 | Val Loss: 0.5637 Acc: 0.7818                                               \n",
      "Epoch 011 | Train Loss: 0.6605 Acc: 0.7501 | Val Loss: 0.6234 Acc: 0.7490                                               \n",
      "Epoch 012 | Train Loss: 0.6160 Acc: 0.7748 | Val Loss: 0.6597 Acc: 0.7513                                               \n",
      "Epoch 013 | Train Loss: 0.5407 Acc: 0.8030 | Val Loss: 0.4447 Acc: 0.8364                                               \n",
      "Epoch 014 | Train Loss: 0.4553 Acc: 0.8274 | Val Loss: 0.3942 Acc: 0.8579                                               \n",
      "Epoch 015 | Train Loss: 0.4027 Acc: 0.8491 | Val Loss: 0.3513 Acc: 0.8564                                               \n",
      "Epoch 016 | Train Loss: 0.3833 Acc: 0.8539 | Val Loss: 0.2785 Acc: 0.8899                                               \n",
      "Epoch 017 | Train Loss: 0.3333 Acc: 0.8782 | Val Loss: 0.2613 Acc: 0.8958                                               \n",
      "Epoch 018 | Train Loss: 0.3053 Acc: 0.8895 | Val Loss: 0.3066 Acc: 0.8809                                               \n",
      "Epoch 019 | Train Loss: 0.2643 Acc: 0.9025 | Val Loss: 0.2097 Acc: 0.9167                                               \n",
      "Epoch 020 | Train Loss: 0.2382 Acc: 0.9147 | Val Loss: 0.2166 Acc: 0.9134                                               \n",
      "Epoch 021 | Train Loss: 0.2212 Acc: 0.9206 | Val Loss: 0.2421 Acc: 0.9087                                               \n",
      "Epoch 022 | Train Loss: 0.1990 Acc: 0.9315 | Val Loss: 0.2354 Acc: 0.9110                                               \n",
      "Epoch 023 | Train Loss: 0.1790 Acc: 0.9337 | Val Loss: 0.1802 Acc: 0.9304                                               \n",
      "Epoch 024 | Train Loss: 0.1758 Acc: 0.9383 | Val Loss: 0.1767 Acc: 0.9328                                               \n",
      "Epoch 025 | Train Loss: 0.1558 Acc: 0.9485 | Val Loss: 0.1677 Acc: 0.9382                                               \n",
      "Epoch 026 | Train Loss: 0.1579 Acc: 0.9436 | Val Loss: 0.1755 Acc: 0.9406                                               \n",
      "Epoch 027 | Train Loss: 0.1327 Acc: 0.9561 | Val Loss: 0.1811 Acc: 0.9403                                               \n",
      "Epoch 028 | Train Loss: 0.1243 Acc: 0.9578 | Val Loss: 0.2573 Acc: 0.9101                                               \n",
      "Epoch 029 | Train Loss: 0.1286 Acc: 0.9590 | Val Loss: 0.1898 Acc: 0.9269                                               \n",
      "Epoch 030 | Train Loss: 0.1276 Acc: 0.9574 | Val Loss: 0.1864 Acc: 0.9325                                               \n",
      "Epoch 031 | Train Loss: 0.1183 Acc: 0.9611 | Val Loss: 0.1845 Acc: 0.9391                                               \n",
      "Epoch 032 | Train Loss: 0.1078 Acc: 0.9628 | Val Loss: 0.1332 Acc: 0.9442                                               \n",
      "Epoch 033 | Train Loss: 0.1066 Acc: 0.9641 | Val Loss: 0.1638 Acc: 0.9454                                               \n",
      "Epoch 034 | Train Loss: 0.1055 Acc: 0.9635 | Val Loss: 0.1212 Acc: 0.9606                                               \n",
      "Epoch 035 | Train Loss: 0.1102 Acc: 0.9644 | Val Loss: 0.1450 Acc: 0.9460                                               \n",
      "Epoch 036 | Train Loss: 0.0852 Acc: 0.9722 | Val Loss: 0.2660 Acc: 0.9090                                               \n",
      "Epoch 037 | Train Loss: 0.0820 Acc: 0.9751 | Val Loss: 0.1338 Acc: 0.9507                                               \n",
      "Epoch 038 | Train Loss: 0.0846 Acc: 0.9730 | Val Loss: 0.1445 Acc: 0.9522                                               \n",
      "Epoch 039 | Train Loss: 0.0782 Acc: 0.9741 | Val Loss: 0.1108 Acc: 0.9633                                               \n",
      "Epoch 040 | Train Loss: 0.0896 Acc: 0.9714 | Val Loss: 0.1037 Acc: 0.9663                                               \n",
      "Epoch 041 | Train Loss: 0.0878 Acc: 0.9700 | Val Loss: 0.1550 Acc: 0.9421                                               \n",
      "Epoch 042 | Train Loss: 0.0686 Acc: 0.9784 | Val Loss: 0.1826 Acc: 0.9478                                               \n",
      "Epoch 043 | Train Loss: 0.0813 Acc: 0.9736 | Val Loss: 0.1293 Acc: 0.9561                                               \n",
      "Epoch 044 | Train Loss: 0.0602 Acc: 0.9805 | Val Loss: 0.3584 Acc: 0.8899                                               \n",
      "Epoch 045 | Train Loss: 0.0744 Acc: 0.9751 | Val Loss: 0.1076 Acc: 0.9701                                               \n",
      "Epoch 046 | Train Loss: 0.0551 Acc: 0.9827 | Val Loss: 0.1443 Acc: 0.9546                                               \n",
      "Epoch 047 | Train Loss: 0.0635 Acc: 0.9790 | Val Loss: 0.1204 Acc: 0.9597                                               \n",
      "Epoch 048 | Train Loss: 0.0646 Acc: 0.9791 | Val Loss: 0.0982 Acc: 0.9743                                               \n",
      "Epoch 049 | Train Loss: 0.0457 Acc: 0.9851 | Val Loss: 0.1744 Acc: 0.9540                                               \n",
      "Epoch 050 | Train Loss: 0.0613 Acc: 0.9817 | Val Loss: 0.1084 Acc: 0.9713                                               \n",
      "Epoch 051 | Train Loss: 0.0623 Acc: 0.9784 | Val Loss: 0.1706 Acc: 0.9451                                               \n",
      "Epoch 052 | Train Loss: 0.0627 Acc: 0.9797 | Val Loss: 0.1327 Acc: 0.9543                                               \n",
      "Epoch 053 | Train Loss: 0.0525 Acc: 0.9828 | Val Loss: 0.1815 Acc: 0.9412                                               \n",
      "Epoch 054 | Train Loss: 0.0590 Acc: 0.9809 | Val Loss: 0.0960 Acc: 0.9704                                               \n",
      "Epoch 055 | Train Loss: 0.0738 Acc: 0.9770 | Val Loss: 0.1579 Acc: 0.9510                                               \n",
      "Epoch 056 | Train Loss: 0.0570 Acc: 0.9826 | Val Loss: 0.1697 Acc: 0.9579                                               \n",
      "Epoch 057 | Train Loss: 0.0464 Acc: 0.9843 | Val Loss: 0.1066 Acc: 0.9687                                               \n",
      "Epoch 058 | Train Loss: 0.0432 Acc: 0.9859 | Val Loss: 0.1672 Acc: 0.9522                                               \n",
      "Epoch 059 | Train Loss: 0.0512 Acc: 0.9833 | Val Loss: 0.1231 Acc: 0.9642                                               \n",
      "Epoch 060 | Train Loss: 0.0510 Acc: 0.9830 | Val Loss: 0.4008 Acc: 0.8943                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 256, 'cnn_dropout': 0.46087873390012435, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 96, 'learning_rate': 0.006828663464980135, 'lstm_dense': 256, 'lstm_hidden_size': 128, 'lstm_layers': 1, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 27.8949 Acc: 0.4127 | Val Loss: 1.2321 Acc: 0.4490                                              \n",
      "Epoch 002 | Train Loss: 1.2307 Acc: 0.4456 | Val Loss: 1.2023 Acc: 0.4600                                               \n",
      "Epoch 003 | Train Loss: 1.2043 Acc: 0.4494 | Val Loss: 1.2131 Acc: 0.4582                                               \n",
      "Epoch 004 | Train Loss: 1.1962 Acc: 0.4447 | Val Loss: 1.2299 Acc: 0.4421                                               \n",
      "Epoch 005 | Train Loss: 1.1819 Acc: 0.4437 | Val Loss: 1.1412 Acc: 0.4173                                               \n",
      "Epoch 006 | Train Loss: 1.2274 Acc: 0.4454 | Val Loss: 1.2381 Acc: 0.4421                                               \n",
      "Epoch 007 | Train Loss: 1.2360 Acc: 0.4412 | Val Loss: 1.2422 Acc: 0.4421                                               \n",
      "Epoch 008 | Train Loss: 1.2134 Acc: 0.4389 | Val Loss: 1.2419 Acc: 0.4421                                               \n",
      "Epoch 009 | Train Loss: 1.2486 Acc: 0.4376 | Val Loss: 1.2420 Acc: 0.4421                                               \n",
      "Epoch 010 | Train Loss: 1.2753 Acc: 0.4397 | Val Loss: 1.2386 Acc: 0.4316                                               \n",
      "Epoch 011 | Train Loss: 1.2437 Acc: 0.4403 | Val Loss: 1.2420 Acc: 0.4421                                               \n",
      "Epoch 012 | Train Loss: 1.2619 Acc: 0.4405 | Val Loss: 1.1695 Acc: 0.4490                                               \n",
      "Epoch 013 | Train Loss: 1.2237 Acc: 0.4376 | Val Loss: 1.2578 Acc: 0.4421                                               \n",
      "Epoch 014 | Train Loss: 1.2372 Acc: 0.4391 | Val Loss: 1.2323 Acc: 0.4421                                               \n",
      "Epoch 015 | Train Loss: 1.1285 Acc: 0.4327 | Val Loss: 1.0643 Acc: 0.4421                                               \n",
      "Epoch 016 | Train Loss: 1.1205 Acc: 0.4381 | Val Loss: 0.9947 Acc: 0.5018                                               \n",
      "Epoch 017 | Train Loss: 1.1252 Acc: 0.4306 | Val Loss: 1.0736 Acc: 0.4412                                               \n",
      "Epoch 018 | Train Loss: 1.1148 Acc: 0.4399 | Val Loss: 1.0104 Acc: 0.4260                                               \n",
      "Epoch 019 | Train Loss: 1.1684 Acc: 0.4289 | Val Loss: 1.1999 Acc: 0.2513                                               \n",
      "Epoch 020 | Train Loss: 1.1687 Acc: 0.4166 | Val Loss: 1.1198 Acc: 0.4421                                               \n",
      "Epoch 021 | Train Loss: 1.3723 Acc: 0.4341 | Val Loss: 1.0481 Acc: 0.4421                                               \n",
      "Epoch 022 | Train Loss: 1.4572 Acc: 0.4340 | Val Loss: 1.1414 Acc: 0.4687                                               \n",
      "Epoch 023 | Train Loss: 1.2773 Acc: 0.4444 | Val Loss: 1.1279 Acc: 0.4421                                               \n",
      "Epoch 024 | Train Loss: 1.1383 Acc: 0.4393 | Val Loss: 1.0786 Acc: 0.4672                                               \n",
      "Epoch 025 | Train Loss: 1.1916 Acc: 0.4366 | Val Loss: 1.0390 Acc: 0.4421                                               \n",
      "Epoch 026 | Train Loss: 1.1148 Acc: 0.4386 | Val Loss: 1.0759 Acc: 0.4149                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 36, 'cnn_dense': 64, 'cnn_dropout': 0.5392462348226111, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 64, 'cnn_kernels_2': 32, 'learning_rate': 1.6035443249221515e-05, 'lstm_dense': 256, 'lstm_hidden_size': 96, 'lstm_layers': 4, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 51.3432 Acc: 0.3507 | Val Loss: 12.6209 Acc: 0.4424                                             \n",
      "Epoch 002 | Train Loss: 33.7743 Acc: 0.3753 | Val Loss: 7.0340 Acc: 0.4884                                              \n",
      "Epoch 003 | Train Loss: 25.3622 Acc: 0.3932 | Val Loss: 6.9327 Acc: 0.5093                                              \n",
      "Epoch 004 | Train Loss: 21.2658 Acc: 0.4036 | Val Loss: 5.7212 Acc: 0.5149                                              \n",
      "Epoch 005 | Train Loss: 18.0973 Acc: 0.4052 | Val Loss: 5.0340 Acc: 0.5394                                              \n",
      "Epoch 006 | Train Loss: 15.8545 Acc: 0.4082 | Val Loss: 5.0375 Acc: 0.5349                                              \n",
      "Epoch 007 | Train Loss: 13.8340 Acc: 0.4202 | Val Loss: 4.3570 Acc: 0.4985                                              \n",
      "Epoch 008 | Train Loss: 12.2506 Acc: 0.4276 | Val Loss: 4.4280 Acc: 0.5481                                              \n",
      "Epoch 009 | Train Loss: 10.8420 Acc: 0.4315 | Val Loss: 3.6983 Acc: 0.5110                                              \n",
      "Epoch 010 | Train Loss: 10.0371 Acc: 0.4329 | Val Loss: 3.5332 Acc: 0.4854                                              \n",
      "Epoch 011 | Train Loss: 9.0213 Acc: 0.4298 | Val Loss: 3.2956 Acc: 0.4836                                               \n",
      "Epoch 012 | Train Loss: 8.0366 Acc: 0.4423 | Val Loss: 2.9612 Acc: 0.5385                                               \n",
      "Epoch 013 | Train Loss: 7.2488 Acc: 0.4569 | Val Loss: 2.5180 Acc: 0.5469                                               \n",
      "Epoch 014 | Train Loss: 6.5102 Acc: 0.4701 | Val Loss: 2.4474 Acc: 0.5701                                               \n",
      "Epoch 015 | Train Loss: 5.9340 Acc: 0.4727 | Val Loss: 2.1349 Acc: 0.5681                                               \n",
      "Epoch 016 | Train Loss: 5.5233 Acc: 0.4756 | Val Loss: 2.1482 Acc: 0.5636                                               \n",
      "Epoch 017 | Train Loss: 4.9615 Acc: 0.4885 | Val Loss: 2.1471 Acc: 0.5612                                               \n",
      "Epoch 018 | Train Loss: 4.5633 Acc: 0.4959 | Val Loss: 1.7405 Acc: 0.6158                                               \n",
      "Epoch 019 | Train Loss: 4.3020 Acc: 0.4981 | Val Loss: 1.5347 Acc: 0.6418                                               \n",
      "Epoch 020 | Train Loss: 3.8915 Acc: 0.5103 | Val Loss: 1.2586 Acc: 0.6746                                               \n",
      "Epoch 021 | Train Loss: 3.6380 Acc: 0.5153 | Val Loss: 1.3398 Acc: 0.6230                                               \n",
      "Epoch 022 | Train Loss: 3.4267 Acc: 0.5252 | Val Loss: 1.2364 Acc: 0.6827                                               \n",
      "Epoch 023 | Train Loss: 3.1929 Acc: 0.5312 | Val Loss: 1.1889 Acc: 0.6878                                               \n",
      "Epoch 024 | Train Loss: 2.9781 Acc: 0.5427 | Val Loss: 1.1154 Acc: 0.6955                                               \n",
      "Epoch 025 | Train Loss: 2.8967 Acc: 0.5472 | Val Loss: 1.0575 Acc: 0.7054                                               \n",
      "Epoch 026 | Train Loss: 2.7439 Acc: 0.5509 | Val Loss: 1.1387 Acc: 0.6991                                               \n",
      "Epoch 027 | Train Loss: 2.6047 Acc: 0.5594 | Val Loss: 0.8701 Acc: 0.7176                                               \n",
      "Epoch 028 | Train Loss: 2.4236 Acc: 0.5706 | Val Loss: 1.0959 Acc: 0.7122                                               \n",
      "Epoch 029 | Train Loss: 2.3248 Acc: 0.5765 | Val Loss: 1.0289 Acc: 0.7164                                               \n",
      "Epoch 030 | Train Loss: 2.3266 Acc: 0.5797 | Val Loss: 0.9188 Acc: 0.7400                                               \n",
      "Epoch 031 | Train Loss: 2.1227 Acc: 0.5960 | Val Loss: 0.9288 Acc: 0.7522                                               \n",
      "Epoch 032 | Train Loss: 2.0996 Acc: 0.5953 | Val Loss: 0.8982 Acc: 0.7331                                               \n",
      "Epoch 033 | Train Loss: 1.9349 Acc: 0.6035 | Val Loss: 0.9145 Acc: 0.7337                                               \n",
      "Epoch 034 | Train Loss: 1.9064 Acc: 0.6089 | Val Loss: 0.8699 Acc: 0.7478                                               \n",
      "Epoch 035 | Train Loss: 1.7932 Acc: 0.6097 | Val Loss: 0.9952 Acc: 0.7212                                               \n",
      "Epoch 036 | Train Loss: 1.7269 Acc: 0.6140 | Val Loss: 0.9468 Acc: 0.7349                                               \n",
      "Epoch 037 | Train Loss: 1.6913 Acc: 0.6218 | Val Loss: 1.0132 Acc: 0.7322                                               \n",
      "Epoch 038 | Train Loss: 1.6243 Acc: 0.6358 | Val Loss: 0.7095 Acc: 0.7904                                               \n",
      "Epoch 039 | Train Loss: 1.5011 Acc: 0.6503 | Val Loss: 0.7495 Acc: 0.7827                                               \n",
      "Epoch 040 | Train Loss: 1.4687 Acc: 0.6439 | Val Loss: 0.8810 Acc: 0.7624                                               \n",
      "Epoch 041 | Train Loss: 1.4646 Acc: 0.6553 | Val Loss: 0.8386 Acc: 0.7624                                               \n",
      "Epoch 042 | Train Loss: 1.3730 Acc: 0.6667 | Val Loss: 0.7490 Acc: 0.7854                                               \n",
      "Epoch 043 | Train Loss: 1.2925 Acc: 0.6699 | Val Loss: 0.7918 Acc: 0.7776                                               \n",
      "Epoch 044 | Train Loss: 1.2859 Acc: 0.6759 | Val Loss: 0.7463 Acc: 0.7812                                               \n",
      "Epoch 045 | Train Loss: 1.2338 Acc: 0.6840 | Val Loss: 0.9196 Acc: 0.7484                                               \n",
      "Epoch 046 | Train Loss: 1.2138 Acc: 0.6865 | Val Loss: 0.8165 Acc: 0.7651                                               \n",
      "Epoch 047 | Train Loss: 1.1494 Acc: 0.6972 | Val Loss: 0.5862 Acc: 0.8248                                               \n",
      "Epoch 048 | Train Loss: 1.1182 Acc: 0.7037 | Val Loss: 0.7074 Acc: 0.7988                                               \n",
      "Epoch 049 | Train Loss: 1.0851 Acc: 0.7082 | Val Loss: 0.9293 Acc: 0.7618                                               \n",
      "Epoch 050 | Train Loss: 1.0371 Acc: 0.7156 | Val Loss: 0.8585 Acc: 0.7719                                               \n",
      "Epoch 051 | Train Loss: 1.0195 Acc: 0.7227 | Val Loss: 0.6834 Acc: 0.8075                                               \n",
      "Epoch 052 | Train Loss: 0.9713 Acc: 0.7239 | Val Loss: 0.6243 Acc: 0.8030                                               \n",
      "Epoch 053 | Train Loss: 0.9384 Acc: 0.7256 | Val Loss: 0.6192 Acc: 0.8215                                               \n",
      "Epoch 054 | Train Loss: 0.9340 Acc: 0.7287 | Val Loss: 0.6770 Acc: 0.8039                                               \n",
      "Epoch 055 | Train Loss: 0.8824 Acc: 0.7448 | Val Loss: 0.5389 Acc: 0.8355                                               \n",
      "Epoch 056 | Train Loss: 0.8558 Acc: 0.7473 | Val Loss: 0.5719 Acc: 0.8293                                               \n",
      "Epoch 057 | Train Loss: 0.8490 Acc: 0.7460 | Val Loss: 0.6029 Acc: 0.8313                                               \n",
      "Epoch 058 | Train Loss: 0.8066 Acc: 0.7618 | Val Loss: 0.6415 Acc: 0.8107                                               \n",
      "Epoch 059 | Train Loss: 0.8303 Acc: 0.7524 | Val Loss: 0.6927 Acc: 0.8119                                               \n",
      "Epoch 060 | Train Loss: 0.7720 Acc: 0.7651 | Val Loss: 0.5749 Acc: 0.8349                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 32, 'cnn_dropout': 0.25434219345076897, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 48, 'cnn_kernels_2': 16, 'learning_rate': 0.00016968232464119155, 'lstm_dense': 256, 'lstm_hidden_size': 128, 'lstm_layers': 3, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 46.9410 Acc: 0.3438 | Val Loss: 6.5009 Acc: 0.3307                                              \n",
      "Epoch 002 | Train Loss: 10.0461 Acc: 0.4130 | Val Loss: 2.5414 Acc: 0.4788                                              \n",
      "Epoch 003 | Train Loss: 6.2761 Acc: 0.4285 | Val Loss: 2.1295 Acc: 0.4821                                               \n",
      "Epoch 004 | Train Loss: 4.4903 Acc: 0.4501 | Val Loss: 1.3367 Acc: 0.5403                                               \n",
      "Epoch 005 | Train Loss: 3.6173 Acc: 0.4674 | Val Loss: 0.9836 Acc: 0.5600                                               \n",
      "Epoch 006 | Train Loss: 2.7494 Acc: 0.5237 | Val Loss: 1.0869 Acc: 0.6266                                               \n",
      "Epoch 007 | Train Loss: 2.2418 Acc: 0.5693 | Val Loss: 0.9803 Acc: 0.6761                                               \n",
      "Epoch 008 | Train Loss: 1.8352 Acc: 0.6145 | Val Loss: 0.7776 Acc: 0.7564                                               \n",
      "Epoch 009 | Train Loss: 1.5674 Acc: 0.6542 | Val Loss: 0.7730 Acc: 0.7591                                               \n",
      "Epoch 010 | Train Loss: 1.4247 Acc: 0.6793 | Val Loss: 0.7344 Acc: 0.7860                                               \n",
      "Epoch 011 | Train Loss: 1.2055 Acc: 0.7127 | Val Loss: 0.6531 Acc: 0.8110                                               \n",
      "Epoch 012 | Train Loss: 0.9397 Acc: 0.7653 | Val Loss: 0.4967 Acc: 0.8588                                               \n",
      "Epoch 013 | Train Loss: 0.8558 Acc: 0.7827 | Val Loss: 0.5732 Acc: 0.8322                                               \n",
      "Epoch 014 | Train Loss: 0.6935 Acc: 0.8062 | Val Loss: 0.3697 Acc: 0.8848                                               \n",
      "Epoch 015 | Train Loss: 0.6114 Acc: 0.8269 | Val Loss: 0.3971 Acc: 0.8785                                               \n",
      "Epoch 016 | Train Loss: 0.5054 Acc: 0.8555 | Val Loss: 0.3336 Acc: 0.8922                                               \n",
      "Epoch 017 | Train Loss: 0.4466 Acc: 0.8721 | Val Loss: 0.3852 Acc: 0.8752                                               \n",
      "Epoch 018 | Train Loss: 0.3954 Acc: 0.8880 | Val Loss: 0.4074 Acc: 0.8860                                               \n",
      "Epoch 019 | Train Loss: 0.3605 Acc: 0.8954 | Val Loss: 0.3988 Acc: 0.8845                                               \n",
      "Epoch 020 | Train Loss: 0.3238 Acc: 0.9051 | Val Loss: 0.2591 Acc: 0.9146                                               \n",
      "Epoch 021 | Train Loss: 0.3219 Acc: 0.9057 | Val Loss: 0.2561 Acc: 0.9096                                               \n",
      "Epoch 022 | Train Loss: 0.2600 Acc: 0.9210 | Val Loss: 0.2377 Acc: 0.9212                                               \n",
      "Epoch 023 | Train Loss: 0.2589 Acc: 0.9207 | Val Loss: 0.1896 Acc: 0.9385                                               \n",
      "Epoch 024 | Train Loss: 0.2474 Acc: 0.9275 | Val Loss: 0.2996 Acc: 0.9060                                               \n",
      "Epoch 025 | Train Loss: 0.2209 Acc: 0.9314 | Val Loss: 0.3712 Acc: 0.8997                                               \n",
      "Epoch 026 | Train Loss: 0.2218 Acc: 0.9355 | Val Loss: 0.2659 Acc: 0.9152                                               \n",
      "Epoch 027 | Train Loss: 0.1949 Acc: 0.9412 | Val Loss: 0.3227 Acc: 0.9012                                               \n",
      "Epoch 028 | Train Loss: 0.1811 Acc: 0.9459 | Val Loss: 0.2650 Acc: 0.9218                                               \n",
      "Epoch 029 | Train Loss: 0.1499 Acc: 0.9531 | Val Loss: 0.2582 Acc: 0.9185                                               \n",
      "Epoch 030 | Train Loss: 0.1799 Acc: 0.9481 | Val Loss: 0.2842 Acc: 0.9137                                               \n",
      "Epoch 031 | Train Loss: 0.1707 Acc: 0.9494 | Val Loss: 0.2382 Acc: 0.9287                                               \n",
      "Epoch 032 | Train Loss: 0.1266 Acc: 0.9592 | Val Loss: 0.1841 Acc: 0.9340                                               \n",
      "Epoch 033 | Train Loss: 0.1340 Acc: 0.9578 | Val Loss: 0.1889 Acc: 0.9424                                               \n",
      "Epoch 034 | Train Loss: 0.1137 Acc: 0.9645 | Val Loss: 0.1552 Acc: 0.9484                                               \n",
      "Epoch 035 | Train Loss: 0.1098 Acc: 0.9654 | Val Loss: 0.2565 Acc: 0.9367                                               \n",
      "Epoch 036 | Train Loss: 0.1083 Acc: 0.9658 | Val Loss: 0.1903 Acc: 0.9448                                               \n",
      "Epoch 037 | Train Loss: 0.1077 Acc: 0.9670 | Val Loss: 0.1239 Acc: 0.9666                                               \n",
      "Epoch 038 | Train Loss: 0.0924 Acc: 0.9713 | Val Loss: 0.2136 Acc: 0.9463                                               \n",
      "Epoch 039 | Train Loss: 0.0995 Acc: 0.9661 | Val Loss: 0.1297 Acc: 0.9657                                               \n",
      "Epoch 040 | Train Loss: 0.0887 Acc: 0.9718 | Val Loss: 0.1227 Acc: 0.9609                                               \n",
      "Epoch 041 | Train Loss: 0.0843 Acc: 0.9737 | Val Loss: 0.1422 Acc: 0.9651                                               \n",
      "Epoch 042 | Train Loss: 0.0646 Acc: 0.9814 | Val Loss: 0.1135 Acc: 0.9693                                               \n",
      "Epoch 043 | Train Loss: 0.0690 Acc: 0.9792 | Val Loss: 0.1076 Acc: 0.9669                                               \n",
      "Epoch 044 | Train Loss: 0.0719 Acc: 0.9778 | Val Loss: 0.1120 Acc: 0.9675                                               \n",
      "Epoch 045 | Train Loss: 0.0792 Acc: 0.9755 | Val Loss: 0.1424 Acc: 0.9627                                               \n",
      "Epoch 046 | Train Loss: 0.0717 Acc: 0.9782 | Val Loss: 0.1075 Acc: 0.9657                                               \n",
      "Epoch 047 | Train Loss: 0.0687 Acc: 0.9784 | Val Loss: 0.1587 Acc: 0.9579                                               \n",
      "Epoch 048 | Train Loss: 0.0559 Acc: 0.9828 | Val Loss: 0.1485 Acc: 0.9639                                               \n",
      "Epoch 049 | Train Loss: 0.0460 Acc: 0.9851 | Val Loss: 0.1569 Acc: 0.9612                                               \n",
      "Epoch 050 | Train Loss: 0.0479 Acc: 0.9842 | Val Loss: 0.0879 Acc: 0.9743                                               \n",
      "Epoch 051 | Train Loss: 0.0428 Acc: 0.9867 | Val Loss: 0.1428 Acc: 0.9633                                               \n",
      "Epoch 052 | Train Loss: 0.0486 Acc: 0.9866 | Val Loss: 0.1022 Acc: 0.9728                                               \n",
      "Epoch 053 | Train Loss: 0.0394 Acc: 0.9879 | Val Loss: 0.2561 Acc: 0.9355                                               \n",
      "Epoch 054 | Train Loss: 0.0443 Acc: 0.9865 | Val Loss: 0.1115 Acc: 0.9722                                               \n",
      "Epoch 055 | Train Loss: 0.0609 Acc: 0.9834 | Val Loss: 0.1063 Acc: 0.9734                                               \n",
      "Epoch 056 | Train Loss: 0.0611 Acc: 0.9829 | Val Loss: 0.1137 Acc: 0.9690                                               \n",
      "Epoch 057 | Train Loss: 0.0342 Acc: 0.9898 | Val Loss: 0.0793 Acc: 0.9779                                               \n",
      "Epoch 058 | Train Loss: 0.0348 Acc: 0.9882 | Val Loss: 0.1264 Acc: 0.9648                                               \n",
      "Epoch 059 | Train Loss: 0.0456 Acc: 0.9863 | Val Loss: 0.0986 Acc: 0.9770                                               \n",
      "Epoch 060 | Train Loss: 0.0320 Acc: 0.9900 | Val Loss: 0.1235 Acc: 0.9678                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 128, 'cnn_dropout': 0.3584155618035024, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 32, 'cnn_kernels_2': 32, 'learning_rate': 5.7105142638052414e-05, 'lstm_dense': 128, 'lstm_hidden_size': 32, 'lstm_layers': 1, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 85.2106 Acc: 0.3327 | Val Loss: 11.5145 Acc: 0.4913                                             \n",
      "Epoch 002 | Train Loss: 26.1994 Acc: 0.3845 | Val Loss: 6.9191 Acc: 0.4290                                              \n",
      "Epoch 003 | Train Loss: 16.7510 Acc: 0.3875 | Val Loss: 5.0157 Acc: 0.4794                                              \n",
      "Epoch 004 | Train Loss: 11.7185 Acc: 0.4073 | Val Loss: 3.6158 Acc: 0.4609                                              \n",
      "Epoch 005 | Train Loss: 9.2126 Acc: 0.4113 | Val Loss: 2.6575 Acc: 0.5039                                               \n",
      "Epoch 006 | Train Loss: 7.3354 Acc: 0.4149 | Val Loss: 2.2299 Acc: 0.5030                                               \n",
      "Epoch 007 | Train Loss: 5.9223 Acc: 0.4290 | Val Loss: 2.6843 Acc: 0.5352                                               \n",
      "Epoch 008 | Train Loss: 5.2765 Acc: 0.4233 | Val Loss: 1.8493 Acc: 0.5293                                               \n",
      "Epoch 009 | Train Loss: 4.5344 Acc: 0.4329 | Val Loss: 1.6961 Acc: 0.5418                                               \n",
      "Epoch 010 | Train Loss: 3.9497 Acc: 0.4440 | Val Loss: 2.6206 Acc: 0.5224                                               \n",
      "Epoch 011 | Train Loss: 3.4845 Acc: 0.4494 | Val Loss: 1.1891 Acc: 0.5693                                               \n",
      "Epoch 012 | Train Loss: 3.1088 Acc: 0.4472 | Val Loss: 1.5013 Acc: 0.5397                                               \n",
      "Epoch 013 | Train Loss: 2.7518 Acc: 0.4569 | Val Loss: 1.3372 Acc: 0.5830                                               \n",
      "Epoch 014 | Train Loss: 2.4005 Acc: 0.4745 | Val Loss: 1.1999 Acc: 0.5639                                               \n",
      "Epoch 015 | Train Loss: 2.1799 Acc: 0.4808 | Val Loss: 1.0225 Acc: 0.5904                                               \n",
      "Epoch 016 | Train Loss: 2.0514 Acc: 0.4885 | Val Loss: 1.0056 Acc: 0.5818                                               \n",
      "Epoch 017 | Train Loss: 1.7949 Acc: 0.4964 | Val Loss: 0.9253 Acc: 0.6099                                               \n",
      "Epoch 018 | Train Loss: 1.6777 Acc: 0.5032 | Val Loss: 0.9793 Acc: 0.5591                                               \n",
      "Epoch 019 | Train Loss: 1.5859 Acc: 0.5158 | Val Loss: 1.0537 Acc: 0.5549                                               \n",
      "Epoch 020 | Train Loss: 1.5013 Acc: 0.5147 | Val Loss: 0.7738 Acc: 0.6436                                               \n",
      "Epoch 021 | Train Loss: 1.3795 Acc: 0.5286 | Val Loss: 0.8483 Acc: 0.6490                                               \n",
      "Epoch 022 | Train Loss: 1.2710 Acc: 0.5450 | Val Loss: 0.9340 Acc: 0.5782                                               \n",
      "Epoch 023 | Train Loss: 1.2235 Acc: 0.5438 | Val Loss: 0.7781 Acc: 0.6063                                               \n",
      "Epoch 024 | Train Loss: 1.1423 Acc: 0.5580 | Val Loss: 0.7587 Acc: 0.6143                                               \n",
      "Epoch 025 | Train Loss: 1.0717 Acc: 0.5758 | Val Loss: 0.8278 Acc: 0.5597                                               \n",
      "Epoch 026 | Train Loss: 1.0324 Acc: 0.5833 | Val Loss: 0.7010 Acc: 0.6281                                               \n",
      "Epoch 027 | Train Loss: 1.0049 Acc: 0.5754 | Val Loss: 0.7316 Acc: 0.6460                                               \n",
      "Epoch 028 | Train Loss: 0.9786 Acc: 0.5827 | Val Loss: 0.6964 Acc: 0.6370                                               \n",
      "Epoch 029 | Train Loss: 0.9564 Acc: 0.5826 | Val Loss: 0.7524 Acc: 0.5833                                               \n",
      "Epoch 030 | Train Loss: 0.9192 Acc: 0.5936 | Val Loss: 0.8052 Acc: 0.5722                                               \n",
      "Epoch 031 | Train Loss: 0.9066 Acc: 0.5966 | Val Loss: 0.6454 Acc: 0.7278                                               \n",
      "Epoch 032 | Train Loss: 0.8704 Acc: 0.6049 | Val Loss: 0.6789 Acc: 0.6522                                               \n",
      "Epoch 033 | Train Loss: 0.8412 Acc: 0.6230 | Val Loss: 0.7862 Acc: 0.6290                                               \n",
      "Epoch 034 | Train Loss: 0.8644 Acc: 0.6171 | Val Loss: 0.6372 Acc: 0.7403                                               \n",
      "Epoch 035 | Train Loss: 0.8444 Acc: 0.6153 | Val Loss: 0.7150 Acc: 0.6406                                               \n",
      "Epoch 036 | Train Loss: 0.8329 Acc: 0.6196 | Val Loss: 0.6266 Acc: 0.7657                                               \n",
      "Epoch 037 | Train Loss: 0.8560 Acc: 0.6096 | Val Loss: 0.6453 Acc: 0.7391                                               \n",
      "Epoch 038 | Train Loss: 0.8307 Acc: 0.6174 | Val Loss: 0.6472 Acc: 0.7200                                               \n",
      "Epoch 039 | Train Loss: 0.8169 Acc: 0.6189 | Val Loss: 0.7065 Acc: 0.6173                                               \n",
      "Epoch 040 | Train Loss: 0.7940 Acc: 0.6303 | Val Loss: 0.6418 Acc: 0.7078                                               \n",
      "Epoch 041 | Train Loss: 0.8172 Acc: 0.6193 | Val Loss: 0.7050 Acc: 0.6230                                               \n",
      "Epoch 042 | Train Loss: 0.7975 Acc: 0.6295 | Val Loss: 0.6869 Acc: 0.6740                                               \n",
      "Epoch 043 | Train Loss: 0.7822 Acc: 0.6264 | Val Loss: 0.6220 Acc: 0.7021                                               \n",
      "Epoch 044 | Train Loss: 0.7819 Acc: 0.6313 | Val Loss: 0.6637 Acc: 0.6985                                               \n",
      "Epoch 045 | Train Loss: 0.7632 Acc: 0.6414 | Val Loss: 0.6093 Acc: 0.7475                                               \n",
      "Epoch 046 | Train Loss: 0.7636 Acc: 0.6434 | Val Loss: 0.6385 Acc: 0.7299                                               \n",
      "Epoch 047 | Train Loss: 0.7617 Acc: 0.6377 | Val Loss: 0.6183 Acc: 0.7104                                               \n",
      "Epoch 048 | Train Loss: 0.7568 Acc: 0.6378 | Val Loss: 0.6412 Acc: 0.6519                                               \n",
      "Epoch 049 | Train Loss: 0.7545 Acc: 0.6456 | Val Loss: 0.6854 Acc: 0.6221                                               \n",
      "Epoch 050 | Train Loss: 0.7529 Acc: 0.6488 | Val Loss: 0.6410 Acc: 0.6937                                               \n",
      "Epoch 051 | Train Loss: 0.7325 Acc: 0.6648 | Val Loss: 0.5991 Acc: 0.7507                                               \n",
      "Epoch 052 | Train Loss: 0.7274 Acc: 0.6677 | Val Loss: 0.6398 Acc: 0.7101                                               \n",
      "Epoch 053 | Train Loss: 0.7214 Acc: 0.6628 | Val Loss: 0.6161 Acc: 0.7439                                               \n",
      "Epoch 054 | Train Loss: 0.7108 Acc: 0.6730 | Val Loss: 0.5888 Acc: 0.7725                                               \n",
      "Epoch 055 | Train Loss: 0.7157 Acc: 0.6724 | Val Loss: 0.6533 Acc: 0.7099                                               \n",
      "Epoch 056 | Train Loss: 0.7288 Acc: 0.6568 | Val Loss: 0.6829 Acc: 0.6681                                               \n",
      "Epoch 057 | Train Loss: 0.7119 Acc: 0.6688 | Val Loss: 0.6625 Acc: 0.6516                                               \n",
      "Epoch 058 | Train Loss: 0.7138 Acc: 0.6689 | Val Loss: 0.6855 Acc: 0.6364                                               \n",
      "Epoch 059 | Train Loss: 0.7099 Acc: 0.6746 | Val Loss: 0.6703 Acc: 0.6937                                               \n",
      "Epoch 060 | Train Loss: 0.7014 Acc: 0.6763 | Val Loss: 0.6137 Acc: 0.7445                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 32, 'cnn_dropout': 0.518814487050888, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 16, 'learning_rate': 1.4691032851414804e-05, 'lstm_dense': 128, 'lstm_hidden_size': 64, 'lstm_layers': 5, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 55.9804 Acc: 0.3212 | Val Loss: 27.8779 Acc: 0.4421                                             \n",
      "Epoch 002 | Train Loss: 39.7834 Acc: 0.3203 | Val Loss: 18.4832 Acc: 0.4421                                             \n",
      "Epoch 003 | Train Loss: 31.8103 Acc: 0.3256 | Val Loss: 11.5531 Acc: 0.4304                                             \n",
      "Epoch 004 | Train Loss: 25.9079 Acc: 0.3267 | Val Loss: 7.5157 Acc: 0.4343                                              \n",
      "Epoch 005 | Train Loss: 21.8337 Acc: 0.3421 | Val Loss: 5.8413 Acc: 0.4418                                              \n",
      "Epoch 006 | Train Loss: 18.7387 Acc: 0.3518 | Val Loss: 4.2077 Acc: 0.4379                                              \n",
      "Epoch 007 | Train Loss: 16.4611 Acc: 0.3620 | Val Loss: 3.1022 Acc: 0.4155                                              \n",
      "Epoch 008 | Train Loss: 14.5789 Acc: 0.3678 | Val Loss: 2.9220 Acc: 0.4740                                              \n",
      "Epoch 009 | Train Loss: 12.9404 Acc: 0.3812 | Val Loss: 2.6839 Acc: 0.4809                                              \n",
      "Epoch 010 | Train Loss: 12.0150 Acc: 0.3752 | Val Loss: 2.7177 Acc: 0.4988                                              \n",
      "Epoch 011 | Train Loss: 10.5189 Acc: 0.3883 | Val Loss: 2.5078 Acc: 0.5137                                              \n",
      "Epoch 012 | Train Loss: 9.4481 Acc: 0.3956 | Val Loss: 2.1188 Acc: 0.5230                                               \n",
      "Epoch 013 | Train Loss: 8.7701 Acc: 0.4020 | Val Loss: 1.7263 Acc: 0.5427                                               \n",
      "Epoch 014 | Train Loss: 7.8853 Acc: 0.4134 | Val Loss: 1.6406 Acc: 0.5179                                               \n",
      "Epoch 015 | Train Loss: 7.1315 Acc: 0.4161 | Val Loss: 1.5230 Acc: 0.5397                                               \n",
      "Epoch 016 | Train Loss: 6.6623 Acc: 0.4259 | Val Loss: 1.3857 Acc: 0.5734                                               \n",
      "Epoch 017 | Train Loss: 5.9421 Acc: 0.4448 | Val Loss: 1.2590 Acc: 0.5913                                               \n",
      "Epoch 018 | Train Loss: 5.6057 Acc: 0.4423 | Val Loss: 1.1167 Acc: 0.6266                                               \n",
      "Epoch 019 | Train Loss: 5.2791 Acc: 0.4517 | Val Loss: 1.1955 Acc: 0.6633                                               \n",
      "Epoch 020 | Train Loss: 4.8469 Acc: 0.4525 | Val Loss: 1.2393 Acc: 0.6212                                               \n",
      "Epoch 021 | Train Loss: 4.5569 Acc: 0.4626 | Val Loss: 1.1065 Acc: 0.6713                                               \n",
      "Epoch 022 | Train Loss: 4.3053 Acc: 0.4743 | Val Loss: 1.1241 Acc: 0.6484                                               \n",
      "Epoch 023 | Train Loss: 3.9328 Acc: 0.4914 | Val Loss: 1.0731 Acc: 0.6573                                               \n",
      "Epoch 024 | Train Loss: 3.7548 Acc: 0.4843 | Val Loss: 1.1306 Acc: 0.6376                                               \n",
      "Epoch 025 | Train Loss: 3.4898 Acc: 0.5059 | Val Loss: 0.9984 Acc: 0.6185                                               \n",
      "Epoch 026 | Train Loss: 3.4067 Acc: 0.5038 | Val Loss: 1.0716 Acc: 0.6331                                               \n",
      "Epoch 027 | Train Loss: 3.2142 Acc: 0.5107 | Val Loss: 1.0132 Acc: 0.6460                                               \n",
      "Epoch 028 | Train Loss: 3.0998 Acc: 0.5135 | Val Loss: 1.3795 Acc: 0.5967                                               \n",
      "Epoch 029 | Train Loss: 2.8517 Acc: 0.5281 | Val Loss: 1.3369 Acc: 0.6263                                               \n",
      "Epoch 030 | Train Loss: 2.7577 Acc: 0.5294 | Val Loss: 1.0781 Acc: 0.6376                                               \n",
      "Epoch 031 | Train Loss: 2.6989 Acc: 0.5336 | Val Loss: 1.2864 Acc: 0.6278                                               \n",
      "Epoch 032 | Train Loss: 2.5342 Acc: 0.5406 | Val Loss: 0.8980 Acc: 0.6696                                               \n",
      "Epoch 033 | Train Loss: 2.5166 Acc: 0.5417 | Val Loss: 0.8614 Acc: 0.6728                                               \n",
      "Epoch 034 | Train Loss: 2.3134 Acc: 0.5521 | Val Loss: 1.1074 Acc: 0.6361                                               \n",
      "Epoch 035 | Train Loss: 2.1634 Acc: 0.5524 | Val Loss: 0.9040 Acc: 0.6722                                               \n",
      "Epoch 036 | Train Loss: 2.0903 Acc: 0.5609 | Val Loss: 0.9905 Acc: 0.6690                                               \n",
      "Epoch 037 | Train Loss: 2.1111 Acc: 0.5701 | Val Loss: 1.2325 Acc: 0.6275                                               \n",
      "Epoch 038 | Train Loss: 1.9907 Acc: 0.5762 | Val Loss: 0.9482 Acc: 0.6958                                               \n",
      "Epoch 039 | Train Loss: 1.8318 Acc: 0.5809 | Val Loss: 0.9307 Acc: 0.7003                                               \n",
      "Epoch 040 | Train Loss: 1.8037 Acc: 0.5887 | Val Loss: 1.4564 Acc: 0.5964                                               \n",
      "Epoch 041 | Train Loss: 1.7999 Acc: 0.5895 | Val Loss: 0.8042 Acc: 0.7373                                               \n",
      "Epoch 042 | Train Loss: 1.6977 Acc: 0.5971 | Val Loss: 0.8952 Acc: 0.7075                                               \n",
      "Epoch 043 | Train Loss: 1.6730 Acc: 0.6017 | Val Loss: 0.9077 Acc: 0.7066                                               \n",
      "Epoch 044 | Train Loss: 1.5823 Acc: 0.6162 | Val Loss: 0.8221 Acc: 0.7096                                               \n",
      "Epoch 045 | Train Loss: 1.5008 Acc: 0.6118 | Val Loss: 1.0792 Acc: 0.6660                                               \n",
      "Epoch 046 | Train Loss: 1.4631 Acc: 0.6199 | Val Loss: 0.7443 Acc: 0.7087                                               \n",
      "Epoch 047 | Train Loss: 1.4554 Acc: 0.6287 | Val Loss: 0.8353 Acc: 0.7215                                               \n",
      "Epoch 048 | Train Loss: 1.4240 Acc: 0.6289 | Val Loss: 0.7918 Acc: 0.7397                                               \n",
      "Epoch 049 | Train Loss: 1.3338 Acc: 0.6382 | Val Loss: 0.9595 Acc: 0.6806                                               \n",
      "Epoch 050 | Train Loss: 1.3033 Acc: 0.6422 | Val Loss: 0.8376 Acc: 0.7143                                               \n",
      "Epoch 051 | Train Loss: 1.2895 Acc: 0.6449 | Val Loss: 0.7026 Acc: 0.7594                                               \n",
      "Epoch 052 | Train Loss: 1.2704 Acc: 0.6485 | Val Loss: 0.8784 Acc: 0.7191                                               \n",
      "Epoch 053 | Train Loss: 1.2076 Acc: 0.6592 | Val Loss: 0.9832 Acc: 0.6851                                               \n",
      "Epoch 054 | Train Loss: 1.2215 Acc: 0.6595 | Val Loss: 0.7248 Acc: 0.7397                                               \n",
      "Epoch 055 | Train Loss: 1.1790 Acc: 0.6610 | Val Loss: 0.8623 Acc: 0.7099                                               \n",
      "Epoch 056 | Train Loss: 1.1394 Acc: 0.6697 | Val Loss: 0.7495 Acc: 0.7433                                               \n",
      "Epoch 057 | Train Loss: 1.0883 Acc: 0.6728 | Val Loss: 0.8441 Acc: 0.7287                                               \n",
      "Epoch 058 | Train Loss: 1.0457 Acc: 0.6818 | Val Loss: 0.7924 Acc: 0.7182                                               \n",
      "Epoch 059 | Train Loss: 1.1164 Acc: 0.6776 | Val Loss: 0.7049 Acc: 0.7355                                               \n",
      "Epoch 060 | Train Loss: 1.0414 Acc: 0.6861 | Val Loss: 0.6994 Acc: 0.7722                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 128, 'cnn_dropout': 0.40988286613780683, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 16, 'learning_rate': 0.0006541317865181193, 'lstm_dense': 128, 'lstm_hidden_size': 96, 'lstm_layers': 5, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 18.8594 Acc: 0.4263 | Val Loss: 19.8051 Acc: 0.4421                                             \n",
      "Epoch 002 | Train Loss: 5.4871 Acc: 0.4853 | Val Loss: 1.9816 Acc: 0.5800                                               \n",
      "Epoch 003 | Train Loss: 2.6680 Acc: 0.5355 | Val Loss: 1.1181 Acc: 0.6260                                               \n",
      "Epoch 004 | Train Loss: 1.2004 Acc: 0.6315 | Val Loss: 0.7725 Acc: 0.7042                                               \n",
      "Epoch 005 | Train Loss: 0.7585 Acc: 0.7116 | Val Loss: 0.5363 Acc: 0.7684                                               \n",
      "Epoch 006 | Train Loss: 0.6118 Acc: 0.7650 | Val Loss: 0.4416 Acc: 0.8370                                               \n",
      "Epoch 007 | Train Loss: 0.5126 Acc: 0.8122 | Val Loss: 0.5356 Acc: 0.7925                                               \n",
      "Epoch 008 | Train Loss: 0.4456 Acc: 0.8445 | Val Loss: 0.3276 Acc: 0.8684                                               \n",
      "Epoch 009 | Train Loss: 0.3771 Acc: 0.8610 | Val Loss: 0.3587 Acc: 0.8645                                               \n",
      "Epoch 010 | Train Loss: 0.3285 Acc: 0.8863 | Val Loss: 0.5077 Acc: 0.8749                                               \n",
      "Epoch 011 | Train Loss: 0.2931 Acc: 0.8991 | Val Loss: 0.3079 Acc: 0.8824                                               \n",
      "Epoch 012 | Train Loss: 0.2563 Acc: 0.9140 | Val Loss: 0.2128 Acc: 0.9191                                               \n",
      "Epoch 013 | Train Loss: 0.2360 Acc: 0.9189 | Val Loss: 0.2087 Acc: 0.9272                                               \n",
      "Epoch 014 | Train Loss: 0.2141 Acc: 0.9254 | Val Loss: 0.1924 Acc: 0.9296                                               \n",
      "Epoch 015 | Train Loss: 0.1893 Acc: 0.9357 | Val Loss: 0.2740 Acc: 0.9137                                               \n",
      "Epoch 016 | Train Loss: 0.1801 Acc: 0.9395 | Val Loss: 0.2283 Acc: 0.9191                                               \n",
      "Epoch 017 | Train Loss: 0.1684 Acc: 0.9429 | Val Loss: 0.1802 Acc: 0.9427                                               \n",
      "Epoch 018 | Train Loss: 0.1557 Acc: 0.9489 | Val Loss: 0.1173 Acc: 0.9630                                               \n",
      "Epoch 019 | Train Loss: 0.1430 Acc: 0.9512 | Val Loss: 0.1024 Acc: 0.9645                                               \n",
      "Epoch 020 | Train Loss: 0.1329 Acc: 0.9572 | Val Loss: 0.3701 Acc: 0.8851                                               \n",
      "Epoch 021 | Train Loss: 0.1207 Acc: 0.9601 | Val Loss: 0.1404 Acc: 0.9460                                               \n",
      "Epoch 022 | Train Loss: 0.1177 Acc: 0.9613 | Val Loss: 0.1068 Acc: 0.9609                                               \n",
      "Epoch 023 | Train Loss: 0.1072 Acc: 0.9656 | Val Loss: 0.1418 Acc: 0.9493                                               \n",
      "Epoch 024 | Train Loss: 0.1014 Acc: 0.9657 | Val Loss: 0.0789 Acc: 0.9743                                               \n",
      "Epoch 025 | Train Loss: 0.0960 Acc: 0.9691 | Val Loss: 1.2661 Acc: 0.7078                                               \n",
      "Epoch 026 | Train Loss: 0.0991 Acc: 0.9695 | Val Loss: 0.1051 Acc: 0.9675                                               \n",
      "Epoch 027 | Train Loss: 0.0788 Acc: 0.9730 | Val Loss: 0.1615 Acc: 0.9496                                               \n",
      "Epoch 028 | Train Loss: 0.0911 Acc: 0.9703 | Val Loss: 0.0758 Acc: 0.9728                                               \n",
      "Epoch 029 | Train Loss: 0.0752 Acc: 0.9751 | Val Loss: 0.0655 Acc: 0.9770                                               \n",
      "Epoch 030 | Train Loss: 0.0863 Acc: 0.9709 | Val Loss: 0.1559 Acc: 0.9525                                               \n",
      "Epoch 031 | Train Loss: 0.0724 Acc: 0.9776 | Val Loss: 0.1939 Acc: 0.9409                                               \n",
      "Epoch 032 | Train Loss: 0.0819 Acc: 0.9733 | Val Loss: 0.0937 Acc: 0.9719                                               \n",
      "Epoch 033 | Train Loss: 0.0594 Acc: 0.9804 | Val Loss: 0.0812 Acc: 0.9749                                               \n",
      "Epoch 034 | Train Loss: 0.0691 Acc: 0.9786 | Val Loss: 0.0755 Acc: 0.9785                                               \n",
      "Epoch 035 | Train Loss: 0.0707 Acc: 0.9775 | Val Loss: 0.0469 Acc: 0.9857                                               \n",
      "Epoch 036 | Train Loss: 0.0717 Acc: 0.9778 | Val Loss: 0.0494 Acc: 0.9836                                               \n",
      "Epoch 037 | Train Loss: 0.0590 Acc: 0.9799 | Val Loss: 0.1800 Acc: 0.9424                                               \n",
      "Epoch 038 | Train Loss: 0.0577 Acc: 0.9817 | Val Loss: 0.0566 Acc: 0.9824                                               \n",
      "Epoch 039 | Train Loss: 0.0645 Acc: 0.9805 | Val Loss: 0.0713 Acc: 0.9749                                               \n",
      "Epoch 040 | Train Loss: 0.0532 Acc: 0.9830 | Val Loss: 0.1128 Acc: 0.9630                                               \n",
      "Epoch 041 | Train Loss: 0.0542 Acc: 0.9816 | Val Loss: 0.0693 Acc: 0.9791                                               \n",
      "Epoch 042 | Train Loss: 0.0570 Acc: 0.9828 | Val Loss: 0.0514 Acc: 0.9854                                               \n",
      "Epoch 043 | Train Loss: 0.0537 Acc: 0.9808 | Val Loss: 0.1674 Acc: 0.9415                                               \n",
      "Epoch 044 | Train Loss: 0.0612 Acc: 0.9804 | Val Loss: 0.1000 Acc: 0.9690                                               \n",
      "Epoch 045 | Train Loss: 0.0442 Acc: 0.9848 | Val Loss: 0.1111 Acc: 0.9615                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 36, 'cnn_dense': 32, 'cnn_dropout': 0.6702627572131722, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 32, 'cnn_kernels_2': 64, 'learning_rate': 0.0033668198939886683, 'lstm_dense': 32, 'lstm_hidden_size': 128, 'lstm_layers': 4, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 5.5483 Acc: 0.4785 | Val Loss: 0.7942 Acc: 0.6158                                               \n",
      "Epoch 002 | Train Loss: 0.8683 Acc: 0.6009 | Val Loss: 0.6311 Acc: 0.7558                                               \n",
      "Epoch 003 | Train Loss: 0.7310 Acc: 0.6553 | Val Loss: 0.6001 Acc: 0.7803                                               \n",
      "Epoch 004 | Train Loss: 0.6650 Acc: 0.6988 | Val Loss: 0.5686 Acc: 0.8081                                               \n",
      "Epoch 005 | Train Loss: 0.6584 Acc: 0.7038 | Val Loss: 0.5887 Acc: 0.7636                                               \n",
      "Epoch 006 | Train Loss: 0.6633 Acc: 0.6926 | Val Loss: 0.6153 Acc: 0.7087                                               \n",
      "Epoch 007 | Train Loss: 0.6536 Acc: 0.6979 | Val Loss: 0.6409 Acc: 0.7191                                               \n",
      "Epoch 008 | Train Loss: 0.6603 Acc: 0.6943 | Val Loss: 0.5585 Acc: 0.7606                                               \n",
      "Epoch 009 | Train Loss: 0.6678 Acc: 0.6889 | Val Loss: 0.8576 Acc: 0.6099                                               \n",
      "Epoch 010 | Train Loss: 0.6363 Acc: 0.7093 | Val Loss: 0.6554 Acc: 0.6699                                               \n",
      "Epoch 011 | Train Loss: 0.6581 Acc: 0.6966 | Val Loss: 0.6714 Acc: 0.7134                                               \n",
      "Epoch 012 | Train Loss: 0.6595 Acc: 0.6951 | Val Loss: 0.7288 Acc: 0.6827                                               \n",
      "Epoch 013 | Train Loss: 0.6751 Acc: 0.6891 | Val Loss: 0.6961 Acc: 0.6182                                               \n",
      "Epoch 014 | Train Loss: 0.6636 Acc: 0.6969 | Val Loss: 0.5768 Acc: 0.7737                                               \n",
      "Epoch 015 | Train Loss: 0.6512 Acc: 0.6989 | Val Loss: 0.7997 Acc: 0.6263                                               \n",
      "Epoch 016 | Train Loss: 0.6596 Acc: 0.6901 | Val Loss: 0.8423 Acc: 0.6767                                               \n",
      "Epoch 017 | Train Loss: 0.6956 Acc: 0.6660 | Val Loss: 0.6813 Acc: 0.6770                                               \n",
      "Epoch 018 | Train Loss: 0.6891 Acc: 0.6715 | Val Loss: 0.7791 Acc: 0.6549                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 32, 'cnn_dropout': 0.06940473977303235, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 64, 'learning_rate': 0.008351796214530636, 'lstm_dense': 256, 'lstm_hidden_size': 64, 'lstm_layers': 6, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 10.3181 Acc: 0.5154 | Val Loss: 0.7725 Acc: 0.6466                                              \n",
      "Epoch 002 | Train Loss: 0.7937 Acc: 0.6271 | Val Loss: 0.7564 Acc: 0.6212                                               \n",
      "Epoch 003 | Train Loss: 0.7325 Acc: 0.6653 | Val Loss: 0.6772 Acc: 0.6958                                               \n",
      "Epoch 004 | Train Loss: 0.7126 Acc: 0.6741 | Val Loss: 0.6147 Acc: 0.7367                                               \n",
      "Epoch 005 | Train Loss: 0.7029 Acc: 0.6786 | Val Loss: 0.6454 Acc: 0.7009                                               \n",
      "Epoch 006 | Train Loss: 0.6880 Acc: 0.6874 | Val Loss: 0.7930 Acc: 0.6027                                               \n",
      "Epoch 007 | Train Loss: 0.7180 Acc: 0.6521 | Val Loss: 0.7372 Acc: 0.6218                                               \n",
      "Epoch 008 | Train Loss: 0.7114 Acc: 0.6629 | Val Loss: 0.8530 Acc: 0.5594                                               \n",
      "Epoch 009 | Train Loss: 0.6783 Acc: 0.6868 | Val Loss: 0.7617 Acc: 0.6546                                               \n",
      "Epoch 010 | Train Loss: 0.6764 Acc: 0.6886 | Val Loss: 0.7280 Acc: 0.6346                                               \n",
      "Epoch 011 | Train Loss: 0.6666 Acc: 0.6871 | Val Loss: 0.7551 Acc: 0.5916                                               \n",
      "Epoch 012 | Train Loss: 0.7170 Acc: 0.6589 | Val Loss: 0.6303 Acc: 0.7248                                               \n",
      "Epoch 013 | Train Loss: 0.6812 Acc: 0.6800 | Val Loss: 0.8425 Acc: 0.5364                                               \n",
      "Epoch 014 | Train Loss: 0.6796 Acc: 0.6785 | Val Loss: 0.8369 Acc: 0.6313                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 64, 'cnn_dropout': 0.3071879352802247, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 16, 'learning_rate': 0.0024434874613210177, 'lstm_dense': 256, 'lstm_hidden_size': 32, 'lstm_layers': 2, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 91.9850 Acc: 0.4334 | Val Loss: 1.2639 Acc: 0.4421                                              \n",
      "Epoch 002 | Train Loss: 1.2543 Acc: 0.4422 | Val Loss: 1.2474 Acc: 0.4421                                               \n",
      "Epoch 003 | Train Loss: 1.2443 Acc: 0.4422 | Val Loss: 1.2426 Acc: 0.4421                                               \n",
      "Epoch 004 | Train Loss: 1.2404 Acc: 0.4422 | Val Loss: 1.2380 Acc: 0.4421                                               \n",
      "Epoch 005 | Train Loss: 1.2363 Acc: 0.4422 | Val Loss: 1.2315 Acc: 0.4409                                               \n",
      "Epoch 006 | Train Loss: 1.2259 Acc: 0.4418 | Val Loss: 1.2159 Acc: 0.4421                                               \n",
      "Epoch 007 | Train Loss: 1.2025 Acc: 0.4532 | Val Loss: 1.1817 Acc: 0.4749                                               \n",
      "Epoch 008 | Train Loss: 1.1668 Acc: 0.4762 | Val Loss: 1.1460 Acc: 0.4887                                               \n",
      "Epoch 009 | Train Loss: 1.1382 Acc: 0.4905 | Val Loss: 1.1417 Acc: 0.4594                                               \n",
      "Epoch 010 | Train Loss: 1.1172 Acc: 0.4992 | Val Loss: 1.1056 Acc: 0.5036                                               \n",
      "Epoch 011 | Train Loss: 1.0895 Acc: 0.5131 | Val Loss: 1.1362 Acc: 0.4958                                               \n",
      "Epoch 012 | Train Loss: 1.0662 Acc: 0.5211 | Val Loss: 1.0586 Acc: 0.5218                                               \n",
      "Epoch 013 | Train Loss: 1.0257 Acc: 0.5490 | Val Loss: 1.0751 Acc: 0.5215                                               \n",
      "Epoch 014 | Train Loss: 0.9954 Acc: 0.5715 | Val Loss: 0.9311 Acc: 0.6173                                               \n",
      "Epoch 015 | Train Loss: 0.9320 Acc: 0.6024 | Val Loss: 0.9144 Acc: 0.6045                                               \n",
      "Epoch 016 | Train Loss: 0.9155 Acc: 0.6169 | Val Loss: 0.8693 Acc: 0.6373                                               \n",
      "Epoch 017 | Train Loss: 0.9095 Acc: 0.6224 | Val Loss: 0.8543 Acc: 0.6507                                               \n",
      "Epoch 018 | Train Loss: 0.8558 Acc: 0.6548 | Val Loss: 0.8205 Acc: 0.6761                                               \n",
      "Epoch 019 | Train Loss: 0.8315 Acc: 0.6668 | Val Loss: 0.7910 Acc: 0.6854                                               \n",
      "Epoch 020 | Train Loss: 0.7814 Acc: 0.6943 | Val Loss: 0.7067 Acc: 0.7367                                               \n",
      "Epoch 021 | Train Loss: 0.7438 Acc: 0.7106 | Val Loss: 0.6785 Acc: 0.7400                                               \n",
      "Epoch 022 | Train Loss: 0.6965 Acc: 0.7329 | Val Loss: 0.7137 Acc: 0.7170                                               \n",
      "Epoch 023 | Train Loss: 0.6727 Acc: 0.7462 | Val Loss: 0.6763 Acc: 0.7397                                               \n",
      "Epoch 024 | Train Loss: 0.6633 Acc: 0.7473 | Val Loss: 0.5923 Acc: 0.7869                                               \n",
      "Epoch 025 | Train Loss: 0.6227 Acc: 0.7652 | Val Loss: 0.5836 Acc: 0.7887                                               \n",
      "Epoch 026 | Train Loss: 0.5859 Acc: 0.7780 | Val Loss: 0.5539 Acc: 0.7937                                               \n",
      "Epoch 027 | Train Loss: 0.5497 Acc: 0.7931 | Val Loss: 0.4934 Acc: 0.8331                                               \n",
      "Epoch 028 | Train Loss: 0.5045 Acc: 0.8146 | Val Loss: 0.5729 Acc: 0.8066                                               \n",
      "Epoch 029 | Train Loss: 0.5036 Acc: 0.8144 | Val Loss: 0.4698 Acc: 0.8367                                               \n",
      "Epoch 030 | Train Loss: 0.4504 Acc: 0.8393 | Val Loss: 0.5696 Acc: 0.8036                                               \n",
      "Epoch 031 | Train Loss: 0.4504 Acc: 0.8410 | Val Loss: 0.4301 Acc: 0.8564                                               \n",
      "Epoch 032 | Train Loss: 0.4238 Acc: 0.8516 | Val Loss: 0.4238 Acc: 0.8510                                               \n",
      "Epoch 033 | Train Loss: 0.4153 Acc: 0.8517 | Val Loss: 0.3808 Acc: 0.8696                                               \n",
      "Epoch 034 | Train Loss: 0.3823 Acc: 0.8651 | Val Loss: 0.3924 Acc: 0.8636                                               \n",
      "Epoch 035 | Train Loss: 0.3700 Acc: 0.8717 | Val Loss: 0.4576 Acc: 0.8442                                               \n",
      "Epoch 036 | Train Loss: 0.3519 Acc: 0.8779 | Val Loss: 0.3425 Acc: 0.8830                                               \n",
      "Epoch 037 | Train Loss: 0.3467 Acc: 0.8771 | Val Loss: 0.3556 Acc: 0.8746                                               \n",
      "Epoch 038 | Train Loss: 0.3321 Acc: 0.8832 | Val Loss: 0.3357 Acc: 0.8896                                               \n",
      "Epoch 039 | Train Loss: 0.3368 Acc: 0.8836 | Val Loss: 0.3397 Acc: 0.8901                                               \n",
      "Epoch 040 | Train Loss: 0.3116 Acc: 0.8923 | Val Loss: 0.3580 Acc: 0.8806                                               \n",
      "Epoch 041 | Train Loss: 0.2936 Acc: 0.8963 | Val Loss: 0.3715 Acc: 0.8779                                               \n",
      "Epoch 042 | Train Loss: 0.3161 Acc: 0.8907 | Val Loss: 0.3268 Acc: 0.8955                                               \n",
      "Epoch 043 | Train Loss: 0.2965 Acc: 0.8980 | Val Loss: 0.2808 Acc: 0.9072                                               \n",
      "Epoch 044 | Train Loss: 0.2899 Acc: 0.8981 | Val Loss: 0.2860 Acc: 0.9021                                               \n",
      "Epoch 045 | Train Loss: 0.2785 Acc: 0.9022 | Val Loss: 0.4533 Acc: 0.8418                                               \n",
      "Epoch 046 | Train Loss: 0.3033 Acc: 0.8937 | Val Loss: 0.2930 Acc: 0.9012                                               \n",
      "Epoch 047 | Train Loss: 0.2647 Acc: 0.9076 | Val Loss: 0.2635 Acc: 0.9116                                               \n",
      "Epoch 048 | Train Loss: 0.2624 Acc: 0.9068 | Val Loss: 0.3416 Acc: 0.8854                                               \n",
      "Epoch 049 | Train Loss: 0.2748 Acc: 0.9015 | Val Loss: 0.2840 Acc: 0.9021                                               \n",
      "Epoch 050 | Train Loss: 0.2528 Acc: 0.9128 | Val Loss: 0.2802 Acc: 0.9113                                               \n",
      "Epoch 051 | Train Loss: 0.2516 Acc: 0.9114 | Val Loss: 0.2608 Acc: 0.9173                                               \n",
      "Epoch 052 | Train Loss: 0.2605 Acc: 0.9085 | Val Loss: 0.2824 Acc: 0.9060                                               \n",
      "Epoch 053 | Train Loss: 0.2338 Acc: 0.9190 | Val Loss: 0.2516 Acc: 0.9182                                               \n",
      "Epoch 054 | Train Loss: 0.2197 Acc: 0.9242 | Val Loss: 0.2876 Acc: 0.9069                                               \n",
      "Epoch 055 | Train Loss: 0.2387 Acc: 0.9201 | Val Loss: 0.2566 Acc: 0.9161                                               \n",
      "Epoch 056 | Train Loss: 0.2221 Acc: 0.9234 | Val Loss: 0.2450 Acc: 0.9236                                               \n",
      "Epoch 057 | Train Loss: 0.2125 Acc: 0.9271 | Val Loss: 0.2481 Acc: 0.9215                                               \n",
      "Epoch 058 | Train Loss: 0.2020 Acc: 0.9281 | Val Loss: 0.2420 Acc: 0.9209                                               \n",
      "Epoch 059 | Train Loss: 0.1971 Acc: 0.9313 | Val Loss: 0.2678 Acc: 0.9125                                               \n",
      "Epoch 060 | Train Loss: 0.2501 Acc: 0.9108 | Val Loss: 0.2848 Acc: 0.9125                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 128, 'cnn_dropout': 0.12000445300761385, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 16, 'cnn_kernels_2': 96, 'learning_rate': 0.00011298094479701163, 'lstm_dense': 32, 'lstm_hidden_size': 64, 'lstm_layers': 4, 'optimizer': 'adam'}\n",
      "Epoch 001 | Train Loss: 43.0336 Acc: 0.3606 | Val Loss: 5.1148 Acc: 0.4830                                              \n",
      "Epoch 002 | Train Loss: 12.6530 Acc: 0.3947 | Val Loss: 2.1826 Acc: 0.5134                                              \n",
      "Epoch 003 | Train Loss: 8.2466 Acc: 0.4179 | Val Loss: 1.9159 Acc: 0.4475                                               \n",
      "Epoch 004 | Train Loss: 6.4944 Acc: 0.4284 | Val Loss: 2.1164 Acc: 0.4472                                               \n",
      "Epoch 005 | Train Loss: 5.0106 Acc: 0.4455 | Val Loss: 1.3099 Acc: 0.5657                                               \n",
      "Epoch 006 | Train Loss: 4.0401 Acc: 0.4750 | Val Loss: 1.2169 Acc: 0.5860                                               \n",
      "Epoch 007 | Train Loss: 3.5150 Acc: 0.4905 | Val Loss: 1.1566 Acc: 0.6260                                               \n",
      "Epoch 008 | Train Loss: 3.0340 Acc: 0.5102 | Val Loss: 1.2397 Acc: 0.5988                                               \n",
      "Epoch 009 | Train Loss: 2.6672 Acc: 0.5271 | Val Loss: 1.1696 Acc: 0.6230                                               \n",
      "Epoch 010 | Train Loss: 2.2727 Acc: 0.5523 | Val Loss: 0.9028 Acc: 0.6925                                               \n",
      "Epoch 011 | Train Loss: 1.9768 Acc: 0.5765 | Val Loss: 1.2840 Acc: 0.6322                                               \n",
      "Epoch 012 | Train Loss: 1.8263 Acc: 0.5891 | Val Loss: 0.8150 Acc: 0.7233                                               \n",
      "Epoch 013 | Train Loss: 1.6367 Acc: 0.6206 | Val Loss: 0.8619 Acc: 0.7394                                               \n",
      "Epoch 014 | Train Loss: 1.4104 Acc: 0.6536 | Val Loss: 0.7608 Acc: 0.7561                                               \n",
      "Epoch 015 | Train Loss: 1.2425 Acc: 0.6856 | Val Loss: 0.6821 Acc: 0.7869                                               \n",
      "Epoch 016 | Train Loss: 1.1205 Acc: 0.7095 | Val Loss: 0.5661 Acc: 0.8099                                               \n",
      "Epoch 017 | Train Loss: 0.9869 Acc: 0.7361 | Val Loss: 0.7131 Acc: 0.7967                                               \n",
      "Epoch 018 | Train Loss: 0.8909 Acc: 0.7567 | Val Loss: 0.5541 Acc: 0.8442                                               \n",
      "Epoch 019 | Train Loss: 0.8306 Acc: 0.7775 | Val Loss: 0.4659 Acc: 0.8469                                               \n",
      "Epoch 020 | Train Loss: 0.7243 Acc: 0.7933 | Val Loss: 0.5274 Acc: 0.8316                                               \n",
      "Epoch 021 | Train Loss: 0.6777 Acc: 0.8092 | Val Loss: 0.3829 Acc: 0.8904                                               \n",
      "Epoch 022 | Train Loss: 0.6147 Acc: 0.8257 | Val Loss: 0.4474 Acc: 0.8618                                               \n",
      "Epoch 023 | Train Loss: 0.5964 Acc: 0.8293 | Val Loss: 0.5178 Acc: 0.8325                                               \n",
      "Epoch 024 | Train Loss: 0.5032 Acc: 0.8490 | Val Loss: 0.4561 Acc: 0.8490                                               \n",
      "Epoch 025 | Train Loss: 0.4809 Acc: 0.8587 | Val Loss: 0.3644 Acc: 0.8845                                               \n",
      "Epoch 026 | Train Loss: 0.4254 Acc: 0.8745 | Val Loss: 0.5214 Acc: 0.8618                                               \n",
      "Epoch 027 | Train Loss: 0.4149 Acc: 0.8828 | Val Loss: 0.3692 Acc: 0.8812                                               \n",
      "Epoch 028 | Train Loss: 0.3927 Acc: 0.8852 | Val Loss: 0.2992 Acc: 0.9099                                               \n",
      "Epoch 029 | Train Loss: 0.3795 Acc: 0.8888 | Val Loss: 0.2947 Acc: 0.9110                                               \n",
      "Epoch 030 | Train Loss: 0.3322 Acc: 0.8978 | Val Loss: 0.4526 Acc: 0.8755                                               \n",
      "Epoch 031 | Train Loss: 0.3237 Acc: 0.9048 | Val Loss: 0.3140 Acc: 0.9143                                               \n",
      "Epoch 032 | Train Loss: 0.2710 Acc: 0.9197 | Val Loss: 0.3248 Acc: 0.9048                                               \n",
      "Epoch 033 | Train Loss: 0.2686 Acc: 0.9208 | Val Loss: 0.3745 Acc: 0.8899                                               \n",
      "Epoch 034 | Train Loss: 0.2542 Acc: 0.9263 | Val Loss: 0.3726 Acc: 0.8967                                               \n",
      "Epoch 035 | Train Loss: 0.2639 Acc: 0.9225 | Val Loss: 0.2593 Acc: 0.9340                                               \n",
      "Epoch 036 | Train Loss: 0.2263 Acc: 0.9313 | Val Loss: 0.2559 Acc: 0.9236                                               \n",
      "Epoch 037 | Train Loss: 0.2036 Acc: 0.9373 | Val Loss: 0.3231 Acc: 0.9009                                               \n",
      "Epoch 038 | Train Loss: 0.2174 Acc: 0.9351 | Val Loss: 0.2797 Acc: 0.9206                                               \n",
      "Epoch 039 | Train Loss: 0.1972 Acc: 0.9451 | Val Loss: 0.2392 Acc: 0.9334                                               \n",
      "Epoch 040 | Train Loss: 0.1823 Acc: 0.9491 | Val Loss: 0.2612 Acc: 0.9260                                               \n",
      "Epoch 041 | Train Loss: 0.1748 Acc: 0.9511 | Val Loss: 0.2801 Acc: 0.9230                                               \n",
      "Epoch 042 | Train Loss: 0.1749 Acc: 0.9479 | Val Loss: 0.2382 Acc: 0.9397                                               \n",
      "Epoch 043 | Train Loss: 0.1692 Acc: 0.9513 | Val Loss: 0.2491 Acc: 0.9448                                               \n",
      "Epoch 044 | Train Loss: 0.1606 Acc: 0.9529 | Val Loss: 0.2647 Acc: 0.9236                                               \n",
      "Epoch 045 | Train Loss: 0.1381 Acc: 0.9593 | Val Loss: 0.2835 Acc: 0.9197                                               \n",
      "Epoch 046 | Train Loss: 0.1424 Acc: 0.9595 | Val Loss: 0.2461 Acc: 0.9373                                               \n",
      "Epoch 047 | Train Loss: 0.1370 Acc: 0.9661 | Val Loss: 0.3226 Acc: 0.9084                                               \n",
      "Epoch 048 | Train Loss: 0.1349 Acc: 0.9648 | Val Loss: 0.2457 Acc: 0.9322                                               \n",
      "Epoch 049 | Train Loss: 0.1259 Acc: 0.9633 | Val Loss: 0.2146 Acc: 0.9603                                               \n",
      "Epoch 050 | Train Loss: 0.1291 Acc: 0.9654 | Val Loss: 0.2854 Acc: 0.9263                                               \n",
      "Epoch 051 | Train Loss: 0.1007 Acc: 0.9718 | Val Loss: 0.2675 Acc: 0.9236                                               \n",
      "Epoch 052 | Train Loss: 0.1148 Acc: 0.9702 | Val Loss: 0.2485 Acc: 0.9409                                               \n",
      "Epoch 053 | Train Loss: 0.0911 Acc: 0.9754 | Val Loss: 0.2074 Acc: 0.9609                                               \n",
      "Epoch 054 | Train Loss: 0.0892 Acc: 0.9762 | Val Loss: 0.3711 Acc: 0.9152                                               \n",
      "Epoch 055 | Train Loss: 0.0727 Acc: 0.9819 | Val Loss: 0.3917 Acc: 0.8943                                               \n",
      "Epoch 056 | Train Loss: 0.1068 Acc: 0.9702 | Val Loss: 0.2635 Acc: 0.9197                                               \n",
      "Epoch 057 | Train Loss: 0.0753 Acc: 0.9807 | Val Loss: 0.2469 Acc: 0.9388                                               \n",
      "Epoch 058 | Train Loss: 0.0855 Acc: 0.9746 | Val Loss: 0.2427 Acc: 0.9424                                               \n",
      "Epoch 059 | Train Loss: 0.0646 Acc: 0.9828 | Val Loss: 0.2073 Acc: 0.9576                                               \n",
      "Epoch 060 | Train Loss: 0.0606 Acc: 0.9843 | Val Loss: 0.2196 Acc: 0.9612                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 64, 'cnn_dropout': 0.5715132104729871, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 16, 'learning_rate': 8.095096994136718e-05, 'lstm_dense': 128, 'lstm_hidden_size': 64, 'lstm_layers': 3, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 6.2986 Acc: 0.4243 | Val Loss: 1.1403 Acc: 0.4367                                               \n",
      "Epoch 002 | Train Loss: 1.0855 Acc: 0.4847 | Val Loss: 1.0229 Acc: 0.5875                                               \n",
      "Epoch 003 | Train Loss: 1.0120 Acc: 0.4828 | Val Loss: 0.9244 Acc: 0.4955                                               \n",
      "Epoch 004 | Train Loss: 0.9545 Acc: 0.5115 | Val Loss: 0.8635 Acc: 0.5472                                               \n",
      "Epoch 005 | Train Loss: 0.9361 Acc: 0.5269 | Val Loss: 0.8468 Acc: 0.5776                                               \n",
      "Epoch 006 | Train Loss: 0.9030 Acc: 0.5477 | Val Loss: 0.8290 Acc: 0.6403                                               \n",
      "Epoch 007 | Train Loss: 0.8839 Acc: 0.5483 | Val Loss: 0.8148 Acc: 0.6015                                               \n",
      "Epoch 008 | Train Loss: 0.8779 Acc: 0.5490 | Val Loss: 0.8082 Acc: 0.5955                                               \n",
      "Epoch 009 | Train Loss: 0.8413 Acc: 0.5633 | Val Loss: 0.7842 Acc: 0.5899                                               \n",
      "Epoch 010 | Train Loss: 0.8353 Acc: 0.5793 | Val Loss: 0.7632 Acc: 0.6358                                               \n",
      "Epoch 011 | Train Loss: 0.8292 Acc: 0.5768 | Val Loss: 0.7552 Acc: 0.6385                                               \n",
      "Epoch 012 | Train Loss: 0.8242 Acc: 0.5852 | Val Loss: 0.7412 Acc: 0.5833                                               \n",
      "Epoch 013 | Train Loss: 0.8188 Acc: 0.5846 | Val Loss: 0.7467 Acc: 0.6078                                               \n",
      "Epoch 014 | Train Loss: 0.8126 Acc: 0.5900 | Val Loss: 0.7350 Acc: 0.6376                                               \n",
      "Epoch 015 | Train Loss: 0.8027 Acc: 0.5941 | Val Loss: 0.7240 Acc: 0.6430                                               \n",
      "Epoch 016 | Train Loss: 0.7993 Acc: 0.5974 | Val Loss: 0.7242 Acc: 0.6678                                               \n",
      "Epoch 017 | Train Loss: 0.7980 Acc: 0.5903 | Val Loss: 0.7191 Acc: 0.6370                                               \n",
      "Epoch 018 | Train Loss: 0.7946 Acc: 0.5987 | Val Loss: 0.7193 Acc: 0.6487                                               \n",
      "Epoch 019 | Train Loss: 0.7901 Acc: 0.6041 | Val Loss: 0.7081 Acc: 0.6666                                               \n",
      "Epoch 020 | Train Loss: 0.7899 Acc: 0.6031 | Val Loss: 0.7269 Acc: 0.6364                                               \n",
      "Epoch 021 | Train Loss: 0.7919 Acc: 0.6047 | Val Loss: 0.7120 Acc: 0.6585                                               \n",
      "Epoch 022 | Train Loss: 0.7870 Acc: 0.6078 | Val Loss: 0.7499 Acc: 0.5907                                               \n",
      "Epoch 023 | Train Loss: 0.7792 Acc: 0.6155 | Val Loss: 0.6959 Acc: 0.6657                                               \n",
      "Epoch 024 | Train Loss: 0.7707 Acc: 0.6153 | Val Loss: 0.6833 Acc: 0.6755                                               \n",
      "Epoch 025 | Train Loss: 0.7725 Acc: 0.6285 | Val Loss: 0.6793 Acc: 0.7015                                               \n",
      "Epoch 026 | Train Loss: 0.7682 Acc: 0.6166 | Val Loss: 0.6802 Acc: 0.6660                                               \n",
      "Epoch 027 | Train Loss: 0.7616 Acc: 0.6258 | Val Loss: 0.6942 Acc: 0.6713                                               \n",
      "Epoch 028 | Train Loss: 0.7688 Acc: 0.6227 | Val Loss: 0.6689 Acc: 0.6881                                               \n",
      "Epoch 029 | Train Loss: 0.7636 Acc: 0.6280 | Val Loss: 0.6972 Acc: 0.7107                                               \n",
      "Epoch 030 | Train Loss: 0.7658 Acc: 0.6249 | Val Loss: 0.6755 Acc: 0.7033                                               \n",
      "Epoch 031 | Train Loss: 0.7618 Acc: 0.6312 | Val Loss: 0.6751 Acc: 0.6875                                               \n",
      "Epoch 032 | Train Loss: 0.7574 Acc: 0.6325 | Val Loss: 0.6748 Acc: 0.7051                                               \n",
      "Epoch 033 | Train Loss: 0.7485 Acc: 0.6332 | Val Loss: 0.6938 Acc: 0.7179                                               \n",
      "Epoch 034 | Train Loss: 0.7530 Acc: 0.6328 | Val Loss: 0.6475 Acc: 0.7087                                               \n",
      "Epoch 035 | Train Loss: 0.7534 Acc: 0.6297 | Val Loss: 0.6770 Acc: 0.6901                                               \n",
      "Epoch 036 | Train Loss: 0.7459 Acc: 0.6416 | Val Loss: 0.6583 Acc: 0.6943                                               \n",
      "Epoch 037 | Train Loss: 0.7502 Acc: 0.6371 | Val Loss: 0.6631 Acc: 0.7161                                               \n",
      "Epoch 038 | Train Loss: 0.7502 Acc: 0.6368 | Val Loss: 0.6626 Acc: 0.6943                                               \n",
      "Epoch 039 | Train Loss: 0.7472 Acc: 0.6389 | Val Loss: 0.6415 Acc: 0.6899                                               \n",
      "Epoch 040 | Train Loss: 0.7360 Acc: 0.6385 | Val Loss: 0.6426 Acc: 0.7075                                               \n",
      "Epoch 041 | Train Loss: 0.7361 Acc: 0.6461 | Val Loss: 0.6475 Acc: 0.7248                                               \n",
      "Epoch 042 | Train Loss: 0.7390 Acc: 0.6444 | Val Loss: 0.6737 Acc: 0.7057                                               \n",
      "Epoch 043 | Train Loss: 0.7390 Acc: 0.6530 | Val Loss: 0.6593 Acc: 0.6934                                               \n",
      "Epoch 044 | Train Loss: 0.7503 Acc: 0.6388 | Val Loss: 0.6414 Acc: 0.7236                                               \n",
      "Epoch 045 | Train Loss: 0.7348 Acc: 0.6491 | Val Loss: 0.6469 Acc: 0.7463                                               \n",
      "Epoch 046 | Train Loss: 0.7348 Acc: 0.6492 | Val Loss: 0.6386 Acc: 0.7188                                               \n",
      "Epoch 047 | Train Loss: 0.7368 Acc: 0.6515 | Val Loss: 0.6863 Acc: 0.6860                                               \n",
      "Epoch 048 | Train Loss: 0.7427 Acc: 0.6430 | Val Loss: 0.6391 Acc: 0.6907                                               \n",
      "Epoch 049 | Train Loss: 0.7389 Acc: 0.6406 | Val Loss: 0.6763 Acc: 0.6976                                               \n",
      "Epoch 050 | Train Loss: 0.7339 Acc: 0.6492 | Val Loss: 0.6373 Acc: 0.7245                                               \n",
      "Epoch 051 | Train Loss: 0.7310 Acc: 0.6477 | Val Loss: 0.6581 Acc: 0.7018                                               \n",
      "Epoch 052 | Train Loss: 0.7324 Acc: 0.6505 | Val Loss: 0.6360 Acc: 0.7024                                               \n",
      "Epoch 053 | Train Loss: 0.7307 Acc: 0.6510 | Val Loss: 0.6944 Acc: 0.6839                                               \n",
      "Epoch 054 | Train Loss: 0.7346 Acc: 0.6475 | Val Loss: 0.6298 Acc: 0.7182                                               \n",
      "Epoch 055 | Train Loss: 0.7226 Acc: 0.6527 | Val Loss: 0.6488 Acc: 0.7072                                               \n",
      "Epoch 056 | Train Loss: 0.7282 Acc: 0.6482 | Val Loss: 0.6210 Acc: 0.7197                                               \n",
      "Epoch 057 | Train Loss: 0.7310 Acc: 0.6495 | Val Loss: 0.6289 Acc: 0.6994                                               \n",
      "Epoch 058 | Train Loss: 0.7230 Acc: 0.6490 | Val Loss: 0.6305 Acc: 0.7063                                               \n",
      "Epoch 059 | Train Loss: 0.7164 Acc: 0.6569 | Val Loss: 0.6216 Acc: 0.7045                                               \n",
      "Epoch 060 | Train Loss: 0.7212 Acc: 0.6521 | Val Loss: 0.6542 Acc: 0.7104                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 32, 'cnn_dropout': 0.5623221837148552, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 32, 'cnn_kernels_2': 16, 'learning_rate': 0.0053992126234261185, 'lstm_dense': 256, 'lstm_hidden_size': 64, 'lstm_layers': 6, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 18.2340 Acc: 0.4451 | Val Loss: 1.9672 Acc: 0.4516                                              \n",
      "Epoch 002 | Train Loss: 1.0136 Acc: 0.5254 | Val Loss: 0.8502 Acc: 0.6603                                               \n",
      "Epoch 003 | Train Loss: 0.8756 Acc: 0.5797 | Val Loss: 1.1144 Acc: 0.5057                                               \n",
      "Epoch 004 | Train Loss: 0.8904 Acc: 0.5780 | Val Loss: 0.8733 Acc: 0.5287                                               \n",
      "Epoch 005 | Train Loss: 0.9072 Acc: 0.5438 | Val Loss: 0.8533 Acc: 0.5654                                               \n",
      "Epoch 006 | Train Loss: 0.9122 Acc: 0.5440 | Val Loss: 0.7607 Acc: 0.6009                                               \n",
      "Epoch 007 | Train Loss: 0.8986 Acc: 0.5458 | Val Loss: 0.8723 Acc: 0.5913                                               \n",
      "Epoch 008 | Train Loss: 0.9234 Acc: 0.5359 | Val Loss: 1.0969 Acc: 0.4743                                               \n",
      "Epoch 009 | Train Loss: 0.8881 Acc: 0.5430 | Val Loss: 0.9704 Acc: 0.4197                                               \n",
      "Epoch 010 | Train Loss: 0.9545 Acc: 0.5015 | Val Loss: 0.9181 Acc: 0.4848                                               \n",
      "Epoch 011 | Train Loss: 0.9842 Acc: 0.5015 | Val Loss: 0.8755 Acc: 0.5648                                               \n",
      "Epoch 012 | Train Loss: 0.9732 Acc: 0.5011 | Val Loss: 0.8878 Acc: 0.5567                                               \n",
      "Epoch 013 | Train Loss: 0.9998 Acc: 0.4699 | Val Loss: 1.7261 Acc: 0.4558                                               \n",
      "Epoch 014 | Train Loss: 1.0231 Acc: 0.4847 | Val Loss: 0.9005 Acc: 0.5287                                               \n",
      "Epoch 015 | Train Loss: 0.9536 Acc: 0.4873 | Val Loss: 1.2182 Acc: 0.4588                                               \n",
      "Epoch 016 | Train Loss: 0.9624 Acc: 0.4858 | Val Loss: 0.9125 Acc: 0.5242                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 64, 'cnn_dropout': 0.1233161718677419, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 16, 'cnn_kernels_2': 96, 'learning_rate': 0.0008060508340380918, 'lstm_dense': 64, 'lstm_hidden_size': 96, 'lstm_layers': 1, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 11.3900 Acc: 0.4435 | Val Loss: 1.2634 Acc: 0.4657                                              \n",
      "Epoch 002 | Train Loss: 1.2257 Acc: 0.4843 | Val Loss: 1.2391 Acc: 0.4415                                               \n",
      "Epoch 003 | Train Loss: 1.1972 Acc: 0.4794 | Val Loss: 1.1838 Acc: 0.4848                                               \n",
      "Epoch 004 | Train Loss: 1.1624 Acc: 0.4937 | Val Loss: 1.1601 Acc: 0.4851                                               \n",
      "Epoch 005 | Train Loss: 1.1405 Acc: 0.5060 | Val Loss: 1.1335 Acc: 0.5066                                               \n",
      "Epoch 006 | Train Loss: 1.1185 Acc: 0.5202 | Val Loss: 1.1254 Acc: 0.4997                                               \n",
      "Epoch 007 | Train Loss: 1.1025 Acc: 0.5318 | Val Loss: 1.0904 Acc: 0.5457                                               \n",
      "Epoch 008 | Train Loss: 1.0874 Acc: 0.5365 | Val Loss: 1.1180 Acc: 0.4937                                               \n",
      "Epoch 009 | Train Loss: 1.0649 Acc: 0.5453 | Val Loss: 1.0587 Acc: 0.5510                                               \n",
      "Epoch 010 | Train Loss: 1.0391 Acc: 0.5585 | Val Loss: 1.0315 Acc: 0.5630                                               \n",
      "Epoch 011 | Train Loss: 1.0143 Acc: 0.5730 | Val Loss: 1.0174 Acc: 0.5779                                               \n",
      "Epoch 012 | Train Loss: 0.9769 Acc: 0.5882 | Val Loss: 0.9548 Acc: 0.5928                                               \n",
      "Epoch 013 | Train Loss: 0.9392 Acc: 0.6168 | Val Loss: 0.9118 Acc: 0.6373                                               \n",
      "Epoch 014 | Train Loss: 0.8890 Acc: 0.6582 | Val Loss: 0.8867 Acc: 0.6576                                               \n",
      "Epoch 015 | Train Loss: 0.8255 Acc: 0.7027 | Val Loss: 0.7906 Acc: 0.7122                                               \n",
      "Epoch 016 | Train Loss: 0.7691 Acc: 0.7254 | Val Loss: 0.8256 Acc: 0.6845                                               \n",
      "Epoch 017 | Train Loss: 0.7246 Acc: 0.7388 | Val Loss: 0.7027 Acc: 0.7391                                               \n",
      "Epoch 018 | Train Loss: 0.6527 Acc: 0.7645 | Val Loss: 0.6389 Acc: 0.7734                                               \n",
      "Epoch 019 | Train Loss: 0.6071 Acc: 0.7839 | Val Loss: 0.6088 Acc: 0.7824                                               \n",
      "Epoch 020 | Train Loss: 0.5294 Acc: 0.8111 | Val Loss: 0.5230 Acc: 0.8164                                               \n",
      "Epoch 021 | Train Loss: 0.4853 Acc: 0.8252 | Val Loss: 0.5320 Acc: 0.8054                                               \n",
      "Epoch 022 | Train Loss: 0.4575 Acc: 0.8336 | Val Loss: 0.4371 Acc: 0.8379                                               \n",
      "Epoch 023 | Train Loss: 0.4396 Acc: 0.8419 | Val Loss: 0.4411 Acc: 0.8373                                               \n",
      "Epoch 024 | Train Loss: 0.4075 Acc: 0.8487 | Val Loss: 0.4188 Acc: 0.8439                                               \n",
      "Epoch 025 | Train Loss: 0.3714 Acc: 0.8617 | Val Loss: 0.3632 Acc: 0.8672                                               \n",
      "Epoch 026 | Train Loss: 0.3487 Acc: 0.8720 | Val Loss: 0.3803 Acc: 0.8585                                               \n",
      "Epoch 027 | Train Loss: 0.3384 Acc: 0.8780 | Val Loss: 0.3456 Acc: 0.8722                                               \n",
      "Epoch 028 | Train Loss: 0.2953 Acc: 0.8914 | Val Loss: 0.3955 Acc: 0.8561                                               \n",
      "Epoch 029 | Train Loss: 0.2846 Acc: 0.9003 | Val Loss: 0.3385 Acc: 0.8818                                               \n",
      "Epoch 030 | Train Loss: 0.2696 Acc: 0.9078 | Val Loss: 0.3398 Acc: 0.8881                                               \n",
      "Epoch 031 | Train Loss: 0.2508 Acc: 0.9184 | Val Loss: 0.3491 Acc: 0.8884                                               \n",
      "Epoch 032 | Train Loss: 0.2386 Acc: 0.9207 | Val Loss: 0.3726 Acc: 0.8776                                               \n",
      "Epoch 033 | Train Loss: 0.2087 Acc: 0.9288 | Val Loss: 0.3123 Acc: 0.9087                                               \n",
      "Epoch 034 | Train Loss: 0.2173 Acc: 0.9300 | Val Loss: 0.2881 Acc: 0.9110                                               \n",
      "Epoch 035 | Train Loss: 0.2032 Acc: 0.9354 | Val Loss: 0.3146 Acc: 0.9009                                               \n",
      "Epoch 036 | Train Loss: 0.2245 Acc: 0.9279 | Val Loss: 0.2502 Acc: 0.9200                                               \n",
      "Epoch 037 | Train Loss: 0.1808 Acc: 0.9442 | Val Loss: 0.2562 Acc: 0.9248                                               \n",
      "Epoch 038 | Train Loss: 0.1495 Acc: 0.9545 | Val Loss: 0.3117 Acc: 0.9081                                               \n",
      "Epoch 039 | Train Loss: 0.1557 Acc: 0.9531 | Val Loss: 0.2967 Acc: 0.9143                                               \n",
      "Epoch 040 | Train Loss: 0.1438 Acc: 0.9556 | Val Loss: 0.2394 Acc: 0.9304                                               \n",
      "Epoch 041 | Train Loss: 0.1346 Acc: 0.9587 | Val Loss: 0.2409 Acc: 0.9257                                               \n",
      "Epoch 042 | Train Loss: 0.1265 Acc: 0.9607 | Val Loss: 0.2374 Acc: 0.9299                                               \n",
      "Epoch 043 | Train Loss: 0.1051 Acc: 0.9686 | Val Loss: 0.2362 Acc: 0.9334                                               \n",
      "Epoch 044 | Train Loss: 0.1153 Acc: 0.9638 | Val Loss: 0.2080 Acc: 0.9439                                               \n",
      "Epoch 045 | Train Loss: 0.1169 Acc: 0.9629 | Val Loss: 0.2120 Acc: 0.9418                                               \n",
      "Epoch 046 | Train Loss: 0.0907 Acc: 0.9717 | Val Loss: 0.2112 Acc: 0.9406                                               \n",
      "Epoch 047 | Train Loss: 0.0849 Acc: 0.9752 | Val Loss: 0.2053 Acc: 0.9448                                               \n",
      "Epoch 048 | Train Loss: 0.0939 Acc: 0.9719 | Val Loss: 0.2412 Acc: 0.9373                                               \n",
      "Epoch 049 | Train Loss: 0.0769 Acc: 0.9770 | Val Loss: 0.1925 Acc: 0.9472                                               \n",
      "Epoch 050 | Train Loss: 0.0684 Acc: 0.9797 | Val Loss: 0.2265 Acc: 0.9418                                               \n",
      "Epoch 051 | Train Loss: 0.0923 Acc: 0.9725 | Val Loss: 0.2276 Acc: 0.9442                                               \n",
      "Epoch 052 | Train Loss: 0.0728 Acc: 0.9790 | Val Loss: 0.1841 Acc: 0.9513                                               \n",
      "Epoch 053 | Train Loss: 0.0744 Acc: 0.9787 | Val Loss: 0.1806 Acc: 0.9499                                               \n",
      "Epoch 054 | Train Loss: 0.0557 Acc: 0.9850 | Val Loss: 0.2014 Acc: 0.9487                                               \n",
      "Epoch 055 | Train Loss: 0.0837 Acc: 0.9757 | Val Loss: 0.2703 Acc: 0.9307                                               \n",
      "Epoch 056 | Train Loss: 0.0765 Acc: 0.9766 | Val Loss: 0.1832 Acc: 0.9573                                               \n",
      "Epoch 057 | Train Loss: 0.0500 Acc: 0.9869 | Val Loss: 0.1721 Acc: 0.9615                                               \n",
      "Epoch 058 | Train Loss: 0.0514 Acc: 0.9848 | Val Loss: 0.1951 Acc: 0.9537                                               \n",
      "Epoch 059 | Train Loss: 0.0469 Acc: 0.9877 | Val Loss: 0.2243 Acc: 0.9528                                               \n",
      "Epoch 060 | Train Loss: 0.0648 Acc: 0.9816 | Val Loss: 0.1983 Acc: 0.9507                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 32, 'cnn_dropout': 0.20926946523904152, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 64, 'cnn_kernels_2': 32, 'learning_rate': 1.1604056341997761e-05, 'lstm_dense': 128, 'lstm_hidden_size': 128, 'lstm_layers': 2, 'optimizer': 'sgd'}\n",
      "Epoch 001 | Train Loss: 6.6825 Acc: 0.4014 | Val Loss: 1.2716 Acc: 0.4576                                               \n",
      "Epoch 002 | Train Loss: 1.2805 Acc: 0.4417 | Val Loss: 1.0951 Acc: 0.4513                                               \n",
      "Epoch 003 | Train Loss: 1.1513 Acc: 0.4621 | Val Loss: 0.9953 Acc: 0.4839                                               \n",
      "Epoch 004 | Train Loss: 1.0844 Acc: 0.4692 | Val Loss: 0.9693 Acc: 0.5424                                               \n",
      "Epoch 005 | Train Loss: 1.0412 Acc: 0.4586 | Val Loss: 0.9613 Acc: 0.4779                                               \n",
      "Epoch 006 | Train Loss: 1.0173 Acc: 0.4606 | Val Loss: 0.9160 Acc: 0.5119                                               \n",
      "Epoch 007 | Train Loss: 0.9936 Acc: 0.4721 | Val Loss: 0.9012 Acc: 0.4866                                               \n",
      "Epoch 008 | Train Loss: 0.9723 Acc: 0.4684 | Val Loss: 0.9118 Acc: 0.4973                                               \n",
      "Epoch 009 | Train Loss: 0.9629 Acc: 0.4706 | Val Loss: 0.8888 Acc: 0.5325                                               \n",
      "Epoch 010 | Train Loss: 0.9526 Acc: 0.4777 | Val Loss: 0.8908 Acc: 0.4976                                               \n",
      "Epoch 011 | Train Loss: 0.9461 Acc: 0.4700 | Val Loss: 0.8716 Acc: 0.4827                                               \n",
      "Epoch 012 | Train Loss: 0.9428 Acc: 0.4731 | Val Loss: 0.8729 Acc: 0.4934                                               \n",
      "Epoch 013 | Train Loss: 0.9303 Acc: 0.4768 | Val Loss: 0.8540 Acc: 0.4806                                               \n",
      "Epoch 014 | Train Loss: 0.9271 Acc: 0.4815 | Val Loss: 0.8526 Acc: 0.4719                                               \n",
      "Epoch 015 | Train Loss: 0.9243 Acc: 0.4836 | Val Loss: 0.8597 Acc: 0.5424                                               \n",
      "Epoch 016 | Train Loss: 0.9090 Acc: 0.4940 | Val Loss: 0.8390 Acc: 0.5430                                               \n",
      "Epoch 017 | Train Loss: 0.9023 Acc: 0.4918 | Val Loss: 0.8330 Acc: 0.5749                                               \n",
      "Epoch 018 | Train Loss: 0.9049 Acc: 0.4965 | Val Loss: 0.8347 Acc: 0.5442                                               \n",
      "Epoch 019 | Train Loss: 0.8987 Acc: 0.5094 | Val Loss: 0.8323 Acc: 0.5567                                               \n",
      "Epoch 020 | Train Loss: 0.8980 Acc: 0.5064 | Val Loss: 0.8235 Acc: 0.5215                                               \n",
      "Epoch 021 | Train Loss: 0.8893 Acc: 0.5069 | Val Loss: 0.8060 Acc: 0.5543                                               \n",
      "Epoch 022 | Train Loss: 0.8943 Acc: 0.5029 | Val Loss: 0.8132 Acc: 0.5591                                               \n",
      "Epoch 023 | Train Loss: 0.8852 Acc: 0.5115 | Val Loss: 0.8182 Acc: 0.5454                                               \n",
      "Epoch 024 | Train Loss: 0.8850 Acc: 0.5113 | Val Loss: 0.8334 Acc: 0.5788                                               \n",
      "Epoch 025 | Train Loss: 0.8880 Acc: 0.5141 | Val Loss: 0.8350 Acc: 0.5293                                               \n",
      "Epoch 026 | Train Loss: 0.8813 Acc: 0.5167 | Val Loss: 0.8150 Acc: 0.5394                                               \n",
      "Epoch 027 | Train Loss: 0.8709 Acc: 0.5248 | Val Loss: 0.7990 Acc: 0.5585                                               \n",
      "Epoch 028 | Train Loss: 0.8700 Acc: 0.5324 | Val Loss: 0.7839 Acc: 0.5991                                               \n",
      "Epoch 029 | Train Loss: 0.8703 Acc: 0.5291 | Val Loss: 0.7959 Acc: 0.6048                                               \n",
      "Epoch 030 | Train Loss: 0.8618 Acc: 0.5342 | Val Loss: 0.7860 Acc: 0.6176                                               \n",
      "Epoch 031 | Train Loss: 0.8706 Acc: 0.5231 | Val Loss: 0.8034 Acc: 0.5546                                               \n",
      "Epoch 032 | Train Loss: 0.8610 Acc: 0.5326 | Val Loss: 0.7923 Acc: 0.5758                                               \n",
      "Epoch 033 | Train Loss: 0.8611 Acc: 0.5372 | Val Loss: 0.7884 Acc: 0.5681                                               \n",
      "Epoch 034 | Train Loss: 0.8487 Acc: 0.5440 | Val Loss: 0.7980 Acc: 0.5633                                               \n",
      "Epoch 035 | Train Loss: 0.8555 Acc: 0.5351 | Val Loss: 0.7874 Acc: 0.5845                                               \n",
      "Epoch 036 | Train Loss: 0.8488 Acc: 0.5453 | Val Loss: 0.7624 Acc: 0.6155                                               \n",
      "Epoch 037 | Train Loss: 0.8473 Acc: 0.5459 | Val Loss: 0.7741 Acc: 0.6099                                               \n",
      "Epoch 038 | Train Loss: 0.8511 Acc: 0.5468 | Val Loss: 0.7669 Acc: 0.5627                                               \n",
      "Epoch 039 | Train Loss: 0.8403 Acc: 0.5468 | Val Loss: 0.7910 Acc: 0.5893                                               \n",
      "Epoch 040 | Train Loss: 0.8439 Acc: 0.5491 | Val Loss: 0.7567 Acc: 0.5919                                               \n",
      "Epoch 041 | Train Loss: 0.8356 Acc: 0.5534 | Val Loss: 0.7819 Acc: 0.5540                                               \n",
      "Epoch 042 | Train Loss: 0.8404 Acc: 0.5531 | Val Loss: 0.7589 Acc: 0.6045                                               \n",
      "Epoch 043 | Train Loss: 0.8310 Acc: 0.5559 | Val Loss: 0.7567 Acc: 0.5952                                               \n",
      "Epoch 044 | Train Loss: 0.8320 Acc: 0.5538 | Val Loss: 0.7502 Acc: 0.6287                                               \n",
      "Epoch 045 | Train Loss: 0.8344 Acc: 0.5562 | Val Loss: 0.7593 Acc: 0.6322                                               \n",
      "Epoch 046 | Train Loss: 0.8334 Acc: 0.5598 | Val Loss: 0.7639 Acc: 0.5794                                               \n",
      "Epoch 047 | Train Loss: 0.8318 Acc: 0.5589 | Val Loss: 0.7846 Acc: 0.6251                                               \n",
      "Epoch 048 | Train Loss: 0.8275 Acc: 0.5603 | Val Loss: 0.7476 Acc: 0.6355                                               \n",
      "Epoch 049 | Train Loss: 0.8229 Acc: 0.5610 | Val Loss: 0.7376 Acc: 0.6209                                               \n",
      "Epoch 050 | Train Loss: 0.8185 Acc: 0.5625 | Val Loss: 0.7667 Acc: 0.5493                                               \n",
      "Epoch 051 | Train Loss: 0.8205 Acc: 0.5635 | Val Loss: 0.7540 Acc: 0.6230                                               \n",
      "Epoch 052 | Train Loss: 0.8211 Acc: 0.5641 | Val Loss: 0.7599 Acc: 0.5958                                               \n",
      "Epoch 053 | Train Loss: 0.8149 Acc: 0.5653 | Val Loss: 0.7551 Acc: 0.5627                                               \n",
      "Epoch 054 | Train Loss: 0.8200 Acc: 0.5638 | Val Loss: 0.7359 Acc: 0.6009                                               \n",
      "Epoch 055 | Train Loss: 0.8168 Acc: 0.5631 | Val Loss: 0.7389 Acc: 0.6173                                               \n",
      "Epoch 056 | Train Loss: 0.8116 Acc: 0.5655 | Val Loss: 0.7366 Acc: 0.6045                                               \n",
      "Epoch 057 | Train Loss: 0.8116 Acc: 0.5638 | Val Loss: 0.7471 Acc: 0.5815                                               \n",
      "Epoch 058 | Train Loss: 0.8120 Acc: 0.5650 | Val Loss: 0.7304 Acc: 0.6537                                               \n",
      "Epoch 059 | Train Loss: 0.8106 Acc: 0.5653 | Val Loss: 0.7340 Acc: 0.6006                                               \n",
      "Epoch 060 | Train Loss: 0.8126 Acc: 0.5676 | Val Loss: 0.7272 Acc: 0.5693                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 80, 'cnn_dense': 128, 'cnn_dropout': 0.0003285701467588331, 'cnn_kernel_size_1': 3, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 16, 'cnn_kernels_2': 64, 'learning_rate': 2.9163494469250913e-05, 'lstm_dense': 256, 'lstm_hidden_size': 64, 'lstm_layers': 2, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 53.1345 Acc: 0.3204 | Val Loss: 14.2861 Acc: 0.4200                                             \n",
      "Epoch 002 | Train Loss: 34.0400 Acc: 0.3752 | Val Loss: 10.7924 Acc: 0.4690                                             \n",
      "Epoch 003 | Train Loss: 24.5711 Acc: 0.3756 | Val Loss: 9.9796 Acc: 0.4475                                              \n",
      "Epoch 004 | Train Loss: 19.1274 Acc: 0.3868 | Val Loss: 8.3039 Acc: 0.4546                                              \n",
      "Epoch 005 | Train Loss: 15.4461 Acc: 0.3953 | Val Loss: 6.7181 Acc: 0.4272                                              \n",
      "Epoch 006 | Train Loss: 12.4488 Acc: 0.4164 | Val Loss: 5.2730 Acc: 0.5140                                              \n",
      "Epoch 007 | Train Loss: 10.7707 Acc: 0.4259 | Val Loss: 4.3652 Acc: 0.4764                                              \n",
      "Epoch 008 | Train Loss: 9.7233 Acc: 0.4343 | Val Loss: 3.3739 Acc: 0.4913                                               \n",
      "Epoch 009 | Train Loss: 8.3957 Acc: 0.4370 | Val Loss: 2.6503 Acc: 0.5045                                               \n",
      "Epoch 010 | Train Loss: 7.4508 Acc: 0.4529 | Val Loss: 2.5074 Acc: 0.5388                                               \n",
      "Epoch 011 | Train Loss: 6.7436 Acc: 0.4544 | Val Loss: 1.8898 Acc: 0.5669                                               \n",
      "Epoch 012 | Train Loss: 6.0933 Acc: 0.4699 | Val Loss: 1.8468 Acc: 0.6116                                               \n",
      "Epoch 013 | Train Loss: 5.4697 Acc: 0.4813 | Val Loss: 1.5473 Acc: 0.6158                                               \n",
      "Epoch 014 | Train Loss: 5.0037 Acc: 0.4904 | Val Loss: 1.3983 Acc: 0.5901                                               \n",
      "Epoch 015 | Train Loss: 4.5697 Acc: 0.4954 | Val Loss: 1.3682 Acc: 0.6415                                               \n",
      "Epoch 016 | Train Loss: 4.0954 Acc: 0.5115 | Val Loss: 1.1179 Acc: 0.6299                                               \n",
      "Epoch 017 | Train Loss: 3.8798 Acc: 0.5192 | Val Loss: 1.1724 Acc: 0.6463                                               \n",
      "Epoch 018 | Train Loss: 3.6576 Acc: 0.5255 | Val Loss: 1.0978 Acc: 0.6722                                               \n",
      "Epoch 019 | Train Loss: 3.4668 Acc: 0.5361 | Val Loss: 0.9657 Acc: 0.6827                                               \n",
      "Epoch 020 | Train Loss: 3.1968 Acc: 0.5361 | Val Loss: 0.9383 Acc: 0.7107                                               \n",
      "Epoch 021 | Train Loss: 3.0341 Acc: 0.5474 | Val Loss: 1.0731 Acc: 0.6442                                               \n",
      "Epoch 022 | Train Loss: 2.8491 Acc: 0.5573 | Val Loss: 0.9677 Acc: 0.6519                                               \n",
      "Epoch 023 | Train Loss: 2.7527 Acc: 0.5588 | Val Loss: 1.0336 Acc: 0.6275                                               \n",
      "Epoch 024 | Train Loss: 2.5111 Acc: 0.5711 | Val Loss: 0.9427 Acc: 0.7051                                               \n",
      "Epoch 025 | Train Loss: 2.3428 Acc: 0.5905 | Val Loss: 0.8451 Acc: 0.7176                                               \n",
      "Epoch 026 | Train Loss: 2.3136 Acc: 0.5891 | Val Loss: 0.7680 Acc: 0.7215                                               \n",
      "Epoch 027 | Train Loss: 2.2092 Acc: 0.5906 | Val Loss: 0.8193 Acc: 0.6919                                               \n",
      "Epoch 028 | Train Loss: 1.9897 Acc: 0.6112 | Val Loss: 0.7214 Acc: 0.7430                                               \n",
      "Epoch 029 | Train Loss: 1.9782 Acc: 0.6167 | Val Loss: 0.8158 Acc: 0.6803                                               \n",
      "Epoch 030 | Train Loss: 1.8240 Acc: 0.6218 | Val Loss: 0.8591 Acc: 0.6851                                               \n",
      "Epoch 031 | Train Loss: 1.7717 Acc: 0.6234 | Val Loss: 0.7131 Acc: 0.7343                                               \n",
      "Epoch 032 | Train Loss: 1.7823 Acc: 0.6250 | Val Loss: 1.1396 Acc: 0.6666                                               \n",
      "Epoch 033 | Train Loss: 1.6593 Acc: 0.6424 | Val Loss: 0.7850 Acc: 0.7152                                               \n",
      "Epoch 034 | Train Loss: 1.5068 Acc: 0.6567 | Val Loss: 0.6454 Acc: 0.7516                                               \n",
      "Epoch 035 | Train Loss: 1.5154 Acc: 0.6492 | Val Loss: 1.0352 Acc: 0.6913                                               \n",
      "Epoch 036 | Train Loss: 1.4723 Acc: 0.6575 | Val Loss: 0.6119 Acc: 0.7669                                               \n",
      "Epoch 037 | Train Loss: 1.4122 Acc: 0.6668 | Val Loss: 0.6120 Acc: 0.7672                                               \n",
      "Epoch 038 | Train Loss: 1.3233 Acc: 0.6724 | Val Loss: 0.8960 Acc: 0.6675                                               \n",
      "Epoch 039 | Train Loss: 1.2905 Acc: 0.6774 | Val Loss: 0.6370 Acc: 0.7531                                               \n",
      "Epoch 040 | Train Loss: 1.2086 Acc: 0.6845 | Val Loss: 0.7741 Acc: 0.7254                                               \n",
      "Epoch 041 | Train Loss: 1.2086 Acc: 0.6914 | Val Loss: 0.6334 Acc: 0.7603                                               \n",
      "Epoch 042 | Train Loss: 1.1294 Acc: 0.7011 | Val Loss: 0.5632 Acc: 0.7857                                               \n",
      "Epoch 043 | Train Loss: 1.1030 Acc: 0.7062 | Val Loss: 0.5450 Acc: 0.7940                                               \n",
      "Epoch 044 | Train Loss: 1.0406 Acc: 0.7161 | Val Loss: 0.5307 Acc: 0.7973                                               \n",
      "Epoch 045 | Train Loss: 1.0227 Acc: 0.7176 | Val Loss: 0.5285 Acc: 0.7994                                               \n",
      "Epoch 046 | Train Loss: 0.9596 Acc: 0.7245 | Val Loss: 0.5281 Acc: 0.8051                                               \n",
      "Epoch 047 | Train Loss: 0.9407 Acc: 0.7294 | Val Loss: 0.6566 Acc: 0.7687                                               \n",
      "Epoch 048 | Train Loss: 0.9228 Acc: 0.7418 | Val Loss: 0.7484 Acc: 0.7442                                               \n",
      "Epoch 049 | Train Loss: 0.8932 Acc: 0.7439 | Val Loss: 0.6743 Acc: 0.7704                                               \n",
      "Epoch 050 | Train Loss: 0.8897 Acc: 0.7440 | Val Loss: 0.5504 Acc: 0.8075                                               \n",
      "Epoch 051 | Train Loss: 0.8267 Acc: 0.7565 | Val Loss: 0.5174 Acc: 0.8233                                               \n",
      "Epoch 052 | Train Loss: 0.8030 Acc: 0.7627 | Val Loss: 0.6332 Acc: 0.7672                                               \n",
      "Epoch 053 | Train Loss: 0.8004 Acc: 0.7633 | Val Loss: 0.4769 Acc: 0.8284                                               \n",
      "Epoch 054 | Train Loss: 0.7620 Acc: 0.7705 | Val Loss: 0.5218 Acc: 0.8042                                               \n",
      "Epoch 055 | Train Loss: 0.7258 Acc: 0.7830 | Val Loss: 0.4727 Acc: 0.8388                                               \n",
      "Epoch 056 | Train Loss: 0.7072 Acc: 0.7795 | Val Loss: 0.6336 Acc: 0.8003                                               \n",
      "Epoch 057 | Train Loss: 0.7074 Acc: 0.7836 | Val Loss: 0.4347 Acc: 0.8519                                               \n",
      "Epoch 058 | Train Loss: 0.6839 Acc: 0.7954 | Val Loss: 0.5486 Acc: 0.8116                                               \n",
      "Epoch 059 | Train Loss: 0.6666 Acc: 0.7925 | Val Loss: 0.4718 Acc: 0.8328                                               \n",
      "Epoch 060 | Train Loss: 0.6365 Acc: 0.8020 | Val Loss: 0.4242 Acc: 0.8507                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 128, 'cnn_dropout': 0.2557507722181842, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 48, 'cnn_kernels_2': 16, 'learning_rate': 0.00021437124013032343, 'lstm_dense': 64, 'lstm_hidden_size': 128, 'lstm_layers': 3, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 16.7902 Acc: 0.3968 | Val Loss: 3.3078 Acc: 0.4982                                              \n",
      "Epoch 002 | Train Loss: 6.7530 Acc: 0.4182 | Val Loss: 2.6101 Acc: 0.5009                                               \n",
      "Epoch 003 | Train Loss: 4.1009 Acc: 0.4466 | Val Loss: 3.4994 Acc: 0.5681                                               \n",
      "Epoch 004 | Train Loss: 2.8760 Acc: 0.4791 | Val Loss: 2.7290 Acc: 0.4875                                               \n",
      "Epoch 005 | Train Loss: 2.1596 Acc: 0.5116 | Val Loss: 1.6095 Acc: 0.5290                                               \n",
      "Epoch 006 | Train Loss: 1.6619 Acc: 0.5557 | Val Loss: 2.4034 Acc: 0.5839                                               \n",
      "Epoch 007 | Train Loss: 1.3186 Acc: 0.5999 | Val Loss: 1.0950 Acc: 0.6943                                               \n",
      "Epoch 008 | Train Loss: 1.0906 Acc: 0.6430 | Val Loss: 0.8261 Acc: 0.6334                                               \n",
      "Epoch 009 | Train Loss: 0.8997 Acc: 0.6892 | Val Loss: 0.7260 Acc: 0.7194                                               \n",
      "Epoch 010 | Train Loss: 0.7253 Acc: 0.7439 | Val Loss: 0.7370 Acc: 0.7588                                               \n",
      "Epoch 011 | Train Loss: 0.6115 Acc: 0.7820 | Val Loss: 0.5144 Acc: 0.8015                                               \n",
      "Epoch 012 | Train Loss: 0.5341 Acc: 0.8109 | Val Loss: 0.4275 Acc: 0.8582                                               \n",
      "Epoch 013 | Train Loss: 0.4532 Acc: 0.8400 | Val Loss: 0.3441 Acc: 0.8776                                               \n",
      "Epoch 014 | Train Loss: 0.3883 Acc: 0.8628 | Val Loss: 0.3115 Acc: 0.8964                                               \n",
      "Epoch 015 | Train Loss: 0.3356 Acc: 0.8815 | Val Loss: 0.3180 Acc: 0.8893                                               \n",
      "Epoch 016 | Train Loss: 0.2902 Acc: 0.8978 | Val Loss: 0.2295 Acc: 0.9179                                               \n",
      "Epoch 017 | Train Loss: 0.2660 Acc: 0.9095 | Val Loss: 0.2523 Acc: 0.9087                                               \n",
      "Epoch 018 | Train Loss: 0.2245 Acc: 0.9251 | Val Loss: 0.1931 Acc: 0.9301                                               \n",
      "Epoch 019 | Train Loss: 0.1993 Acc: 0.9316 | Val Loss: 0.2555 Acc: 0.9093                                               \n",
      "Epoch 020 | Train Loss: 0.1750 Acc: 0.9417 | Val Loss: 0.1288 Acc: 0.9528                                               \n",
      "Epoch 021 | Train Loss: 0.1503 Acc: 0.9503 | Val Loss: 0.1834 Acc: 0.9397                                               \n",
      "Epoch 022 | Train Loss: 0.1342 Acc: 0.9568 | Val Loss: 0.1838 Acc: 0.9460                                               \n",
      "Epoch 023 | Train Loss: 0.1242 Acc: 0.9602 | Val Loss: 0.3042 Acc: 0.8991                                               \n",
      "Epoch 024 | Train Loss: 0.1131 Acc: 0.9625 | Val Loss: 0.1751 Acc: 0.9436                                               \n",
      "Epoch 025 | Train Loss: 0.1025 Acc: 0.9677 | Val Loss: 0.1551 Acc: 0.9472                                               \n",
      "Epoch 026 | Train Loss: 0.0962 Acc: 0.9701 | Val Loss: 0.1598 Acc: 0.9466                                               \n",
      "Epoch 027 | Train Loss: 0.0933 Acc: 0.9695 | Val Loss: 0.2149 Acc: 0.9331                                               \n",
      "Epoch 028 | Train Loss: 0.0801 Acc: 0.9746 | Val Loss: 0.1042 Acc: 0.9696                                               \n",
      "Epoch 029 | Train Loss: 0.0756 Acc: 0.9742 | Val Loss: 0.2294 Acc: 0.9322                                               \n",
      "Epoch 030 | Train Loss: 0.0744 Acc: 0.9759 | Val Loss: 0.2114 Acc: 0.9293                                               \n",
      "Epoch 031 | Train Loss: 0.0547 Acc: 0.9837 | Val Loss: 0.1418 Acc: 0.9525                                               \n",
      "Epoch 032 | Train Loss: 0.0559 Acc: 0.9823 | Val Loss: 0.0836 Acc: 0.9749                                               \n",
      "Epoch 033 | Train Loss: 0.0536 Acc: 0.9817 | Val Loss: 0.0703 Acc: 0.9779                                               \n",
      "Epoch 034 | Train Loss: 0.0518 Acc: 0.9834 | Val Loss: 0.1236 Acc: 0.9633                                               \n",
      "Epoch 035 | Train Loss: 0.0441 Acc: 0.9866 | Val Loss: 0.1194 Acc: 0.9651                                               \n",
      "Epoch 036 | Train Loss: 0.0443 Acc: 0.9868 | Val Loss: 1.0299 Acc: 0.7919                                               \n",
      "Epoch 037 | Train Loss: 0.0479 Acc: 0.9848 | Val Loss: 0.0794 Acc: 0.9740                                               \n",
      "Epoch 038 | Train Loss: 0.0430 Acc: 0.9872 | Val Loss: 0.1027 Acc: 0.9713                                               \n",
      "Epoch 039 | Train Loss: 0.0363 Acc: 0.9886 | Val Loss: 0.0954 Acc: 0.9761                                               \n",
      "Epoch 040 | Train Loss: 0.0408 Acc: 0.9879 | Val Loss: 0.0770 Acc: 0.9791                                               \n",
      "Epoch 041 | Train Loss: 0.0358 Acc: 0.9891 | Val Loss: 0.1376 Acc: 0.9558                                               \n",
      "Epoch 042 | Train Loss: 0.0321 Acc: 0.9896 | Val Loss: 1.0466 Acc: 0.8466                                               \n",
      "Epoch 043 | Train Loss: 0.0436 Acc: 0.9871 | Val Loss: 0.0663 Acc: 0.9830                                               \n",
      "Epoch 044 | Train Loss: 0.0248 Acc: 0.9916 | Val Loss: 0.1145 Acc: 0.9675                                               \n",
      "Epoch 045 | Train Loss: 0.0364 Acc: 0.9895 | Val Loss: 0.0573 Acc: 0.9845                                               \n",
      "Epoch 046 | Train Loss: 0.0197 Acc: 0.9934 | Val Loss: 0.1039 Acc: 0.9731                                               \n",
      "Epoch 047 | Train Loss: 0.0246 Acc: 0.9913 | Val Loss: 0.0723 Acc: 0.9836                                               \n",
      "Epoch 048 | Train Loss: 0.0302 Acc: 0.9903 | Val Loss: 0.0879 Acc: 0.9755                                               \n",
      "Epoch 049 | Train Loss: 0.0274 Acc: 0.9908 | Val Loss: 0.0660 Acc: 0.9833                                               \n",
      "Epoch 050 | Train Loss: 0.0264 Acc: 0.9927 | Val Loss: 0.0695 Acc: 0.9845                                               \n",
      "Epoch 051 | Train Loss: 0.0253 Acc: 0.9928 | Val Loss: 0.0805 Acc: 0.9788                                               \n",
      "Epoch 052 | Train Loss: 0.0225 Acc: 0.9931 | Val Loss: 0.0523 Acc: 0.9878                                               \n",
      "Epoch 053 | Train Loss: 0.0238 Acc: 0.9925 | Val Loss: 0.0645 Acc: 0.9821                                               \n",
      "Epoch 054 | Train Loss: 0.0271 Acc: 0.9916 | Val Loss: 0.0843 Acc: 0.9803                                               \n",
      "Epoch 055 | Train Loss: 0.0218 Acc: 0.9927 | Val Loss: 0.1222 Acc: 0.9696                                               \n",
      "Epoch 056 | Train Loss: 0.0197 Acc: 0.9946 | Val Loss: 0.0777 Acc: 0.9842                                               \n",
      "Epoch 057 | Train Loss: 0.0202 Acc: 0.9937 | Val Loss: 0.1080 Acc: 0.9767                                               \n",
      "Epoch 058 | Train Loss: 0.0162 Acc: 0.9943 | Val Loss: 1.5169 Acc: 0.7672                                               \n",
      "Epoch 059 | Train Loss: 0.0288 Acc: 0.9926 | Val Loss: 0.0917 Acc: 0.9794                                               \n",
      "Epoch 060 | Train Loss: 0.0100 Acc: 0.9972 | Val Loss: 0.0769 Acc: 0.9830                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 128, 'cnn_dropout': 0.4021989997493572, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 48, 'cnn_kernels_2': 16, 'learning_rate': 0.00029582119595722905, 'lstm_dense': 64, 'lstm_hidden_size': 96, 'lstm_layers': 3, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 12.2144 Acc: 0.4262 | Val Loss: 4.5084 Acc: 0.4806                                              \n",
      "Epoch 002 | Train Loss: 4.6471 Acc: 0.4606 | Val Loss: 2.8501 Acc: 0.5516                                               \n",
      "Epoch 003 | Train Loss: 2.9263 Acc: 0.4926 | Val Loss: 1.4609 Acc: 0.5349                                               \n",
      "Epoch 004 | Train Loss: 2.0769 Acc: 0.5344 | Val Loss: 1.4418 Acc: 0.5773                                               \n",
      "Epoch 005 | Train Loss: 1.5949 Acc: 0.5816 | Val Loss: 0.5776 Acc: 0.7684                                               \n",
      "Epoch 006 | Train Loss: 1.1480 Acc: 0.6333 | Val Loss: 1.3482 Acc: 0.5991                                               \n",
      "Epoch 007 | Train Loss: 0.9470 Acc: 0.6882 | Val Loss: 0.5574 Acc: 0.7746                                               \n",
      "Epoch 008 | Train Loss: 0.7360 Acc: 0.7317 | Val Loss: 0.5286 Acc: 0.7955                                               \n",
      "Epoch 009 | Train Loss: 0.6018 Acc: 0.7745 | Val Loss: 0.4141 Acc: 0.8451                                               \n",
      "Epoch 010 | Train Loss: 0.5311 Acc: 0.8031 | Val Loss: 1.2443 Acc: 0.6531                                               \n",
      "Epoch 011 | Train Loss: 0.4558 Acc: 0.8369 | Val Loss: 0.4986 Acc: 0.8048                                               \n",
      "Epoch 012 | Train Loss: 0.3726 Acc: 0.8629 | Val Loss: 0.2069 Acc: 0.9272                                               \n",
      "Epoch 013 | Train Loss: 0.3229 Acc: 0.8812 | Val Loss: 0.2064 Acc: 0.9209                                               \n",
      "Epoch 014 | Train Loss: 0.2804 Acc: 0.9013 | Val Loss: 0.1567 Acc: 0.9469                                               \n",
      "Epoch 015 | Train Loss: 0.2401 Acc: 0.9159 | Val Loss: 0.1901 Acc: 0.9301                                               \n",
      "Epoch 016 | Train Loss: 0.2125 Acc: 0.9269 | Val Loss: 0.1321 Acc: 0.9445                                               \n",
      "Epoch 017 | Train Loss: 0.1778 Acc: 0.9411 | Val Loss: 0.1633 Acc: 0.9355                                               \n",
      "Epoch 018 | Train Loss: 0.1676 Acc: 0.9421 | Val Loss: 0.1798 Acc: 0.9316                                               \n",
      "Epoch 019 | Train Loss: 0.1395 Acc: 0.9531 | Val Loss: 0.1376 Acc: 0.9531                                               \n",
      "Epoch 020 | Train Loss: 0.1273 Acc: 0.9584 | Val Loss: 0.1016 Acc: 0.9669                                               \n",
      "Epoch 021 | Train Loss: 0.1147 Acc: 0.9600 | Val Loss: 0.0792 Acc: 0.9731                                               \n",
      "Epoch 022 | Train Loss: 0.1062 Acc: 0.9660 | Val Loss: 0.0784 Acc: 0.9734                                               \n",
      "Epoch 023 | Train Loss: 0.1052 Acc: 0.9644 | Val Loss: 0.0734 Acc: 0.9776                                               \n",
      "Epoch 024 | Train Loss: 0.0936 Acc: 0.9705 | Val Loss: 0.0712 Acc: 0.9764                                               \n",
      "Epoch 025 | Train Loss: 0.0877 Acc: 0.9708 | Val Loss: 0.1396 Acc: 0.9558                                               \n",
      "Epoch 026 | Train Loss: 0.0827 Acc: 0.9731 | Val Loss: 0.0947 Acc: 0.9687                                               \n",
      "Epoch 027 | Train Loss: 0.0810 Acc: 0.9728 | Val Loss: 0.0618 Acc: 0.9821                                               \n",
      "Epoch 028 | Train Loss: 0.0702 Acc: 0.9787 | Val Loss: 0.0820 Acc: 0.9752                                               \n",
      "Epoch 029 | Train Loss: 0.0755 Acc: 0.9776 | Val Loss: 0.0885 Acc: 0.9725                                               \n",
      "Epoch 030 | Train Loss: 0.0661 Acc: 0.9792 | Val Loss: 0.0575 Acc: 0.9815                                               \n",
      "Epoch 031 | Train Loss: 0.0616 Acc: 0.9800 | Val Loss: 0.0597 Acc: 0.9821                                               \n",
      "Epoch 032 | Train Loss: 0.0563 Acc: 0.9816 | Val Loss: 0.0768 Acc: 0.9791                                               \n",
      "Epoch 033 | Train Loss: 0.0539 Acc: 0.9836 | Val Loss: 0.0716 Acc: 0.9782                                               \n",
      "Epoch 034 | Train Loss: 0.0538 Acc: 0.9835 | Val Loss: 0.0506 Acc: 0.9839                                               \n",
      "Epoch 035 | Train Loss: 0.0518 Acc: 0.9846 | Val Loss: 0.0475 Acc: 0.9872                                               \n",
      "Epoch 036 | Train Loss: 0.0479 Acc: 0.9853 | Val Loss: 0.0543 Acc: 0.9824                                               \n",
      "Epoch 037 | Train Loss: 0.0441 Acc: 0.9866 | Val Loss: 0.0426 Acc: 0.9869                                               \n",
      "Epoch 038 | Train Loss: 0.0489 Acc: 0.9847 | Val Loss: 0.0495 Acc: 0.9857                                               \n",
      "Epoch 039 | Train Loss: 0.0453 Acc: 0.9854 | Val Loss: 0.0787 Acc: 0.9758                                               \n",
      "Epoch 040 | Train Loss: 0.0482 Acc: 0.9856 | Val Loss: 0.0842 Acc: 0.9752                                               \n",
      "Epoch 041 | Train Loss: 0.0379 Acc: 0.9882 | Val Loss: 0.0542 Acc: 0.9842                                               \n",
      "Epoch 042 | Train Loss: 0.0426 Acc: 0.9861 | Val Loss: 0.0447 Acc: 0.9815                                               \n",
      "Epoch 043 | Train Loss: 0.0466 Acc: 0.9858 | Val Loss: 0.0559 Acc: 0.9830                                               \n",
      "Epoch 044 | Train Loss: 0.0311 Acc: 0.9896 | Val Loss: 0.0526 Acc: 0.9848                                               \n",
      "Epoch 045 | Train Loss: 0.0344 Acc: 0.9892 | Val Loss: 0.0444 Acc: 0.9866                                               \n",
      "Epoch 046 | Train Loss: 0.0403 Acc: 0.9881 | Val Loss: 0.0469 Acc: 0.9872                                               \n",
      "Epoch 047 | Train Loss: 0.0348 Acc: 0.9894 | Val Loss: 0.1315 Acc: 0.9684                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 128, 'cnn_dropout': 0.4321547152793202, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 48, 'cnn_kernels_2': 16, 'learning_rate': 0.00038395834790170256, 'lstm_dense': 64, 'lstm_hidden_size': 96, 'lstm_layers': 3, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 11.4644 Acc: 0.4169 | Val Loss: 2.4467 Acc: 0.5412                                              \n",
      "Epoch 002 | Train Loss: 4.0603 Acc: 0.4751 | Val Loss: 1.2811 Acc: 0.6069                                               \n",
      "Epoch 003 | Train Loss: 2.6070 Acc: 0.5032 | Val Loss: 1.8905 Acc: 0.5976                                               \n",
      "Epoch 004 | Train Loss: 1.8285 Acc: 0.5341 | Val Loss: 0.8902 Acc: 0.6699                                               \n",
      "Epoch 005 | Train Loss: 1.3312 Acc: 0.5812 | Val Loss: 0.8875 Acc: 0.6510                                               \n",
      "Epoch 006 | Train Loss: 1.0932 Acc: 0.6036 | Val Loss: 0.7141 Acc: 0.6522                                               \n",
      "Epoch 007 | Train Loss: 0.9404 Acc: 0.6301 | Val Loss: 0.7640 Acc: 0.6940                                               \n",
      "Epoch 008 | Train Loss: 0.8219 Acc: 0.6595 | Val Loss: 0.8377 Acc: 0.6609                                               \n",
      "Epoch 009 | Train Loss: 0.7310 Acc: 0.6943 | Val Loss: 0.6025 Acc: 0.7370                                               \n",
      "Epoch 010 | Train Loss: 0.6520 Acc: 0.7286 | Val Loss: 0.4147 Acc: 0.8424                                               \n",
      "Epoch 011 | Train Loss: 0.6057 Acc: 0.7548 | Val Loss: 0.4536 Acc: 0.7958                                               \n",
      "Epoch 012 | Train Loss: 0.5222 Acc: 0.7930 | Val Loss: 0.5891 Acc: 0.7594                                               \n",
      "Epoch 013 | Train Loss: 0.4496 Acc: 0.8292 | Val Loss: 0.4852 Acc: 0.7713                                               \n",
      "Epoch 014 | Train Loss: 0.4333 Acc: 0.8437 | Val Loss: 0.5060 Acc: 0.8096                                               \n",
      "Epoch 015 | Train Loss: 0.3696 Acc: 0.8641 | Val Loss: 0.4279 Acc: 0.8245                                               \n",
      "Epoch 016 | Train Loss: 0.3252 Acc: 0.8851 | Val Loss: 0.2441 Acc: 0.9054                                               \n",
      "Epoch 017 | Train Loss: 0.2935 Acc: 0.8945 | Val Loss: 0.2568 Acc: 0.9036                                               \n",
      "Epoch 018 | Train Loss: 0.2611 Acc: 0.9058 | Val Loss: 0.2522 Acc: 0.9113                                               \n",
      "Epoch 019 | Train Loss: 0.2461 Acc: 0.9126 | Val Loss: 0.1379 Acc: 0.9567                                               \n",
      "Epoch 020 | Train Loss: 0.2189 Acc: 0.9222 | Val Loss: 0.1523 Acc: 0.9487                                               \n",
      "Epoch 021 | Train Loss: 0.2158 Acc: 0.9304 | Val Loss: 0.1362 Acc: 0.9484                                               \n",
      "Epoch 022 | Train Loss: 0.1837 Acc: 0.9357 | Val Loss: 0.2073 Acc: 0.9239                                               \n",
      "Epoch 023 | Train Loss: 0.1746 Acc: 0.9410 | Val Loss: 0.1602 Acc: 0.9418                                               \n",
      "Epoch 024 | Train Loss: 0.1616 Acc: 0.9442 | Val Loss: 0.1263 Acc: 0.9537                                               \n",
      "Epoch 025 | Train Loss: 0.1469 Acc: 0.9487 | Val Loss: 0.1685 Acc: 0.9376                                               \n",
      "Epoch 026 | Train Loss: 0.1396 Acc: 0.9534 | Val Loss: 0.2940 Acc: 0.8943                                               \n",
      "Epoch 027 | Train Loss: 0.1436 Acc: 0.9528 | Val Loss: 0.0967 Acc: 0.9669                                               \n",
      "Epoch 028 | Train Loss: 0.1179 Acc: 0.9590 | Val Loss: 0.2151 Acc: 0.9257                                               \n",
      "Epoch 029 | Train Loss: 0.1204 Acc: 0.9575 | Val Loss: 0.0964 Acc: 0.9681                                               \n",
      "Epoch 030 | Train Loss: 0.1117 Acc: 0.9631 | Val Loss: 0.0875 Acc: 0.9716                                               \n",
      "Epoch 031 | Train Loss: 0.1028 Acc: 0.9651 | Val Loss: 0.0879 Acc: 0.9737                                               \n",
      "Epoch 032 | Train Loss: 0.0976 Acc: 0.9674 | Val Loss: 0.1006 Acc: 0.9710                                               \n",
      "Epoch 033 | Train Loss: 0.0993 Acc: 0.9690 | Val Loss: 0.1463 Acc: 0.9531                                               \n",
      "Epoch 034 | Train Loss: 0.0999 Acc: 0.9678 | Val Loss: 0.0807 Acc: 0.9767                                               \n",
      "Epoch 035 | Train Loss: 0.0876 Acc: 0.9704 | Val Loss: 0.0854 Acc: 0.9728                                               \n",
      "Epoch 036 | Train Loss: 0.0861 Acc: 0.9728 | Val Loss: 0.0786 Acc: 0.9764                                               \n",
      "Epoch 037 | Train Loss: 0.0780 Acc: 0.9728 | Val Loss: 0.0885 Acc: 0.9731                                               \n",
      "Epoch 038 | Train Loss: 0.0740 Acc: 0.9760 | Val Loss: 0.1354 Acc: 0.9567                                               \n",
      "Epoch 039 | Train Loss: 0.0749 Acc: 0.9769 | Val Loss: 0.0780 Acc: 0.9794                                               \n",
      "Epoch 040 | Train Loss: 0.0776 Acc: 0.9760 | Val Loss: 0.0940 Acc: 0.9758                                               \n",
      "Epoch 041 | Train Loss: 0.0728 Acc: 0.9755 | Val Loss: 0.0613 Acc: 0.9830                                               \n",
      "Epoch 042 | Train Loss: 0.0665 Acc: 0.9759 | Val Loss: 0.0777 Acc: 0.9782                                               \n",
      "Epoch 043 | Train Loss: 0.0644 Acc: 0.9778 | Val Loss: 0.0748 Acc: 0.9791                                               \n",
      "Epoch 044 | Train Loss: 0.0574 Acc: 0.9801 | Val Loss: 0.1667 Acc: 0.9442                                               \n",
      "Epoch 045 | Train Loss: 0.0577 Acc: 0.9808 | Val Loss: 0.0915 Acc: 0.9633                                               \n",
      "Epoch 046 | Train Loss: 0.0614 Acc: 0.9799 | Val Loss: 0.0847 Acc: 0.9758                                               \n",
      "Epoch 047 | Train Loss: 0.0506 Acc: 0.9846 | Val Loss: 0.0911 Acc: 0.9767                                               \n",
      "Epoch 048 | Train Loss: 0.0550 Acc: 0.9816 | Val Loss: 0.1200 Acc: 0.9573                                               \n",
      "Epoch 049 | Train Loss: 0.0541 Acc: 0.9823 | Val Loss: 0.1316 Acc: 0.9513                                               \n",
      "Epoch 050 | Train Loss: 0.0623 Acc: 0.9814 | Val Loss: 0.1040 Acc: 0.9645                                               \n",
      "Epoch 051 | Train Loss: 0.0473 Acc: 0.9846 | Val Loss: 0.0584 Acc: 0.9854                                               \n",
      "Epoch 052 | Train Loss: 0.0497 Acc: 0.9840 | Val Loss: 0.1211 Acc: 0.9630                                               \n",
      "Epoch 053 | Train Loss: 0.0514 Acc: 0.9835 | Val Loss: 0.0539 Acc: 0.9854                                               \n",
      "Epoch 054 | Train Loss: 0.0523 Acc: 0.9837 | Val Loss: 0.1774 Acc: 0.9472                                               \n",
      "Epoch 055 | Train Loss: 0.0468 Acc: 0.9840 | Val Loss: 0.0654 Acc: 0.9833                                               \n",
      "Epoch 056 | Train Loss: 0.0482 Acc: 0.9834 | Val Loss: 0.1574 Acc: 0.9463                                               \n",
      "Epoch 057 | Train Loss: 0.0472 Acc: 0.9853 | Val Loss: 0.0814 Acc: 0.9707                                               \n",
      "Epoch 058 | Train Loss: 0.0388 Acc: 0.9872 | Val Loss: 0.1683 Acc: 0.9567                                               \n",
      "Epoch 059 | Train Loss: 0.0407 Acc: 0.9866 | Val Loss: 0.0549 Acc: 0.9830                                               \n",
      "Epoch 060 | Train Loss: 0.0415 Acc: 0.9866 | Val Loss: 0.0581 Acc: 0.9821                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 128, 'cnn_dropout': 0.39984674963307026, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 16, 'learning_rate': 0.00043158913517213067, 'lstm_dense': 64, 'lstm_hidden_size': 96, 'lstm_layers': 3, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 12.1871 Acc: 0.4143 | Val Loss: 4.4383 Acc: 0.4797                                              \n",
      "Epoch 002 | Train Loss: 3.8795 Acc: 0.4881 | Val Loss: 3.3147 Acc: 0.5346                                               \n",
      "Epoch 003 | Train Loss: 2.3995 Acc: 0.5359 | Val Loss: 2.6997 Acc: 0.5161                                               \n",
      "Epoch 004 | Train Loss: 1.5794 Acc: 0.5987 | Val Loss: 0.9090 Acc: 0.7128                                               \n",
      "Epoch 005 | Train Loss: 1.0917 Acc: 0.6511 | Val Loss: 0.9258 Acc: 0.7090                                               \n",
      "Epoch 006 | Train Loss: 0.8444 Acc: 0.6989 | Val Loss: 0.8746 Acc: 0.6457                                               \n",
      "Epoch 007 | Train Loss: 0.6892 Acc: 0.7431 | Val Loss: 1.2293 Acc: 0.6472                                               \n",
      "Epoch 008 | Train Loss: 0.5284 Acc: 0.7957 | Val Loss: 0.5080 Acc: 0.8027                                               \n",
      "Epoch 009 | Train Loss: 0.4387 Acc: 0.8337 | Val Loss: 0.6149 Acc: 0.7373                                               \n",
      "Epoch 010 | Train Loss: 0.3725 Acc: 0.8603 | Val Loss: 0.4874 Acc: 0.7988                                               \n",
      "Epoch 011 | Train Loss: 0.3175 Acc: 0.8792 | Val Loss: 0.2163 Acc: 0.9188                                               \n",
      "Epoch 012 | Train Loss: 0.2900 Acc: 0.8975 | Val Loss: 0.2551 Acc: 0.9054                                               \n",
      "Epoch 013 | Train Loss: 0.2363 Acc: 0.9171 | Val Loss: 0.2760 Acc: 0.9024                                               \n",
      "Epoch 014 | Train Loss: 0.2186 Acc: 0.9237 | Val Loss: 0.2170 Acc: 0.9099                                               \n",
      "Epoch 015 | Train Loss: 0.1955 Acc: 0.9323 | Val Loss: 0.1807 Acc: 0.9358                                               \n",
      "Epoch 016 | Train Loss: 0.1750 Acc: 0.9401 | Val Loss: 0.1593 Acc: 0.9504                                               \n",
      "Epoch 017 | Train Loss: 0.1509 Acc: 0.9478 | Val Loss: 0.1926 Acc: 0.9322                                               \n",
      "Epoch 018 | Train Loss: 0.1490 Acc: 0.9525 | Val Loss: 0.4075 Acc: 0.8719                                               \n",
      "Epoch 019 | Train Loss: 0.1289 Acc: 0.9567 | Val Loss: 0.1384 Acc: 0.9475                                               \n",
      "Epoch 020 | Train Loss: 0.1261 Acc: 0.9594 | Val Loss: 0.1133 Acc: 0.9582                                               \n",
      "Epoch 021 | Train Loss: 0.1177 Acc: 0.9599 | Val Loss: 0.1117 Acc: 0.9612                                               \n",
      "Epoch 022 | Train Loss: 0.1172 Acc: 0.9614 | Val Loss: 0.1048 Acc: 0.9693                                               \n",
      "Epoch 023 | Train Loss: 0.1028 Acc: 0.9663 | Val Loss: 0.1498 Acc: 0.9540                                               \n",
      "Epoch 024 | Train Loss: 0.0979 Acc: 0.9694 | Val Loss: 0.1145 Acc: 0.9618                                               \n",
      "Epoch 025 | Train Loss: 0.0893 Acc: 0.9718 | Val Loss: 0.1542 Acc: 0.9472                                               \n",
      "Epoch 026 | Train Loss: 0.0774 Acc: 0.9751 | Val Loss: 0.2295 Acc: 0.9310                                               \n",
      "Epoch 027 | Train Loss: 0.0849 Acc: 0.9748 | Val Loss: 0.1536 Acc: 0.9537                                               \n",
      "Epoch 028 | Train Loss: 0.0813 Acc: 0.9742 | Val Loss: 0.1387 Acc: 0.9549                                               \n",
      "Epoch 029 | Train Loss: 0.0803 Acc: 0.9746 | Val Loss: 0.0979 Acc: 0.9701                                               \n",
      "Epoch 030 | Train Loss: 0.0708 Acc: 0.9769 | Val Loss: 0.0668 Acc: 0.9806                                               \n",
      "Epoch 031 | Train Loss: 0.0653 Acc: 0.9778 | Val Loss: 0.0755 Acc: 0.9740                                               \n",
      "Epoch 032 | Train Loss: 0.0598 Acc: 0.9805 | Val Loss: 0.0816 Acc: 0.9746                                               \n",
      "Epoch 033 | Train Loss: 0.0596 Acc: 0.9803 | Val Loss: 0.1122 Acc: 0.9675                                               \n",
      "Epoch 034 | Train Loss: 0.0542 Acc: 0.9810 | Val Loss: 0.0687 Acc: 0.9785                                               \n",
      "Epoch 035 | Train Loss: 0.0513 Acc: 0.9832 | Val Loss: 0.0847 Acc: 0.9752                                               \n",
      "Epoch 036 | Train Loss: 0.0589 Acc: 0.9813 | Val Loss: 0.0533 Acc: 0.9854                                               \n",
      "Epoch 037 | Train Loss: 0.0517 Acc: 0.9837 | Val Loss: 0.1572 Acc: 0.9466                                               \n",
      "Epoch 038 | Train Loss: 0.0416 Acc: 0.9866 | Val Loss: 0.0913 Acc: 0.9773                                               \n",
      "Epoch 039 | Train Loss: 0.0473 Acc: 0.9852 | Val Loss: 0.0614 Acc: 0.9809                                               \n",
      "Epoch 040 | Train Loss: 0.0466 Acc: 0.9848 | Val Loss: 0.0506 Acc: 0.9851                                               \n",
      "Epoch 041 | Train Loss: 0.0427 Acc: 0.9870 | Val Loss: 0.0659 Acc: 0.9815                                               \n",
      "Epoch 042 | Train Loss: 0.0424 Acc: 0.9856 | Val Loss: 0.0653 Acc: 0.9809                                               \n",
      "Epoch 043 | Train Loss: 0.0419 Acc: 0.9872 | Val Loss: 0.0833 Acc: 0.9746                                               \n",
      "Epoch 044 | Train Loss: 0.0417 Acc: 0.9863 | Val Loss: 0.0623 Acc: 0.9818                                               \n",
      "Epoch 045 | Train Loss: 0.0425 Acc: 0.9870 | Val Loss: 0.0578 Acc: 0.9824                                               \n",
      "Epoch 046 | Train Loss: 0.0391 Acc: 0.9886 | Val Loss: 0.0539 Acc: 0.9830                                               \n",
      "Epoch 047 | Train Loss: 0.0395 Acc: 0.9890 | Val Loss: 0.0513 Acc: 0.9866                                               \n",
      "Epoch 048 | Train Loss: 0.0326 Acc: 0.9896 | Val Loss: 0.0569 Acc: 0.9815                                               \n",
      "Epoch 049 | Train Loss: 0.0322 Acc: 0.9894 | Val Loss: 0.0545 Acc: 0.9842                                               \n",
      "Epoch 050 | Train Loss: 0.0413 Acc: 0.9869 | Val Loss: 0.0557 Acc: 0.9851                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 128, 'cnn_dropout': 0.4808816074834695, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 48, 'cnn_kernels_2': 16, 'learning_rate': 0.0016358119493234111, 'lstm_dense': 64, 'lstm_hidden_size': 96, 'lstm_layers': 5, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 6.3100 Acc: 0.4817 | Val Loss: 1.6554 Acc: 0.5385                                               \n",
      "Epoch 002 | Train Loss: 1.0572 Acc: 0.5831 | Val Loss: 0.7215 Acc: 0.6349                                               \n",
      "Epoch 003 | Train Loss: 0.8210 Acc: 0.6272 | Val Loss: 0.7503 Acc: 0.6322                                               \n",
      "Epoch 004 | Train Loss: 0.7239 Acc: 0.6677 | Val Loss: 0.6152 Acc: 0.7340                                               \n",
      "Epoch 005 | Train Loss: 0.6756 Acc: 0.6889 | Val Loss: 0.5503 Acc: 0.7430                                               \n",
      "Epoch 006 | Train Loss: 0.6495 Acc: 0.7060 | Val Loss: 0.6538 Acc: 0.6910                                               \n",
      "Epoch 007 | Train Loss: 0.6137 Acc: 0.7186 | Val Loss: 0.6354 Acc: 0.7087                                               \n",
      "Epoch 008 | Train Loss: 0.5867 Acc: 0.7283 | Val Loss: 0.9450 Acc: 0.6236                                               \n",
      "Epoch 009 | Train Loss: 0.5771 Acc: 0.7415 | Val Loss: 0.5228 Acc: 0.7478                                               \n",
      "Epoch 010 | Train Loss: 0.5583 Acc: 0.7538 | Val Loss: 0.5873 Acc: 0.7722                                               \n",
      "Epoch 011 | Train Loss: 0.5423 Acc: 0.7585 | Val Loss: 0.5008 Acc: 0.7875                                               \n",
      "Epoch 012 | Train Loss: 0.5383 Acc: 0.7621 | Val Loss: 0.5285 Acc: 0.8200                                               \n",
      "Epoch 013 | Train Loss: 0.5445 Acc: 0.7580 | Val Loss: 0.7062 Acc: 0.7081                                               \n",
      "Epoch 014 | Train Loss: 0.5224 Acc: 0.7704 | Val Loss: 0.5493 Acc: 0.7952                                               \n",
      "Epoch 015 | Train Loss: 0.5270 Acc: 0.7646 | Val Loss: 0.5656 Acc: 0.7699                                               \n",
      "Epoch 016 | Train Loss: 0.5166 Acc: 0.7695 | Val Loss: 0.6891 Acc: 0.7496                                               \n",
      "Epoch 017 | Train Loss: 0.5074 Acc: 0.7756 | Val Loss: 0.7754 Acc: 0.7104                                               \n",
      "Epoch 018 | Train Loss: 0.4945 Acc: 0.7818 | Val Loss: 0.6161 Acc: 0.7579                                               \n",
      "Epoch 019 | Train Loss: 0.4932 Acc: 0.7828 | Val Loss: 0.8580 Acc: 0.7296                                               \n",
      "Epoch 020 | Train Loss: 0.4851 Acc: 0.7907 | Val Loss: 0.5585 Acc: 0.7994                                               \n",
      "Epoch 021 | Train Loss: 0.4927 Acc: 0.7916 | Val Loss: 0.6543 Acc: 0.7197                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 128, 'cnn_dropout': 0.3723458002759457, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 16, 'learning_rate': 0.0002611328563356821, 'lstm_dense': 64, 'lstm_hidden_size': 96, 'lstm_layers': 3, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 13.5344 Acc: 0.3962 | Val Loss: 3.1190 Acc: 0.4615                                              \n",
      "Epoch 002 | Train Loss: 4.8773 Acc: 0.4225 | Val Loss: 1.5249 Acc: 0.5349                                               \n",
      "Epoch 003 | Train Loss: 3.0904 Acc: 0.4551 | Val Loss: 2.4842 Acc: 0.5033                                               \n",
      "Epoch 004 | Train Loss: 2.2113 Acc: 0.4848 | Val Loss: 0.9011 Acc: 0.5919                                               \n",
      "Epoch 005 | Train Loss: 1.7213 Acc: 0.5189 | Val Loss: 0.9009 Acc: 0.6701                                               \n",
      "Epoch 006 | Train Loss: 1.4056 Acc: 0.5635 | Val Loss: 1.0004 Acc: 0.6609                                               \n",
      "Epoch 007 | Train Loss: 1.1230 Acc: 0.6130 | Val Loss: 0.7751 Acc: 0.7042                                               \n",
      "Epoch 008 | Train Loss: 0.9716 Acc: 0.6572 | Val Loss: 0.8372 Acc: 0.6600                                               \n",
      "Epoch 009 | Train Loss: 0.8392 Acc: 0.6974 | Val Loss: 1.0952 Acc: 0.6516                                               \n",
      "Epoch 010 | Train Loss: 0.7553 Acc: 0.7262 | Val Loss: 0.8759 Acc: 0.7173                                               \n",
      "Epoch 011 | Train Loss: 0.6474 Acc: 0.7621 | Val Loss: 0.4018 Acc: 0.8573                                               \n",
      "Epoch 012 | Train Loss: 0.5693 Acc: 0.7898 | Val Loss: 0.5137 Acc: 0.8167                                               \n",
      "Epoch 013 | Train Loss: 0.5203 Acc: 0.8071 | Val Loss: 0.5835 Acc: 0.8185                                               \n",
      "Epoch 014 | Train Loss: 0.4568 Acc: 0.8331 | Val Loss: 0.5744 Acc: 0.7976                                               \n",
      "Epoch 015 | Train Loss: 0.3989 Acc: 0.8535 | Val Loss: 0.4392 Acc: 0.8304                                               \n",
      "Epoch 016 | Train Loss: 0.3752 Acc: 0.8666 | Val Loss: 0.4508 Acc: 0.8385                                               \n",
      "Epoch 017 | Train Loss: 0.3212 Acc: 0.8886 | Val Loss: 0.2968 Acc: 0.8913                                               \n",
      "Epoch 018 | Train Loss: 0.2843 Acc: 0.8998 | Val Loss: 0.3937 Acc: 0.8570                                               \n",
      "Epoch 019 | Train Loss: 0.2437 Acc: 0.9155 | Val Loss: 0.1974 Acc: 0.9269                                               \n",
      "Epoch 020 | Train Loss: 0.2186 Acc: 0.9251 | Val Loss: 0.3053 Acc: 0.8904                                               \n",
      "Epoch 021 | Train Loss: 0.1939 Acc: 0.9351 | Val Loss: 0.3129 Acc: 0.8976                                               \n",
      "Epoch 022 | Train Loss: 0.1767 Acc: 0.9379 | Val Loss: 0.1501 Acc: 0.9478                                               \n",
      "Epoch 023 | Train Loss: 0.1593 Acc: 0.9461 | Val Loss: 0.1227 Acc: 0.9546                                               \n",
      "Epoch 024 | Train Loss: 0.1511 Acc: 0.9499 | Val Loss: 0.1511 Acc: 0.9421                                               \n",
      "Epoch 025 | Train Loss: 0.1323 Acc: 0.9569 | Val Loss: 0.1181 Acc: 0.9531                                               \n",
      "Epoch 026 | Train Loss: 0.1256 Acc: 0.9590 | Val Loss: 0.1750 Acc: 0.9364                                               \n",
      "Epoch 027 | Train Loss: 0.1139 Acc: 0.9622 | Val Loss: 0.1064 Acc: 0.9657                                               \n",
      "Epoch 028 | Train Loss: 0.1015 Acc: 0.9679 | Val Loss: 0.0860 Acc: 0.9710                                               \n",
      "Epoch 029 | Train Loss: 0.0976 Acc: 0.9675 | Val Loss: 0.2923 Acc: 0.9087                                               \n",
      "Epoch 030 | Train Loss: 0.0980 Acc: 0.9684 | Val Loss: 0.0777 Acc: 0.9734                                               \n",
      "Epoch 031 | Train Loss: 0.0822 Acc: 0.9731 | Val Loss: 0.0925 Acc: 0.9696                                               \n",
      "Epoch 032 | Train Loss: 0.0755 Acc: 0.9757 | Val Loss: 0.0651 Acc: 0.9776                                               \n",
      "Epoch 033 | Train Loss: 0.0751 Acc: 0.9748 | Val Loss: 0.0864 Acc: 0.9716                                               \n",
      "Epoch 034 | Train Loss: 0.0706 Acc: 0.9773 | Val Loss: 0.0991 Acc: 0.9684                                               \n",
      "Epoch 035 | Train Loss: 0.0657 Acc: 0.9792 | Val Loss: 0.1039 Acc: 0.9666                                               \n",
      "Epoch 036 | Train Loss: 0.0625 Acc: 0.9808 | Val Loss: 0.0731 Acc: 0.9779                                               \n",
      "Epoch 037 | Train Loss: 0.0601 Acc: 0.9804 | Val Loss: 0.1106 Acc: 0.9615                                               \n",
      "Epoch 038 | Train Loss: 0.0548 Acc: 0.9822 | Val Loss: 0.0709 Acc: 0.9776                                               \n",
      "Epoch 039 | Train Loss: 0.0524 Acc: 0.9826 | Val Loss: 0.0580 Acc: 0.9815                                               \n",
      "Epoch 040 | Train Loss: 0.0564 Acc: 0.9821 | Val Loss: 0.4520 Acc: 0.8973                                               \n",
      "Epoch 041 | Train Loss: 0.0527 Acc: 0.9834 | Val Loss: 0.0708 Acc: 0.9791                                               \n",
      "Epoch 042 | Train Loss: 0.0505 Acc: 0.9845 | Val Loss: 0.0623 Acc: 0.9800                                               \n",
      "Epoch 043 | Train Loss: 0.0450 Acc: 0.9855 | Val Loss: 0.0858 Acc: 0.9752                                               \n",
      "Epoch 044 | Train Loss: 0.0425 Acc: 0.9869 | Val Loss: 0.1773 Acc: 0.9475                                               \n",
      "Epoch 045 | Train Loss: 0.0445 Acc: 0.9857 | Val Loss: 0.1020 Acc: 0.9734                                               \n",
      "Epoch 046 | Train Loss: 0.0369 Acc: 0.9880 | Val Loss: 0.0767 Acc: 0.9764                                               \n",
      "Epoch 047 | Train Loss: 0.0352 Acc: 0.9880 | Val Loss: 0.0831 Acc: 0.9731                                               \n",
      "Epoch 048 | Train Loss: 0.0432 Acc: 0.9862 | Val Loss: 0.0712 Acc: 0.9821                                               \n",
      "Epoch 049 | Train Loss: 0.0373 Acc: 0.9880 | Val Loss: 0.1905 Acc: 0.9448                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 64, 'cnn_dense': 128, 'cnn_dropout': 0.5996632055951256, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 16, 'learning_rate': 0.00012737063878449583, 'lstm_dense': 128, 'lstm_hidden_size': 96, 'lstm_layers': 6, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 27.0479 Acc: 0.3444 | Val Loss: 5.5467 Acc: 0.4690                                              \n",
      "Epoch 002 | Train Loss: 11.0535 Acc: 0.3941 | Val Loss: 4.3219 Acc: 0.4484                                              \n",
      "Epoch 003 | Train Loss: 7.5790 Acc: 0.4182 | Val Loss: 2.9123 Acc: 0.4797                                               \n",
      "Epoch 004 | Train Loss: 5.7423 Acc: 0.4460 | Val Loss: 3.7441 Acc: 0.4922                                               \n",
      "Epoch 005 | Train Loss: 4.3772 Acc: 0.4665 | Val Loss: 1.7129 Acc: 0.5770                                               \n",
      "Epoch 006 | Train Loss: 3.4859 Acc: 0.4944 | Val Loss: 2.9948 Acc: 0.5528                                               \n",
      "Epoch 007 | Train Loss: 2.9781 Acc: 0.5171 | Val Loss: 1.6388 Acc: 0.6337                                               \n",
      "Epoch 008 | Train Loss: 2.4571 Acc: 0.5431 | Val Loss: 2.9327 Acc: 0.5499                                               \n",
      "Epoch 009 | Train Loss: 2.1226 Acc: 0.5674 | Val Loss: 1.4890 Acc: 0.5943                                               \n",
      "Epoch 010 | Train Loss: 1.8881 Acc: 0.5832 | Val Loss: 2.7410 Acc: 0.6000                                               \n",
      "Epoch 011 | Train Loss: 1.5819 Acc: 0.6193 | Val Loss: 1.1265 Acc: 0.6364                                               \n",
      "Epoch 012 | Train Loss: 1.4390 Acc: 0.6342 | Val Loss: 0.7542 Acc: 0.7534                                               \n",
      "Epoch 013 | Train Loss: 1.2515 Acc: 0.6631 | Val Loss: 1.0007 Acc: 0.7143                                               \n",
      "Epoch 014 | Train Loss: 1.1816 Acc: 0.6751 | Val Loss: 0.7196 Acc: 0.7707                                               \n",
      "Epoch 015 | Train Loss: 1.0649 Acc: 0.6983 | Val Loss: 0.7916 Acc: 0.7624                                               \n",
      "Epoch 016 | Train Loss: 0.9662 Acc: 0.7132 | Val Loss: 1.1585 Acc: 0.7051                                               \n",
      "Epoch 017 | Train Loss: 0.8463 Acc: 0.7434 | Val Loss: 1.2115 Acc: 0.6755                                               \n",
      "Epoch 018 | Train Loss: 0.7965 Acc: 0.7523 | Val Loss: 0.6498 Acc: 0.7872                                               \n",
      "Epoch 019 | Train Loss: 0.7236 Acc: 0.7703 | Val Loss: 0.7171 Acc: 0.7684                                               \n",
      "Epoch 020 | Train Loss: 0.6691 Acc: 0.7774 | Val Loss: 0.6486 Acc: 0.8078                                               \n",
      "Epoch 021 | Train Loss: 0.6383 Acc: 0.7917 | Val Loss: 0.4803 Acc: 0.8287                                               \n",
      "Epoch 022 | Train Loss: 0.6005 Acc: 0.8031 | Val Loss: 0.6431 Acc: 0.7678                                               \n",
      "Epoch 023 | Train Loss: 0.5583 Acc: 0.8143 | Val Loss: 0.6955 Acc: 0.7934                                               \n",
      "Epoch 024 | Train Loss: 0.5033 Acc: 0.8308 | Val Loss: 0.3023 Acc: 0.8973                                               \n",
      "Epoch 025 | Train Loss: 0.4947 Acc: 0.8364 | Val Loss: 0.7294 Acc: 0.8087                                               \n",
      "Epoch 026 | Train Loss: 0.4597 Acc: 0.8450 | Val Loss: 0.8304 Acc: 0.7528                                               \n",
      "Epoch 027 | Train Loss: 0.4344 Acc: 0.8571 | Val Loss: 0.7765 Acc: 0.7621                                               \n",
      "Epoch 028 | Train Loss: 0.4223 Acc: 0.8604 | Val Loss: 0.7386 Acc: 0.7690                                               \n",
      "Epoch 029 | Train Loss: 0.3895 Acc: 0.8671 | Val Loss: 1.2213 Acc: 0.7081                                               \n",
      "Epoch 030 | Train Loss: 0.3725 Acc: 0.8775 | Val Loss: 0.5929 Acc: 0.7997                                               \n",
      "Epoch 031 | Train Loss: 0.3691 Acc: 0.8760 | Val Loss: 0.8035 Acc: 0.7663                                               \n",
      "Epoch 032 | Train Loss: 0.3512 Acc: 0.8791 | Val Loss: 0.4445 Acc: 0.8403                                               \n",
      "Epoch 033 | Train Loss: 0.3417 Acc: 0.8857 | Val Loss: 0.7167 Acc: 0.7376                                               \n",
      "Epoch 034 | Train Loss: 0.3273 Acc: 0.8931 | Val Loss: 0.3412 Acc: 0.8687                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 96, 'cnn_dense': 128, 'cnn_dropout': 0.2912657301840562, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 16, 'cnn_kernels_2': 16, 'learning_rate': 0.00042485773232741296, 'lstm_dense': 64, 'lstm_hidden_size': 96, 'lstm_layers': 3, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 20.0413 Acc: 0.3944 | Val Loss: 8.2230 Acc: 0.4663                                              \n",
      "Epoch 002 | Train Loss: 7.8295 Acc: 0.4429 | Val Loss: 4.1656 Acc: 0.4385                                               \n",
      "Epoch 003 | Train Loss: 4.9220 Acc: 0.4776 | Val Loss: 3.3999 Acc: 0.5582                                               \n",
      "Epoch 004 | Train Loss: 3.4919 Acc: 0.5210 | Val Loss: 2.6474 Acc: 0.5057                                               \n",
      "Epoch 005 | Train Loss: 2.5070 Acc: 0.5721 | Val Loss: 2.0741 Acc: 0.6104                                               \n",
      "Epoch 006 | Train Loss: 1.7797 Acc: 0.6155 | Val Loss: 2.1328 Acc: 0.6630                                               \n",
      "Epoch 007 | Train Loss: 1.5200 Acc: 0.6450 | Val Loss: 0.7929 Acc: 0.7325                                               \n",
      "Epoch 008 | Train Loss: 1.1450 Acc: 0.6877 | Val Loss: 0.8146 Acc: 0.7466                                               \n",
      "Epoch 009 | Train Loss: 0.9422 Acc: 0.7195 | Val Loss: 1.2243 Acc: 0.6612                                               \n",
      "Epoch 010 | Train Loss: 0.7936 Acc: 0.7482 | Val Loss: 0.5130 Acc: 0.8290                                               \n",
      "Epoch 011 | Train Loss: 0.6049 Acc: 0.7974 | Val Loss: 0.4542 Acc: 0.8340                                               \n",
      "Epoch 012 | Train Loss: 0.5318 Acc: 0.8197 | Val Loss: 0.3121 Acc: 0.8827                                               \n",
      "Epoch 013 | Train Loss: 0.4350 Acc: 0.8586 | Val Loss: 0.3564 Acc: 0.8737                                               \n",
      "Epoch 014 | Train Loss: 0.3279 Acc: 0.8858 | Val Loss: 0.2206 Acc: 0.9266                                               \n",
      "Epoch 015 | Train Loss: 0.3096 Acc: 0.8936 | Val Loss: 0.2134 Acc: 0.9310                                               \n",
      "Epoch 016 | Train Loss: 0.2730 Acc: 0.9073 | Val Loss: 0.1562 Acc: 0.9475                                               \n",
      "Epoch 017 | Train Loss: 0.2231 Acc: 0.9262 | Val Loss: 0.4524 Acc: 0.8648                                               \n",
      "Epoch 018 | Train Loss: 0.2303 Acc: 0.9306 | Val Loss: 0.1932 Acc: 0.9334                                               \n",
      "Epoch 019 | Train Loss: 0.1690 Acc: 0.9449 | Val Loss: 0.1344 Acc: 0.9528                                               \n",
      "Epoch 020 | Train Loss: 0.1738 Acc: 0.9463 | Val Loss: 0.1746 Acc: 0.9388                                               \n",
      "Epoch 021 | Train Loss: 0.1486 Acc: 0.9511 | Val Loss: 0.1506 Acc: 0.9507                                               \n",
      "Epoch 022 | Train Loss: 0.1206 Acc: 0.9599 | Val Loss: 0.1056 Acc: 0.9630                                               \n",
      "Epoch 023 | Train Loss: 0.1310 Acc: 0.9590 | Val Loss: 0.3513 Acc: 0.8949                                               \n",
      "Epoch 024 | Train Loss: 0.1076 Acc: 0.9634 | Val Loss: 0.0928 Acc: 0.9710                                               \n",
      "Epoch 025 | Train Loss: 0.0880 Acc: 0.9701 | Val Loss: 0.0814 Acc: 0.9749                                               \n",
      "Epoch 026 | Train Loss: 0.0898 Acc: 0.9701 | Val Loss: 0.1256 Acc: 0.9633                                               \n",
      "Epoch 027 | Train Loss: 0.1016 Acc: 0.9676 | Val Loss: 0.1284 Acc: 0.9627                                               \n",
      "Epoch 028 | Train Loss: 0.0737 Acc: 0.9780 | Val Loss: 0.0699 Acc: 0.9785                                               \n",
      "Epoch 029 | Train Loss: 0.0759 Acc: 0.9752 | Val Loss: 0.0992 Acc: 0.9681                                               \n",
      "Epoch 030 | Train Loss: 0.0652 Acc: 0.9786 | Val Loss: 0.0670 Acc: 0.9821                                               \n",
      "Epoch 031 | Train Loss: 0.0704 Acc: 0.9776 | Val Loss: 0.0957 Acc: 0.9699                                               \n",
      "Epoch 032 | Train Loss: 0.0605 Acc: 0.9798 | Val Loss: 0.0832 Acc: 0.9767                                               \n",
      "Epoch 033 | Train Loss: 0.0651 Acc: 0.9792 | Val Loss: 0.0686 Acc: 0.9767                                               \n",
      "Epoch 034 | Train Loss: 0.0632 Acc: 0.9787 | Val Loss: 0.1694 Acc: 0.9463                                               \n",
      "Epoch 035 | Train Loss: 0.0525 Acc: 0.9823 | Val Loss: 0.0953 Acc: 0.9713                                               \n",
      "Epoch 036 | Train Loss: 0.0467 Acc: 0.9842 | Val Loss: 0.0860 Acc: 0.9758                                               \n",
      "Epoch 037 | Train Loss: 0.0528 Acc: 0.9831 | Val Loss: 0.0760 Acc: 0.9776                                               \n",
      "Epoch 038 | Train Loss: 0.0517 Acc: 0.9845 | Val Loss: 0.0929 Acc: 0.9797                                               \n",
      "Epoch 039 | Train Loss: 0.0545 Acc: 0.9830 | Val Loss: 0.0835 Acc: 0.9785                                               \n",
      "Epoch 040 | Train Loss: 0.0341 Acc: 0.9890 | Val Loss: 0.0637 Acc: 0.9836                                               \n",
      "Epoch 041 | Train Loss: 0.0434 Acc: 0.9857 | Val Loss: 0.1410 Acc: 0.9651                                               \n",
      "Epoch 042 | Train Loss: 0.0359 Acc: 0.9880 | Val Loss: 0.0869 Acc: 0.9791                                               \n",
      "Epoch 043 | Train Loss: 0.0389 Acc: 0.9875 | Val Loss: 0.0974 Acc: 0.9669                                               \n",
      "Epoch 044 | Train Loss: 0.0376 Acc: 0.9875 | Val Loss: 0.0565 Acc: 0.9851                                               \n",
      "Epoch 045 | Train Loss: 0.0406 Acc: 0.9867 | Val Loss: 0.0546 Acc: 0.9857                                               \n",
      "Epoch 046 | Train Loss: 0.0397 Acc: 0.9876 | Val Loss: 0.0986 Acc: 0.9725                                               \n",
      "Epoch 047 | Train Loss: 0.0320 Acc: 0.9913 | Val Loss: 0.0547 Acc: 0.9857                                               \n",
      "Epoch 048 | Train Loss: 0.0366 Acc: 0.9893 | Val Loss: 0.0898 Acc: 0.9761                                               \n",
      "Epoch 049 | Train Loss: 0.0387 Acc: 0.9865 | Val Loss: 0.0611 Acc: 0.9845                                               \n",
      "Epoch 050 | Train Loss: 0.0313 Acc: 0.9904 | Val Loss: 0.0527 Acc: 0.9878                                               \n",
      "Epoch 051 | Train Loss: 0.0315 Acc: 0.9897 | Val Loss: 0.0549 Acc: 0.9836                                               \n",
      "Epoch 052 | Train Loss: 0.0350 Acc: 0.9900 | Val Loss: 0.0776 Acc: 0.9791                                               \n",
      "Epoch 053 | Train Loss: 0.0295 Acc: 0.9906 | Val Loss: 0.0651 Acc: 0.9857                                               \n",
      "Epoch 054 | Train Loss: 0.0259 Acc: 0.9915 | Val Loss: 0.0480 Acc: 0.9854                                               \n",
      "Epoch 055 | Train Loss: 0.0260 Acc: 0.9919 | Val Loss: 0.0623 Acc: 0.9848                                               \n",
      "Epoch 056 | Train Loss: 0.0361 Acc: 0.9889 | Val Loss: 0.0505 Acc: 0.9869                                               \n",
      "Epoch 057 | Train Loss: 0.0199 Acc: 0.9942 | Val Loss: 0.0664 Acc: 0.9812                                               \n",
      "Epoch 058 | Train Loss: 0.0277 Acc: 0.9912 | Val Loss: 0.0563 Acc: 0.9863                                               \n",
      "Epoch 059 | Train Loss: 0.0274 Acc: 0.9909 | Val Loss: 0.0942 Acc: 0.9755                                               \n",
      "Epoch 060 | Train Loss: 0.0246 Acc: 0.9923 | Val Loss: 0.0562 Acc: 0.9857                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 48, 'cnn_dense': 128, 'cnn_dropout': 0.6295395984769552, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 48, 'cnn_kernels_2': 16, 'learning_rate': 0.001312707705749187, 'lstm_dense': 128, 'lstm_hidden_size': 96, 'lstm_layers': 5, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 9.2652 Acc: 0.4535 | Val Loss: 1.0087 Acc: 0.5910                                               \n",
      "Epoch 002 | Train Loss: 1.2498 Acc: 0.5568 | Val Loss: 0.9631 Acc: 0.6690                                               \n",
      "Epoch 003 | Train Loss: 0.8266 Acc: 0.6243 | Val Loss: 0.8187 Acc: 0.6003                                               \n",
      "Epoch 004 | Train Loss: 0.7268 Acc: 0.6703 | Val Loss: 0.8352 Acc: 0.5937                                               \n",
      "Epoch 005 | Train Loss: 0.6393 Acc: 0.7105 | Val Loss: 0.4952 Acc: 0.7797                                               \n",
      "Epoch 006 | Train Loss: 0.5883 Acc: 0.7384 | Val Loss: 0.4838 Acc: 0.7887                                               \n",
      "Epoch 007 | Train Loss: 0.5373 Acc: 0.7685 | Val Loss: 0.7494 Acc: 0.7534                                               \n",
      "Epoch 008 | Train Loss: 0.5018 Acc: 0.7765 | Val Loss: 0.9726 Acc: 0.6758                                               \n",
      "Epoch 009 | Train Loss: 0.4821 Acc: 0.7953 | Val Loss: 0.7175 Acc: 0.6946                                               \n",
      "Epoch 010 | Train Loss: 0.4694 Acc: 0.8027 | Val Loss: 0.4613 Acc: 0.8397                                               \n",
      "Epoch 011 | Train Loss: 0.4506 Acc: 0.8116 | Val Loss: 0.5500 Acc: 0.7478                                               \n",
      "Epoch 012 | Train Loss: 0.4672 Acc: 0.8048 | Val Loss: 0.5533 Acc: 0.7797                                               \n",
      "Epoch 013 | Train Loss: 0.4455 Acc: 0.8148 | Val Loss: 0.5279 Acc: 0.7848                                               \n",
      "Epoch 014 | Train Loss: 0.4541 Acc: 0.8136 | Val Loss: 0.5011 Acc: 0.7940                                               \n",
      "Epoch 015 | Train Loss: 0.4422 Acc: 0.8090 | Val Loss: 0.6003 Acc: 0.7427                                               \n",
      "Epoch 016 | Train Loss: 0.4485 Acc: 0.8139 | Val Loss: 0.5104 Acc: 0.7848                                               \n",
      "Epoch 017 | Train Loss: 0.4434 Acc: 0.8144 | Val Loss: 0.6351 Acc: 0.7585                                               \n",
      "Epoch 018 | Train Loss: 0.4411 Acc: 0.8163 | Val Loss: 0.7583 Acc: 0.7215                                               \n",
      "Epoch 019 | Train Loss: 0.4601 Acc: 0.8095 | Val Loss: 0.5161 Acc: 0.7979                                               \n",
      "Epoch 020 | Train Loss: 0.4818 Acc: 0.8070 | Val Loss: 0.5685 Acc: 0.7660                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "Trial params:                                                                                                           \n",
      "{'batch_size': 32, 'cnn_dense': 128, 'cnn_dropout': 0.3993656040824247, 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 32, 'cnn_kernels_2': 64, 'learning_rate': 0.0022008962879353933, 'lstm_dense': 128, 'lstm_hidden_size': 96, 'lstm_layers': 5, 'optimizer': 'rmsprop'}\n",
      "Epoch 001 | Train Loss: 3.6073 Acc: 0.5347 | Val Loss: 0.7415 Acc: 0.6916                                               \n",
      "Epoch 002 | Train Loss: 0.8943 Acc: 0.5899 | Val Loss: 0.7488 Acc: 0.6501                                               \n",
      "Epoch 003 | Train Loss: 0.8562 Acc: 0.5962 | Val Loss: 0.9085 Acc: 0.5907                                               \n",
      "Epoch 004 | Train Loss: 0.7879 Acc: 0.6256 | Val Loss: 0.6739 Acc: 0.6907                                               \n",
      "Epoch 005 | Train Loss: 0.7621 Acc: 0.6554 | Val Loss: 0.8161 Acc: 0.5666                                               \n",
      "Epoch 006 | Train Loss: 0.7477 Acc: 0.6615 | Val Loss: 0.7466 Acc: 0.6481                                               \n",
      "Epoch 007 | Train Loss: 0.7313 Acc: 0.6683 | Val Loss: 0.9014 Acc: 0.5693                                               \n",
      "Epoch 008 | Train Loss: 0.7488 Acc: 0.6635 | Val Loss: 0.7561 Acc: 0.6537                                               \n",
      "Epoch 009 | Train Loss: 0.7284 Acc: 0.6766 | Val Loss: 0.6218 Acc: 0.7101                                               \n",
      "Epoch 010 | Train Loss: 0.7525 Acc: 0.6716 | Val Loss: 0.8239 Acc: 0.5997                                               \n",
      "Epoch 011 | Train Loss: 0.7237 Acc: 0.6745 | Val Loss: 0.8059 Acc: 0.6424                                               \n",
      "Epoch 012 | Train Loss: 0.7072 Acc: 0.6957 | Val Loss: 0.7690 Acc: 0.6478                                               \n",
      "Epoch 013 | Train Loss: 0.7109 Acc: 0.6852 | Val Loss: 0.7516 Acc: 0.6982                                               \n",
      "Epoch 014 | Train Loss: 0.7105 Acc: 0.6762 | Val Loss: 0.7278 Acc: 0.6394                                               \n",
      "Epoch 015 | Train Loss: 0.7055 Acc: 0.6777 | Val Loss: 0.8954 Acc: 0.5842                                               \n",
      "Epoch 016 | Train Loss: 0.7077 Acc: 0.6871 | Val Loss: 0.9267 Acc: 0.6284                                               \n",
      "Epoch 017 | Train Loss: 0.6880 Acc: 0.6935 | Val Loss: 0.7836 Acc: 0.6128                                               \n",
      "Epoch 018 | Train Loss: 0.6707 Acc: 0.7004 | Val Loss: 0.7865 Acc: 0.6436                                               \n",
      "Epoch 019 | Train Loss: 0.6939 Acc: 0.6964 | Val Loss: 0.6684 Acc: 0.7140                                               \n",
      "Early stopping triggered.                                                                                               \n",
      "100%|████████████████████████████████████████████████| 30/30 [24:25<00:00, 48.86s/trial, best loss: 0.04260263274020668]\n",
      "Best hyperparameters: {'batch_size': np.int64(3), 'cnn_dense': np.int64(2), 'cnn_dropout': np.float64(0.4021989997493572), 'cnn_kernel_size_1': np.int64(1), 'cnn_kernel_size_2': np.int64(0), 'cnn_kernels_1': np.int64(2), 'cnn_kernels_2': np.int64(0), 'learning_rate': np.float64(0.00029582119595722905), 'lstm_dense': np.int64(1), 'lstm_hidden_size': np.int64(2), 'lstm_layers': np.int64(2), 'optimizer': np.int64(1)}\n",
      "TPE search finished in 1465.97 seconds\n",
      "Best (raw indices): {'batch_size': np.int64(3), 'cnn_dense': np.int64(2), 'cnn_dropout': np.float64(0.4021989997493572), 'cnn_kernel_size_1': np.int64(1), 'cnn_kernel_size_2': np.int64(0), 'cnn_kernels_1': np.int64(2), 'cnn_kernels_2': np.int64(0), 'learning_rate': np.float64(0.00029582119595722905), 'lstm_dense': np.int64(1), 'lstm_hidden_size': np.int64(2), 'lstm_layers': np.int64(2), 'optimizer': np.int64(1)}\n",
      "Best (interpreted): {'batch_size': 64, 'cnn_dense': 128, 'cnn_dropout': np.float64(0.4021989997493572), 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 48, 'cnn_kernels_2': 16, 'learning_rate': np.float64(0.00029582119595722905), 'lstm_dense': 64, 'lstm_hidden_size': 96, 'lstm_layers': 3, 'optimizer': 'rmsprop'}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type Trials is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mall_trials.json\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mw+\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresults\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial_results\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/json/__init__.py:179\u001b[39m, in \u001b[36mdump\u001b[39m\u001b[34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[39m\n\u001b[32m    173\u001b[39m     iterable = \u001b[38;5;28mcls\u001b[39m(skipkeys=skipkeys, ensure_ascii=ensure_ascii,\n\u001b[32m    174\u001b[39m         check_circular=check_circular, allow_nan=allow_nan, indent=indent,\n\u001b[32m    175\u001b[39m         separators=separators,\n\u001b[32m    176\u001b[39m         default=default, sort_keys=sort_keys, **kw).iterencode(obj)\n\u001b[32m    177\u001b[39m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/json/encoder.py:432\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode\u001b[39m\u001b[34m(o, _current_indent_level)\u001b[39m\n\u001b[32m    430\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    434\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/json/encoder.py:406\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode_dict\u001b[39m\u001b[34m(dct, _current_indent_level)\u001b[39m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    405\u001b[39m             chunks = _iterencode(value, _current_indent_level)\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    408\u001b[39m     _current_indent_level -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/json/encoder.py:326\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode_list\u001b[39m\u001b[34m(lst, _current_indent_level)\u001b[39m\n\u001b[32m    324\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    325\u001b[39m             chunks = _iterencode(value, _current_indent_level)\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    328\u001b[39m     _current_indent_level -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/json/encoder.py:406\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode_dict\u001b[39m\u001b[34m(dct, _current_indent_level)\u001b[39m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    405\u001b[39m             chunks = _iterencode(value, _current_indent_level)\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    408\u001b[39m     _current_indent_level -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/json/encoder.py:439\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode\u001b[39m\u001b[34m(o, _current_indent_level)\u001b[39m\n\u001b[32m    437\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCircular reference detected\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    438\u001b[39m     markers[markerid] = o\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m o = \u001b[43m_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/json/encoder.py:180\u001b[39m, in \u001b[36mJSONEncoder.default\u001b[39m\u001b[34m(self, o)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[32m    162\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[33;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[33;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    178\u001b[39m \n\u001b[32m    179\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    181\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mis not JSON serializable\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: Object of type Trials is not JSON serializable"
     ]
    }
   ],
   "source": [
    "trial_results = []\n",
    "\n",
    "for i in range(10):\n",
    "    trials, params = hyperparameter_search()\n",
    "    trial_results.append({\"trials\": trials, \"params\": params})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f3309ea-f41e-4d51-a860-176d3821da54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ATTACH::0::history': {'train_accs': array([0.34599597, 0.3630868 , 0.38995447, 0.4072692 , 0.41525487,\n",
      "       0.4153295 , 0.42674826, 0.42585268, 0.42973356, 0.43674901,\n",
      "       0.44212255, 0.44466005, 0.45204866, 0.47451302, 0.47339354,\n",
      "       0.48234943, 0.48824539, 0.49652959, 0.5039182 , 0.51235167,\n",
      "       0.51690425, 0.52212852, 0.53675647, 0.53780133, 0.53638331,\n",
      "       0.54705575, 0.55145906, 0.56422121, 0.56272856, 0.57631167,\n",
      "       0.58220763, 0.58511829, 0.5876558 , 0.59773117, 0.60810508,\n",
      "       0.61004553, 0.60788119, 0.62803194, 0.63206209, 0.64780954,\n",
      "       0.64370475, 0.66124338, 0.66415404, 0.67758788, 0.67773714,\n",
      "       0.69132025, 0.6949026 , 0.70027614, 0.71572505, 0.71863572,\n",
      "       0.73415927, 0.73624897, 0.73774162, 0.74796627, 0.7531159 ,\n",
      "       0.76281812, 0.7699082 , 0.77281887, 0.78043138, 0.78953653]), 'train_losses': array([102.24673   ,  45.86756107,  30.85426743,  21.31211104,\n",
      "        16.43628355,  14.0938445 ,  11.95519647,  10.34000997,\n",
      "         9.12098165,   8.21643394,   7.46169677,   6.76228488,\n",
      "         6.10770766,   5.52442156,   5.02120907,   4.64939098,\n",
      "         4.38490412,   3.87742824,   3.68002026,   3.49149608,\n",
      "         3.18305036,   3.03428998,   2.77490494,   2.59125106,\n",
      "         2.5242765 ,   2.35763429,   2.26098155,   2.1448995 ,\n",
      "         2.01171557,   1.86550832,   1.78122471,   1.75462903,\n",
      "         1.66740254,   1.5967971 ,   1.48885523,   1.47545266,\n",
      "         1.41249255,   1.31836458,   1.29671774,   1.21358742,\n",
      "         1.17284643,   1.1074343 ,   1.06990955,   1.05668863,\n",
      "         1.00657646,   0.96172294,   0.93930234,   0.93090037,\n",
      "         0.89240315,   0.84535406,   0.79896226,   0.7953719 ,\n",
      "         0.79065821,   0.73872895,   0.7275792 ,   0.69143489,\n",
      "         0.6711643 ,   0.66533762,   0.63883728,   0.61990271]), 'val_accs': array([0.44686567, 0.41552239, 0.35014925, 0.37552239, 0.38597015,\n",
      "       0.43641791, 0.42029851, 0.46537313, 0.44835821, 0.46059701,\n",
      "       0.52238806, 0.55074627, 0.55850746, 0.59402985, 0.61223881,\n",
      "       0.6238806 , 0.55313433, 0.6238806 , 0.66328358, 0.62119403,\n",
      "       0.65014925, 0.65641791, 0.65791045, 0.65641791, 0.66567164,\n",
      "       0.67552239, 0.65671642, 0.69522388, 0.69223881, 0.6841791 ,\n",
      "       0.69432836, 0.69731343, 0.71044776, 0.69910448, 0.69761194,\n",
      "       0.72447761, 0.73641791, 0.72776119, 0.68298507, 0.72089552,\n",
      "       0.73820896, 0.75253731, 0.74776119, 0.75731343, 0.77253731,\n",
      "       0.75791045, 0.75910448, 0.77164179, 0.7561194 , 0.76119403,\n",
      "       0.79432836, 0.80656716, 0.76358209, 0.81373134, 0.78537313,\n",
      "       0.80328358, 0.78238806, 0.81671642, 0.81701493, 0.82238806]), 'val_losses': array([33.23892691, 12.70504869,  9.50805994,  7.06570473,  4.94824946,\n",
      "        3.61004412,  2.96347355,  2.82079618,  2.98318986,  2.47977703,\n",
      "        2.45519699,  2.53267783,  2.45275367,  1.75242561,  1.90156643,\n",
      "        1.63054783,  1.62741891,  1.47217801,  1.44096764,  1.5730863 ,\n",
      "        1.38383893,  1.33992147,  1.2142296 ,  1.43636464,  1.33907823,\n",
      "        1.30809587,  1.31614909,  1.10952878,  1.09793755,  0.9690518 ,\n",
      "        1.09671533,  1.07359411,  1.073589  ,  1.06465426,  0.98522576,\n",
      "        0.85594721,  0.81755496,  0.90659317,  1.00680155,  0.80093285,\n",
      "        0.84728788,  0.75266179,  0.78312121,  0.70850531,  0.70265887,\n",
      "        0.73006191,  0.79216015,  0.67812999,  0.77594398,  0.76490465,\n",
      "        0.59955582,  0.63492508,  0.7296029 ,  0.58788578,  0.58772978,\n",
      "        0.58710401,  0.65045378,  0.53431475,  0.55652149,  0.52501685])}, 'ATTACH::0::best_val_loss': 0.5250168463365356, 'ATTACH::1::history': {'train_accs': array([0.36338533, 0.42644974, 0.46682588, 0.50473916, 0.54474214,\n",
      "       0.57123666, 0.58646168, 0.61034406, 0.628629  , 0.6401224 ,\n",
      "       0.66101948, 0.67646839, 0.70012688, 0.70818718, 0.72094932,\n",
      "       0.73430853, 0.75401149, 0.76326592, 0.7758788 , 0.79655198,\n",
      "       0.80259721, 0.81968804, 0.83573401, 0.82946489, 0.84439137,\n",
      "       0.85924323, 0.86297485, 0.87521457, 0.8861109 , 0.88693186,\n",
      "       0.88894694, 0.89588775, 0.903351  , 0.90253004, 0.91297858,\n",
      "       0.91685947, 0.91939697, 0.92574073, 0.92947235, 0.93290544,\n",
      "       0.92365102, 0.93745802, 0.94133891, 0.93984626, 0.94372714,\n",
      "       0.93939846, 0.95014553, 0.95141428, 0.9532801 , 0.95402642,\n",
      "       0.95410105, 0.95768341, 0.95798194, 0.95828047, 0.96343011,\n",
      "       0.96044481, 0.96634077, 0.96462423, 0.96425106, 0.96738563]), 'train_losses': array([30.66973831,  8.59710992,  5.21037223,  3.69031145,  2.78049919,\n",
      "        2.33470617,  2.02411305,  1.68583198,  1.53368666,  1.39747282,\n",
      "        1.18138458,  1.08936232,  0.9686152 ,  0.8913395 ,  0.84163308,\n",
      "        0.78963477,  0.71127515,  0.6796717 ,  0.62029923,  0.57219427,\n",
      "        0.54676074,  0.49715723,  0.47149131,  0.47637416,  0.42577223,\n",
      "        0.38083756,  0.38027312,  0.34942576,  0.31567922,  0.30947305,\n",
      "        0.30290736,  0.29311634,  0.27337658,  0.27207875,  0.24405362,\n",
      "        0.23606271,  0.22412163,  0.2137423 ,  0.19855627,  0.19254228,\n",
      "        0.21420868,  0.17586794,  0.16906076,  0.17095008,  0.16410042,\n",
      "        0.18146037,  0.14388738,  0.14204458,  0.13697027,  0.132826  ,\n",
      "        0.13376824,  0.12155167,  0.12118343,  0.121448  ,  0.10846832,\n",
      "        0.1182346 ,  0.09990554,  0.09946538,  0.10184903,  0.09784361]), 'val_accs': array([0.42686567, 0.57432836, 0.60179104, 0.61313433, 0.59164179,\n",
      "       0.62298507, 0.66238806, 0.60776119, 0.73462687, 0.63253731,\n",
      "       0.7319403 , 0.77850746, 0.75343284, 0.76746269, 0.65283582,\n",
      "       0.80149254, 0.80119403, 0.82925373, 0.84567164, 0.81283582,\n",
      "       0.8719403 , 0.87761194, 0.86447761, 0.88626866, 0.87313433,\n",
      "       0.87970149, 0.89910448, 0.9041791 , 0.90447761, 0.91850746,\n",
      "       0.89820896, 0.9161194 , 0.9       , 0.91134328, 0.90925373,\n",
      "       0.92328358, 0.89462687, 0.92895522, 0.93313433, 0.94895522,\n",
      "       0.95432836, 0.92865672, 0.94746269, 0.93283582, 0.95910448,\n",
      "       0.96089552, 0.94716418, 0.95701493, 0.95731343, 0.95283582,\n",
      "       0.95522388, 0.96746269, 0.96149254, 0.96298507, 0.96985075,\n",
      "       0.96746269, 0.9761194 , 0.9719403 , 0.97313433, 0.96      ]), 'val_losses': array([5.15779496, 2.21449373, 2.45785398, 1.94994076, 1.57816106,\n",
      "       1.79774827, 0.93617901, 1.1645944 , 0.78246256, 1.07919717,\n",
      "       0.69868653, 0.58600477, 0.72324309, 0.59059275, 0.98782125,\n",
      "       0.50659468, 0.55569807, 0.42044   , 0.41304808, 0.52489108,\n",
      "       0.34096359, 0.34750372, 0.37080916, 0.32373744, 0.35494405,\n",
      "       0.35451365, 0.27893944, 0.28103586, 0.27622868, 0.21970298,\n",
      "       0.28389371, 0.22766661, 0.29907662, 0.23940673, 0.26496895,\n",
      "       0.2148404 , 0.30615166, 0.19290762, 0.17997357, 0.14849921,\n",
      "       0.1430918 , 0.19886409, 0.16150348, 0.16962481, 0.12404715,\n",
      "       0.11601282, 0.14024728, 0.11822733, 0.1259343 , 0.12966896,\n",
      "       0.12179111, 0.10434617, 0.11290807, 0.11147955, 0.10144992,\n",
      "       0.09632141, 0.08149725, 0.08499823, 0.08419742, 0.12014827])}, 'ATTACH::1::best_val_loss': 0.08149724610705873, 'ATTACH::2::history': {'train_accs': array([0.45816852, 0.53638331, 0.51294873, 0.50884394, 0.4763042 ,\n",
      "       0.46190014, 0.45824315, 0.47070677, 0.46749757, 0.4406299 ,\n",
      "       0.43891335, 0.43868945, 0.44212255, 0.44197328]), 'train_losses': array([15.59567429,  1.0137589 ,  1.14091335,  0.99428483,  1.00871591,\n",
      "        1.07820578,  1.07066352,  1.06486438,  1.07334602,  1.30620776,\n",
      "        1.24683456,  1.2514471 ,  1.24072712,  1.26671573]), 'val_accs': array([0.51373134, 0.4961194 , 0.56567164, 0.56835821, 0.49014925,\n",
      "       0.48447761, 0.44746269, 0.48029851, 0.45432836, 0.44208955,\n",
      "       0.43731343, 0.44208955, 0.44208955, 0.44208955]), 'val_losses': array([1.2491509 , 1.29480066, 0.95809291, 0.85382367, 0.92784357,\n",
      "       0.92951376, 1.20471409, 1.0001402 , 1.13782962, 1.24539981,\n",
      "       1.25107718, 1.23875331, 1.23941508, 1.23958034])}, 'ATTACH::2::best_val_loss': 0.8538236742589012, 'ATTACH::3::history': {'train_accs': array([0.39450705, 0.44995895, 0.46167624, 0.47936413, 0.51332189,\n",
      "       0.53295022, 0.54727965, 0.56869916, 0.60855288, 0.61653855,\n",
      "       0.6451974 , 0.67087096, 0.68206583, 0.72333756, 0.74401075,\n",
      "       0.76259422, 0.78804388, 0.81655347, 0.84080902, 0.86872155,\n",
      "       0.8916337 , 0.89917158, 0.9054407 , 0.92805433, 0.93424882,\n",
      "       0.94521979, 0.94887678, 0.96059407, 0.96066871, 0.96357937,\n",
      "       0.96649004, 0.97343085, 0.97507277, 0.97649078, 0.97544593,\n",
      "       0.97805806, 0.98305844, 0.98126726, 0.97731174, 0.98977536,\n",
      "       0.98432719, 0.98828271, 0.98626763, 0.98731249, 0.99186506,\n",
      "       0.98955146, 0.99037242, 0.99268602, 0.98567057, 0.99432793,\n",
      "       0.99261139, 0.99268602, 0.99111874, 0.99328308, 0.99305918,\n",
      "       0.99343235, 0.99432793, 0.99522352, 0.99261139, 0.99246212]), 'train_losses': array([1.44122423e+01, 5.07738849e+00, 3.52525348e+00, 2.59750359e+00,\n",
      "       1.93815813e+00, 1.54323993e+00, 1.30823238e+00, 1.10692593e+00,\n",
      "       9.65088168e-01, 8.94974800e-01, 8.05223670e-01, 7.53428361e-01,\n",
      "       7.20070668e-01, 6.64738314e-01, 6.18263306e-01, 5.87307341e-01,\n",
      "       5.26071452e-01, 4.72468738e-01, 4.11932859e-01, 3.54498761e-01,\n",
      "       3.01667498e-01, 2.78285275e-01, 2.56291988e-01, 2.01439026e-01,\n",
      "       1.84879416e-01, 1.58849797e-01, 1.50849698e-01, 1.32859541e-01,\n",
      "       1.18912664e-01, 1.07118731e-01, 1.00525217e-01, 8.09574396e-02,\n",
      "       7.85952191e-02, 7.30900585e-02, 7.03400759e-02, 8.10197799e-02,\n",
      "       5.23778881e-02, 5.93160109e-02, 8.62907191e-02, 3.26464885e-02,\n",
      "       5.10740263e-02, 3.49529776e-02, 4.58956016e-02, 4.16130500e-02,\n",
      "       2.33537429e-02, 3.59748669e-02, 3.14260513e-02, 2.34983378e-02,\n",
      "       5.32193130e-02, 1.93301612e-02, 2.50936427e-02, 2.37611508e-02,\n",
      "       3.04509297e-02, 2.14671677e-02, 2.21719262e-02, 2.48212280e-02,\n",
      "       1.78040273e-02, 1.38993250e-02, 1.97947266e-02, 2.67570286e-02]), 'val_accs': array([0.50626866, 0.50746269, 0.49164179, 0.54298507, 0.56567164,\n",
      "       0.5238806 , 0.52059701, 0.59373134, 0.64746269, 0.69910448,\n",
      "       0.64567164, 0.78119403, 0.68238806, 0.70567164, 0.75791045,\n",
      "       0.84179104, 0.86507463, 0.88835821, 0.91880597, 0.92238806,\n",
      "       0.94059701, 0.85104478, 0.94298507, 0.88      , 0.94716418,\n",
      "       0.93910448, 0.9561194 , 0.95761194, 0.96597015, 0.96059701,\n",
      "       0.97850746, 0.95791045, 0.9680597 , 0.98059701, 0.97671642,\n",
      "       0.97910448, 0.97731343, 0.96119403, 0.96686567, 0.98059701,\n",
      "       0.9758209 , 0.96835821, 0.98268657, 0.96746269, 0.98328358,\n",
      "       0.98059701, 0.97552239, 0.96895522, 0.97970149, 0.97522388,\n",
      "       0.98746269, 0.96059701, 0.97850746, 0.96716418, 0.95820896,\n",
      "       0.98507463, 0.98298507, 0.98626866, 0.98328358, 0.95253731]), 'val_losses': array([2.52093625, 2.43357202, 3.23511918, 2.15024043, 1.03924379,\n",
      "       0.88948063, 1.09536788, 0.93840368, 0.75085045, 0.67082018,\n",
      "       0.69322153, 0.54429666, 0.64200665, 0.60198225, 0.53527405,\n",
      "       0.41971895, 0.35703476, 0.32166719, 0.25707622, 0.23187627,\n",
      "       0.18307254, 0.34762243, 0.16819797, 0.29887705, 0.13566392,\n",
      "       0.15842649, 0.11896826, 0.11405461, 0.08851247, 0.10840838,\n",
      "       0.06166278, 0.11265259, 0.08827353, 0.05467703, 0.06791053,\n",
      "       0.06531226, 0.06321937, 0.1022989 , 0.09449282, 0.055757  ,\n",
      "       0.06920619, 0.09698814, 0.0506088 , 0.10602247, 0.0529637 ,\n",
      "       0.0595622 , 0.06687191, 0.09206408, 0.05721398, 0.078965  ,\n",
      "       0.04066932, 0.12964421, 0.05864975, 0.11968107, 0.12922013,\n",
      "       0.04877682, 0.05297201, 0.04522058, 0.06325825, 0.16893687])}, 'ATTACH::3::best_val_loss': 0.040669322038875584, 'ATTACH::4::history': {'train_accs': array([0.3821927 , 0.44182402, 0.4599597 , 0.49869393, 0.53086051,\n",
      "       0.56526606, 0.6069856 , 0.64176431, 0.67176655, 0.72483021,\n",
      "       0.7503545 , 0.77169938, 0.79670125, 0.81162773, 0.83140533,\n",
      "       0.84207777, 0.86103441, 0.87543847, 0.87663258, 0.89178297,\n",
      "       0.90932159, 0.90603776, 0.91626241, 0.92290469, 0.92663632,\n",
      "       0.93768192, 0.93977162, 0.93827898, 0.94633928, 0.94283155,\n",
      "       0.95484738, 0.9555937 , 0.95760878, 0.95939996, 0.9583551 ,\n",
      "       0.96499739, 0.96857974, 0.96910217, 0.97104262, 0.96514665,\n",
      "       0.97253526, 0.97552056, 0.97656542, 0.97686395, 0.977461  ,\n",
      "       0.97686395, 0.97708784, 0.97828196, 0.97984924, 0.97992387,\n",
      "       0.977461  , 0.97372938, 0.98134189, 0.98104336, 0.97887902,\n",
      "       0.98425256, 0.98410329, 0.98283454, 0.98499888]), 'train_losses': array([24.56132423,  6.11717244,  3.68051259,  2.51267634,  1.90595933,\n",
      "        1.64506493,  1.29985446,  1.12275284,  1.00214872,  0.85248706,\n",
      "        0.75947777,  0.66691004,  0.60228306,  0.55842116,  0.49127751,\n",
      "        0.45663191,  0.40402037,  0.36014524,  0.35768369,  0.31327744,\n",
      "        0.26939677,  0.27885959,  0.25267591,  0.23527423,  0.2337406 ,\n",
      "        0.20038109,  0.19595714,  0.19670618,  0.16944471,  0.1784615 ,\n",
      "        0.14740164,  0.14621028,  0.13942308,  0.13582422,  0.13017314,\n",
      "        0.11524181,  0.10013439,  0.10726028,  0.09343738,  0.11544604,\n",
      "        0.08991936,  0.08272661,  0.07850033,  0.07565144,  0.07639718,\n",
      "        0.07545848,  0.06992918,  0.06717404,  0.06335753,  0.06644066,\n",
      "        0.07920079,  0.08549391,  0.05833369,  0.06371546,  0.06939935,\n",
      "        0.05527954,  0.05400826,  0.05521074,  0.04748732]), 'val_accs': array([0.49731343, 0.52298507, 0.53044776, 0.63253731, 0.59761194,\n",
      "       0.7319403 , 0.70119403, 0.68      , 0.70626866, 0.72626866,\n",
      "       0.82029851, 0.78597015, 0.82447761, 0.79462687, 0.83313433,\n",
      "       0.86865672, 0.86925373, 0.84298507, 0.88955224, 0.88179104,\n",
      "       0.8519403 , 0.89850746, 0.91641791, 0.92059701, 0.90537313,\n",
      "       0.90149254, 0.91283582, 0.92895522, 0.92835821, 0.9241791 ,\n",
      "       0.91880597, 0.92029851, 0.92865672, 0.93910448, 0.92686567,\n",
      "       0.91970149, 0.9161194 , 0.94507463, 0.94776119, 0.93253731,\n",
      "       0.93910448, 0.9119403 , 0.96328358, 0.94746269, 0.93373134,\n",
      "       0.96238806, 0.96776119, 0.96597015, 0.97253731, 0.97014925,\n",
      "       0.94119403, 0.94865672, 0.9638806 , 0.93462687, 0.95432836,\n",
      "       0.96865672, 0.95820896, 0.96626866, 0.96059701]), 'val_losses': array([2.88830385, 1.88728926, 1.50411537, 1.0322923 , 1.01857682,\n",
      "       0.71122023, 0.86378971, 0.85582438, 0.8564164 , 0.66250721,\n",
      "       0.50253451, 0.64764892, 0.48542749, 0.568445  , 0.49068692,\n",
      "       0.37394653, 0.39220344, 0.49288872, 0.27633066, 0.31426542,\n",
      "       0.47470608, 0.24817192, 0.22437741, 0.2465965 , 0.27213507,\n",
      "       0.31904624, 0.2215957 , 0.20620287, 0.19033933, 0.22184887,\n",
      "       0.24701053, 0.21907019, 0.22778769, 0.17894504, 0.23584061,\n",
      "       0.29800319, 0.25897531, 0.17937725, 0.15449221, 0.23758382,\n",
      "       0.1830411 , 0.29419956, 0.1378835 , 0.16874857, 0.20218347,\n",
      "       0.12938828, 0.13356543, 0.1361827 , 0.10609243, 0.11637439,\n",
      "       0.21603694, 0.16219007, 0.14540283, 0.24999675, 0.13980454,\n",
      "       0.13419931, 0.17203287, 0.13800812, 0.17235502])}, 'ATTACH::4::best_val_loss': 0.10609242803125239, 'ATTACH::5::history': {'train_accs': array([0.36778864, 0.43174864, 0.46973655, 0.49996268, 0.50645571,\n",
      "       0.5345175 , 0.55907157, 0.56795283, 0.58198373, 0.59504441,\n",
      "       0.61041869, 0.61870289, 0.63489813, 0.64168968, 0.64460034,\n",
      "       0.65892977, 0.65728786, 0.65766102, 0.66572132, 0.6677364 ,\n",
      "       0.67378163, 0.67766251, 0.67542354, 0.67811031, 0.68042391,\n",
      "       0.68631987, 0.69624599, 0.70311217, 0.71124711, 0.72206881,\n",
      "       0.74192104, 0.75565341, 0.76401224, 0.76707217, 0.78267035,\n",
      "       0.79714904, 0.81162773, 0.82953952, 0.83924173, 0.85483991,\n",
      "       0.86394507, 0.87357265, 0.88902157, 0.89126054, 0.89954474,\n",
      "       0.90730652, 0.9194716 , 0.92103888, 0.92424808, 0.93238301,\n",
      "       0.93283081, 0.93798045, 0.94253302, 0.94447347, 0.94939921,\n",
      "       0.9528323 , 0.95395179, 0.95708635, 0.95611613, 0.96148966]), 'train_losses': array([19.31765646,  6.60714965,  3.97079499,  2.74044389,  2.0955074 ,\n",
      "        1.68283961,  1.41296477,  1.26386002,  1.16017921,  1.06419978,\n",
      "        0.99479281,  0.93707787,  0.90713268,  0.86742289,  0.85715702,\n",
      "        0.80168275,  0.8101669 ,  0.79275276,  0.77060024,  0.77538459,\n",
      "        0.75407227,  0.74253275,  0.74990437,  0.74631566,  0.72385477,\n",
      "        0.72723449,  0.70477686,  0.69573117,  0.68022191,  0.66209285,\n",
      "        0.62713697,  0.62553034,  0.59132337,  0.57845385,  0.55492383,\n",
      "        0.51993645,  0.49276365,  0.45941922,  0.42775914,  0.39324461,\n",
      "        0.37812263,  0.35103944,  0.32250436,  0.30410162,  0.28076778,\n",
      "        0.27131527,  0.23210229,  0.23506386,  0.21967283,  0.20045623,\n",
      "        0.19507932,  0.18603302,  0.17330887,  0.16477027,  0.14946709,\n",
      "        0.14969132,  0.14387126,  0.1353039 ,  0.13642263,  0.11645295]), 'val_accs': array([0.45820896, 0.50895522, 0.46089552, 0.56      , 0.57462687,\n",
      "       0.60835821, 0.68298507, 0.65223881, 0.67970149, 0.62268657,\n",
      "       0.70865672, 0.67701493, 0.61134328, 0.66059701, 0.6561194 ,\n",
      "       0.67820896, 0.7158209 , 0.72059701, 0.77283582, 0.63343284,\n",
      "       0.58626866, 0.75970149, 0.76447761, 0.77134328, 0.78059701,\n",
      "       0.72746269, 0.79313433, 0.74567164, 0.78477612, 0.78746269,\n",
      "       0.70955224, 0.77462687, 0.67432836, 0.72895522, 0.82507463,\n",
      "       0.83014925, 0.8358209 , 0.82208955, 0.84537313, 0.87671642,\n",
      "       0.88029851, 0.87522388, 0.8880597 , 0.92746269, 0.94      ,\n",
      "       0.93014925, 0.90179104, 0.92686567, 0.94776119, 0.92597015,\n",
      "       0.91253731, 0.92895522, 0.93104478, 0.95373134, 0.95104478,\n",
      "       0.89761194, 0.96447761, 0.90776119, 0.95910448, 0.95791045]), 'val_losses': array([4.18080133, 2.13807927, 2.16196754, 1.31476403, 1.15277477,\n",
      "       0.89182204, 0.73553154, 0.71909085, 0.78748584, 0.70755403,\n",
      "       0.65374267, 0.67748949, 0.71224337, 0.73866059, 0.68788497,\n",
      "       0.712999  , 0.64829829, 0.62758003, 0.53256391, 0.66275478,\n",
      "       0.79481716, 0.56207322, 0.55733621, 0.55009089, 0.56908712,\n",
      "       0.56819333, 0.53619892, 0.56782927, 0.49648625, 0.4949311 ,\n",
      "       0.61001863, 0.53987029, 0.60495879, 0.52168972, 0.44960872,\n",
      "       0.42685625, 0.4028807 , 0.42986562, 0.36993766, 0.3132794 ,\n",
      "       0.30383202, 0.31999768, 0.28612366, 0.20194193, 0.18818781,\n",
      "       0.20868978, 0.26175139, 0.19876609, 0.15510153, 0.19626567,\n",
      "       0.23195122, 0.19839108, 0.19270342, 0.13837888, 0.14279729,\n",
      "       0.30639488, 0.10313278, 0.25993916, 0.12290219, 0.12625929])}, 'ATTACH::5::best_val_loss': 0.10313278396850202, 'ATTACH::6::history': {'train_accs': array([0.44264497, 0.53362191, 0.54966789, 0.55422046, 0.56310172,\n",
      "       0.56451974, 0.57855064, 0.57952086, 0.57011717, 0.57101276,\n",
      "       0.56190761, 0.54213001, 0.55414583, 0.56952011, 0.58952161,\n",
      "       0.60370177, 0.60623927, 0.63236062, 0.64124188]), 'train_losses': array([5.00427823, 0.9635039 , 0.93365438, 0.91006185, 0.90879877,\n",
      "       0.88996187, 0.86578317, 0.85617866, 0.85700915, 0.85918684,\n",
      "       0.8552368 , 0.891947  , 0.90781017, 0.87695781, 0.87030311,\n",
      "       0.83945722, 0.84385638, 0.80112392, 0.79057536]), 'val_accs': array([0.5761194 , 0.61313433, 0.52179104, 0.63164179, 0.55552239,\n",
      "       0.56119403, 0.49402985, 0.62716418, 0.63522388, 0.65313433,\n",
      "       0.63492537, 0.59850746, 0.58895522, 0.52776119, 0.62      ,\n",
      "       0.65701493, 0.68597015, 0.62179104, 0.63223881]), 'val_losses': array([0.94319753, 0.81170675, 0.8356802 , 0.79750477, 0.85580404,\n",
      "       0.85804278, 0.93647076, 0.80157645, 0.7225855 , 0.79394312,\n",
      "       0.81117075, 0.81278348, 0.79952769, 0.8202316 , 0.79789461,\n",
      "       0.74027466, 0.76020775, 0.76996715, 0.76380561])}, 'ATTACH::6::best_val_loss': 0.7225855045887961, 'ATTACH::7::history': {'train_accs': array([0.41398612, 0.47279648, 0.48093141, 0.49548474, 0.50876931,\n",
      "       0.50862005, 0.50809762, 0.5149638 , 0.5287708 , 0.53630868,\n",
      "       0.53466677, 0.53712964, 0.54765281, 0.55048884, 0.55929547,\n",
      "       0.56220613, 0.57116203, 0.57228151, 0.57743115, 0.57534144,\n",
      "       0.57623703, 0.57922233, 0.59385029, 0.60183596, 0.6040003 ,\n",
      "       0.60280618, 0.61019479, 0.60847824, 0.62131502, 0.61698634,\n",
      "       0.62698709, 0.6233301 , 0.62892753, 0.63153967, 0.63594298,\n",
      "       0.6316143 , 0.63504739, 0.64743638, 0.65004851, 0.65198895,\n",
      "       0.65997462, 0.65639227, 0.65325771, 0.66087021, 0.66810956,\n",
      "       0.67363236, 0.66594522, 0.66654228, 0.67094559, 0.68184193,\n",
      "       0.6813195 , 0.67840884, 0.68594671, 0.68497649, 0.69273826,\n",
      "       0.69296216, 0.69758937, 0.69169341, 0.69848496, 0.7038585 ]), 'train_losses': array([3.84295824, 0.99346437, 0.9527395 , 0.92754843, 0.91490922,\n",
      "       0.90996293, 0.90264101, 0.89557153, 0.8842957 , 0.87463032,\n",
      "       0.87227348, 0.86581735, 0.84706538, 0.84640654, 0.83882313,\n",
      "       0.8315797 , 0.82598341, 0.81942799, 0.81780202, 0.80950993,\n",
      "       0.81927082, 0.80870653, 0.79078229, 0.78892085, 0.78375467,\n",
      "       0.7803359 , 0.77652495, 0.76873382, 0.76082447, 0.75384452,\n",
      "       0.75423496, 0.75466814, 0.74872701, 0.74322815, 0.73323414,\n",
      "       0.74315194, 0.74449825, 0.72069665, 0.72578512, 0.72542674,\n",
      "       0.71680952, 0.72345252, 0.71315148, 0.70735379, 0.70373477,\n",
      "       0.69320545, 0.70741422, 0.70077403, 0.69217244, 0.68080105,\n",
      "       0.67700191, 0.68155543, 0.67228385, 0.67138168, 0.66475228,\n",
      "       0.66883255, 0.66120803, 0.6576773 , 0.65823766, 0.63775538]), 'val_accs': array([0.47343284, 0.52298507, 0.51402985, 0.54358209, 0.56567164,\n",
      "       0.56507463, 0.51731343, 0.52119403, 0.52358209, 0.52477612,\n",
      "       0.59552239, 0.61522388, 0.60716418, 0.61373134, 0.60686567,\n",
      "       0.60477612, 0.59671642, 0.61701493, 0.6358209 , 0.61970149,\n",
      "       0.60238806, 0.62      , 0.63432836, 0.64059701, 0.60328358,\n",
      "       0.58      , 0.6480597 , 0.62567164, 0.60537313, 0.6       ,\n",
      "       0.64507463, 0.62656716, 0.61820896, 0.65761194, 0.65910448,\n",
      "       0.62119403, 0.68      , 0.7       , 0.69313433, 0.7241791 ,\n",
      "       0.65820896, 0.64716418, 0.65970149, 0.71253731, 0.6880597 ,\n",
      "       0.71134328, 0.70328358, 0.6719403 , 0.71283582, 0.70626866,\n",
      "       0.69134328, 0.72626866, 0.7641791 , 0.68925373, 0.72955224,\n",
      "       0.7719403 , 0.7519403 , 0.76447761, 0.75641791, 0.75820896]), 'val_losses': array([0.9854617 , 0.90900745, 0.88749185, 0.85697506, 0.84280958,\n",
      "       0.83313799, 0.83475961, 0.81740626, 0.81321408, 0.81799946,\n",
      "       0.8064083 , 0.78531964, 0.78175956, 0.79834236, 0.76642892,\n",
      "       0.76174775, 0.77103928, 0.75792953, 0.73656663, 0.74745726,\n",
      "       0.74393009, 0.74361616, 0.72434919, 0.73435384, 0.70994014,\n",
      "       0.72346235, 0.69346067, 0.68463832, 0.69759032, 0.70305624,\n",
      "       0.69064947, 0.6909776 , 0.69212862, 0.68509715, 0.66091338,\n",
      "       0.69212481, 0.66364418, 0.6532345 , 0.65141627, 0.64498851,\n",
      "       0.64324687, 0.64625702, 0.64486889, 0.62280476, 0.62727905,\n",
      "       0.62713132, 0.61698696, 0.65433607, 0.60060548, 0.59745424,\n",
      "       0.60306214, 0.59072521, 0.59729793, 0.61229861, 0.56189102,\n",
      "       0.56426045, 0.55651494, 0.55827935, 0.56500981, 0.55179981])}, 'ATTACH::7::best_val_loss': 0.5517998110002547, 'ATTACH::8::history': {'train_accs': array([0.42592731, 0.47175162, 0.54101052, 0.5899694 , 0.68990223,\n",
      "       0.76363908, 0.82715128, 0.86319875, 0.89088738, 0.91148593,\n",
      "       0.92126278, 0.92999478, 0.93529368, 0.94372714, 0.95036943,\n",
      "       0.95014553, 0.95686245, 0.95790731, 0.95492201, 0.95887753,\n",
      "       0.96051944, 0.96305694, 0.96171356, 0.96850511, 0.96678857,\n",
      "       0.97044556, 0.96820658, 0.9670871 , 0.96604224, 0.97059482,\n",
      "       0.97335622, 0.972386  , 0.97208747, 0.96350474, 0.97134114]), 'train_losses': array([12.71689276,  3.28835707,  1.83508381,  1.21865015,  0.834115  ,\n",
      "        0.6192166 ,  0.4444996 ,  0.36434545,  0.29029603,  0.24119897,\n",
      "        0.21658164,  0.19645216,  0.18229589,  0.16460016,  0.14309021,\n",
      "        0.1417247 ,  0.12872341,  0.12421963,  0.13745071,  0.12878742,\n",
      "        0.1150051 ,  0.10367333,  0.11688919,  0.09321183,  0.0950673 ,\n",
      "        0.08927215,  0.09112212,  0.0962059 ,  0.10717069,  0.0901574 ,\n",
      "        0.07928489,  0.07964868,  0.07898974,  0.10866474,  0.09113433]), 'val_accs': array([0.5080597 , 0.46477612, 0.62507463, 0.68537313, 0.7561194 ,\n",
      "       0.81522388, 0.85850746, 0.89850746, 0.91940299, 0.90208955,\n",
      "       0.91970149, 0.93940299, 0.94895522, 0.9480597 , 0.9438806 ,\n",
      "       0.96      , 0.95761194, 0.94567164, 0.95044776, 0.95432836,\n",
      "       0.96119403, 0.96776119, 0.9638806 , 0.96179104, 0.96686567,\n",
      "       0.95074627, 0.97074627, 0.96925373, 0.97343284, 0.96776119,\n",
      "       0.9719403 , 0.97164179, 0.95283582, 0.96537313, 0.97044776]), 'val_losses': array([2.77934903, 2.35473034, 0.98209244, 0.68761176, 0.59909991,\n",
      "       0.4428882 , 0.3692983 , 0.25521847, 0.20629519, 0.29371259,\n",
      "       0.20206848, 0.1605745 , 0.14703332, 0.16064255, 0.14102683,\n",
      "       0.13200729, 0.1246354 , 0.2240199 , 0.15507496, 0.13479967,\n",
      "       0.11823765, 0.09565809, 0.11378902, 0.13839469, 0.09063659,\n",
      "       0.15187173, 0.09101075, 0.10256343, 0.09191764, 0.10574285,\n",
      "       0.0944747 , 0.09458563, 0.15822762, 0.10861189, 0.09197826])}, 'ATTACH::8::best_val_loss': 0.09063658644459141, 'ATTACH::9::history': {'train_accs': array([0.39062617, 0.43645048, 0.46010896, 0.46122845, 0.46175088,\n",
      "       0.46951265, 0.48175237, 0.49152922, 0.49212628, 0.48578252,\n",
      "       0.49936562, 0.49817151, 0.4960818 , 0.49675349, 0.50585865,\n",
      "       0.5066796 , 0.50496306, 0.51070975, 0.5211583 , 0.50959027,\n",
      "       0.52250168, 0.51697888, 0.51481454, 0.52638257, 0.51571013,\n",
      "       0.51354579, 0.52332264, 0.52436749, 0.52757668, 0.52190462,\n",
      "       0.52511381, 0.5289947 , 0.52309874, 0.53071125, 0.53071125,\n",
      "       0.53444287, 0.54317486, 0.53742817, 0.53272632, 0.53138294,\n",
      "       0.52742742, 0.53145757, 0.53242779, 0.52750205, 0.53802523,\n",
      "       0.54168222, 0.54227927, 0.54981715, 0.54757818, 0.54683185,\n",
      "       0.54780207, 0.55213076, 0.54683185, 0.55339951, 0.55175759,\n",
      "       0.54817524, 0.55586238, 0.55616091, 0.55877304, 0.55847451]), 'train_losses': array([13.42036551,  2.32160096,  1.61418707,  1.36075047,  1.23038899,\n",
      "        1.18627386,  1.10426658,  1.04720843,  1.0470049 ,  1.00409568,\n",
      "        1.00244354,  1.0049514 ,  0.96693002,  0.99116379,  0.96620375,\n",
      "        0.95202088,  0.9504192 ,  0.93947166,  0.93146143,  0.92851118,\n",
      "        0.91751355,  0.92446851,  0.91869306,  0.90094345,  0.90991868,\n",
      "        0.91645677,  0.91986764,  0.89434534,  0.89473416,  0.90197762,\n",
      "        0.88940992,  0.89066162,  0.88989232,  0.87726078,  0.87671512,\n",
      "        0.87583605,  0.87350551,  0.87591478,  0.87963323,  0.88095191,\n",
      "        0.86269025,  0.86608419,  0.86633773,  0.87292609,  0.85453254,\n",
      "        0.8573234 ,  0.85174684,  0.85091434,  0.8473221 ,  0.8562435 ,\n",
      "        0.8479172 ,  0.84768111,  0.8532288 ,  0.83919753,  0.84419375,\n",
      "        0.85554736,  0.83977807,  0.83951567,  0.83623289,  0.84051408]), 'val_accs': array([0.47343284, 0.52298507, 0.50686567, 0.46955224, 0.53492537,\n",
      "       0.46477612, 0.59223881, 0.53074627, 0.55940299, 0.53522388,\n",
      "       0.54298507, 0.56268657, 0.58328358, 0.55940299, 0.56238806,\n",
      "       0.55402985, 0.52149254, 0.58268657, 0.55940299, 0.61074627,\n",
      "       0.54447761, 0.58895522, 0.59343284, 0.53940299, 0.5638806 ,\n",
      "       0.57522388, 0.60119403, 0.56895522, 0.55880597, 0.57850746,\n",
      "       0.56955224, 0.55283582, 0.59522388, 0.60059701, 0.54298507,\n",
      "       0.55522388, 0.57552239, 0.5758209 , 0.59164179, 0.59074627,\n",
      "       0.62298507, 0.57731343, 0.54358209, 0.57402985, 0.56089552,\n",
      "       0.63164179, 0.62507463, 0.56567164, 0.59671642, 0.56985075,\n",
      "       0.57940299, 0.62776119, 0.57910448, 0.55223881, 0.55492537,\n",
      "       0.64597015, 0.63791045, 0.63940299, 0.63552239, 0.59373134]), 'val_losses': array([1.44998891, 0.93213847, 0.99311971, 0.98601681, 0.84511819,\n",
      "       0.90217527, 0.84647142, 0.83783819, 0.82146397, 0.88191405,\n",
      "       0.85514528, 0.83549611, 0.81912406, 0.83163677, 0.82329448,\n",
      "       0.81308891, 0.80390984, 0.80163892, 0.78951062, 0.79869721,\n",
      "       0.80932157, 0.79282365, 0.79181583, 0.85294158, 0.78044465,\n",
      "       0.79747812, 0.79177129, 0.81650767, 0.80058821, 0.78301175,\n",
      "       0.82452294, 0.78544544, 0.79669267, 0.77449772, 0.78670644,\n",
      "       0.7957868 , 0.77629517, 0.7900732 , 0.76770213, 0.77634621,\n",
      "       0.76825191, 0.77728431, 0.77059252, 0.77489227, 0.77756706,\n",
      "       0.7620151 , 0.80457578, 0.76371879, 0.76007309, 0.77038655,\n",
      "       0.75476958, 0.7558043 , 0.76159147, 0.7566121 , 0.75029215,\n",
      "       0.74211497, 0.74554599, 0.75485269, 0.74994898, 0.74569028])}, 'ATTACH::9::best_val_loss': 0.7421149697944299, 'ATTACH::10::history': {'train_accs': array([0.43562952, 0.51309799, 0.53765206, 0.54705575, 0.57586387,\n",
      "       0.6069856 , 0.67594597, 0.72505411, 0.77595343, 0.8060303 ,\n",
      "       0.84065975, 0.875737  , 0.894619  , 0.90066423, 0.92118815,\n",
      "       0.92603926, 0.93380103, 0.93551758, 0.94402567, 0.94066721,\n",
      "       0.94566759, 0.95432495, 0.95499664, 0.95969848, 0.958579  ,\n",
      "       0.95350399, 0.95634003, 0.96171356, 0.96604224, 0.96499739,\n",
      "       0.96245989, 0.96843048, 0.9611165 , 0.96447496, 0.96760952,\n",
      "       0.96925144, 0.97163967, 0.96835585, 0.97059482, 0.97477424,\n",
      "       0.97044556, 0.97462497, 0.96872901, 0.9666393 , 0.9751474 ,\n",
      "       0.97731174, 0.97313232, 0.97611762, 0.9749235 , 0.97544593,\n",
      "       0.97507277, 0.97387865]), 'train_losses': array([11.48096983,  2.66752696,  1.67754616,  1.43067349,  1.08742053,\n",
      "        0.90793591,  0.77891848,  0.65988477,  0.5493925 ,  0.47367718,\n",
      "        0.40464117,  0.33444155,  0.28465209,  0.25852034,  0.22068014,\n",
      "        0.20470378,  0.18850766,  0.18088674,  0.15978126,  0.16931527,\n",
      "        0.15440354,  0.13015841,  0.13162682,  0.11576302,  0.1183739 ,\n",
      "        0.13488587,  0.128273  ,  0.1138774 ,  0.10488052,  0.1008923 ,\n",
      "        0.11240287,  0.09392922,  0.11587643,  0.10709227,  0.09836407,\n",
      "        0.0905333 ,  0.08185131,  0.09632811,  0.08357257,  0.07880281,\n",
      "        0.08634299,  0.07601904,  0.09593771,  0.09738446,  0.07458216,\n",
      "        0.07239142,  0.07777992,  0.07002823,  0.0751608 ,  0.06839998,\n",
      "        0.07365642,  0.07960505]), 'val_accs': array([0.48776119, 0.51432836, 0.63641791, 0.6961194 , 0.66597015,\n",
      "       0.59373134, 0.65701493, 0.74955224, 0.82328358, 0.85910448,\n",
      "       0.86477612, 0.89373134, 0.89522388, 0.92955224, 0.92597015,\n",
      "       0.93432836, 0.94029851, 0.93432836, 0.95432836, 0.94537313,\n",
      "       0.93552239, 0.94865672, 0.96149254, 0.94597015, 0.95641791,\n",
      "       0.96089552, 0.9561194 , 0.95432836, 0.95791045, 0.96567164,\n",
      "       0.96447761, 0.96835821, 0.95761194, 0.95761194, 0.96925373,\n",
      "       0.97552239, 0.97044776, 0.96686567, 0.97492537, 0.9761194 ,\n",
      "       0.96985075, 0.97343284, 0.96358209, 0.97701493, 0.97164179,\n",
      "       0.97223881, 0.97432836, 0.9761194 , 0.97134328, 0.97373134,\n",
      "       0.96119403, 0.97462687]), 'val_losses': array([2.51397597, 1.78125734, 1.01889893, 0.80359958, 0.74039203,\n",
      "       0.91505669, 0.68146938, 0.60481437, 0.49891497, 0.3510019 ,\n",
      "       0.32349992, 0.29545342, 0.27137604, 0.20815602, 0.22201206,\n",
      "       0.17398469, 0.16981547, 0.18111001, 0.13270393, 0.16668359,\n",
      "       0.1866537 , 0.1518144 , 0.11596153, 0.1741717 , 0.15379914,\n",
      "       0.12358016, 0.14031373, 0.12694093, 0.11655454, 0.11600621,\n",
      "       0.12594899, 0.11246691, 0.14357581, 0.12743767, 0.10748232,\n",
      "       0.0930233 , 0.09834434, 0.10834891, 0.09087068, 0.07704611,\n",
      "       0.09206134, 0.07492105, 0.1120897 , 0.07771747, 0.08211617,\n",
      "       0.10017   , 0.08875166, 0.07751771, 0.10242616, 0.07735485,\n",
      "       0.11137559, 0.08827851])}, 'ATTACH::10::best_val_loss': 0.07492105230133035, 'ATTACH::11::history': {'train_accs': array([0.33338309, 0.33062169, 0.34793641, 0.3654004 , 0.36689305,\n",
      "       0.37928204, 0.38181954, 0.40592582, 0.40883648, 0.41868796,\n",
      "       0.42174789, 0.42182252, 0.42122546, 0.43525636, 0.44301814,\n",
      "       0.44816777, 0.44951116, 0.45988507, 0.46533323, 0.47719979,\n",
      "       0.49414135, 0.50011195, 0.51093365, 0.52018807, 0.53212926,\n",
      "       0.54063736, 0.54824987, 0.56832599, 0.56608702, 0.57750578,\n",
      "       0.59220837, 0.58855138, 0.60855288, 0.62347936, 0.62698709,\n",
      "       0.63295768, 0.64280916, 0.64557056, 0.66184044, 0.66348235,\n",
      "       0.67519964, 0.67654303, 0.68258825, 0.69885812, 0.69773864,\n",
      "       0.70609747, 0.71691917, 0.72430778, 0.72527801, 0.7337861 ,\n",
      "       0.74139861, 0.74162251, 0.7508023 , 0.75677289, 0.76960967,\n",
      "       0.75826554, 0.77834167, 0.77983432, 0.78304351, 0.78998433]), 'train_losses': array([112.9164407 ,  82.47012098,  57.08764674,  35.48643307,\n",
      "        24.00677815,  19.69209455,  16.55001363,  14.25943526,\n",
      "        12.99179955,  11.59796868,  11.02771941,  10.72210122,\n",
      "         9.65380291,   9.04028746,   8.37597635,   7.88729411,\n",
      "         7.38847708,   6.96333461,   6.66484291,   6.30327794,\n",
      "         5.79280131,   5.64969743,   5.13911909,   4.9563325 ,\n",
      "         4.78037147,   4.36314758,   4.33454971,   3.8987369 ,\n",
      "         3.89850471,   3.59988836,   3.52081105,   3.35179632,\n",
      "         3.08245035,   2.96973657,   2.8415733 ,   2.63094674,\n",
      "         2.59881617,   2.51269264,   2.43449776,   2.22574268,\n",
      "         2.19926367,   2.12331426,   2.00572486,   1.9285079 ,\n",
      "         1.8467397 ,   1.74390231,   1.64886238,   1.62634305,\n",
      "         1.56330404,   1.47142298,   1.39809933,   1.37422152,\n",
      "         1.35430809,   1.26873721,   1.21307594,   1.2801588 ,\n",
      "         1.16675779,   1.10111124,   1.14446922,   1.06827939]), 'val_accs': array([0.42865672, 0.44208955, 0.43313433, 0.45940299, 0.45044776,\n",
      "       0.40029851, 0.46298507, 0.41373134, 0.44985075, 0.41850746,\n",
      "       0.46029851, 0.46089552, 0.53552239, 0.51164179, 0.5561194 ,\n",
      "       0.53044776, 0.57014925, 0.57402985, 0.59761194, 0.58179104,\n",
      "       0.59432836, 0.60268657, 0.63343284, 0.63492537, 0.62059701,\n",
      "       0.67283582, 0.67522388, 0.60985075, 0.66626866, 0.69701493,\n",
      "       0.7119403 , 0.73343284, 0.70358209, 0.75671642, 0.69820896,\n",
      "       0.77253731, 0.7280597 , 0.74537313, 0.74238806, 0.72985075,\n",
      "       0.79402985, 0.79701493, 0.77701493, 0.80059701, 0.77791045,\n",
      "       0.82358209, 0.78865672, 0.80089552, 0.83373134, 0.8361194 ,\n",
      "       0.84716418, 0.84238806, 0.8319403 , 0.84716418, 0.84626866,\n",
      "       0.82716418, 0.85044776, 0.83910448, 0.85671642, 0.85283582]), 'val_losses': array([60.89382116, 40.38525213, 21.07685024,  6.72481187,  5.61632057,\n",
      "        4.61230294,  4.91864127,  4.18306712,  3.91721831,  3.70210769,\n",
      "        3.94204881,  3.27886181,  2.8004072 ,  3.4330589 ,  2.46061499,\n",
      "        3.05627846,  2.65241933,  2.67218797,  2.3798696 ,  2.34114229,\n",
      "        2.46578824,  2.18154441,  1.84855455,  1.88662521,  2.00415636,\n",
      "        1.68576223,  1.60333393,  1.84838707,  1.55091173,  1.41239974,\n",
      "        1.2909581 ,  1.13590809,  1.31201789,  1.05370492,  1.36749644,\n",
      "        0.94881192,  1.16594778,  1.02406811,  1.08409771,  1.08865713,\n",
      "        0.86186163,  0.79432862,  0.93641198,  0.79953122,  0.87244712,\n",
      "        0.65623416,  0.85163086,  0.69318355,  0.64169442,  0.58001139,\n",
      "        0.54407178,  0.57968687,  0.59964609,  0.51476007,  0.53580245,\n",
      "        0.58824584,  0.54613338,  0.59968752,  0.49346333,  0.49067434])}, 'ATTACH::11::best_val_loss': 0.49067433728210963, 'ATTACH::12::history': {'train_accs': array([0.41637436, 0.44630196, 0.44607807, 0.4459288 , 0.44585417,\n",
      "       0.44600343, 0.44607807, 0.44548101, 0.44533174, 0.44533174,\n",
      "       0.4457049 , 0.4459288 , 0.44622733, 0.44607807, 0.44600343,\n",
      "       0.4463766 , 0.44585417, 0.44630196, 0.44555564, 0.4459288 ,\n",
      "       0.4457049 , 0.44577954, 0.44533174, 0.44607807, 0.44577954,\n",
      "       0.44645123, 0.44727218, 0.45600418, 0.46712441, 0.48727517,\n",
      "       0.50369431, 0.51257557, 0.52153146, 0.52571087, 0.5342936 ,\n",
      "       0.53877155, 0.54287633, 0.54175685, 0.54489141, 0.54929472,\n",
      "       0.55951937, 0.56459437, 0.55966863, 0.56631092, 0.56772893,\n",
      "       0.5678782 , 0.57504291, 0.57153519, 0.5874319 , 0.60526905,\n",
      "       0.61743414, 0.62527054, 0.6290768 , 0.6371371 , 0.64810807,\n",
      "       0.65616837, 0.66340772, 0.67587133, 0.67430405, 0.68885738]), 'train_losses': array([11.22939333,  1.31659989,  1.2948835 ,  1.27917695,  1.26730844,\n",
      "        1.25809968,  1.25092118,  1.24677271,  1.24285805,  1.23915009,\n",
      "        1.23516256,  1.2319987 ,  1.22894425,  1.22681827,  1.22487842,\n",
      "        1.22225968,  1.22149407,  1.21844108,  1.218267  ,  1.21543596,\n",
      "        1.2132528 ,  1.2104058 ,  1.20787486,  1.20115832,  1.19580443,\n",
      "        1.18815287,  1.1819779 ,  1.17236147,  1.16025927,  1.15103134,\n",
      "        1.13601369,  1.12360358,  1.11032086,  1.09310304,  1.0821422 ,\n",
      "        1.06902795,  1.05367763,  1.04436768,  1.03896817,  1.0251956 ,\n",
      "        1.00926517,  0.99642708,  0.98858533,  0.97987814,  0.96941099,\n",
      "        0.95812788,  0.94769132,  0.94470068,  0.93327449,  0.91216277,\n",
      "        0.90208471,  0.88645281,  0.88088853,  0.8742253 ,  0.85670297,\n",
      "        0.83986558,  0.82624455,  0.80026586,  0.80279866,  0.77291401]), 'val_accs': array([0.44895522, 0.44895522, 0.44895522, 0.44895522, 0.44895522,\n",
      "       0.44895522, 0.44895522, 0.44895522, 0.44895522, 0.44895522,\n",
      "       0.44895522, 0.44895522, 0.44895522, 0.44895522, 0.44895522,\n",
      "       0.44895522, 0.44895522, 0.44895522, 0.44895522, 0.44895522,\n",
      "       0.44895522, 0.44895522, 0.44895522, 0.44895522, 0.44925373,\n",
      "       0.44895522, 0.46      , 0.47791045, 0.49492537, 0.50477612,\n",
      "       0.50537313, 0.46477612, 0.47134328, 0.50477612, 0.50716418,\n",
      "       0.52268657, 0.53402985, 0.51462687, 0.52208955, 0.54626866,\n",
      "       0.52507463, 0.53134328, 0.53641791, 0.5280597 , 0.55134328,\n",
      "       0.54716418, 0.56179104, 0.58716418, 0.60537313, 0.59850746,\n",
      "       0.62537313, 0.61432836, 0.63940299, 0.63731343, 0.64149254,\n",
      "       0.67164179, 0.68626866, 0.69134328, 0.68985075, 0.69910448]), 'val_losses': array([1.32443907, 1.29836076, 1.27933238, 1.26540934, 1.25482044,\n",
      "       1.24658415, 1.24029362, 1.23515236, 1.2309896 , 1.22749715,\n",
      "       1.22430006, 1.221394  , 1.2190328 , 1.21688115, 1.21463698,\n",
      "       1.21249448, 1.21029947, 1.20793478, 1.20586021, 1.20381547,\n",
      "       1.2012141 , 1.19792614, 1.19381756, 1.18886033, 1.17939472,\n",
      "       1.17551272, 1.16648706, 1.15713546, 1.1419054 , 1.1506487 ,\n",
      "       1.13511518, 1.15553361, 1.13885444, 1.08728514, 1.07270965,\n",
      "       1.05683318, 1.03411513, 1.04012772, 1.0352432 , 0.99555433,\n",
      "       1.03884853, 1.00789049, 0.9965411 , 1.00108916, 0.9567736 ,\n",
      "       0.96438139, 0.9401667 , 0.90447754, 0.90214746, 0.9134105 ,\n",
      "       0.88195632, 0.88895587, 0.86846447, 0.8485953 , 0.88395421,\n",
      "       0.81563914, 0.7907447 , 0.77808603, 0.77840755, 0.75360506])}, 'ATTACH::12::best_val_loss': 0.7536050564851334, 'ATTACH::13::history': {'train_accs': array([0.46742294, 0.55205612, 0.60997089, 0.63325621, 0.67042317,\n",
      "       0.69796253, 0.70833644, 0.72527801, 0.72393462, 0.74012986,\n",
      "       0.73893574, 0.74065229, 0.76147474, 0.76475856, 0.76863945,\n",
      "       0.77707292, 0.76632585, 0.7811777 , 0.77072916]), 'train_losses': array([7.35763196, 1.29204882, 0.85271301, 0.77825108, 0.71467778,\n",
      "       0.65582942, 0.64304564, 0.60813433, 0.60336045, 0.58195343,\n",
      "       0.57108793, 0.57938774, 0.54191492, 0.52600446, 0.52341652,\n",
      "       0.49724308, 0.52217934, 0.49374586, 0.5210588 ]), 'val_accs': array([0.61074627, 0.60447761, 0.60955224, 0.65671642, 0.61432836,\n",
      "       0.72059701, 0.71552239, 0.74179104, 0.78029851, 0.77044776,\n",
      "       0.72447761, 0.73880597, 0.74835821, 0.73731343, 0.71552239,\n",
      "       0.75820896, 0.78746269, 0.64537313, 0.6919403 ]), 'val_losses': array([1.18394974, 0.80105361, 0.79507662, 0.71137485, 0.69281396,\n",
      "       0.61119051, 0.62182824, 0.52977875, 0.48967018, 0.5951628 ,\n",
      "       0.69278063, 0.66099493, 0.60112129, 0.73402992, 0.68849192,\n",
      "       0.7133475 , 0.51239581, 0.7835789 , 0.84533675])}, 'ATTACH::13::best_val_loss': 0.489670181808187, 'ATTACH::14::history': {'train_accs': array([0.43719681, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718]), 'train_losses': array([5.21786751, 1.29723384, 1.27520512, 1.261855  , 1.25386121,\n",
      "       1.24859625, 1.24557712, 1.24314615, 1.24142191, 1.24023359,\n",
      "       1.23941415, 1.23871625, 1.23836609, 1.23794198, 1.23797112,\n",
      "       1.23754877, 1.23760691, 1.23756547, 1.23760772, 1.2371638 ,\n",
      "       1.23709715, 1.23701527, 1.2374798 , 1.23711871, 1.23695773,\n",
      "       1.23691684, 1.23736042, 1.23715381, 1.2371556 , 1.2371044 ,\n",
      "       1.23729425, 1.2368095 , 1.23727834, 1.23686265, 1.23712601,\n",
      "       1.2370449 , 1.23701803, 1.23676722, 1.23704423, 1.23703825,\n",
      "       1.23715868, 1.2370853 , 1.23677187, 1.23705341, 1.2372861 ,\n",
      "       1.23687857, 1.23722584, 1.2369096 , 1.2373911 , 1.23703788,\n",
      "       1.23685377, 1.23676733, 1.23698433, 1.23685452, 1.23714235,\n",
      "       1.2372469 , 1.23691249, 1.23709037, 1.23681915, 1.2368108 ]), 'val_accs': array([0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955]), 'val_losses': array([1.31306683, 1.28464001, 1.26767133, 1.25747631, 1.25124322,\n",
      "       1.24711161, 1.24435392, 1.24240605, 1.24099958, 1.23999019,\n",
      "       1.23926175, 1.23871272, 1.23831602, 1.23805228, 1.23781291,\n",
      "       1.23763967, 1.2375134 , 1.23742198, 1.23734837, 1.23730333,\n",
      "       1.23728927, 1.23723571, 1.23719783, 1.23718128, 1.23718794,\n",
      "       1.23715409, 1.23714251, 1.23713312, 1.23712459, 1.23711822,\n",
      "       1.23711237, 1.23711966, 1.23710132, 1.2371203 , 1.23709203,\n",
      "       1.23709036, 1.23709463, 1.23708555, 1.23707973, 1.23709103,\n",
      "       1.23707833, 1.23707256, 1.23707717, 1.23707391, 1.23706714,\n",
      "       1.23706587, 1.23706378, 1.2370631 , 1.23706381, 1.23706025,\n",
      "       1.23705938, 1.23705919, 1.23706498, 1.23705617, 1.23705673,\n",
      "       1.23705487, 1.23705357, 1.23705519, 1.23705071, 1.23704507])}, 'ATTACH::14::best_val_loss': 1.2370450688831842, 'ATTACH::15::history': {'train_accs': array([0.39667139, 0.43779387, 0.44249571, 0.44428689, 0.45548175,\n",
      "       0.45294425, 0.45070528, 0.45361594, 0.46854243, 0.47712516,\n",
      "       0.48309575, 0.49876856, 0.49854467, 0.49891783, 0.50899321,\n",
      "       0.50347041, 0.51444138, 0.51727741, 0.52354653, 0.54354803,\n",
      "       0.5570565 , 0.56063885, 0.56638555, 0.56892305, 0.57466975,\n",
      "       0.57220688, 0.58056571, 0.58564072, 0.58713337, 0.5848944 ,\n",
      "       0.58862602, 0.58616315, 0.58795432, 0.5897455 , 0.5954922 ,\n",
      "       0.601015  , 0.60108963, 0.60295544, 0.60870214, 0.60444809,\n",
      "       0.60832898, 0.61258303, 0.61220987, 0.61325472, 0.61153817,\n",
      "       0.61594149, 0.57922233, 0.59302933, 0.60064184, 0.61624002,\n",
      "       0.62415106, 0.63101724, 0.63422643, 0.63176356, 0.63751026,\n",
      "       0.64116725, 0.64974998, 0.65422793, 0.65057094, 0.64489887]), 'train_losses': array([5.50790145, 1.15282734, 1.08226546, 1.03371631, 1.00387324,\n",
      "       0.98929412, 0.96294049, 0.95472461, 0.93555026, 0.92230116,\n",
      "       0.90761138, 0.89776074, 0.89131213, 0.88803488, 0.87930241,\n",
      "       0.87801282, 0.8678524 , 0.86467965, 0.85886902, 0.8464835 ,\n",
      "       0.84215734, 0.84457802, 0.82551105, 0.82867691, 0.82124321,\n",
      "       0.82079474, 0.81208769, 0.80809939, 0.80725694, 0.81024973,\n",
      "       0.80172553, 0.79477754, 0.80915336, 0.78969098, 0.78378587,\n",
      "       0.77910997, 0.77143348, 0.77229832, 0.76534444, 0.76598915,\n",
      "       0.76735005, 0.7615381 , 0.75805999, 0.75610015, 0.75249102,\n",
      "       0.75406318, 0.78722972, 0.76400593, 0.75794055, 0.74922675,\n",
      "       0.74738421, 0.74353306, 0.73684968, 0.73376901, 0.72767488,\n",
      "       0.72988106, 0.72746646, 0.71301906, 0.72011293, 0.73116569]), 'val_accs': array([0.38477612, 0.47432836, 0.42447761, 0.44208955, 0.45402985,\n",
      "       0.50925373, 0.49552239, 0.46477612, 0.50238806, 0.51880597,\n",
      "       0.46507463, 0.53492537, 0.5680597 , 0.53074627, 0.54238806,\n",
      "       0.53134328, 0.52029851, 0.5519403 , 0.53283582, 0.57940299,\n",
      "       0.61701493, 0.61164179, 0.62238806, 0.61492537, 0.61552239,\n",
      "       0.61880597, 0.60716418, 0.61462687, 0.62447761, 0.59880597,\n",
      "       0.63014925, 0.65164179, 0.62477612, 0.62567164, 0.63373134,\n",
      "       0.62      , 0.63402985, 0.66597015, 0.65164179, 0.67701493,\n",
      "       0.65044776, 0.69014925, 0.68776119, 0.6241791 , 0.67343284,\n",
      "       0.59910448, 0.58358209, 0.61462687, 0.61880597, 0.63432836,\n",
      "       0.6719403 , 0.63402985, 0.6358209 , 0.63880597, 0.69731343,\n",
      "       0.66477612, 0.71552239, 0.68149254, 0.7038806 , 0.72716418]), 'val_losses': array([1.12759317, 1.03524236, 1.05459047, 0.96735076, 0.94249044,\n",
      "       0.91393043, 0.85699834, 0.90592757, 0.84414941, 0.82924793,\n",
      "       0.82010975, 0.81884346, 0.82236531, 0.7974989 , 0.8376784 ,\n",
      "       0.79427496, 0.79211936, 0.79504751, 0.8022579 , 0.77685157,\n",
      "       0.749182  , 0.78443048, 0.75468833, 0.74985592, 0.73224693,\n",
      "       0.72455773, 0.7322602 , 0.71439264, 0.7268444 , 0.7369697 ,\n",
      "       0.70756919, 0.70919937, 0.71414809, 0.69970738, 0.69057586,\n",
      "       0.70603317, 0.67964309, 0.68600839, 0.67274525, 0.67647524,\n",
      "       0.68951842, 0.67334793, 0.71033226, 0.68855497, 0.66999895,\n",
      "       0.71320711, 0.69581811, 0.68781584, 0.68304035, 0.66401127,\n",
      "       0.65104879, 0.66844508, 0.65656744, 0.65490732, 0.64640665,\n",
      "       0.68300199, 0.65585806, 0.64366726, 0.65388978, 0.63234457])}, 'ATTACH::15::best_val_loss': 0.63234457329138, 'ATTACH::16::history': {'train_accs': array([0.42756922, 0.44212255, 0.44219718, 0.44227181, 0.48406598,\n",
      "       0.52794985, 0.52690499, 0.53847302, 0.58519292, 0.61444884,\n",
      "       0.64646615, 0.67422942, 0.69206657, 0.7204269 , 0.73460706,\n",
      "       0.73766699, 0.76296739, 0.7728935 , 0.7869244 , 0.80177625,\n",
      "       0.81423987, 0.83297261, 0.8451377 , 0.85887006, 0.86043735,\n",
      "       0.86715427, 0.87708038, 0.88349877, 0.8861109 , 0.88917083,\n",
      "       0.9005896 , 0.90409732, 0.91148593, 0.89648481, 0.90932159,\n",
      "       0.91939697, 0.92074035, 0.92394955, 0.92029256, 0.93081573,\n",
      "       0.92678558, 0.92962161, 0.93238301, 0.93977162, 0.93484588,\n",
      "       0.93693559, 0.93805508, 0.94529442, 0.94245839, 0.94253302,\n",
      "       0.94693634, 0.94589148, 0.94924994, 0.95253377, 0.94835435,\n",
      "       0.95022017, 0.94790656, 0.95081723, 0.95596686, 0.95693708]), 'train_losses': array([26.7475853 ,  1.25756427,  1.22069464,  1.18600837,  1.14269909,\n",
      "        1.10476321,  1.08415819,  1.06432222,  0.99243581,  0.94666751,\n",
      "        0.89714718,  0.83833306,  0.79370976,  0.73635519,  0.70274476,\n",
      "        0.68521415,  0.61957409,  0.58909392,  0.55854757,  0.52997531,\n",
      "        0.49412042,  0.45870202,  0.43346376,  0.41548346,  0.40173438,\n",
      "        0.38472621,  0.36125006,  0.34379493,  0.33353064,  0.31831757,\n",
      "        0.29058983,  0.28053715,  0.25631337,  0.2961091 ,  0.25274363,\n",
      "        0.23910849,  0.23192913,  0.22004945,  0.22458292,  0.20396151,\n",
      "        0.20928705,  0.20149565,  0.19468741,  0.17738175,  0.18657252,\n",
      "        0.18192774,  0.18521036,  0.15744807,  0.16669716,  0.16148333,\n",
      "        0.15684808,  0.15752314,  0.14849839,  0.14079736,  0.15132478,\n",
      "        0.14419534,  0.14867845,  0.14264936,  0.1284228 ,  0.12693859]), 'val_accs': array([0.44208955, 0.44208955, 0.44208955, 0.44238806, 0.48746269,\n",
      "       0.54208955, 0.54895522, 0.52447761, 0.6080597 , 0.64597015,\n",
      "       0.63373134, 0.65701493, 0.7158209 , 0.72328358, 0.75432836,\n",
      "       0.75940299, 0.78149254, 0.79044776, 0.80626866, 0.82597015,\n",
      "       0.8       , 0.8438806 , 0.8438806 , 0.85820896, 0.8558209 ,\n",
      "       0.88268657, 0.88925373, 0.87552239, 0.88238806, 0.87791045,\n",
      "       0.88567164, 0.90597015, 0.87552239, 0.89910448, 0.90507463,\n",
      "       0.88507463, 0.89701493, 0.87761194, 0.90776119, 0.90985075,\n",
      "       0.91701493, 0.88597015, 0.90746269, 0.91373134, 0.93731343,\n",
      "       0.91432836, 0.92358209, 0.93402985, 0.92059701, 0.93731343,\n",
      "       0.91492537, 0.91820896, 0.92358209, 0.93044776, 0.93104478,\n",
      "       0.93701493, 0.94686567, 0.93432836, 0.94507463, 0.93671642]), 'val_losses': array([1.27856583, 1.23996455, 1.20281817, 1.18868654, 1.15187859,\n",
      "       1.07865336, 1.0502675 , 1.0624818 , 0.95840171, 0.89212359,\n",
      "       0.89252411, 0.86721613, 0.76138242, 0.7152202 , 0.65781675,\n",
      "       0.64594765, 0.58172375, 0.56538821, 0.52074788, 0.48233964,\n",
      "       0.5358217 , 0.47015962, 0.45513829, 0.42651378, 0.4434275 ,\n",
      "       0.36505798, 0.33836428, 0.38085359, 0.36749509, 0.35740061,\n",
      "       0.33699405, 0.27834703, 0.3774016 , 0.29395815, 0.29485979,\n",
      "       0.34897583, 0.31336834, 0.38630512, 0.29131953, 0.27586012,\n",
      "       0.25547745, 0.33532569, 0.27983308, 0.26444184, 0.20163139,\n",
      "       0.25625555, 0.23341352, 0.21180004, 0.25018493, 0.19731552,\n",
      "       0.25743672, 0.25197681, 0.23251123, 0.22923546, 0.21647756,\n",
      "       0.1946322 , 0.17359356, 0.18718993, 0.18120051, 0.20883186])}, 'ATTACH::16::best_val_loss': 0.17359355723679956, 'ATTACH::17::history': {'train_accs': array([0.42906187, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44548101, 0.48324502, 0.51369505,\n",
      "       0.54720502, 0.46704978, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718]), 'train_losses': array([614.75888094,   1.24536138,   1.24544398,   1.2444709 ,\n",
      "         1.24342161,   1.24591576,   1.24428571,   1.24519616,\n",
      "         1.24425104,   1.24444781,   1.24346675,   1.24303174,\n",
      "         1.24384809,   1.24252966,   1.24199301,   1.24177842,\n",
      "         1.23806742,   1.21015375,   1.15385688,   1.09382146,\n",
      "         1.03107707,   1.16091906,   1.24476356,   1.24251484,\n",
      "         1.24231534,   1.24244401,   1.24261627,   1.24267953,\n",
      "         1.24262936,   1.24257184,   1.2422407 ]), 'val_accs': array([0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.48029851, 0.52089552, 0.50119403,\n",
      "       0.57223881, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955]), 'val_losses': array([1.24506891, 1.24200425, 1.24198638, 1.24737179, 1.2544599 ,\n",
      "       1.24558252, 1.24664754, 1.24224041, 1.24365938, 1.24145314,\n",
      "       1.24253085, 1.24385392, 1.24082614, 1.24173066, 1.23933511,\n",
      "       1.23947003, 1.2391177 , 1.16867395, 1.11321932, 1.10886225,\n",
      "       1.0571007 , 1.25078987, 1.24283873, 1.24246413, 1.24371421,\n",
      "       1.24199521, 1.2419579 , 1.24189424, 1.24200745, 1.24186865,\n",
      "       1.24181921])}, 'ATTACH::17::best_val_loss': 1.0571007005492252, 'ATTACH::18::history': {'train_accs': array([0.43771923, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44204791, 0.44227181, 0.44309277, 0.44697365, 0.45734756,\n",
      "       0.47190089, 0.49040973, 0.5016046 , 0.51384432, 0.52033734,\n",
      "       0.53048735, 0.53466677, 0.54347339, 0.54772744, 0.55205612,\n",
      "       0.55526532, 0.56340025, 0.56310172, 0.5736249 , 0.57444585,\n",
      "       0.57213225, 0.58131204, 0.58407344, 0.58071498, 0.59444735,\n",
      "       0.59362639, 0.60026868, 0.60773192, 0.61340399, 0.59758191]), 'train_losses': array([10.21080038,  1.29196341,  1.27481932,  1.26261905,  1.25437534,\n",
      "        1.24779215,  1.24331862,  1.24051982,  1.23747733,  1.23538473,\n",
      "        1.23311248,  1.23152138,  1.23008634,  1.22990574,  1.22904835,\n",
      "        1.22831251,  1.22685532,  1.22595194,  1.22541965,  1.22472791,\n",
      "        1.22356106,  1.22285364,  1.22174337,  1.22101423,  1.2193321 ,\n",
      "        1.2177371 ,  1.21668911,  1.21399297,  1.21130377,  1.20910741,\n",
      "        1.20649774,  1.20207752,  1.19850992,  1.19218168,  1.18392459,\n",
      "        1.17552101,  1.16747258,  1.1588854 ,  1.14848557,  1.13855323,\n",
      "        1.12763596,  1.11796182,  1.10229662,  1.08893967,  1.08057789,\n",
      "        1.06551719,  1.05288293,  1.04544396,  1.02396576,  1.0158494 ,\n",
      "        1.01457189,  0.99559815,  0.98571187,  0.98721328,  0.96001153,\n",
      "        0.95474251,  0.94251764,  0.92784574,  0.91480976,  0.94886457]), 'val_accs': array([0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.4441791 , 0.46865672, 0.48119403,\n",
      "       0.5158209 , 0.5238806 , 0.53373134, 0.54119403, 0.54686567,\n",
      "       0.54925373, 0.54268657, 0.52238806, 0.55940299, 0.52597015,\n",
      "       0.55014925, 0.55253731, 0.57104478, 0.57313433, 0.50059701,\n",
      "       0.5838806 , 0.56716418, 0.56268657, 0.6       , 0.58447761,\n",
      "       0.60597015, 0.61373134, 0.58537313, 0.60179104, 0.5958209 ]), 'val_losses': array([1.30151593, 1.28090353, 1.26618989, 1.25580933, 1.24882094,\n",
      "       1.24361875, 1.23964823, 1.236588  , 1.23400488, 1.2320803 ,\n",
      "       1.23037135, 1.22897101, 1.22773629, 1.22676205, 1.22579188,\n",
      "       1.2248724 , 1.22404844, 1.22318334, 1.22234184, 1.22150065,\n",
      "       1.22047704, 1.21954466, 1.21845141, 1.21710102, 1.21581152,\n",
      "       1.21410693, 1.21218755, 1.21148187, 1.20831277, 1.20453062,\n",
      "       1.20082531, 1.1980168 , 1.19182608, 1.18897443, 1.17310734,\n",
      "       1.16877075, 1.15667597, 1.14798749, 1.14027785, 1.12408069,\n",
      "       1.11157567, 1.09989963, 1.11110138, 1.0663646 , 1.08720046,\n",
      "       1.05191286, 1.03816686, 1.01211694, 1.00733126, 1.10276308,\n",
      "       0.98318217, 0.99124242, 0.98921227, 0.94845381, 0.96125783,\n",
      "       0.91661305, 0.92939769, 0.93228181, 0.90887422, 0.93251197])}, 'ATTACH::18::best_val_loss': 0.9088742201363862, 'ATTACH::19::history': {'train_accs': array([0.41928502, 0.46958728, 0.48884245, 0.50921711, 0.52884544,\n",
      "       0.53399507, 0.54190611, 0.54966789, 0.55944473, 0.55735503,\n",
      "       0.55772819, 0.57228151, 0.57459512, 0.57116203, 0.57392343,\n",
      "       0.57825211, 0.58123741, 0.59093962, 0.58810359, 0.59952235,\n",
      "       0.5929547 , 0.6040003 , 0.60347787, 0.6042242 , 0.60601537,\n",
      "       0.61146354, 0.61788193, 0.60840361, 0.62347936, 0.61437421,\n",
      "       0.61026942, 0.62765878, 0.62258377, 0.62825584, 0.64094335,\n",
      "       0.62579297, 0.63086798, 0.63870438, 0.63676394, 0.64392865,\n",
      "       0.63698783, 0.64049556, 0.64079409, 0.64900366, 0.64168968,\n",
      "       0.65639227, 0.64766027, 0.65027241, 0.65206359, 0.65706396,\n",
      "       0.65534741, 0.64370475, 0.64960072, 0.64004776, 0.64960072,\n",
      "       0.65049631, 0.64945145, 0.65489962]), 'train_losses': array([5.35423838, 1.04242013, 0.97654965, 0.9585251 , 0.92263024,\n",
      "       0.91850452, 0.90477439, 0.89986363, 0.88633502, 0.89175895,\n",
      "       0.88326155, 0.86331506, 0.86237673, 0.85975583, 0.85375   ,\n",
      "       0.85086264, 0.85483237, 0.84439399, 0.83734143, 0.82875118,\n",
      "       0.83353721, 0.81418866, 0.81772465, 0.80986533, 0.81709263,\n",
      "       0.81583444, 0.79634011, 0.80977774, 0.79446303, 0.7986464 ,\n",
      "       0.81472142, 0.78938894, 0.79828482, 0.78254859, 0.76484444,\n",
      "       0.78932894, 0.77882056, 0.76983803, 0.7722727 , 0.76949452,\n",
      "       0.77300374, 0.76577721, 0.75968919, 0.76081172, 0.75681698,\n",
      "       0.74707844, 0.75832524, 0.74763395, 0.74597193, 0.74016644,\n",
      "       0.74719669, 0.77604366, 0.7548741 , 0.76604797, 0.75741338,\n",
      "       0.75640059, 0.75131966, 0.75018374]), 'val_accs': array([0.45731343, 0.51552239, 0.5680597 , 0.6161194 , 0.60059701,\n",
      "       0.59402985, 0.61910448, 0.60447761, 0.62776119, 0.59283582,\n",
      "       0.64537313, 0.64328358, 0.6480597 , 0.67910448, 0.65850746,\n",
      "       0.66149254, 0.62865672, 0.65164179, 0.64686567, 0.63970149,\n",
      "       0.66119403, 0.69820896, 0.71014925, 0.68059701, 0.66626866,\n",
      "       0.69791045, 0.69641791, 0.68716418, 0.65074627, 0.72298507,\n",
      "       0.72865672, 0.72238806, 0.71552239, 0.72507463, 0.71253731,\n",
      "       0.69313433, 0.71223881, 0.78328358, 0.7319403 , 0.72835821,\n",
      "       0.71313433, 0.74746269, 0.78298507, 0.74447761, 0.74029851,\n",
      "       0.76746269, 0.7680597 , 0.76059701, 0.75671642, 0.74089552,\n",
      "       0.76597015, 0.72716418, 0.7480597 , 0.74746269, 0.74089552,\n",
      "       0.78955224, 0.78208955, 0.74716418]), 'val_losses': array([1.0195892 , 0.91789815, 0.88001146, 0.86283473, 0.82192759,\n",
      "       0.8337877 , 0.8187202 , 0.81663969, 0.7965234 , 0.81261131,\n",
      "       0.77511491, 0.78158157, 0.77445832, 0.75593067, 0.77236283,\n",
      "       0.75734562, 0.75984747, 0.74651507, 0.73438064, 0.7244247 ,\n",
      "       0.73244353, 0.71334958, 0.71250843, 0.69620799, 0.69114488,\n",
      "       0.7141463 , 0.68779161, 0.68855142, 0.69980191, 0.671306  ,\n",
      "       0.67450456, 0.66930563, 0.66649769, 0.65700002, 0.65834562,\n",
      "       0.6502411 , 0.65773756, 0.6362473 , 0.64409565, 0.66162058,\n",
      "       0.64004356, 0.65447582, 0.62877958, 0.60893675, 0.60106875,\n",
      "       0.61032998, 0.62655078, 0.58802358, 0.61837211, 0.59243948,\n",
      "       0.60936202, 0.62766724, 0.60935082, 0.60339171, 0.61295604,\n",
      "       0.60417231, 0.61124651, 0.61819587])}, 'ATTACH::19::best_val_loss': 0.5880235794053149, 'ATTACH::20::history': {'train_accs': array([0.40249272, 0.45570565, 0.51145608, 0.54959325, 0.58302858,\n",
      "       0.61437421, 0.64310769, 0.67087096, 0.69967908, 0.72229271,\n",
      "       0.73662214, 0.74721994, 0.7620718 , 0.77237107, 0.79065602,\n",
      "       0.803045  , 0.81953877, 0.83804762, 0.84782446, 0.85648183,\n",
      "       0.875737  , 0.87499067, 0.89133517, 0.89223076, 0.89670871,\n",
      "       0.90745578, 0.9061124 , 0.91745653, 0.92111352, 0.92671095,\n",
      "       0.92827823, 0.93469662, 0.93939846, 0.9422345 , 0.94402567,\n",
      "       0.95238451, 0.95171281, 0.94790656, 0.95969848, 0.95701172,\n",
      "       0.95962385, 0.96522129, 0.96447496, 0.96171356, 0.96440033,\n",
      "       0.96954997, 0.97440107, 0.9721621 , 0.97283379, 0.9721621 ,\n",
      "       0.97559519, 0.97552056, 0.97649078, 0.97761027, 0.97649078,\n",
      "       0.97992387, 0.97410254, 0.97716247, 0.97887902, 0.98104336]), 'train_losses': array([8.02693983, 2.55992535, 1.60233968, 1.26384564, 1.01189318,\n",
      "       0.86579621, 0.79160368, 0.73034358, 0.69309033, 0.64545932,\n",
      "       0.61729288, 0.59098133, 0.56614977, 0.54277643, 0.51443084,\n",
      "       0.49756167, 0.45922831, 0.42084795, 0.41236285, 0.38490868,\n",
      "       0.33767691, 0.34021303, 0.30513833, 0.30469643, 0.29585461,\n",
      "       0.26387969, 0.26166076, 0.23842335, 0.23236388, 0.21024395,\n",
      "       0.23130508, 0.1954937 , 0.17833304, 0.176445  , 0.16718664,\n",
      "       0.14759414, 0.14972096, 0.15598119, 0.12799407, 0.13509166,\n",
      "       0.12794474, 0.11187092, 0.11040078, 0.11746439, 0.11812363,\n",
      "       0.09695434, 0.08353623, 0.08805913, 0.08591636, 0.09052326,\n",
      "       0.07604314, 0.08316616, 0.07835877, 0.07116084, 0.07503569,\n",
      "       0.05987068, 0.0897561 , 0.07019086, 0.07222601, 0.05783526]), 'val_accs': array([0.42925373, 0.60059701, 0.43462687, 0.58835821, 0.65253731,\n",
      "       0.71641791, 0.67492537, 0.71671642, 0.72149254, 0.72895522,\n",
      "       0.80477612, 0.7319403 , 0.78477612, 0.74925373, 0.82208955,\n",
      "       0.79044776, 0.81731343, 0.79402985, 0.73880597, 0.82985075,\n",
      "       0.78      , 0.79910448, 0.87671642, 0.87074627, 0.86149254,\n",
      "       0.87253731, 0.83731343, 0.89761194, 0.90298507, 0.88268657,\n",
      "       0.90447761, 0.82925373, 0.77970149, 0.86985075, 0.9238806 ,\n",
      "       0.84089552, 0.90985075, 0.88328358, 0.91880597, 0.89044776,\n",
      "       0.93104478, 0.91671642, 0.86955224, 0.94626866, 0.94029851,\n",
      "       0.9319403 , 0.91253731, 0.93074627, 0.96089552, 0.93761194,\n",
      "       0.93044776, 0.93164179, 0.93910448, 0.94208955, 0.96179104,\n",
      "       0.88925373, 0.95462687, 0.90985075, 0.96865672, 0.95402985]), 'val_losses': array([1.51271835, 1.27286093, 3.47293143, 1.07551171, 0.81670235,\n",
      "       0.65618928, 0.66743008, 0.66754632, 0.59902117, 0.61597299,\n",
      "       0.51781746, 0.56516719, 0.50663849, 0.57342141, 0.46139995,\n",
      "       0.44755153, 0.42758145, 0.4672423 , 0.65691213, 0.48258649,\n",
      "       0.53453152, 0.55397713, 0.35291908, 0.33140234, 0.36872616,\n",
      "       0.36895287, 0.39906689, 0.38896164, 0.26938074, 0.47262078,\n",
      "       0.28186569, 0.47922632, 0.90296916, 0.37251356, 0.19422106,\n",
      "       0.45607202, 0.264191  , 0.32008314, 0.28109317, 0.28921427,\n",
      "       0.24324384, 0.30281963, 0.59065039, 0.22175328, 0.15642875,\n",
      "       0.23391438, 0.34904302, 0.23350365, 0.14155547, 0.19429108,\n",
      "       0.23743029, 0.21186984, 0.18681448, 0.1866745 , 0.13584268,\n",
      "       0.45403503, 0.15377831, 0.28906592, 0.08304234, 0.13791775])}, 'ATTACH::20::best_val_loss': 0.08304234458439386, 'ATTACH::21::history': {'train_accs': array([0.43727144, 0.50914247, 0.55750429, 0.6092992 , 0.65564594,\n",
      "       0.68437943, 0.70542578, 0.73505485, 0.7537876 , 0.76513173,\n",
      "       0.7894619 , 0.79953728, 0.79908948, 0.82177774, 0.81864318,\n",
      "       0.82312113, 0.83536085, 0.83416673, 0.84357042, 0.84409284,\n",
      "       0.84155534, 0.85491455, 0.85148145, 0.85901933, 0.86521382,\n",
      "       0.86431823, 0.86819912, 0.86387044, 0.87066199, 0.87103515,\n",
      "       0.86961714, 0.87513994, 0.87521457, 0.87864766, 0.87864766,\n",
      "       0.87596089, 0.88342414, 0.88760355, 0.88454362, 0.88163296,\n",
      "       0.88170759, 0.89014106, 0.88514068, 0.89394731, 0.89073811,\n",
      "       0.89133517, 0.89603702, 0.89372341, 0.89446974, 0.89394731,\n",
      "       0.89432047]), 'train_losses': array([13.04309551,  2.53988382,  1.4073199 ,  0.97035262,  0.77977845,\n",
      "        0.69814292,  0.64667487,  0.59410467,  0.54993702,  0.53628272,\n",
      "        0.4909327 ,  0.46056457,  0.48438947,  0.42539066,  0.4248134 ,\n",
      "        0.42197372,  0.3893221 ,  0.39286802,  0.37965062,  0.37647419,\n",
      "        0.38473451,  0.34816871,  0.35738615,  0.34806298,  0.32756519,\n",
      "        0.33340051,  0.33245327,  0.335459  ,  0.32063778,  0.31611966,\n",
      "        0.32476897,  0.31691083,  0.29852338,  0.30429707,  0.29796493,\n",
      "        0.31536965,  0.2966823 ,  0.28308149,  0.29042911,  0.30344361,\n",
      "        0.29699176,  0.28123969,  0.28632838,  0.26687536,  0.28111314,\n",
      "        0.28072607,  0.26935457,  0.26911623,  0.27449275,  0.27105243,\n",
      "        0.26937403]), 'val_accs': array([0.57791045, 0.53671642, 0.53134328, 0.62268657, 0.7358209 ,\n",
      "       0.74656716, 0.7558209 , 0.78268657, 0.77313433, 0.78208955,\n",
      "       0.7319403 , 0.76537313, 0.82059701, 0.83164179, 0.80358209,\n",
      "       0.79880597, 0.77671642, 0.79074627, 0.8080597 , 0.79044776,\n",
      "       0.76656716, 0.74298507, 0.78537313, 0.81761194, 0.78298507,\n",
      "       0.69343284, 0.7558209 , 0.87492537, 0.82268657, 0.83014925,\n",
      "       0.71820896, 0.84358209, 0.83492537, 0.83671642, 0.81402985,\n",
      "       0.75343284, 0.83820896, 0.8       , 0.87701493, 0.82149254,\n",
      "       0.8841791 , 0.85761194, 0.83104478, 0.75910448, 0.83970149,\n",
      "       0.75761194, 0.81074627, 0.82179104, 0.75850746, 0.79731343,\n",
      "       0.81641791]), 'val_losses': array([1.25479586, 2.33333558, 1.28559974, 0.85355038, 0.59049131,\n",
      "       0.55591787, 0.48524844, 0.47734376, 0.48052345, 0.47290903,\n",
      "       0.55440163, 0.4908702 , 0.41578786, 0.41613141, 0.4181569 ,\n",
      "       0.47280772, 0.42946649, 0.47804436, 0.40352258, 0.45106917,\n",
      "       0.54645409, 0.56025028, 0.38995714, 0.45733788, 0.5197473 ,\n",
      "       0.62169026, 0.55929067, 0.49814996, 0.34841726, 0.41951284,\n",
      "       1.1455146 , 0.4545881 , 0.39609723, 0.3986838 , 0.43330763,\n",
      "       0.61777012, 0.43991462, 0.41910285, 0.31809891, 0.50539762,\n",
      "       0.31648226, 0.37749649, 0.39005493, 0.50548452, 0.48247191,\n",
      "       0.48814392, 0.42912545, 0.38457409, 0.62716961, 0.46002346,\n",
      "       0.46944184])}, 'ATTACH::21::best_val_loss': 0.3164822633586713, 'ATTACH::22::history': {'train_accs': array([0.36987835, 0.40659751, 0.42861408, 0.46033286, 0.48690201,\n",
      "       0.51429211, 0.52936786, 0.5623554 , 0.57989402, 0.61049332,\n",
      "       0.62497201, 0.64743638, 0.66146727, 0.68027465, 0.69311143,\n",
      "       0.71647138, 0.73527875, 0.74744384, 0.77401299, 0.79453691,\n",
      "       0.80826927, 0.81953877, 0.83521158, 0.84924248, 0.86192999,\n",
      "       0.87110978, 0.88439436, 0.8973804 , 0.9001418 , 0.91208299,\n",
      "       0.91723263, 0.93126353, 0.93044257, 0.93835361, 0.94298082,\n",
      "       0.94589148, 0.95208598, 0.95320546, 0.95693708, 0.96119113,\n",
      "       0.96126577, 0.96559445, 0.96298231, 0.97201284, 0.97163967,\n",
      "       0.97231137, 0.972386  , 0.9753713 , 0.97686395, 0.97775953,\n",
      "       0.97671468, 0.97910292, 0.98238675, 0.98007314, 0.98156579,\n",
      "       0.98358086, 0.98484962, 0.98358086, 0.98387939, 0.98656616]), 'train_losses': array([21.01754845,  9.55998102,  6.72962849,  4.98360113,  3.92080163,\n",
      "        3.16803625,  2.72626104,  2.19359483,  1.9050506 ,  1.60733357,\n",
      "        1.44721916,  1.33897587,  1.18008418,  1.10720464,  1.01047717,\n",
      "        0.90403148,  0.82986083,  0.7746689 ,  0.67467083,  0.6137987 ,\n",
      "        0.57120325,  0.52357748,  0.47912436,  0.43889107,  0.41195348,\n",
      "        0.3680727 ,  0.33193201,  0.30259293,  0.28990541,  0.25251574,\n",
      "        0.24073241,  0.20090689,  0.20645037,  0.17798699,  0.17192859,\n",
      "        0.16418271,  0.14397521,  0.1420779 ,  0.12906121,  0.11540531,\n",
      "        0.11219481,  0.09983268,  0.11943323,  0.08939043,  0.08551732,\n",
      "        0.08036854,  0.08523752,  0.07932517,  0.06928191,  0.06585324,\n",
      "        0.07359422,  0.06055714,  0.05114367,  0.06373815,  0.05661549,\n",
      "        0.05256627,  0.04832341,  0.05804411,  0.04872737,  0.04275292]), 'val_accs': array([0.41014925, 0.45970149, 0.52119403, 0.57701493, 0.59164179,\n",
      "       0.63940299, 0.65402985, 0.67134328, 0.64865672, 0.67940299,\n",
      "       0.70119403, 0.64507463, 0.55791045, 0.75850746, 0.66447761,\n",
      "       0.76298507, 0.81432836, 0.82835821, 0.79432836, 0.86149254,\n",
      "       0.7480597 , 0.84656716, 0.85791045, 0.76537313, 0.87701493,\n",
      "       0.86328358, 0.88567164, 0.89253731, 0.92597015, 0.93313433,\n",
      "       0.89791045, 0.86895522, 0.9441791 , 0.9241791 , 0.89970149,\n",
      "       0.94776119, 0.96179104, 0.9641791 , 0.94089552, 0.93283582,\n",
      "       0.93850746, 0.96776119, 0.95522388, 0.89044776, 0.96      ,\n",
      "       0.9519403 , 0.96119403, 0.97044776, 0.97253731, 0.96268657,\n",
      "       0.95492537, 0.96835821, 0.98059701, 0.96179104, 0.95641791,\n",
      "       0.9758209 , 0.9758209 , 0.9480597 , 0.97492537, 0.97373134]), 'val_losses': array([5.12987966, 2.23088978, 1.92767944, 1.25955302, 1.1010187 ,\n",
      "       1.43300068, 0.94336775, 0.82364094, 1.23205688, 0.82908256,\n",
      "       0.78305529, 1.65685109, 1.77542142, 0.69132692, 0.84055933,\n",
      "       0.68173011, 0.47075762, 0.44804204, 0.57918367, 0.42054083,\n",
      "       0.78444558, 0.42321207, 0.41876977, 0.6125445 , 0.38071762,\n",
      "       0.35843659, 0.31389255, 0.32463615, 0.218947  , 0.19799732,\n",
      "       0.29171757, 0.44993506, 0.17236935, 0.22873533, 0.31402814,\n",
      "       0.14898689, 0.13329836, 0.11780961, 0.21938866, 0.23104864,\n",
      "       0.18446056, 0.10654852, 0.12682555, 0.32714596, 0.12379096,\n",
      "       0.1619443 , 0.12198228, 0.09169437, 0.08108896, 0.11624381,\n",
      "       0.14240702, 0.10203919, 0.0714501 , 0.13485999, 0.1767266 ,\n",
      "       0.08730897, 0.08787089, 0.18340026, 0.08838928, 0.08958714])}, 'ATTACH::22::best_val_loss': 0.0714501047034317, 'ATTACH::23::history': {'train_accs': array([0.36987835, 0.3964475 , 0.42488245, 0.45234719, 0.48451377,\n",
      "       0.51085902, 0.53959251, 0.56675871, 0.58705874, 0.60131353,\n",
      "       0.62534518, 0.64579446, 0.6785581 , 0.70923203, 0.73132323,\n",
      "       0.74617509, 0.76498246, 0.77774461, 0.80147772, 0.81670274,\n",
      "       0.81991193, 0.82961415, 0.8417046 , 0.85483991, 0.8617061 ,\n",
      "       0.87446824, 0.89088738, 0.89678334, 0.9031271 , 0.90827674,\n",
      "       0.91156056, 0.92454661, 0.9252183 , 0.93335324, 0.93611464,\n",
      "       0.94171207, 0.94424957, 0.9449959 ]), 'train_losses': array([23.39266055,  9.77237358,  6.82717082,  5.11946195,  3.95949822,\n",
      "        3.33782478,  2.75139028,  2.36601276,  2.07823641,  1.78697299,\n",
      "        1.63978295,  1.45329946,  1.2721019 ,  1.11656398,  1.00671366,\n",
      "        0.91816034,  0.81510691,  0.77395992,  0.69760092,  0.62721032,\n",
      "        0.60184533,  0.56845743,  0.53231755,  0.46409152,  0.43292578,\n",
      "        0.40519128,  0.34812793,  0.32123448,  0.30757774,  0.29183742,\n",
      "        0.28920958,  0.24684548,  0.24152149,  0.2311169 ,  0.20806955,\n",
      "        0.19180962,  0.17723113,  0.18234817]), 'val_accs': array([0.30955224, 0.36597015, 0.48716418, 0.46656716, 0.56626866,\n",
      "       0.61044776, 0.6761194 , 0.67552239, 0.6319403 , 0.65492537,\n",
      "       0.73134328, 0.77522388, 0.75104478, 0.75761194, 0.81134328,\n",
      "       0.83164179, 0.75343284, 0.79970149, 0.82      , 0.84776119,\n",
      "       0.86149254, 0.87701493, 0.89373134, 0.90089552, 0.81402985,\n",
      "       0.87880597, 0.87373134, 0.92865672, 0.91970149, 0.84298507,\n",
      "       0.91641791, 0.85313433, 0.91223881, 0.90268657, 0.91880597,\n",
      "       0.90477612, 0.90716418, 0.8519403 ]), 'val_losses': array([7.20422443, 4.09904768, 2.27382169, 2.60062255, 1.74593213,\n",
      "       1.53388944, 0.95654069, 1.04768626, 1.24306318, 1.34032559,\n",
      "       0.92414339, 0.83263374, 0.78033049, 0.88298741, 0.67467313,\n",
      "       0.59865723, 0.93940041, 0.82188712, 0.63532025, 0.53189812,\n",
      "       0.44398753, 0.36810706, 0.38550058, 0.35340846, 0.61997848,\n",
      "       0.34053881, 0.31393479, 0.22850703, 0.24816292, 0.46151228,\n",
      "       0.23637337, 0.43640848, 0.23397425, 0.27143717, 0.24449634,\n",
      "       0.26871389, 0.38352096, 0.4932852 ])}, 'ATTACH::23::best_val_loss': 0.22850702849786672, 'ATTACH::24::history': {'train_accs': array([0.35875812, 0.41973282, 0.46137771, 0.4903351 , 0.52041197,\n",
      "       0.55414583, 0.5901933 , 0.61974774, 0.65452646, 0.68348384,\n",
      "       0.71012762, 0.74027913, 0.76057915, 0.78744682, 0.79797   ,\n",
      "       0.81700127, 0.83491305, 0.84737667, 0.86013882, 0.87319949,\n",
      "       0.8890962 , 0.89364878, 0.90641093, 0.91335174, 0.92200911,\n",
      "       0.92305396, 0.93633853, 0.9364878 , 0.9417867 , 0.949847  ,\n",
      "       0.95395179, 0.95529517, 0.96425106, 0.9611165 , 0.96805732,\n",
      "       0.96865438, 0.97275916, 0.97425181, 0.97425181, 0.97641615,\n",
      "       0.98141652, 0.97701321, 0.98111799, 0.98283454, 0.98156579,\n",
      "       0.98470035, 0.9832077 , 0.98462572, 0.98701396, 0.98604373,\n",
      "       0.98858124, 0.98984999, 0.98887977, 0.98783491, 0.98656616,\n",
      "       0.99089484, 0.98940219, 0.99037242, 0.99134264, 0.99014852]), 'train_losses': array([15.96160988,  7.08403782,  4.59611655,  3.39349479,  2.63927003,\n",
      "        2.22056994,  1.74903556,  1.48887535,  1.29268186,  1.08157465,\n",
      "        0.94540603,  0.8036979 ,  0.74804046,  0.64612468,  0.61394575,\n",
      "        0.55535012,  0.49885947,  0.46182006,  0.43439803,  0.39815301,\n",
      "        0.35488967,  0.3422233 ,  0.30705147,  0.27983578,  0.2528134 ,\n",
      "        0.24562115,  0.20230402,  0.19760029,  0.18595132,  0.15290185,\n",
      "        0.13911871,  0.14366092,  0.11329245,  0.12388014,  0.10228143,\n",
      "        0.10290336,  0.0924181 ,  0.09056362,  0.08423603,  0.07990398,\n",
      "        0.0629146 ,  0.07570046,  0.06467085,  0.06318625,  0.06360752,\n",
      "        0.05475162,  0.05460082,  0.0509777 ,  0.04712404,  0.04783585,\n",
      "        0.04255186,  0.03591973,  0.04235833,  0.04615028,  0.0524832 ,\n",
      "        0.03291545,  0.03626619,  0.03497154,  0.03168487,  0.03484106]), 'val_accs': array([0.56985075, 0.4841791 , 0.54537313, 0.62477612, 0.64179104,\n",
      "       0.59850746, 0.56686567, 0.72179104, 0.72208955, 0.74507463,\n",
      "       0.74656716, 0.80358209, 0.82776119, 0.81313433, 0.82865672,\n",
      "       0.82328358, 0.81761194, 0.81164179, 0.83492537, 0.88447761,\n",
      "       0.88746269, 0.87343284, 0.88955224, 0.88835821, 0.83283582,\n",
      "       0.92238806, 0.91522388, 0.90626866, 0.9280597 , 0.92835821,\n",
      "       0.94716418, 0.95044776, 0.92626866, 0.95731343, 0.95701493,\n",
      "       0.95164179, 0.94955224, 0.9561194 , 0.95880597, 0.96447761,\n",
      "       0.92597015, 0.96298507, 0.91164179, 0.94895522, 0.96238806,\n",
      "       0.96656716, 0.96746269, 0.93432836, 0.96985075, 0.97223881,\n",
      "       0.9641791 , 0.97104478, 0.95820896, 0.97223881, 0.97492537,\n",
      "       0.97402985, 0.97134328, 0.97462687, 0.96716418, 0.96865672]), 'val_losses': array([2.81275802, 2.54517204, 1.46536216, 1.09909896, 1.25024621,\n",
      "       1.53497057, 1.5989583 , 0.78531148, 0.75756642, 0.76617762,\n",
      "       0.82110356, 0.59996515, 0.50588568, 0.51073561, 0.54130044,\n",
      "       0.49797459, 0.5072867 , 0.54148288, 0.51191857, 0.32852394,\n",
      "       0.30686129, 0.3545293 , 0.34264507, 0.33559651, 0.44645084,\n",
      "       0.23433516, 0.237982  , 0.26002467, 0.19642316, 0.21171292,\n",
      "       0.15505557, 0.13990724, 0.22201387, 0.13980506, 0.1274962 ,\n",
      "       0.15259157, 0.15811985, 0.13620242, 0.11963898, 0.12173733,\n",
      "       0.23810477, 0.12049943, 0.3356007 , 0.22176455, 0.12618401,\n",
      "       0.11603233, 0.11638755, 0.23849401, 0.10720938, 0.10750988,\n",
      "       0.12170769, 0.11351068, 0.18478386, 0.10708443, 0.09870843,\n",
      "       0.10040013, 0.11438198, 0.09530169, 0.12013194, 0.11660299])}, 'ATTACH::24::best_val_loss': 0.09530168798432422, 'ATTACH::25::history': {'train_accs': array([0.33211434, 0.32756176, 0.33539816, 0.34151802, 0.3430853 ,\n",
      "       0.35390701, 0.37189342, 0.37861034, 0.38532726, 0.3907008 ,\n",
      "       0.39570117, 0.40637361, 0.41398612, 0.42077767, 0.42906187,\n",
      "       0.43107695, 0.44727218, 0.44958579, 0.45525786, 0.4601836 ,\n",
      "       0.46145235, 0.47197552, 0.4790656 , 0.47921487, 0.49384282,\n",
      "       0.50182849, 0.51847153, 0.52287484, 0.52011344, 0.52675573,\n",
      "       0.54548847, 0.54145832, 0.55280245, 0.56138518, 0.56250466,\n",
      "       0.56280319, 0.57026644, 0.57168445, 0.57519218, 0.58705874,\n",
      "       0.58608851, 0.5901933 , 0.59556683, 0.59258154, 0.60071647,\n",
      "       0.60989626, 0.60661243, 0.61541906, 0.62556907, 0.62161355,\n",
      "       0.63168893, 0.63624151, 0.6401224 , 0.63810732, 0.64295843,\n",
      "       0.63974924, 0.6569147 , 0.66154191, 0.66549743, 0.67452795]), 'train_losses': array([59.72369859, 41.19414513, 30.64063291, 24.48701814, 19.50154843,\n",
      "       16.89919772, 14.56800707, 13.14033579, 12.1870041 , 10.9680447 ,\n",
      "        9.95816523,  9.08786387,  8.36652258,  7.86480139,  7.3642848 ,\n",
      "        6.8720805 ,  6.12693334,  5.89118508,  5.78263139,  5.39514376,\n",
      "        5.11776686,  4.69424831,  4.53747441,  4.3660583 ,  4.09305261,\n",
      "        3.86125249,  3.5906281 ,  3.43989616,  3.35563916,  3.21119958,\n",
      "        2.96945619,  2.95711362,  2.75791694,  2.61612885,  2.63329026,\n",
      "        2.54957111,  2.45868878,  2.35963526,  2.27305256,  2.25321006,\n",
      "        2.06527661,  2.11700475,  2.01390869,  1.99707172,  1.87062195,\n",
      "        1.77053847,  1.83556669,  1.76525207,  1.67300695,  1.66115118,\n",
      "        1.63811953,  1.55743196,  1.53359426,  1.52662443,  1.47015889,\n",
      "        1.46746492,  1.40109147,  1.38754553,  1.31186931,  1.28651573]), 'val_accs': array([0.31253731, 0.31343284, 0.33462687, 0.33104478, 0.36567164,\n",
      "       0.43044776, 0.43432836, 0.46895522, 0.48447761, 0.45074627,\n",
      "       0.51910448, 0.54059701, 0.50716418, 0.54208955, 0.57343284,\n",
      "       0.56029851, 0.56746269, 0.56447761, 0.5761194 , 0.58208955,\n",
      "       0.60865672, 0.51283582, 0.5558209 , 0.61373134, 0.61731343,\n",
      "       0.62895522, 0.63044776, 0.61104478, 0.61104478, 0.65522388,\n",
      "       0.69850746, 0.65671642, 0.6519403 , 0.68895522, 0.68686567,\n",
      "       0.6438806 , 0.65283582, 0.66208955, 0.66955224, 0.67731343,\n",
      "       0.60656716, 0.69223881, 0.70179104, 0.68895522, 0.67074627,\n",
      "       0.72328358, 0.6838806 , 0.7241791 , 0.73940299, 0.70089552,\n",
      "       0.68626866, 0.67014925, 0.73343284, 0.73492537, 0.7119403 ,\n",
      "       0.74328358, 0.75492537, 0.72925373, 0.73701493, 0.70298507]), 'val_losses': array([22.35210526, 12.32997345,  7.28139739,  5.4243387 ,  4.77258227,\n",
      "        3.48585748,  3.30347867,  2.7415494 ,  2.64276953,  2.59619815,\n",
      "        2.0960219 ,  1.94431489,  1.91471919,  2.16909304,  1.67041379,\n",
      "        1.73891198,  1.92738264,  1.72335121,  1.68973907,  1.70662641,\n",
      "        1.53914725,  1.92478096,  1.95362216,  1.57937244,  1.66333682,\n",
      "        1.61664699,  1.60391036,  1.79488031,  1.87113272,  1.59640062,\n",
      "        1.31966218,  1.51912844,  1.46373131,  1.22071108,  1.37370561,\n",
      "        1.2060883 ,  1.44046853,  1.45711079,  1.11932877,  1.30621515,\n",
      "        1.5839679 ,  1.40887506,  1.31268117,  1.240876  ,  1.40642703,\n",
      "        0.9987335 ,  1.35589326,  1.05301656,  1.03257397,  0.89436807,\n",
      "        1.07254545,  1.11144665,  0.82095088,  0.78730001,  0.98403864,\n",
      "        0.83723449,  0.84515271,  0.91847997,  0.96898568,  1.02619449])}, 'ATTACH::25::best_val_loss': 0.787300010759439, 'ATTACH::26::history': {'train_accs': array([0.38368535, 0.40868722, 0.42786775, 0.45630271, 0.47578177,\n",
      "       0.51063512, 0.52377043, 0.55862378, 0.57019181, 0.59676095,\n",
      "       0.61288156, 0.63064408, 0.64146578, 0.66004926, 0.68878274,\n",
      "       0.71751623, 0.73363684, 0.75953429, 0.78356594, 0.79595492,\n",
      "       0.82065826, 0.83812225, 0.84543623, 0.85901933, 0.86827375,\n",
      "       0.87722964, 0.89058885, 0.89947011, 0.91454586, 0.92006866,\n",
      "       0.92992014, 0.93589074, 0.94081648, 0.94671244, 0.94596612,\n",
      "       0.95044406, 0.95663856, 0.95954922, 0.96462423, 0.96514665,\n",
      "       0.9721621 , 0.97350549, 0.97402791, 0.97880439, 0.97977461,\n",
      "       0.97664005, 0.98238675, 0.97865512, 0.98470035, 0.98649153,\n",
      "       0.98731249, 0.98432719, 0.98611837, 0.98738712, 0.98761102,\n",
      "       0.9889544 , 0.99029778, 0.98581984, 0.99052168, 0.99096948]), 'train_losses': array([25.13877664, 10.44895916,  7.25903611,  5.44750188,  4.34467335,\n",
      "        3.40956348,  2.95464601,  2.39397842,  2.09555084,  1.83866777,\n",
      "        1.68004728,  1.50041009,  1.35967946,  1.22424705,  1.06543129,\n",
      "        0.97046829,  0.87409784,  0.78238283,  0.69825853,  0.64579325,\n",
      "        0.55253335,  0.5129417 ,  0.46742934,  0.42872858,  0.39662434,\n",
      "        0.37776687,  0.33839507,  0.29908046,  0.27264426,  0.23787719,\n",
      "        0.21953234,  0.1963217 ,  0.17775412,  0.17067078,  0.16394992,\n",
      "        0.14816969,  0.13257776,  0.12760114,  0.11204077,  0.1087663 ,\n",
      "        0.08986111,  0.08274602,  0.08424412,  0.07015507,  0.06496908,\n",
      "        0.07900484,  0.05589676,  0.07770135,  0.04865209,  0.04620109,\n",
      "        0.04225105,  0.04752783,  0.04362749,  0.04207978,  0.03889999,\n",
      "        0.03772394,  0.03205179,  0.05435147,  0.03007593,  0.02770892]), 'val_accs': array([0.44567164, 0.50328358, 0.47641791, 0.51283582, 0.60925373,\n",
      "       0.58746269, 0.66208955, 0.67462687, 0.67134328, 0.63074627,\n",
      "       0.61522388, 0.69552239, 0.69074627, 0.66925373, 0.72835821,\n",
      "       0.7558209 , 0.75820896, 0.76179104, 0.83462687, 0.72895522,\n",
      "       0.85432836, 0.87074627, 0.86238806, 0.88268657, 0.87373134,\n",
      "       0.88179104, 0.88925373, 0.87880597, 0.93791045, 0.94179104,\n",
      "       0.9361194 , 0.93940299, 0.95044776, 0.87671642, 0.93074627,\n",
      "       0.93940299, 0.91014925, 0.93970149, 0.95731343, 0.94985075,\n",
      "       0.92865672, 0.93074627, 0.96119403, 0.96238806, 0.96447761,\n",
      "       0.94746269, 0.96029851, 0.97552239, 0.96776119, 0.96238806,\n",
      "       0.97432836, 0.95970149, 0.97223881, 0.97014925, 0.96298507,\n",
      "       0.96776119, 0.96149254, 0.97820896, 0.96925373, 0.95731343]), 'val_losses': array([4.12831754, 3.33693555, 2.67802584, 3.03480983, 1.81521192,\n",
      "       1.25316129, 1.19893754, 0.98567579, 1.00425569, 1.02003509,\n",
      "       1.50745506, 0.86524287, 0.87509562, 1.27492624, 0.83315621,\n",
      "       0.75566199, 0.89802565, 0.72272341, 0.49134224, 0.87764073,\n",
      "       0.43372585, 0.39729805, 0.37760123, 0.38519171, 0.36309874,\n",
      "       0.34708783, 0.31746769, 0.37152565, 0.19075053, 0.1648937 ,\n",
      "       0.20372802, 0.17908942, 0.15445936, 0.37399274, 0.19976382,\n",
      "       0.17657446, 0.2457762 , 0.16185387, 0.14025555, 0.13498604,\n",
      "       0.21162725, 0.18141361, 0.12166372, 0.10471539, 0.11016598,\n",
      "       0.16124519, 0.12394592, 0.08973968, 0.08556458, 0.11883697,\n",
      "       0.07275233, 0.1071246 , 0.08516967, 0.10780912, 0.11972987,\n",
      "       0.10058774, 0.12661433, 0.07606857, 0.09531909, 0.14277528])}, 'ATTACH::26::best_val_loss': 0.07275232570384865, 'ATTACH::27::history': {'train_accs': array([0.41898649, 0.46682588, 0.50190313, 0.51511307, 0.55138443,\n",
      "       0.57437122, 0.58989477, 0.6288529 , 0.66116874, 0.69042466,\n",
      "       0.70378386, 0.71676991, 0.73938354, 0.75453392, 0.76826629,\n",
      "       0.78162549, 0.79020822, 0.80102993, 0.81237406, 0.81229943,\n",
      "       0.82132995, 0.83274871, 0.83461452, 0.83759982, 0.84260019,\n",
      "       0.84819763, 0.85416822, 0.85722815, 0.85745205, 0.85804911,\n",
      "       0.86006418, 0.86640794, 0.86730353, 0.87208001, 0.87327412,\n",
      "       0.87006493, 0.88036421, 0.87708038, 0.87051272, 0.87760281,\n",
      "       0.87767744, 0.88267781, 0.88558848, 0.88812598, 0.88730502,\n",
      "       0.88648407, 0.88417046, 0.88685723, 0.89111128, 0.88999179,\n",
      "       0.89118591]), 'train_losses': array([17.95321323,  6.57337478,  4.2059906 ,  3.03551087,  1.98990045,\n",
      "        1.47461502,  1.15272233,  0.88285976,  0.78770609,  0.71723598,\n",
      "        0.67340161,  0.63428733,  0.59371367,  0.57398622,  0.53973837,\n",
      "        0.50559542,  0.49680845,  0.4817916 ,  0.45867566,  0.46169054,\n",
      "        0.44163409,  0.42085976,  0.41083525,  0.40270277,  0.39724971,\n",
      "        0.38598396,  0.37877552,  0.37270401,  0.35905704,  0.3725078 ,\n",
      "        0.35983125,  0.35147049,  0.34301047,  0.3347802 ,  0.32462421,\n",
      "        0.33504022,  0.30952416,  0.31409955,  0.33261892,  0.31042318,\n",
      "        0.30959041,  0.30650078,  0.29851236,  0.2909527 ,  0.28960394,\n",
      "        0.29028814,  0.29735686,  0.28667645,  0.27056468,  0.28395385,\n",
      "        0.27892877]), 'val_accs': array([0.49910448, 0.49402985, 0.54149254, 0.5280597 , 0.56776119,\n",
      "       0.5080597 , 0.6841791 , 0.69343284, 0.67343284, 0.69074627,\n",
      "       0.80089552, 0.74746269, 0.78447761, 0.83791045, 0.8319403 ,\n",
      "       0.68119403, 0.84477612, 0.81343284, 0.74507463, 0.74686567,\n",
      "       0.83880597, 0.83910448, 0.80328358, 0.77731343, 0.8358209 ,\n",
      "       0.83731343, 0.85402985, 0.85940299, 0.86746269, 0.81671642,\n",
      "       0.88597015, 0.9041791 , 0.80597015, 0.85671642, 0.85880597,\n",
      "       0.76029851, 0.84656716, 0.86179104, 0.89432836, 0.88238806,\n",
      "       0.90208955, 0.83223881, 0.87731343, 0.86835821, 0.80447761,\n",
      "       0.8641791 , 0.85492537, 0.83940299, 0.88776119, 0.8480597 ,\n",
      "       0.89044776]), 'val_losses': array([4.49098644, 4.31099472, 5.69745506, 1.5776457 , 2.68546434,\n",
      "       3.07317362, 0.86921113, 0.71454617, 0.80149726, 0.65882049,\n",
      "       0.52523131, 0.52799356, 0.51867093, 0.41352998, 0.40454663,\n",
      "       0.64703589, 0.36787631, 0.40969076, 0.6618211 , 0.47832825,\n",
      "       0.38112009, 0.38369418, 0.44412746, 0.45855804, 0.41087702,\n",
      "       0.38288443, 0.3496761 , 0.39184714, 0.35517716, 0.4113721 ,\n",
      "       0.28454037, 0.26332759, 0.40947267, 0.33389302, 0.37934074,\n",
      "       0.48899391, 0.33111611, 0.35306536, 0.28367996, 0.26624129,\n",
      "       0.26215639, 0.35786826, 0.30348144, 0.31837581, 0.41692082,\n",
      "       0.37413148, 0.43782231, 0.37423549, 0.28819669, 0.36155734,\n",
      "       0.26442184])}, 'ATTACH::27::best_val_loss': 0.26215639326109813, 'ATTACH::28::history': {'train_accs': array([0.17180387, 0.16807224, 0.24225688, 0.30830659, 0.32599448,\n",
      "       0.34853347, 0.35607135, 0.38435704, 0.38480484, 0.39980596,\n",
      "       0.40779163, 0.41219494, 0.43174864, 0.43316665, 0.4376446 ,\n",
      "       0.44257034, 0.44771998, 0.45249645, 0.4544369 , 0.47413986,\n",
      "       0.47996119, 0.48981267, 0.49242481, 0.50033585, 0.50167923,\n",
      "       0.50653034, 0.51160534, 0.52272558, 0.52212852, 0.53608478,\n",
      "       0.5292186 , 0.54332413, 0.54981715, 0.55586238, 0.5598179 ,\n",
      "       0.56631092, 0.57190835, 0.5759385 , 0.57563997, 0.58877528,\n",
      "       0.58855138, 0.59564147, 0.60362714, 0.60407493, 0.60803045,\n",
      "       0.61668781, 0.61638928, 0.62250914, 0.62721099, 0.62668856,\n",
      "       0.63497276, 0.63467423, 0.64139115, 0.63795806, 0.64937682]), 'train_losses': array([125.55520614,  67.43456445,  36.23256025,  25.95270616,\n",
      "        20.66848803,  16.41542682,  13.45815011,  10.83805305,\n",
      "         9.39785536,   8.13522353,   7.17396882,   6.21737126,\n",
      "         5.51202607,   4.96206258,   4.64461351,   4.17636613,\n",
      "         3.92658969,   3.58421215,   3.36098521,   3.04123716,\n",
      "         2.9147509 ,   2.72566873,   2.5062057 ,   2.39257434,\n",
      "         2.32453113,   2.18017862,   2.05512041,   1.94220829,\n",
      "         1.90367972,   1.79845747,   1.70949493,   1.62206614,\n",
      "         1.5796219 ,   1.49924605,   1.48232411,   1.39987245,\n",
      "         1.35782043,   1.35408342,   1.32788489,   1.27103432,\n",
      "         1.25156426,   1.19066094,   1.1484977 ,   1.13998596,\n",
      "         1.14199667,   1.0988956 ,   1.0836852 ,   1.0655123 ,\n",
      "         1.03399323,   1.03257001,   1.00236585,   1.00292256,\n",
      "         0.98083262,   0.98254525,   0.94569166]), 'val_accs': array([0.19731343, 0.2041791 , 0.43223881, 0.43283582, 0.42716418,\n",
      "       0.43402985, 0.43492537, 0.44537313, 0.45402985, 0.4919403 ,\n",
      "       0.54447761, 0.50865672, 0.5358209 , 0.53970149, 0.54537313,\n",
      "       0.55671642, 0.5958209 , 0.63253731, 0.59104478, 0.61373134,\n",
      "       0.61970149, 0.6358209 , 0.62447761, 0.65164179, 0.66208955,\n",
      "       0.60656716, 0.68716418, 0.68089552, 0.69671642, 0.68268657,\n",
      "       0.64925373, 0.70238806, 0.69223881, 0.66895522, 0.6438806 ,\n",
      "       0.66179104, 0.67791045, 0.65701493, 0.67402985, 0.65731343,\n",
      "       0.69641791, 0.67343284, 0.68      , 0.67164179, 0.68268657,\n",
      "       0.66179104, 0.6841791 , 0.67014925, 0.67253731, 0.65671642,\n",
      "       0.67910448, 0.66895522, 0.70358209, 0.70029851, 0.70149254]), 'val_losses': array([74.25673794, 27.81520858, 15.11936844, 14.07062984, 10.84410499,\n",
      "        7.74044333,  6.65008643,  5.05648341,  4.22599713,  3.05270603,\n",
      "        2.14788299,  2.08361712,  1.62502533,  1.50955745,  1.44085626,\n",
      "        1.31974528,  1.13295953,  1.0436376 ,  0.99247107,  0.95131361,\n",
      "        0.90202699,  0.92349692,  0.91426908,  0.87395214,  0.85816225,\n",
      "        0.87564009,  0.82155698,  0.80103464,  0.78953053,  0.81509087,\n",
      "        0.85638844,  0.79204237,  0.80334536,  0.8015676 ,  0.82606811,\n",
      "        0.7893393 ,  0.7240977 ,  0.80185678,  0.78151346,  0.8010572 ,\n",
      "        0.81865456,  0.70954509,  0.69964138,  0.7551316 ,  0.66767954,\n",
      "        0.73023037,  0.71454121,  0.83535632,  0.82624255,  0.72243405,\n",
      "        0.79268096,  0.84844752,  0.73374448,  0.77215703,  0.70781179])}, 'ATTACH::28::best_val_loss': 0.6676795427478961, 'ATTACH::29::history': {'train_accs': array([0.33009926, 0.37129637, 0.39279051, 0.41719531, 0.44645123,\n",
      "       0.46704978, 0.49429062, 0.50653034, 0.53018882, 0.56332562,\n",
      "       0.58795432, 0.62758415, 0.664975  , 0.69639525, 0.72878573,\n",
      "       0.75483245, 0.77050526, 0.79722367, 0.82132995, 0.83871931,\n",
      "       0.85469065, 0.86797522, 0.87730428, 0.89514143, 0.90514217,\n",
      "       0.91036645, 0.91865065, 0.92491977, 0.93260691, 0.93969699,\n",
      "       0.94402567, 0.94887678, 0.94880215, 0.95760878, 0.95865363,\n",
      "       0.96007165, 0.96328084, 0.96514665, 0.96910217, 0.9694007 ,\n",
      "       0.97044556, 0.97447571, 0.9751474 , 0.97917755, 0.9781327 ,\n",
      "       0.97716247, 0.97977461, 0.97969998, 0.98081946, 0.98417792,\n",
      "       0.98261064]), 'train_losses': array([34.93852264, 14.17090827,  8.55627861,  6.04553928,  4.58099183,\n",
      "        3.75084444,  2.99633383,  2.50987235,  2.18869263,  1.81131064,\n",
      "        1.61676293,  1.36798557,  1.18836854,  1.05822701,  0.91026301,\n",
      "        0.80983812,  0.74137266,  0.65014407,  0.57194742,  0.51236119,\n",
      "        0.46293358,  0.4058176 ,  0.36529171,  0.32231477,  0.284177  ,\n",
      "        0.27138195,  0.24244146,  0.22728815,  0.20159189,  0.18420838,\n",
      "        0.16479243,  0.15542295,  0.15265718,  0.12869127,  0.12428399,\n",
      "        0.12109463,  0.11415964,  0.10284894,  0.09748347,  0.09272663,\n",
      "        0.0870469 ,  0.07800396,  0.07775415,  0.06373929,  0.0677534 ,\n",
      "        0.0675364 ,  0.06229583,  0.06305742,  0.05732638,  0.05183121,\n",
      "        0.05418333]), 'val_accs': array([0.43910448, 0.40626866, 0.50776119, 0.40656716, 0.49343284,\n",
      "       0.55552239, 0.52119403, 0.62865672, 0.64656716, 0.68179104,\n",
      "       0.71313433, 0.76328358, 0.76089552, 0.7641791 , 0.68597015,\n",
      "       0.83880597, 0.75164179, 0.80776119, 0.85134328, 0.81373134,\n",
      "       0.75283582, 0.91074627, 0.87820896, 0.91820896, 0.91701493,\n",
      "       0.87313433, 0.93283582, 0.90776119, 0.91044776, 0.93492537,\n",
      "       0.93432836, 0.89253731, 0.95850746, 0.9038806 , 0.95014925,\n",
      "       0.95462687, 0.96208955, 0.95761194, 0.96477612, 0.95522388,\n",
      "       0.97104478, 0.96179104, 0.93761194, 0.97223881, 0.97134328,\n",
      "       0.95731343, 0.96597015, 0.96447761, 0.95880597, 0.95761194,\n",
      "       0.95074627]), 'val_losses': array([11.21923629,  3.92414272,  2.34930933,  4.76002091,  1.49799959,\n",
      "        1.57177442,  2.14117489,  1.17641077,  1.25857192,  1.03435075,\n",
      "        0.97321571,  0.75003208,  0.75815399,  0.78525717,  0.94021264,\n",
      "        0.53687983,  0.75991449,  0.57512272,  0.41602598,  0.55405684,\n",
      "        0.79890479,  0.27094242,  0.34675765,  0.24498948,  0.25006676,\n",
      "        0.36711901,  0.21024661,  0.27705046,  0.28292036,  0.18353568,\n",
      "        0.2032496 ,  0.33247827,  0.13773776,  0.29678264,  0.15730769,\n",
      "        0.13871605,  0.11718216,  0.12477665,  0.10887735,  0.13425418,\n",
      "        0.09032326,  0.12255203,  0.19477659,  0.09401839,  0.09098694,\n",
      "        0.12492659,  0.10644509,  0.11051663,  0.13480833,  0.13034228,\n",
      "        0.15993768])}, 'ATTACH::29::best_val_loss': 0.0903232555708556}\n",
      "{'ATTACH::0::history': {'train_accs': array([0.47018434, 0.53332338, 0.58944697, 0.63094261, 0.66900515,\n",
      "       0.69415628, 0.71303829, 0.73363684, 0.75244421, 0.76393761,\n",
      "       0.77662512, 0.77386372, 0.7924472 , 0.7977461 , 0.80797074,\n",
      "       0.8115531 , 0.80752295, 0.81692664, 0.82125532, 0.82297186,\n",
      "       0.8251362 , 0.82610643, 0.8340921 , 0.82939025, 0.83692813,\n",
      "       0.83528622]), 'train_losses': array([6.94135378, 1.66929667, 1.08266489, 0.85862377, 0.75270525,\n",
      "       0.67723199, 0.62312361, 0.5871213 , 0.55414816, 0.52763485,\n",
      "       0.50299649, 0.50615176, 0.47851361, 0.46086783, 0.44536016,\n",
      "       0.44081844, 0.4370246 , 0.41845544, 0.41653442, 0.41231252,\n",
      "       0.40974943, 0.40092793, 0.39523602, 0.42487857, 0.37995788,\n",
      "       0.3864721 ]), 'val_accs': array([0.44268657, 0.58328358, 0.61432836, 0.74925373, 0.72328358,\n",
      "       0.64955224, 0.76      , 0.74865672, 0.71910448, 0.78268657,\n",
      "       0.75880597, 0.74686567, 0.72597015, 0.6519403 , 0.74      ,\n",
      "       0.80656716, 0.75671642, 0.72865672, 0.77432836, 0.73880597,\n",
      "       0.71701493, 0.67671642, 0.74567164, 0.77253731, 0.70985075,\n",
      "       0.72059701]), 'val_losses': array([1.98814694, 1.13642239, 0.83661213, 0.5998493 , 0.58618414,\n",
      "       0.64853587, 0.5905516 , 0.52778764, 0.59492737, 0.49817771,\n",
      "       0.68282861, 0.60972887, 0.64657891, 0.73515379, 0.5581127 ,\n",
      "       0.4207231 , 0.48712918, 0.46591428, 0.49427405, 0.57230321,\n",
      "       0.57463492, 0.81231938, 0.55308311, 0.68717102, 0.61727916,\n",
      "       0.5280273 ])}, 'ATTACH::0::best_val_loss': 0.4207231028222326, 'ATTACH::1::history': {'train_accs': array([0.40458243, 0.44689902, 0.47107993, 0.48720054, 0.4986193 ,\n",
      "       0.52712889, 0.55899694, 0.57758042, 0.60728413, 0.63221136,\n",
      "       0.65930293, 0.6734831 , 0.69415628, 0.70400776, 0.71841182,\n",
      "       0.72468095, 0.74139861, 0.73236809, 0.7480409 , 0.75438466,\n",
      "       0.75602657, 0.77401299, 0.76796776, 0.7786402 , 0.77826703,\n",
      "       0.78423763, 0.78513322, 0.7922233 , 0.79043212, 0.80640346,\n",
      "       0.80117919, 0.80886633, 0.81028435, 0.81162773, 0.8230465 ,\n",
      "       0.83028584, 0.84506306, 0.84886932, 0.85678036, 0.86484066,\n",
      "       0.86924397, 0.87678185, 0.88036421, 0.88484215, 0.88708113,\n",
      "       0.8948429 , 0.89797746, 0.9001418 , 0.89939548, 0.90394806,\n",
      "       0.90424659, 0.9114113 , 0.91447123, 0.91768042, 0.92297933,\n",
      "       0.92305396, 0.92745727, 0.93387566, 0.93156206, 0.9415628 ]), 'train_losses': array([19.3079023 ,  7.44011423,  4.61236134,  3.32766622,  2.50600864,\n",
      "        1.8902136 ,  1.46375573,  1.2269328 ,  1.01114601,  0.88893717,\n",
      "        0.78537911,  0.74600896,  0.69788015,  0.67410564,  0.64120261,\n",
      "        0.63391399,  0.60460289,  0.60893692,  0.58209961,  0.56885932,\n",
      "        0.55745938,  0.53997641,  0.53482639,  0.5001555 ,  0.51999487,\n",
      "        0.49807849,  0.48083585,  0.48094256,  0.47649689,  0.46508178,\n",
      "        0.46169058,  0.44576853,  0.44820889,  0.45231876,  0.43288196,\n",
      "        0.42237703,  0.39150056,  0.38990555,  0.36923126,  0.36601869,\n",
      "        0.34977518,  0.33247447,  0.32008522,  0.316043  ,  0.30913528,\n",
      "        0.28563873,  0.28267199,  0.27712481,  0.27813378,  0.26598168,\n",
      "        0.26121625,  0.24385304,  0.24172824,  0.23315916,  0.22147202,\n",
      "        0.21648257,  0.19798576,  0.18590603,  0.20036687,  0.16430377]), 'val_accs': array([0.48865672, 0.45731343, 0.47910448, 0.40955224, 0.6361194 ,\n",
      "       0.62895522, 0.60537313, 0.64686567, 0.61223881, 0.73641791,\n",
      "       0.7119403 , 0.72567164, 0.74328358, 0.7638806 , 0.75761194,\n",
      "       0.75671642, 0.68985075, 0.78477612, 0.70686567, 0.79761194,\n",
      "       0.81014925, 0.82328358, 0.73104478, 0.80268657, 0.68835821,\n",
      "       0.6438806 , 0.78089552, 0.81880597, 0.7       , 0.65313433,\n",
      "       0.85313433, 0.80149254, 0.83492537, 0.81910448, 0.78626866,\n",
      "       0.82089552, 0.80328358, 0.85492537, 0.86597015, 0.86716418,\n",
      "       0.8680597 , 0.83164179, 0.8480597 , 0.86895522, 0.91850746,\n",
      "       0.89880597, 0.8441791 , 0.91104478, 0.91880597, 0.85402985,\n",
      "       0.89044776, 0.85253731, 0.91402985, 0.86537313, 0.91701493,\n",
      "       0.91462687, 0.91880597, 0.9238806 , 0.93850746, 0.9241791 ]), 'val_losses': array([4.85232753, 4.67638938, 5.75960745, 1.98237348, 0.99677914,\n",
      "       1.01981368, 1.26439868, 0.94448181, 0.91574123, 0.65362344,\n",
      "       0.62001019, 0.54552815, 0.52972884, 0.54618917, 0.54436253,\n",
      "       0.5473159 , 0.71823502, 0.48195005, 0.62888511, 0.47179271,\n",
      "       0.44028052, 0.42356708, 0.54630733, 0.4368451 , 0.64367634,\n",
      "       0.89354226, 0.47477697, 0.41418705, 0.59040567, 0.78121579,\n",
      "       0.35369696, 0.48010045, 0.38727972, 0.41747441, 0.43326297,\n",
      "       0.40738874, 0.52704508, 0.39441528, 0.38088819, 0.33462607,\n",
      "       0.32315305, 0.4416808 , 0.32275325, 0.3271361 , 0.28073666,\n",
      "       0.25925885, 0.39470638, 0.25206988, 0.25887599, 0.27621062,\n",
      "       0.31901071, 0.39606476, 0.2314437 , 0.28177205, 0.21547721,\n",
      "       0.20794632, 0.22549686, 0.20891531, 0.18411523, 0.16972513])}, 'ATTACH::1::best_val_loss': 0.16972512834107698, 'ATTACH::2::history': {'train_accs': array([0.43816703, 0.50406747, 0.51571013, 0.53250243, 0.54354803,\n",
      "       0.55369804, 0.57123666, 0.5819091 , 0.59034256, 0.60765729,\n",
      "       0.65303381, 0.68355847, 0.72483021, 0.75602657, 0.77162475,\n",
      "       0.79923875, 0.80595567, 0.82334503, 0.83886857, 0.85163072,\n",
      "       0.85946712, 0.8752892 , 0.8784984 , 0.88200612, 0.89529069,\n",
      "       0.91290395, 0.91469513, 0.92006866, 0.92365102, 0.92715874,\n",
      "       0.93066647, 0.94268229, 0.93835361, 0.9388014 , 0.94051795,\n",
      "       0.94596612, 0.95193671, 0.95320546, 0.96074334, 0.95790731,\n",
      "       0.96454959, 0.95738488, 0.96283305, 0.96186283, 0.9473095 ,\n",
      "       0.94447347, 0.95313083, 0.95663856, 0.96514665, 0.97201284,\n",
      "       0.97081872, 0.97022166, 0.97380402, 0.96148966, 0.96551981,\n",
      "       0.95917606, 0.96768416, 0.97320696, 0.97544593, 0.98111799]), 'train_losses': array([22.35229949,  1.15809793,  1.11660027,  1.07976542,  1.05214973,\n",
      "        1.02312051,  0.99237875,  0.96213921,  0.92937862,  0.89439494,\n",
      "        0.8483586 ,  0.81244845,  0.72693653,  0.66349976,  0.62281716,\n",
      "        0.55002322,  0.52866036,  0.47968104,  0.44568772,  0.41869663,\n",
      "        0.39731581,  0.35336708,  0.34636126,  0.32934295,  0.30454591,\n",
      "        0.26437154,  0.25970493,  0.24104204,  0.23338649,  0.22422094,\n",
      "        0.21164095,  0.17969946,  0.19229225,  0.18324697,  0.18017693,\n",
      "        0.16891938,  0.14335177,  0.14207287,  0.11994992,  0.12979168,\n",
      "        0.11540412,  0.13522494,  0.11734818,  0.11556886,  0.15285509,\n",
      "        0.16493524,  0.14185378,  0.13713738,  0.1107609 ,  0.08926618,\n",
      "        0.08703489,  0.08927276,  0.08052755,  0.11160679,  0.09937594,\n",
      "        0.12056652,  0.09358155,  0.07631592,  0.06889131,  0.05767778]), 'val_accs': array([0.46447761, 0.51850746, 0.51791045, 0.52835821, 0.54298507,\n",
      "       0.54477612, 0.57970149, 0.5758209 , 0.58955224, 0.63253731,\n",
      "       0.6558209 , 0.66298507, 0.73313433, 0.76298507, 0.79820896,\n",
      "       0.73970149, 0.80567164, 0.81343284, 0.8158209 , 0.84507463,\n",
      "       0.84089552, 0.85970149, 0.86925373, 0.88      , 0.89552239,\n",
      "       0.89462687, 0.89940299, 0.91492537, 0.90537313, 0.88865672,\n",
      "       0.9161194 , 0.93671642, 0.9238806 , 0.92149254, 0.91940299,\n",
      "       0.92776119, 0.92      , 0.93164179, 0.93014925, 0.92716418,\n",
      "       0.93731343, 0.93641791, 0.94447761, 0.94179104, 0.9038806 ,\n",
      "       0.91492537, 0.93970149, 0.93253731, 0.93701493, 0.94      ,\n",
      "       0.93343284, 0.93880597, 0.94328358, 0.94567164, 0.93910448,\n",
      "       0.93402985, 0.93850746, 0.95313433, 0.94746269, 0.95373134]), 'val_losses': array([1.2073924 , 1.12833391, 1.09868811, 1.06919814, 1.03935618,\n",
      "       1.02414898, 0.97207995, 0.95296075, 0.92145622, 0.86961272,\n",
      "       0.88268735, 0.829207  , 0.71939148, 0.6366942 , 0.55836695,\n",
      "       0.68114378, 0.54798504, 0.52406778, 0.51367559, 0.43270327,\n",
      "       0.44143248, 0.40082313, 0.37955526, 0.34937699, 0.32258253,\n",
      "       0.31186427, 0.31148055, 0.27729914, 0.2853828 , 0.3387477 ,\n",
      "       0.26276159, 0.21675484, 0.23758382, 0.24372738, 0.25834667,\n",
      "       0.21773898, 0.25161994, 0.22319902, 0.22055465, 0.22729867,\n",
      "       0.20340963, 0.20726048, 0.18892798, 0.20686449, 0.31602859,\n",
      "       0.27166553, 0.21949315, 0.2103283 , 0.21779068, 0.19740492,\n",
      "       0.24343934, 0.22006615, 0.18560535, 0.17662198, 0.21178969,\n",
      "       0.20108106, 0.19788319, 0.15990096, 0.16050279, 0.1640398 ])}, 'ATTACH::2::best_val_loss': 0.15990096034398718, 'ATTACH::3::history': {'train_accs': array([0.43659975, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718]), 'train_losses': array([12.85031624,  1.24097452,  1.23939451,  1.23879241,  1.23878069,\n",
      "        1.23895826,  1.238719  ,  1.23814102,  1.23885784,  1.23850109,\n",
      "        1.23849759,  1.2388916 ,  1.23863921,  1.2380494 ,  1.23875003,\n",
      "        1.23872592,  1.23871351,  1.23850077,  1.23865302,  1.23835151,\n",
      "        1.23891397,  1.23861713,  1.23858417,  1.23868602,  1.23853485,\n",
      "        1.23853917,  1.23798894,  1.2388816 ,  1.23848949,  1.23860262,\n",
      "        1.23828901,  1.23830175,  1.23821401,  1.23800215,  1.23786912,\n",
      "        1.23803038,  1.23848045,  1.23768754,  1.23755863,  1.23810271,\n",
      "        1.23820777,  1.2384791 ,  1.2381344 ,  1.23828573,  1.23819227,\n",
      "        1.23808358,  1.23794641,  1.23806885,  1.23773675,  1.23817095,\n",
      "        1.23785163,  1.2379147 ,  1.23769851,  1.23729003,  1.23701454,\n",
      "        1.23806854,  1.23754399,  1.23736951,  1.23784639,  1.23709083]), 'val_accs': array([0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955]), 'val_losses': array([1.24283324, 1.23851601, 1.23798416, 1.2379355 , 1.2378329 ,\n",
      "       1.23780813, 1.23778591, 1.23777686, 1.23778718, 1.23774712,\n",
      "       1.23772985, 1.23772872, 1.23774563, 1.23770362, 1.23769673,\n",
      "       1.2376762 , 1.23767771, 1.2376537 , 1.23764195, 1.23769384,\n",
      "       1.23763273, 1.2376039 , 1.23759386, 1.2375836 , 1.23758499,\n",
      "       1.23759203, 1.23753472, 1.23751518, 1.23750328, 1.23748131,\n",
      "       1.23746587, 1.23744873, 1.23743887, 1.2374018 , 1.23738577,\n",
      "       1.23736394, 1.23735265, 1.23732163, 1.237298  , 1.23727471,\n",
      "       1.23724326, 1.2372246 , 1.2371908 , 1.23716644, 1.23715752,\n",
      "       1.23711412, 1.23707884, 1.23705378, 1.23705183, 1.23699087,\n",
      "       1.23694046, 1.23690358, 1.23686086, 1.23686955, 1.23682546,\n",
      "       1.23672104, 1.23667194, 1.23661631, 1.2365581 , 1.23649441])}, 'ATTACH::3::best_val_loss': 1.2364944073691297, 'ATTACH::4::history': {'train_accs': array([0.28636465, 0.30270916, 0.32711396, 0.34771252, 0.3654004 ,\n",
      "       0.38353608, 0.39196955, 0.39435779, 0.40458243, 0.40159713,\n",
      "       0.41174715, 0.41144862, 0.41958355, 0.4210762 , 0.42122546,\n",
      "       0.42756922, 0.43727144, 0.43756997, 0.44406299, 0.4546608 ,\n",
      "       0.45361594, 0.45824315, 0.4712292 , 0.47361743, 0.4848123 ,\n",
      "       0.48675274, 0.48578252, 0.50287335, 0.50600791, 0.51108292,\n",
      "       0.50839615, 0.51556086, 0.52041197, 0.53205463, 0.537055  ,\n",
      "       0.54332413, 0.5455631 , 0.55728039, 0.56131055, 0.56892305,\n",
      "       0.57414732, 0.58116277, 0.57899843, 0.59899993, 0.59153668,\n",
      "       0.60989626, 0.61295619, 0.60243302, 0.6205687 , 0.62258377,\n",
      "       0.61974774, 0.63922681, 0.64363012, 0.65221285, 0.64982461,\n",
      "       0.65251138, 0.66034779, 0.66266139, 0.67005   , 0.67796104]), 'train_losses': array([179.05323455,  68.5536591 ,  44.10627077,  30.78064937,\n",
      "        23.8118577 ,  19.09814301,  16.38245776,  13.97051164,\n",
      "        12.27802534,  11.17887844,   9.82196056,   9.12442723,\n",
      "         8.2034842 ,   7.52283679,   7.10003089,   6.48228923,\n",
      "         6.23659151,   5.57294018,   5.27036582,   4.9053824 ,\n",
      "         4.71274962,   4.40321666,   4.07643049,   3.91876937,\n",
      "         3.63211487,   3.44678347,   3.34011546,   3.14511551,\n",
      "         2.95017856,   2.88088256,   2.70209248,   2.71114032,\n",
      "         2.51953028,   2.40114135,   2.24076085,   2.18356192,\n",
      "         2.11812466,   2.05317194,   1.93505319,   1.86130526,\n",
      "         1.83028991,   1.73129753,   1.71837755,   1.60574263,\n",
      "         1.62207577,   1.54236887,   1.46634402,   1.53717698,\n",
      "         1.40954931,   1.40412574,   1.39323543,   1.27698376,\n",
      "         1.25976161,   1.23728779,   1.21113859,   1.17367314,\n",
      "         1.18693579,   1.16580293,   1.11339944,   1.06461084]), 'val_accs': array([0.29522388, 0.35044776, 0.34      , 0.29791045, 0.31283582,\n",
      "       0.38925373, 0.45880597, 0.49313433, 0.46985075, 0.42955224,\n",
      "       0.45910448, 0.47253731, 0.49850746, 0.50507463, 0.53253731,\n",
      "       0.5080597 , 0.53074627, 0.50119403, 0.55791045, 0.50895522,\n",
      "       0.53253731, 0.54865672, 0.52597015, 0.54268657, 0.57134328,\n",
      "       0.56895522, 0.55223881, 0.58447761, 0.56865672, 0.56895522,\n",
      "       0.63134328, 0.61373134, 0.60656716, 0.61552239, 0.58776119,\n",
      "       0.65253731, 0.66      , 0.67522388, 0.67253731, 0.67283582,\n",
      "       0.67850746, 0.64746269, 0.67402985, 0.65522388, 0.63014925,\n",
      "       0.68895522, 0.71074627, 0.6680597 , 0.68179104, 0.70537313,\n",
      "       0.71791045, 0.69343284, 0.72268657, 0.69134328, 0.70149254,\n",
      "       0.72119403, 0.76      , 0.73164179, 0.65880597, 0.71850746]), 'val_losses': array([54.02133604, 27.73003394, 20.71069263, 15.31591949, 10.73071228,\n",
      "        6.91851518,  5.21384889,  3.76052745,  3.72823544,  3.03968488,\n",
      "        2.93515063,  2.43857892,  2.42051808,  2.26831596,  2.05234894,\n",
      "        1.74419322,  2.3818171 ,  1.94395988,  1.65689478,  1.57821376,\n",
      "        1.64270345,  1.65123345,  1.36151045,  1.38061913,  1.80156584,\n",
      "        1.69526775,  1.65865225,  1.1844298 ,  1.19068767,  1.32516728,\n",
      "        1.1920959 ,  1.13001589,  1.59613298,  1.08667078,  1.10816507,\n",
      "        0.96725759,  1.00016294,  0.9386776 ,  0.93247399,  1.1485963 ,\n",
      "        0.84597837,  1.18543669,  0.88222099,  1.08273949,  1.09207541,\n",
      "        0.83971258,  0.90898876,  0.90990512,  0.84137122,  0.93074887,\n",
      "        0.81535439,  1.08090555,  0.84829698,  0.87409229,  0.91065881,\n",
      "        0.7819655 ,  0.6442179 ,  0.70120864,  0.99044556,  0.83507001])}, 'ATTACH::4::best_val_loss': 0.6442179005181612, 'ATTACH::5::history': {'train_accs': array([0.3879394 , 0.45518322, 0.490559  , 0.50750056, 0.53511456,\n",
      "       0.55302635, 0.58661094, 0.59952235, 0.63109187, 0.66072095,\n",
      "       0.6728114 , 0.69064856, 0.70766475, 0.71751623, 0.73192029,\n",
      "       0.74221957, 0.75401149, 0.761624  , 0.77095306, 0.77752071,\n",
      "       0.77378909, 0.77804314, 0.78207329, 0.78281961, 0.78938727,\n",
      "       0.79401448, 0.79528323, 0.8009553 , 0.80938876, 0.80819464,\n",
      "       0.80789611, 0.81535936, 0.81804612, 0.8170759 , 0.82431525,\n",
      "       0.8251362 , 0.82700202, 0.83125606, 0.82901709, 0.83491305,\n",
      "       0.82976342, 0.83312187, 0.84065975, 0.83968953, 0.83968953]), 'train_losses': array([20.69114371,  6.67208513,  4.02267342,  2.89531577,  2.0299075 ,\n",
      "        1.65461153,  1.26715295,  1.09654703,  0.90761946,  0.78964184,\n",
      "        0.74399406,  0.69173973,  0.65043246,  0.63355607,  0.5945209 ,\n",
      "        0.57265886,  0.54947072,  0.53392787,  0.52135579,  0.50560055,\n",
      "        0.50922925,  0.4961409 ,  0.48595057,  0.48557277,  0.4731182 ,\n",
      "        0.46401492,  0.45320669,  0.44874152,  0.43819802,  0.43343697,\n",
      "        0.43377275,  0.41736597,  0.41275911,  0.40551572,  0.40778845,\n",
      "        0.40011234,  0.39825906,  0.38773635,  0.38147121,  0.37531702,\n",
      "        0.38192966,  0.37593209,  0.37241642,  0.36522412,  0.36673482]), 'val_accs': array([0.47223881, 0.56298507, 0.5441791 , 0.45791045, 0.52089552,\n",
      "       0.55970149, 0.66029851, 0.67253731, 0.62716418, 0.72507463,\n",
      "       0.75910448, 0.81343284, 0.74626866, 0.80358209, 0.78626866,\n",
      "       0.7961194 , 0.78597015, 0.77432836, 0.78776119, 0.81104478,\n",
      "       0.78029851, 0.82238806, 0.82955224, 0.85761194, 0.72119403,\n",
      "       0.83910448, 0.80029851, 0.80567164, 0.82268657, 0.82238806,\n",
      "       0.84686567, 0.84656716, 0.82656716, 0.81820896, 0.91432836,\n",
      "       0.87731343, 0.85552239, 0.84626866, 0.88835821, 0.83223881,\n",
      "       0.82447761, 0.85522388, 0.84985075, 0.87014925, 0.90149254]), 'val_losses': array([6.02221199, 2.02897602, 1.34260802, 2.6663379 , 2.67413252,\n",
      "       1.73075323, 1.30279856, 0.80827347, 0.85954849, 0.56936529,\n",
      "       0.58659034, 0.52611333, 0.52740939, 0.48059655, 0.49987163,\n",
      "       0.43767132, 0.51975388, 0.4871517 , 0.4607482 , 0.48852433,\n",
      "       0.49394049, 0.42191642, 0.39227538, 0.38954795, 0.6503532 ,\n",
      "       0.35720371, 0.42027879, 0.48927191, 0.43880101, 0.40274238,\n",
      "       0.32987015, 0.38852946, 0.41162082, 0.41752521, 0.3078902 ,\n",
      "       0.31983273, 0.36081226, 0.3602197 , 0.33446273, 0.34133574,\n",
      "       0.44165031, 0.33005984, 0.39257666, 0.31067094, 0.3140274 ])}, 'ATTACH::5::best_val_loss': 0.30789020052596705, 'ATTACH::6::history': {'train_accs': array([0.27793119, 0.31644153, 0.33860736, 0.33868199, 0.34853347,\n",
      "       0.36054929, 0.37271438, 0.38532726, 0.39241734, 0.4015225 ,\n",
      "       0.41585193, 0.4240615 , 0.42771849, 0.4436152 , 0.45100381,\n",
      "       0.46406448, 0.46361669, 0.47622957, 0.48690201, 0.48428987,\n",
      "       0.49921636, 0.50660497, 0.5234719 , 0.5211583 , 0.5264572 ,\n",
      "       0.5400403 , 0.53914471, 0.55250392, 0.56145981, 0.56190761,\n",
      "       0.57093813, 0.58034182, 0.59653706, 0.59086499, 0.60153743,\n",
      "       0.60735876, 0.61571759, 0.62467348, 0.62877827, 0.63474886,\n",
      "       0.64676468, 0.64952608, 0.65751175, 0.66534816, 0.6675125 ,\n",
      "       0.68102097, 0.68318531, 0.6951265 , 0.6955743 , 0.70594821,\n",
      "       0.70602284, 0.71206806, 0.71841182, 0.72975595, 0.72751698,\n",
      "       0.73333831, 0.74639898, 0.74520487, 0.74602582, 0.75438466]), 'train_losses': array([62.26600809, 33.15729004, 23.6600568 , 18.13948197, 14.32077444,\n",
      "       11.89062352, 10.25454547,  8.98620885,  8.28294824,  7.39923238,\n",
      "        6.70634128,  6.14860147,  5.80221439,  5.23451809,  4.86349762,\n",
      "        4.57043948,  4.33537749,  3.9685745 ,  3.80224292,  3.66646442,\n",
      "        3.41770691,  3.26876145,  3.02196831,  2.95071803,  2.78051327,\n",
      "        2.70891889,  2.56614085,  2.4035449 ,  2.34022661,  2.26186264,\n",
      "        2.15845741,  2.07569684,  1.99671242,  1.98105475,  1.88290485,\n",
      "        1.8035355 ,  1.73549326,  1.62656554,  1.67836311,  1.60609889,\n",
      "        1.50175287,  1.50214526,  1.42107812,  1.4043966 ,  1.35669452,\n",
      "        1.26625418,  1.26210205,  1.21676305,  1.19694097,  1.15407829,\n",
      "        1.14618677,  1.10202185,  1.07555687,  1.03145668,  1.02129374,\n",
      "        1.00644277,  0.96532511,  0.94818558,  0.92642198,  0.88134325]), 'val_accs': array([0.23850746, 0.36686567, 0.3958209 , 0.39522388, 0.43552239,\n",
      "       0.46626866, 0.47970149, 0.53522388, 0.55701493, 0.56746269,\n",
      "       0.57134328, 0.55850746, 0.55492537, 0.59492537, 0.59791045,\n",
      "       0.61134328, 0.66268657, 0.65134328, 0.6558209 , 0.65552239,\n",
      "       0.65402985, 0.6358209 , 0.66835821, 0.66746269, 0.65671642,\n",
      "       0.66      , 0.68835821, 0.66656716, 0.69880597, 0.68626866,\n",
      "       0.70447761, 0.71253731, 0.70925373, 0.72149254, 0.69970149,\n",
      "       0.71671642, 0.75402985, 0.75641791, 0.7641791 , 0.76447761,\n",
      "       0.7558209 , 0.76716418, 0.77731343, 0.71671642, 0.70268657,\n",
      "       0.78328358, 0.77731343, 0.76268657, 0.79791045, 0.76238806,\n",
      "       0.8158209 , 0.81850746, 0.82208955, 0.78716418, 0.80955224,\n",
      "       0.77791045, 0.81552239, 0.82865672, 0.82656716, 0.82059701]), 'val_losses': array([19.99730234,  8.74612384,  6.33749293,  4.35406378,  2.96596602,\n",
      "        2.97742575,  2.68321717,  2.3675436 ,  2.00492017,  1.82405173,\n",
      "        1.61683405,  1.88696956,  1.62183385,  1.39920603,  1.33721305,\n",
      "        1.30396822,  1.29173707,  1.17168755,  1.25184656,  1.0409016 ,\n",
      "        1.15328103,  1.17581891,  1.02764578,  1.24633972,  1.06222877,\n",
      "        1.00961497,  0.94246845,  1.0009329 ,  0.92307784,  0.93025258,\n",
      "        0.88165181,  0.81977602,  0.86078808,  0.85669474,  0.94502201,\n",
      "        0.910631  ,  0.73793688,  0.74421679,  0.69850812,  0.71101333,\n",
      "        0.75353446,  0.70513258,  0.65907945,  0.84555224,  0.81209869,\n",
      "        0.61904841,  0.65865098,  0.72460498,  0.60089061,  0.83946418,\n",
      "        0.54400575,  0.55024689,  0.53137311,  0.62506961,  0.5850839 ,\n",
      "        0.61501909,  0.57923739,  0.47873092,  0.51838799,  0.54478194])}, 'ATTACH::6::best_val_loss': 0.47873091608730717, 'ATTACH::7::history': {'train_accs': array([0.3709232 , 0.40891111, 0.43391298, 0.43682364, 0.44473468,\n",
      "       0.46048213, 0.4624972 , 0.46137771, 0.46451228, 0.46443764,\n",
      "       0.46518397, 0.47227405, 0.48175237, 0.4845884 , 0.48309575,\n",
      "       0.48242406, 0.48563326, 0.4873498 , 0.49130532, 0.48720054,\n",
      "       0.48772296, 0.49078289, 0.495634  , 0.48929025, 0.49503694,\n",
      "       0.4928726 , 0.50003732, 0.49876856, 0.49376819, 0.50100754,\n",
      "       0.49518621, 0.49399209, 0.49376819, 0.50182849, 0.50048511,\n",
      "       0.5039182 , 0.50421673, 0.50996343, 0.51854616, 0.51668035,\n",
      "       0.52056124, 0.52317337, 0.51399358, 0.52399433, 0.5177252 ,\n",
      "       0.53138294, 0.53003956, 0.53399507, 0.53586089, 0.52563624,\n",
      "       0.53377118, 0.5234719 , 0.53839839, 0.52772595, 0.52384506,\n",
      "       0.52362117, 0.5264572 , 0.52989029, 0.53571162, 0.52817374]), 'train_losses': array([11.38857848,  1.57028963,  1.29702267,  1.18284399,  1.14811631,\n",
      "        1.10289815,  1.0776505 ,  1.05766542,  1.02315275,  1.02555595,\n",
      "        1.00495699,  1.00059454,  0.98684821,  0.98786473,  0.97834783,\n",
      "        0.96818918,  0.96554648,  0.961759  ,  0.96238423,  0.95540355,\n",
      "        0.95917744,  0.95795855,  0.94657012,  0.9553684 ,  0.94517073,\n",
      "        0.9519511 ,  0.94456563,  0.94445423,  0.94894101,  0.94074117,\n",
      "        0.94948163,  0.94525702,  0.94912539,  0.9358658 ,  0.93773772,\n",
      "        0.93054942,  0.93448334,  0.9280816 ,  0.92393717,  0.92595622,\n",
      "        0.92962376,  0.92708508,  0.92336201,  0.9201473 ,  0.92215669,\n",
      "        0.91690171,  0.91230523,  0.91622141,  0.91133384,  0.91401444,\n",
      "        0.91126387,  0.92196003,  0.90974324,  0.90670296,  0.9115029 ,\n",
      "        0.91197308,  0.91063158,  0.91126154,  0.91207074,  0.91076465]), 'val_accs': array([0.37522388, 0.44626866, 0.48567164, 0.54298507, 0.51641791,\n",
      "       0.51134328, 0.47074627, 0.47074627, 0.48895522, 0.5038806 ,\n",
      "       0.52835821, 0.45761194, 0.45462687, 0.54      , 0.46537313,\n",
      "       0.57462687, 0.58716418, 0.50537313, 0.53104478, 0.48268657,\n",
      "       0.55402985, 0.5880597 , 0.57343284, 0.54059701, 0.52059701,\n",
      "       0.57552239, 0.55014925, 0.48328358, 0.55134328, 0.48507463,\n",
      "       0.54985075, 0.4838806 , 0.61044776, 0.61910448, 0.5558209 ,\n",
      "       0.49671642, 0.48447761, 0.49552239, 0.53432836, 0.58686567,\n",
      "       0.61940299, 0.53164179, 0.55014925, 0.53044776, 0.49373134,\n",
      "       0.56955224, 0.51014925, 0.62119403, 0.57402985, 0.58447761,\n",
      "       0.55791045, 0.56686567, 0.60179104, 0.58716418, 0.59044776,\n",
      "       0.50238806, 0.60626866, 0.63731343, 0.59223881, 0.60895522]), 'val_losses': array([1.07577685, 0.97881671, 0.99081825, 0.95612639, 0.93653017,\n",
      "       0.92707096, 0.92134716, 0.92276164, 0.8992273 , 0.89547139,\n",
      "       0.88632279, 0.88987847, 0.89512392, 0.88015872, 0.90783238,\n",
      "       0.87416412, 0.86992391, 0.87730804, 0.88048877, 0.90029432,\n",
      "       0.88240347, 0.86768849, 0.86663182, 0.86191642, 0.87308133,\n",
      "       0.85628898, 0.87538069, 0.8638714 , 0.86767603, 0.86197046,\n",
      "       0.87031223, 0.86009805, 0.86208976, 0.84985485, 0.84650572,\n",
      "       0.85592492, 0.87422554, 0.85651987, 0.85522553, 0.86617526,\n",
      "       0.83286152, 0.8444733 , 0.83747053, 0.84956695, 0.8535363 ,\n",
      "       0.84247932, 0.85420574, 0.82410343, 0.83121407, 0.83744117,\n",
      "       0.83518274, 0.83930712, 0.82253416, 0.8228828 , 0.82775841,\n",
      "       0.86873783, 0.84084662, 0.81448169, 0.83943277, 0.8223521 ])}, 'ATTACH::7::best_val_loss': 0.814481692563242, 'ATTACH::8::history': {'train_accs': array([0.35562355, 0.37540115, 0.38764087, 0.41040376, 0.42413613,\n",
      "       0.4293604 , 0.44212255, 0.44980969, 0.44988432, 0.48152847,\n",
      "       0.49630569, 0.50458989, 0.5151877 , 0.52160609, 0.53966714,\n",
      "       0.54213001, 0.54892156, 0.56451974, 0.57929696, 0.58773043,\n",
      "       0.59788044, 0.60504515, 0.61176207, 0.62019554, 0.63609225,\n",
      "       0.63064408, 0.65519815, 0.65504888, 0.66228823, 0.66572132,\n",
      "       0.67967759, 0.69042466, 0.68848422, 0.70005224, 0.70303754,\n",
      "       0.71774013, 0.71430704, 0.73475633, 0.73117397, 0.73841331,\n",
      "       0.74871259, 0.75505635, 0.76393761, 0.76169863, 0.77214718,\n",
      "       0.77393835, 0.78438689, 0.78565565, 0.79782073, 0.79953728,\n",
      "       0.80789611, 0.80744832, 0.80916486, 0.81543399, 0.82289723,\n",
      "       0.82886783, 0.82871856, 0.83677886, 0.83006194, 0.83797298]), 'train_losses': array([63.94539385, 22.5680301 , 14.46310178,  9.66851092,  7.03722984,\n",
      "        5.63897014,  4.57791264,  3.85296241,  3.31358845,  2.83873505,\n",
      "        2.53831187,  2.3378386 ,  2.13569756,  1.90464566,  1.75611722,\n",
      "        1.64650885,  1.56161415,  1.44882886,  1.38945219,  1.2598605 ,\n",
      "        1.20834678,  1.16573254,  1.11548331,  1.07755348,  1.01276446,\n",
      "        0.99112704,  0.94551562,  0.93442188,  0.91730199,  0.90383969,\n",
      "        0.85427772,  0.84879424,  0.82740822,  0.80109841,  0.78595831,\n",
      "        0.7511083 ,  0.74708147,  0.71854554,  0.71890221,  0.7057032 ,\n",
      "        0.67500547,  0.66163324,  0.65226836,  0.649189  ,  0.6343265 ,\n",
      "        0.6147853 ,  0.59873727,  0.60081768,  0.56932319,  0.55275393,\n",
      "        0.54011227,  0.53244329,  0.54367831,  0.5169587 ,  0.50112408,\n",
      "        0.47700894,  0.48292377,  0.45880284,  0.47640209,  0.46087525]), 'val_accs': array([0.41134328, 0.44328358, 0.38119403, 0.4558209 , 0.49014925,\n",
      "       0.50298507, 0.47820896, 0.52835821, 0.59313433, 0.56358209,\n",
      "       0.61701493, 0.62716418, 0.58626866, 0.59253731, 0.63402985,\n",
      "       0.63522388, 0.61791045, 0.6361194 , 0.65731343, 0.64208955,\n",
      "       0.70835821, 0.59044776, 0.7038806 , 0.74686567, 0.68567164,\n",
      "       0.72686567, 0.62208955, 0.67701493, 0.71283582, 0.71492537,\n",
      "       0.74029851, 0.71432836, 0.71283582, 0.65880597, 0.74089552,\n",
      "       0.72537313, 0.77283582, 0.7519403 , 0.78985075, 0.74626866,\n",
      "       0.76507463, 0.7680597 , 0.75910448, 0.77970149, 0.81910448,\n",
      "       0.80059701, 0.81671642, 0.77283582, 0.7958209 , 0.79283582,\n",
      "       0.81432836, 0.79701493, 0.83761194, 0.8438806 , 0.83522388,\n",
      "       0.83880597, 0.77910448, 0.84746269, 0.81492537, 0.83313433]), 'val_losses': array([10.88524269,  4.75287662,  3.28371656,  3.82948904,  1.71716122,\n",
      "        1.46013652,  1.56984651,  1.21359919,  1.26357573,  1.1512626 ,\n",
      "        1.07273967,  0.92603861,  0.8215607 ,  0.90258295,  0.96372507,\n",
      "        0.97897999,  0.91148228,  0.79198261,  0.90195017,  0.89389012,\n",
      "        0.84881137,  1.0176559 ,  0.78432633,  0.70409994,  0.74953802,\n",
      "        0.70169395,  0.75666739,  0.74582536,  0.72744059,  0.69177503,\n",
      "        0.64905871,  0.6504961 ,  0.70727496,  0.62980054,  0.69541123,\n",
      "        0.71658716,  0.64426446,  0.69577723,  0.58739173,  0.70534133,\n",
      "        0.67361054,  0.61079565,  0.5969738 ,  0.59841421,  0.52313573,\n",
      "        0.55621003,  0.54350836,  0.67959813,  0.54475624,  0.52745353,\n",
      "        0.49562563,  0.53085449,  0.46212398,  0.45276936,  0.47582531,\n",
      "        0.43582621,  0.63285519,  0.44591871,  0.52594886,  0.47540179])}, 'ATTACH::8::best_val_loss': 0.4358262139291906, 'ATTACH::9::history': {'train_accs': array([0.38301366, 0.42697216, 0.43704754, 0.4712292 , 0.49749981,\n",
      "       0.50638107, 0.52056124, 0.53974177, 0.56422121, 0.58586462,\n",
      "       0.59720875, 0.60250765, 0.6145981 , 0.62698709, 0.66527353,\n",
      "       0.70296291, 0.74871259, 0.78498395, 0.82364356, 0.84663035,\n",
      "       0.87103515, 0.89126054, 0.90663482, 0.91447123, 0.92283006,\n",
      "       0.92939772, 0.93783118, 0.94193597, 0.94148817, 0.95096649,\n",
      "       0.95133965, 0.9611165 , 0.95634003, 0.96380327, 0.96148966,\n",
      "       0.9638779 , 0.96320621, 0.96857974, 0.97186357, 0.97380402,\n",
      "       0.97074409, 0.9781327 , 0.9749235 , 0.97731174, 0.97126651,\n",
      "       0.97693858, 0.98126726, 0.97887902, 0.9829838 , 0.97955071,\n",
      "       0.98126726, 0.98373013, 0.98156579, 0.98231211, 0.98261064,\n",
      "       0.98290917, 0.98104336, 0.98350623, 0.98447645, 0.98835734]), 'train_losses': array([26.1832255 ,  7.38361743,  4.21793721,  2.78861905,  2.07220846,\n",
      "        1.66002965,  1.48974989,  1.32810618,  1.13486228,  1.00825334,\n",
      "        0.97004941,  0.92223421,  0.87323303,  0.81394866,  0.76554954,\n",
      "        0.70164   ,  0.6145054 ,  0.54568043,  0.45542335,  0.40363159,\n",
      "        0.34204724,  0.29959913,  0.25773535,  0.23863732,  0.21949376,\n",
      "        0.19815749,  0.1778866 ,  0.16952359,  0.16135346,  0.14150006,\n",
      "        0.13857544,  0.11429637,  0.12854761,  0.10511121,  0.10914001,\n",
      "        0.10378656,  0.10609049,  0.09468282,  0.07836283,  0.08156216,\n",
      "        0.08601348,  0.06640204,  0.07290399,  0.06614898,  0.07941264,\n",
      "        0.06738479,  0.05740012,  0.06165123,  0.05138988,  0.05838265,\n",
      "        0.05390769,  0.04895386,  0.05427842,  0.04993441,  0.05245748,\n",
      "        0.05370849,  0.05208124,  0.04867862,  0.04498242,  0.03791413]), 'val_accs': array([0.48895522, 0.4558209 , 0.47432836, 0.65791045, 0.60895522,\n",
      "       0.71014925, 0.53522388, 0.5841791 , 0.7       , 0.65104478,\n",
      "       0.61044776, 0.70626866, 0.70328358, 0.64447761, 0.73492537,\n",
      "       0.70208955, 0.80776119, 0.84298507, 0.87373134, 0.9038806 ,\n",
      "       0.87970149, 0.90776119, 0.92686567, 0.93074627, 0.9280597 ,\n",
      "       0.94477612, 0.9358209 , 0.95373134, 0.95253731, 0.9558209 ,\n",
      "       0.95074627, 0.95910448, 0.94298507, 0.96238806, 0.95014925,\n",
      "       0.9641791 , 0.96119403, 0.97223881, 0.95910448, 0.96328358,\n",
      "       0.96686567, 0.96865672, 0.98029851, 0.96477612, 0.96746269,\n",
      "       0.97074627, 0.98089552, 0.97701493, 0.97761194, 0.97462687,\n",
      "       0.98029851, 0.97820896, 0.9638806 , 0.97761194, 0.97253731,\n",
      "       0.97731343, 0.97701493, 0.97731343, 0.97880597, 0.98      ]), 'val_losses': array([2.34733585, 2.52001894, 1.24058271, 0.85052424, 0.87684867,\n",
      "       0.76428189, 1.53955891, 1.04859109, 0.71082747, 0.82645735,\n",
      "       0.85974367, 0.69222572, 0.67792548, 0.70373517, 0.64035638,\n",
      "       0.65039105, 0.47665651, 0.42443096, 0.3376251 , 0.2742234 ,\n",
      "       0.30343276, 0.25216552, 0.20087051, 0.18889385, 0.19763787,\n",
      "       0.14203576, 0.16766654, 0.14206471, 0.13278434, 0.12649053,\n",
      "       0.14629049, 0.11835781, 0.15071076, 0.10348229, 0.13556534,\n",
      "       0.11602079, 0.10673195, 0.07941926, 0.11709887, 0.10343073,\n",
      "       0.1020757 , 0.08109319, 0.06434001, 0.10641704, 0.09814407,\n",
      "       0.08256489, 0.06235612, 0.07386911, 0.0663878 , 0.06653051,\n",
      "       0.0522749 , 0.06529135, 0.10697829, 0.0647487 , 0.07855862,\n",
      "       0.06689449, 0.07011843, 0.06189335, 0.05920878, 0.06699118])}, 'ATTACH::9::best_val_loss': 0.05227490153636283, 'ATTACH::10::history': {'train_accs': array([0.46384059, 0.56981864, 0.62855437, 0.65519815, 0.67639376,\n",
      "       0.68564818, 0.66781103, 0.69438018, 0.70117173, 0.70161952,\n",
      "       0.71423241, 0.70176879, 0.69669378, 0.68908127, 0.6928129 ,\n",
      "       0.69072319]), 'train_losses': array([5.97748274, 0.87744114, 0.77921942, 0.73434163, 0.7031655 ,\n",
      "       0.67810402, 0.71870281, 0.66328073, 0.64641144, 0.65930316,\n",
      "       0.63656834, 0.65514649, 0.67686224, 0.67714641, 0.66435588,\n",
      "       0.68038983]), 'val_accs': array([0.5561194 , 0.64059701, 0.62119403, 0.69164179, 0.72895522,\n",
      "       0.76507463, 0.69223881, 0.68268657, 0.67134328, 0.68358209,\n",
      "       0.69522388, 0.75492537, 0.64208955, 0.63731343, 0.6561194 ,\n",
      "       0.69492537]), 'val_losses': array([0.86044056, 0.73240481, 0.67411982, 0.63292435, 0.61176688,\n",
      "       0.57268779, 0.66884119, 0.68590421, 0.61515033, 0.64101233,\n",
      "       0.66974157, 0.66334501, 0.68043824, 0.73940192, 0.72538493,\n",
      "       0.66492739])}, 'ATTACH::10::best_val_loss': 0.5726877931338638, 'ATTACH::11::history': {'train_accs': array([0.43525636, 0.5287708 , 0.57213225, 0.61086648, 0.63504739,\n",
      "       0.66131801, 0.6783342 , 0.6949026 , 0.70393313, 0.72527801,\n",
      "       0.75781775, 0.77252034, 0.79259646, 0.79334279, 0.8058064 ,\n",
      "       0.80558251, 0.81909098, 0.83140533, 0.83655497, 0.84431674,\n",
      "       0.85931786, 0.85543697, 0.86551235, 0.85095903, 0.86849765,\n",
      "       0.85476528, 0.86685574, 0.86984103, 0.88902157, 0.89267856,\n",
      "       0.88894694, 0.89596239, 0.88223002, 0.8833495 , 0.89387268,\n",
      "       0.89469363, 0.90506754, 0.90491828, 0.8948429 , 0.88834988,\n",
      "       0.89342488, 0.90484365, 0.90297783, 0.91603851, 0.90685872,\n",
      "       0.90320173, 0.90409732, 0.903351  , 0.90708262, 0.90208224,\n",
      "       0.917158  , 0.91447123, 0.92312859, 0.91514292, 0.91208299,\n",
      "       0.91245615, 0.92506904, 0.92633779, 0.91902381, 0.92342712]), 'train_losses': array([9.06363754, 1.73610885, 1.10080979, 0.89905305, 0.80843399,\n",
      "       0.7329136 , 0.70106791, 0.65904972, 0.63868701, 0.6057923 ,\n",
      "       0.56614462, 0.53106369, 0.49795576, 0.49430291, 0.47317611,\n",
      "       0.47830777, 0.43253415, 0.41891838, 0.40482689, 0.38228129,\n",
      "       0.358466  , 0.36286219, 0.33675606, 0.37131086, 0.33423983,\n",
      "       0.36109644, 0.32668846, 0.32885053, 0.28932499, 0.27727711,\n",
      "       0.29568667, 0.26895201, 0.31469794, 0.30949757, 0.27692832,\n",
      "       0.27213722, 0.25699353, 0.24663774, 0.27896272, 0.28994264,\n",
      "       0.27170818, 0.24145963, 0.25253917, 0.22477803, 0.24917911,\n",
      "       0.25708303, 0.24862447, 0.25523307, 0.246345  , 0.26814606,\n",
      "       0.22064049, 0.23034741, 0.21382875, 0.23278936, 0.23964765,\n",
      "       0.24151833, 0.20024018, 0.19578137, 0.21541609, 0.2088387 ]), 'val_accs': array([0.59253731, 0.53223881, 0.62238806, 0.62238806, 0.69731343,\n",
      "       0.69462687, 0.72597015, 0.73343284, 0.72208955, 0.77134328,\n",
      "       0.74298507, 0.79462687, 0.81761194, 0.75492537, 0.81522388,\n",
      "       0.8038806 , 0.79492537, 0.81462687, 0.84029851, 0.86      ,\n",
      "       0.78328358, 0.87671642, 0.84776119, 0.85940299, 0.86      ,\n",
      "       0.81253731, 0.87731343, 0.87671642, 0.90298507, 0.86298507,\n",
      "       0.88149254, 0.86746269, 0.8638806 , 0.88656716, 0.85850746,\n",
      "       0.9080597 , 0.90358209, 0.91313433, 0.89104478, 0.89820896,\n",
      "       0.85731343, 0.92089552, 0.8880597 , 0.89432836, 0.90507463,\n",
      "       0.92656716, 0.92268657, 0.87373134, 0.90238806, 0.88      ,\n",
      "       0.8958209 , 0.90955224, 0.91223881, 0.93313433, 0.88328358,\n",
      "       0.93134328, 0.92059701, 0.92656716, 0.92597015, 0.91373134]), 'val_losses': array([1.65202883, 1.02637202, 0.97420509, 0.76055734, 0.65707904,\n",
      "       0.69159207, 0.59930715, 0.62026474, 0.58040037, 0.52150272,\n",
      "       0.56811954, 0.51418197, 0.44951524, 0.49802371, 0.41635124,\n",
      "       0.44833739, 0.45725895, 0.42028747, 0.39464159, 0.31931853,\n",
      "       0.4736846 , 0.30681793, 0.3240573 , 0.3148607 , 0.36314382,\n",
      "       0.47604608, 0.30057713, 0.30110959, 0.25868112, 0.29680946,\n",
      "       0.32182692, 0.31498728, 0.35907925, 0.2896774 , 0.30486514,\n",
      "       0.22220804, 0.28167444, 0.21794122, 0.26757757, 0.22537855,\n",
      "       0.35156504, 0.22567721, 0.29797668, 0.25569878, 0.26471062,\n",
      "       0.19937474, 0.20351564, 0.34179596, 0.26527106, 0.30456562,\n",
      "       0.28884823, 0.23895214, 0.23645251, 0.18481081, 0.33834815,\n",
      "       0.17650044, 0.21342652, 0.19399187, 0.21157696, 0.25304729])}, 'ATTACH::11::best_val_loss': 0.17650044354929853, 'ATTACH::12::history': {'train_accs': array([0.42876334, 0.49660422, 0.52518845, 0.56571386, 0.6092992 ,\n",
      "       0.63773416, 0.66460184, 0.69624599, 0.71378461, 0.73430853,\n",
      "       0.7590865 , 0.79155161, 0.81095604, 0.82879319, 0.85185462,\n",
      "       0.85185462, 0.87163221, 0.88021494, 0.89126054, 0.89767893,\n",
      "       0.9054407 , 0.90350026, 0.91827748, 0.9277558 , 0.92924845,\n",
      "       0.93715949, 0.93566684, 0.94230913, 0.94126427, 0.94872752,\n",
      "       0.94947384, 0.95290693, 0.95611613, 0.9526084 , 0.95566833,\n",
      "       0.95678782, 0.96395253, 0.96529592, 0.96044481, 0.96596761,\n",
      "       0.96320621, 0.96678857, 0.96731099, 0.97081872, 0.97283379,\n",
      "       0.97074409, 0.97268453, 0.97320696, 0.97268453]), 'train_losses': array([12.09385067,  3.1450828 ,  1.95558771,  1.28194335,  0.96075856,\n",
      "        0.81939129,  0.74534727,  0.66183696,  0.6272236 ,  0.59800101,\n",
      "        0.55728548,  0.49847285,  0.44825555,  0.41152694,  0.36980409,\n",
      "        0.36592057,  0.32526333,  0.3050235 ,  0.2841273 ,  0.27129364,\n",
      "        0.25613806,  0.26200592,  0.22204234,  0.20136999,  0.20555265,\n",
      "        0.18550158,  0.18334666,  0.16363811,  0.17146038,  0.15373606,\n",
      "        0.14594547,  0.13598027,  0.12844159,  0.14918899,  0.13635344,\n",
      "        0.12201562,  0.12239038,  0.11398385,  0.11266636,  0.10481421,\n",
      "        0.10788995,  0.10171776,  0.10135928,  0.08997882,  0.08819075,\n",
      "        0.09079797,  0.08451643,  0.08886235,  0.08395983]), 'val_accs': array([0.53492537, 0.59970149, 0.56537313, 0.66716418, 0.70835821,\n",
      "       0.73462687, 0.70895522, 0.74925373, 0.78      , 0.74119403,\n",
      "       0.7558209 , 0.79731343, 0.83522388, 0.85791045, 0.87432836,\n",
      "       0.88119403, 0.86686567, 0.87850746, 0.93880597, 0.8919403 ,\n",
      "       0.90626866, 0.88537313, 0.92925373, 0.93164179, 0.92746269,\n",
      "       0.9319403 , 0.94925373, 0.95432836, 0.94835821, 0.95910448,\n",
      "       0.9480597 , 0.92865672, 0.96955224, 0.9519403 , 0.97104478,\n",
      "       0.96985075, 0.97671642, 0.93880597, 0.97283582, 0.96955224,\n",
      "       0.96626866, 0.96179104, 0.95402985, 0.96149254, 0.92149254,\n",
      "       0.93970149, 0.95104478, 0.97492537, 0.96686567]), 'val_losses': array([1.63327027, 1.06255492, 1.41029896, 0.99125619, 0.66623692,\n",
      "       0.61141667, 0.59908493, 0.50965462, 0.48873197, 0.53627682,\n",
      "       0.56714358, 0.39652505, 0.42232126, 0.38504349, 0.30147085,\n",
      "       0.29681086, 0.32523444, 0.27488314, 0.18896845, 0.30227824,\n",
      "       0.23306018, 0.27469869, 0.18017496, 0.18355687, 0.21399527,\n",
      "       0.18723556, 0.1417893 , 0.14526753, 0.12214433, 0.13217278,\n",
      "       0.15001234, 0.19997409, 0.09101014, 0.13793158, 0.09741186,\n",
      "       0.09291484, 0.08938814, 0.16791757, 0.07696341, 0.0951937 ,\n",
      "       0.08723502, 0.12788195, 0.15789817, 0.10523937, 0.23819713,\n",
      "       0.19685221, 0.12185437, 0.08164969, 0.09887206])}, 'ATTACH::12::best_val_loss': 0.07696341358681223, 'ATTACH::13::history': {'train_accs': array([0.43271886, 0.50279872, 0.56563923, 0.59474588, 0.62004627,\n",
      "       0.6596761 , 0.67549817, 0.697664  , 0.7006493 , 0.71199343,\n",
      "       0.71632211, 0.7257258 , 0.73080081, 0.73781625, 0.73856258,\n",
      "       0.73617434, 0.74095082, 0.7475931 , 0.75117546, 0.74169714,\n",
      "       0.75401149, 0.77125159, 0.76266886, 0.75550414, 0.75639973,\n",
      "       0.76893798, 0.77162475, 0.76617658, 0.76722143, 0.77184865,\n",
      "       0.76348981]), 'train_losses': array([9.65071879, 1.61496097, 1.07204892, 0.93839375, 0.81616299,\n",
      "       0.73890854, 0.71415479, 0.67103551, 0.65407813, 0.62043895,\n",
      "       0.62919223, 0.60025899, 0.59069472, 0.58228796, 0.56361496,\n",
      "       0.57263981, 0.54677753, 0.55097919, 0.53991961, 0.5647651 ,\n",
      "       0.52972635, 0.50475755, 0.53007652, 0.53734722, 0.53609536,\n",
      "       0.5161804 , 0.51087159, 0.52027877, 0.51417872, 0.50591777,\n",
      "       0.52127086]), 'val_accs': array([0.54507463, 0.60238806, 0.6841791 , 0.57134328, 0.70567164,\n",
      "       0.6958209 , 0.66358209, 0.7119403 , 0.66716418, 0.72776119,\n",
      "       0.7480597 , 0.66955224, 0.70865672, 0.78328358, 0.68985075,\n",
      "       0.69850746, 0.70238806, 0.71701493, 0.70776119, 0.70089552,\n",
      "       0.7958209 , 0.71253731, 0.74029851, 0.5958209 , 0.70895522,\n",
      "       0.73970149, 0.69223881, 0.72179104, 0.67164179, 0.79910448,\n",
      "       0.68358209]), 'val_losses': array([1.61215827, 1.15165423, 0.73360956, 0.96976257, 0.66123269,\n",
      "       0.66868297, 0.65257597, 0.62599122, 0.66549891, 0.58588544,\n",
      "       0.56491815, 0.68325766, 0.63096689, 0.52700236, 0.63548566,\n",
      "       0.62592646, 0.69247783, 0.59574006, 0.65408115, 0.64245927,\n",
      "       0.50424129, 0.66940069, 0.61061371, 0.87106615, 0.62780672,\n",
      "       0.62097003, 0.69710951, 0.62567788, 0.6591544 , 0.54149777,\n",
      "       0.69126967])}, 'ATTACH::13::best_val_loss': 0.5042412868898306, 'ATTACH::14::history': {'train_accs': array([0.25382491, 0.33883126, 0.3603254 , 0.37517725, 0.38786477,\n",
      "       0.38592432, 0.39667139, 0.40085081, 0.40562729, 0.40644824,\n",
      "       0.412792  , 0.42413613, 0.42413613, 0.42480782, 0.4293604 ,\n",
      "       0.43279349, 0.43227107, 0.43868945, 0.43883872, 0.44451078,\n",
      "       0.45861631, 0.46839316, 0.46540787, 0.47667736, 0.48294649,\n",
      "       0.4903351 , 0.49294723, 0.50862005, 0.50996343, 0.51974028,\n",
      "       0.5234719 , 0.53078588, 0.53563699, 0.54287633, 0.55339951,\n",
      "       0.55436973, 0.57004254, 0.5731771 , 0.58056571, 0.59116352,\n",
      "       0.59676095, 0.60250765, 0.60683633, 0.61750877, 0.62265841,\n",
      "       0.62750952, 0.63004702, 0.63698783, 0.64751101, 0.64370475,\n",
      "       0.65325771, 0.66004926, 0.67318457, 0.67564744, 0.68243899,\n",
      "       0.68482723, 0.69990298, 0.69945518, 0.70072393, 0.70997836]), 'train_losses': array([112.47402093,  53.47023205,  40.51257504,  32.10369686,\n",
      "        26.49047202,  23.33046925,  19.1310783 ,  17.14616282,\n",
      "        15.00423827,  13.78557944,  12.30399837,  10.74028753,\n",
      "        10.18897932,   9.11654398,   8.43765233,   7.91455384,\n",
      "         7.42050484,   6.94791778,   6.47949561,   6.11784304,\n",
      "         5.79607604,   5.4665101 ,   4.99782097,   4.67931734,\n",
      "         4.41089327,   4.11987309,   3.96424007,   3.80122482,\n",
      "         3.54669541,   3.44116715,   3.28602529,   2.92395097,\n",
      "         2.84775366,   2.74997132,   2.62548281,   2.60442039,\n",
      "         2.36911368,   2.30248639,   2.19966444,   2.10780459,\n",
      "         1.98747461,   1.95089604,   1.89127594,   1.75055516,\n",
      "         1.75042592,   1.66415791,   1.59348751,   1.55727881,\n",
      "         1.49356233,   1.47347198,   1.41088779,   1.33478678,\n",
      "         1.27978267,   1.26955614,   1.19523414,   1.17448979,\n",
      "         1.12731052,   1.10894589,   1.1036834 ,   1.03344413]), 'val_accs': array([0.43641791, 0.4680597 , 0.46656716, 0.46      , 0.46238806,\n",
      "       0.46686567, 0.48149254, 0.47283582, 0.4919403 , 0.50716418,\n",
      "       0.47522388, 0.47492537, 0.50925373, 0.49343284, 0.51850746,\n",
      "       0.50746269, 0.51970149, 0.54119403, 0.54865672, 0.58029851,\n",
      "       0.58865672, 0.58208955, 0.60507463, 0.56656716, 0.65432836,\n",
      "       0.60955224, 0.64328358, 0.61910448, 0.66985075, 0.68238806,\n",
      "       0.65880597, 0.63343284, 0.67074627, 0.70029851, 0.71104478,\n",
      "       0.67701493, 0.70358209, 0.70985075, 0.69641791, 0.68686567,\n",
      "       0.72119403, 0.74029851, 0.72238806, 0.72179104, 0.71940299,\n",
      "       0.75164179, 0.73223881, 0.75701493, 0.74626866, 0.76686567,\n",
      "       0.78358209, 0.76626866, 0.75343284, 0.77641791, 0.7638806 ,\n",
      "       0.77402985, 0.78686567, 0.78626866, 0.78059701, 0.76626866]), 'val_losses': array([41.0764483 , 27.91510241, 22.07951757, 19.97145812, 16.46551029,\n",
      "       13.08748666, 10.66235383,  9.82487147,  6.90203613,  5.07627812,\n",
      "        4.60285726,  4.28224307,  3.46326121,  3.73466377,  2.88594626,\n",
      "        2.62047178,  2.34213716,  2.10602837,  2.05905169,  1.95534658,\n",
      "        1.652559  ,  1.77418472,  1.40206811,  1.88509571,  1.4358295 ,\n",
      "        1.49502281,  1.30024203,  1.51909677,  1.10931234,  1.14024274,\n",
      "        1.16972844,  1.20316575,  1.01742614,  0.92539376,  0.88667705,\n",
      "        0.95031798,  0.88981984,  0.81894534,  0.86440693,  0.85110224,\n",
      "        0.75097316,  0.73145667,  0.70848658,  0.73019158,  0.76207729,\n",
      "        0.64328989,  0.63012441,  0.61730868,  0.67467355,  0.60223907,\n",
      "        0.56848766,  0.5995911 ,  0.68928444,  0.55930229,  0.55374601,\n",
      "        0.56523959,  0.50577328,  0.50575706,  0.51856322,  0.58222832])}, 'ATTACH::14::best_val_loss': 0.5057570644634873, 'ATTACH::15::history': {'train_accs': array([0.43227107, 0.48331965, 0.51376968, 0.54541384, 0.56675871,\n",
      "       0.60780655, 0.62847974, 0.66549743, 0.68758863, 0.70102246,\n",
      "       0.72669602, 0.75281737, 0.76819166, 0.775431  , 0.79117845,\n",
      "       0.80282111, 0.81028435, 0.81297112, 0.82767371, 0.83871931,\n",
      "       0.84640645, 0.8614822 , 0.87342339, 0.89655944, 0.90350026,\n",
      "       0.92768117, 0.9279797 , 0.94096574, 0.94521979, 0.95596686,\n",
      "       0.95268304, 0.9613404 , 0.96865438, 0.96119113, 0.97313232,\n",
      "       0.97096798, 0.97372938, 0.97932682, 0.97805806, 0.97858049,\n",
      "       0.97917755, 0.97701321, 0.98201358, 0.98275991, 0.98462572,\n",
      "       0.98014777, 0.98402866, 0.98373013, 0.98604373, 0.98567057,\n",
      "       0.98477498, 0.98402866]), 'train_losses': array([14.37339427,  4.29101869,  2.49055198,  1.69894797,  1.35251307,\n",
      "        1.00132623,  0.88859047,  0.75981231,  0.70304354,  0.66645904,\n",
      "        0.60937805,  0.57291335,  0.54130245,  0.51678436,  0.49652588,\n",
      "        0.4702626 ,  0.46078731,  0.45242724,  0.42212158,  0.38763602,\n",
      "        0.37014535,  0.3440691 ,  0.32358686,  0.27704678,  0.26078119,\n",
      "        0.20761583,  0.20341402,  0.17286488,  0.16561911,  0.1354672 ,\n",
      "        0.14003487,  0.11550396,  0.0956866 ,  0.11523695,  0.08416385,\n",
      "        0.09888018,  0.08834919,  0.06552753,  0.067197  ,  0.07125677,\n",
      "        0.06637483,  0.07582508,  0.05410371,  0.0532164 ,  0.04979446,\n",
      "        0.06023151,  0.05040384,  0.04944506,  0.04746163,  0.04459341,\n",
      "        0.04652749,  0.05622177]), 'val_accs': array([0.46      , 0.62985075, 0.55731343, 0.49134328, 0.65044776,\n",
      "       0.7238806 , 0.72716418, 0.64      , 0.7358209 , 0.78268657,\n",
      "       0.73552239, 0.80328358, 0.78746269, 0.81641791, 0.82776119,\n",
      "       0.83701493, 0.85522388, 0.80925373, 0.84955224, 0.79014925,\n",
      "       0.82149254, 0.82298507, 0.89940299, 0.86059701, 0.93791045,\n",
      "       0.87044776, 0.91044776, 0.92208955, 0.96029851, 0.97641791,\n",
      "       0.87253731, 0.96865672, 0.97671642, 0.95402985, 0.9838806 ,\n",
      "       0.98328358, 0.97343284, 0.96776119, 0.97104478, 0.94179104,\n",
      "       0.98298507, 0.9880597 , 0.98537313, 0.96895522, 0.93970149,\n",
      "       0.96      , 0.9838806 , 0.9758209 , 0.96716418, 0.95850746,\n",
      "       0.98089552, 0.9841791 ]), 'val_losses': array([4.80549816, 1.38102643, 1.09178694, 2.40851295, 0.86261709,\n",
      "       0.68921008, 0.61161131, 0.73454347, 0.60276617, 0.48952608,\n",
      "       0.58545385, 0.47747879, 0.47615519, 0.45141193, 0.40491171,\n",
      "       0.41776986, 0.38686254, 0.39848335, 0.38751087, 0.39392352,\n",
      "       0.38940259, 0.33876056, 0.2323251 , 0.30883495, 0.19869177,\n",
      "       0.30415935, 0.24773521, 0.26412828, 0.10763952, 0.0915495 ,\n",
      "       0.43410425, 0.09713137, 0.08799396, 0.15621193, 0.05794075,\n",
      "       0.05941874, 0.08881516, 0.07577887, 0.09593478, 0.17923226,\n",
      "       0.06748948, 0.04315035, 0.04867152, 0.09144855, 0.17718851,\n",
      "       0.10687093, 0.05532612, 0.07174678, 0.08157706, 0.12738336,\n",
      "       0.05832325, 0.0527083 ])}, 'ATTACH::15::best_val_loss': 0.043150351097557084, 'ATTACH::16::history': {'train_accs': array([0.25404881, 0.35495186, 0.37913277, 0.40764236, 0.41107545,\n",
      "       0.42600194, 0.43480857, 0.44428689, 0.44398836, 0.46645272,\n",
      "       0.46787074, 0.47025897, 0.4816031 , 0.490559  , 0.50279872,\n",
      "       0.50794835, 0.52220315, 0.54295097, 0.55026495, 0.56086275,\n",
      "       0.57459512, 0.58541682, 0.60437346, 0.60235838, 0.62250914,\n",
      "       0.62795731, 0.63870438, 0.6396746 , 0.65213822, 0.66295992,\n",
      "       0.67116949, 0.67505038, 0.68340921, 0.68781252, 0.69572356,\n",
      "       0.70147026, 0.71311292, 0.7144563 , 0.72072543, 0.73236809,\n",
      "       0.74736921, 0.76237033, 0.77438615, 0.78677513, 0.7949847 ,\n",
      "       0.81379207, 0.81655347, 0.8389432 , 0.84648108, 0.84939175,\n",
      "       0.8640197 , 0.87387118, 0.88066274, 0.88663333, 0.89521606,\n",
      "       0.89797746, 0.90805284, 0.90417195, 0.91641167, 0.9116352 ]), 'train_losses': array([60.12666074, 24.00626695, 18.00807876, 13.51667613, 11.426263  ,\n",
      "        9.31395884,  8.17455988,  6.85704167,  6.27434091,  5.31899067,\n",
      "        4.7620413 ,  4.29294746,  3.90365055,  3.55273647,  3.19760603,\n",
      "        2.91772442,  2.68159513,  2.36962424,  2.24205262,  1.99012332,\n",
      "        1.83902932,  1.66033366,  1.55036884,  1.46196006,  1.35720362,\n",
      "        1.25444145,  1.20726207,  1.16041411,  1.05174871,  1.03381333,\n",
      "        0.97110078,  0.92347276,  0.89605588,  0.86039594,  0.79403356,\n",
      "        0.8052133 ,  0.75506059,  0.73985161,  0.73222711,  0.69626393,\n",
      "        0.66939242,  0.6373764 ,  0.60320204,  0.57503897,  0.5590214 ,\n",
      "        0.50768454,  0.51045837,  0.46831472,  0.43939214,  0.43839413,\n",
      "        0.40654293,  0.37350704,  0.37330266,  0.34298377,  0.31050924,\n",
      "        0.30412704,  0.28668617,  0.28626435,  0.25424304,  0.26453853]), 'val_accs': array([0.45432836, 0.50597015, 0.53462687, 0.48865672, 0.50208955,\n",
      "       0.53343284, 0.53820896, 0.55313433, 0.51223881, 0.55283582,\n",
      "       0.51283582, 0.54925373, 0.51731343, 0.57253731, 0.56925373,\n",
      "       0.62955224, 0.64776119, 0.58208955, 0.63671642, 0.69641791,\n",
      "       0.63850746, 0.64686567, 0.70835821, 0.63850746, 0.68656716,\n",
      "       0.70179104, 0.70298507, 0.73492537, 0.67044776, 0.70925373,\n",
      "       0.69791045, 0.75164179, 0.71313433, 0.69402985, 0.75970149,\n",
      "       0.76477612, 0.79492537, 0.70686567, 0.71731343, 0.77462687,\n",
      "       0.71880597, 0.83731343, 0.83014925, 0.8       , 0.85850746,\n",
      "       0.83074627, 0.85641791, 0.76776119, 0.87701493, 0.87402985,\n",
      "       0.90656716, 0.89104478, 0.91373134, 0.89522388, 0.92089552,\n",
      "       0.90447761, 0.89850746, 0.93164179, 0.91850746, 0.92059701]), 'val_losses': array([8.34630322, 7.49743314, 5.66816794, 5.14971966, 3.59825759,\n",
      "       3.12435461, 2.51425452, 2.57454975, 2.14699477, 2.39886822,\n",
      "       1.68721666, 1.43887928, 1.15771904, 1.34404203, 1.03611747,\n",
      "       1.0532791 , 0.97357015, 0.89373862, 0.92388127, 0.86831057,\n",
      "       0.79323928, 0.83339837, 0.63567305, 0.85537846, 0.79711547,\n",
      "       0.66122288, 0.61087201, 0.61416428, 0.68772439, 0.56822791,\n",
      "       1.21080862, 0.55512246, 0.51883374, 0.53428322, 0.57565288,\n",
      "       0.49435256, 0.45632771, 0.55775096, 0.57530051, 0.61118188,\n",
      "       0.64750218, 0.44079934, 0.4146506 , 0.42535825, 0.38760886,\n",
      "       0.40686976, 0.37707536, 0.57492169, 0.32424386, 0.34169602,\n",
      "       0.26827866, 0.27257867, 0.26068949, 0.27455265, 0.2314596 ,\n",
      "       0.2733816 , 0.27703747, 0.20075494, 0.22035542, 0.22236737])}, 'ATTACH::16::best_val_loss': 0.20075493730715852, 'ATTACH::17::history': {'train_accs': array([0.31300843, 0.36928129, 0.40055228, 0.4102545 , 0.43704754,\n",
      "       0.44174938, 0.44749608, 0.45018285, 0.47197552, 0.48906635,\n",
      "       0.51399358, 0.54250317, 0.56750504, 0.58795432, 0.61967311,\n",
      "       0.64161505, 0.66422867, 0.69520113, 0.71333682, 0.74012986,\n",
      "       0.74669751, 0.77744608, 0.78438689, 0.79252183, 0.81013508,\n",
      "       0.81797149, 0.82774834, 0.8419285 , 0.85304873, 0.86476603,\n",
      "       0.87484141, 0.87797597, 0.88566311, 0.89849989, 0.89655944,\n",
      "       0.90454512, 0.9114113 , 0.91111277, 0.91514292, 0.9224569 ,\n",
      "       0.93021867, 0.92708411, 0.93387566, 0.93641317, 0.94036868,\n",
      "       0.94141354, 0.94641391, 0.9422345 , 0.95454885, 0.94917531,\n",
      "       0.94820509, 0.95439958, 0.95969848, 0.9555937 , 0.9583551 ,\n",
      "       0.96178819, 0.96671393, 0.96604224, 0.96604224, 0.9694007 ]), 'train_losses': array([65.17911047, 26.04117139, 14.1983184 , 10.0455737 ,  7.80298067,\n",
      "        6.49891841,  5.60494294,  4.82358394,  4.18619766,  3.72379119,\n",
      "        3.28826473,  2.7804224 ,  2.54156593,  2.29536406,  1.96617855,\n",
      "        1.80458412,  1.67241653,  1.46963371,  1.37375529,  1.20385224,\n",
      "        1.10062188,  0.97442786,  0.92442731,  0.86565799,  0.80715766,\n",
      "        0.72973337,  0.67297057,  0.63740247,  0.53751915,  0.50568194,\n",
      "        0.47420707,  0.43780691,  0.41304061,  0.36095816,  0.35542432,\n",
      "        0.32485653,  0.2983731 ,  0.30188558,  0.29224504,  0.26802533,\n",
      "        0.2377285 ,  0.24746513,  0.22611565,  0.21319941,  0.20012593,\n",
      "        0.19372287,  0.18194807,  0.18331534,  0.15129588,  0.1630407 ,\n",
      "        0.1592176 ,  0.15232603,  0.12236953,  0.14202478,  0.13245354,\n",
      "        0.12261313,  0.11150848,  0.11684665,  0.10688525,  0.09502838]), 'val_accs': array([0.42597015, 0.43552239, 0.42656716, 0.43850746, 0.49970149,\n",
      "       0.49343284, 0.53910448, 0.52537313, 0.5280597 , 0.56597015,\n",
      "       0.65522388, 0.69283582, 0.64507463, 0.68656716, 0.76835821,\n",
      "       0.74537313, 0.77850746, 0.79373134, 0.82746269, 0.80358209,\n",
      "       0.83880597, 0.84059701, 0.86955224, 0.85283582, 0.86716418,\n",
      "       0.88298507, 0.89014925, 0.88447761, 0.89552239, 0.90089552,\n",
      "       0.90179104, 0.90985075, 0.92686567, 0.92746269, 0.92358209,\n",
      "       0.92716418, 0.92029851, 0.9361194 , 0.93701493, 0.93701493,\n",
      "       0.9438806 , 0.94179104, 0.95402985, 0.93820896, 0.94      ,\n",
      "       0.94925373, 0.93701493, 0.95283582, 0.9438806 , 0.93014925,\n",
      "       0.94746269, 0.96089552, 0.94835821, 0.95343284, 0.96626866,\n",
      "       0.95074627, 0.95940299, 0.96149254, 0.96179104, 0.96119403]), 'val_losses': array([6.69028556, 4.10151852, 4.37480765, 4.50370083, 2.39260392,\n",
      "       3.74315105, 1.53845405, 1.80125512, 1.84917411, 1.24821446,\n",
      "       0.85552254, 0.78009164, 1.03774466, 0.9340317 , 0.69794751,\n",
      "       0.93234531, 0.72954524, 0.67617165, 0.58809896, 0.71453692,\n",
      "       0.56787954, 0.57481159, 0.44045565, 0.49335443, 0.49557006,\n",
      "       0.35690063, 0.32579477, 0.38466982, 0.3824325 , 0.33241781,\n",
      "       0.34196818, 0.32308627, 0.26197263, 0.26262996, 0.2601458 ,\n",
      "       0.23274887, 0.25843531, 0.23891706, 0.22939635, 0.2404978 ,\n",
      "       0.20095884, 0.21001907, 0.1758235 , 0.20941257, 0.20307632,\n",
      "       0.1860348 , 0.23188743, 0.18805725, 0.20474139, 0.24400779,\n",
      "       0.20382166, 0.14673167, 0.19882378, 0.19632746, 0.13225973,\n",
      "       0.16784477, 0.158399  , 0.15035484, 0.14063175, 0.15020939])}, 'ATTACH::17::best_val_loss': 0.13225973212896888, 'ATTACH::18::history': {'train_accs': array([0.41518024, 0.45182476, 0.46578103, 0.48331965, 0.50242555,\n",
      "       0.50003732, 0.50973953, 0.51317262, 0.51787447, 0.52085977,\n",
      "       0.52757668, 0.51235167, 0.49257407, 0.51354579, 0.53093514,\n",
      "       0.50824688, 0.53033808, 0.5264572 , 0.53115904, 0.53145757,\n",
      "       0.53302485, 0.54310023, 0.51503844, 0.53690574, 0.4877976 ,\n",
      "       0.50600791, 0.49936562, 0.44689902]), 'train_losses': array([2.49391567, 1.12971814, 1.05155996, 1.00056772, 0.9809143 ,\n",
      "       0.95584384, 0.93563252, 0.95000098, 0.93882348, 0.9306302 ,\n",
      "       0.91822861, 0.93050052, 0.97257493, 0.94447755, 0.89662916,\n",
      "       0.94327029, 0.90259001, 0.89511366, 0.8976149 , 0.88829403,\n",
      "       0.89671811, 0.87744178, 0.92118161, 0.88773285, 0.95722693,\n",
      "       0.93400229, 0.93374902, 0.99923837]), 'val_accs': array([0.45313433, 0.44835821, 0.51552239, 0.47522388, 0.56985075,\n",
      "       0.53014925, 0.62268657, 0.60208955, 0.62089552, 0.58686567,\n",
      "       0.54537313, 0.61492537, 0.54059701, 0.66      , 0.54358209,\n",
      "       0.60686567, 0.55940299, 0.58089552, 0.63820896, 0.58716418,\n",
      "       0.53074627, 0.56597015, 0.56686567, 0.58477612, 0.58      ,\n",
      "       0.5441791 , 0.51283582, 0.49940299]), 'val_losses': array([1.19850296, 1.06104737, 0.94803676, 0.96646859, 0.90847243,\n",
      "       0.85768046, 0.84095924, 0.89750003, 0.8735873 , 0.83379145,\n",
      "       0.87030534, 0.87298629, 0.8794009 , 0.81678809, 0.84735612,\n",
      "       0.83058205, 0.81689045, 0.79976993, 0.8239496 , 0.8013477 ,\n",
      "       0.83946201, 0.80378102, 0.84178597, 0.81410184, 0.83063866,\n",
      "       0.82358447, 0.83192849, 0.87101402])}, 'ATTACH::18::best_val_loss': 0.7997699282774284, 'ATTACH::19::history': {'train_accs': array([0.4102545 , 0.46578103, 0.50847078, 0.54944399, 0.59034256,\n",
      "       0.63236062, 0.6815434 , 0.72124785, 0.75751922, 0.78871558,\n",
      "       0.8225987 , 0.85700425, 0.88297634, 0.89126054, 0.90984402,\n",
      "       0.92036719, 0.93141279, 0.93492052, 0.94783193, 0.94887678,\n",
      "       0.95268304, 0.95925069, 0.96529592, 0.96596761, 0.96925144,\n",
      "       0.9719382 , 0.97104262, 0.97701321, 0.97432644, 0.98305844,\n",
      "       0.98074483, 0.98440182, 0.9832077 , 0.98186432, 0.98522278,\n",
      "       0.9864169 , 0.98529741, 0.98947683, 0.98746175, 0.9891783 ,\n",
      "       0.98865587, 0.98865587, 0.9891783 , 0.99208896, 0.99029778,\n",
      "       0.99089484, 0.99059631, 0.99328308, 0.9919397 , 0.99104411,\n",
      "       0.99343235, 0.99238749, 0.99343235]), 'train_losses': array([13.66959334,  5.52519168,  3.71370573,  2.6917308 ,  2.003098  ,\n",
      "        1.62365989,  1.2605685 ,  1.0223442 ,  0.83918324,  0.67850911,\n",
      "        0.54673249,  0.44808671,  0.36779425,  0.33507198,  0.27617189,\n",
      "        0.2386658 ,  0.21652099,  0.2086225 ,  0.16439766,  0.16124263,\n",
      "        0.14477837,  0.12256277,  0.10736041,  0.10502429,  0.10012841,\n",
      "        0.09548042,  0.08522842,  0.06930355,  0.07604418,  0.05484823,\n",
      "        0.05818754,  0.0477141 ,  0.05100791,  0.054466  ,  0.04700356,\n",
      "        0.03953888,  0.04451688,  0.03366566,  0.04115535,  0.03224075,\n",
      "        0.03524636,  0.03328888,  0.0338966 ,  0.02461835,  0.03226385,\n",
      "        0.0272659 ,  0.03241689,  0.0199922 ,  0.024047  ,  0.02705836,\n",
      "        0.02028057,  0.02406142,  0.01961496]), 'val_accs': array([0.46985075, 0.51820896, 0.54089552, 0.55880597, 0.55731343,\n",
      "       0.67014925, 0.67402985, 0.82268657, 0.81552239, 0.80985075,\n",
      "       0.90059701, 0.84      , 0.76089552, 0.84656716, 0.91253731,\n",
      "       0.87850746, 0.76208955, 0.92985075, 0.94447761, 0.93850746,\n",
      "       0.91731343, 0.95641791, 0.91492537, 0.95522388, 0.93134328,\n",
      "       0.93820896, 0.96567164, 0.96686567, 0.94656716, 0.96835821,\n",
      "       0.96985075, 0.97373134, 0.96298507, 0.96      , 0.97283582,\n",
      "       0.97462687, 0.97910448, 0.98      , 0.96358209, 0.9719403 ,\n",
      "       0.97641791, 0.9719403 , 0.98268657, 0.97074627, 0.9838806 ,\n",
      "       0.97850746, 0.96298507, 0.97522388, 0.97880597, 0.98029851,\n",
      "       0.98      , 0.98477612, 0.98298507]), 'val_losses': array([3.24484817, 3.43519173, 3.35698977, 2.23989587, 3.07760436,\n",
      "       1.40291897, 1.55115457, 0.44651277, 0.5336857 , 0.5907266 ,\n",
      "       0.29544325, 0.4802306 , 0.86213313, 0.47269051, 0.2497886 ,\n",
      "       0.35490306, 0.8796525 , 0.211696  , 0.14897749, 0.18824129,\n",
      "       0.25233495, 0.16129462, 0.25936507, 0.12966775, 0.23397014,\n",
      "       0.1582655 , 0.09378662, 0.10260997, 0.16235023, 0.09698608,\n",
      "       0.11362075, 0.08467342, 0.13880495, 0.1305444 , 0.09897218,\n",
      "       0.08780144, 0.06548158, 0.06799711, 0.12806172, 0.08717942,\n",
      "       0.08648926, 0.09560506, 0.05577285, 0.09168352, 0.06430772,\n",
      "       0.07196345, 0.11778837, 0.09064172, 0.078564  , 0.07947401,\n",
      "       0.06872719, 0.05841296, 0.06862999])}, 'ATTACH::19::best_val_loss': 0.05577285366244058, 'ATTACH::20::history': {'train_accs': array([0.49869393, 0.55675797, 0.56922158, 0.54922009, 0.54205538,\n",
      "       0.53556236, 0.53742817, 0.54586163, 0.52242705, 0.49182775,\n",
      "       0.53809986, 0.49817151, 0.48929025]), 'train_losses': array([4.5997106 , 0.88882851, 0.85418672, 0.8836224 , 0.88890879,\n",
      "       0.89680372, 0.90004207, 0.89097067, 0.90913076, 0.93281233,\n",
      "       0.90315585, 0.95462304, 0.9907158 ]), 'val_accs': array([0.60358209, 0.57731343, 0.6638806 , 0.58298507, 0.61731343,\n",
      "       0.57552239, 0.4680597 , 0.55462687, 0.47014925, 0.59044776,\n",
      "       0.58895522, 0.52597015, 0.46716418]), 'val_losses': array([0.89715561, 0.82114651, 0.75714308, 0.78735812, 0.79738341,\n",
      "       0.83927851, 0.90862057, 0.82371541, 0.8843621 , 0.79064184,\n",
      "       0.80874881, 0.95585866, 0.92901324])}, 'ATTACH::20::best_val_loss': 0.7571430815511675, 'ATTACH::21::history': {'train_accs': array([0.33263676, 0.38353608, 0.39592507, 0.40913501, 0.42122546,\n",
      "       0.42562878, 0.44540637, 0.44458542, 0.46003433, 0.46555713,\n",
      "       0.47093067, 0.49615643, 0.49399209, 0.50264945, 0.50503769,\n",
      "       0.50832152, 0.52108366, 0.53899545, 0.56108665, 0.58034182,\n",
      "       0.58802896, 0.60392567, 0.61295619, 0.63034555, 0.64773491,\n",
      "       0.67400552, 0.68079707, 0.7010971 , 0.71535189, 0.72990522,\n",
      "       0.76446003, 0.7675946 , 0.79438764, 0.79528323, 0.81185163,\n",
      "       0.82050899, 0.82804687, 0.8423763 , 0.84924248, 0.85976565,\n",
      "       0.86902008, 0.87513994, 0.87834913, 0.88752892, 0.89133517,\n",
      "       0.89058885, 0.90760505, 0.9169341 , 0.92342712, 0.92603926,\n",
      "       0.92305396, 0.94171207, 0.94096574, 0.94783193, 0.95111575,\n",
      "       0.95089186, 0.95208598, 0.95753414, 0.95566833, 0.96283305]), 'train_losses': array([83.2153551 , 34.13533194, 23.12517536, 17.73750298, 13.89796276,\n",
      "       12.0132661 ,  9.75792406,  8.19379106,  6.72126573,  5.81123098,\n",
      "        5.05324313,  4.15962312,  3.93521275,  3.44667286,  2.97282385,\n",
      "        2.80184885,  2.50496119,  2.15287283,  1.98239029,  1.76505515,\n",
      "        1.65975044,  1.48589928,  1.36257978,  1.32882628,  1.20935813,\n",
      "        1.16536551,  1.02300674,  0.95580303,  0.89676485,  0.83031345,\n",
      "        0.71276098,  0.70572165,  0.62692959,  0.63424841,  0.55700783,\n",
      "        0.52354186,  0.51804474,  0.46268313,  0.45096121,  0.40443377,\n",
      "        0.37871856,  0.36510394,  0.35686465,  0.32025886,  0.31499294,\n",
      "        0.31444238,  0.27969888,  0.2400055 ,  0.23208971,  0.2255976 ,\n",
      "        0.22641438,  0.17543764,  0.18118736,  0.17043396,  0.1441755 ,\n",
      "        0.1545045 ,  0.14941804,  0.12946439,  0.13687992,  0.11742514]), 'val_accs': array([0.46149254, 0.45283582, 0.43970149, 0.48507463, 0.46865672,\n",
      "       0.53283582, 0.51164179, 0.55432836, 0.54      , 0.56268657,\n",
      "       0.6080597 , 0.56985075, 0.56358209, 0.57402985, 0.58268657,\n",
      "       0.53164179, 0.57402985, 0.57522388, 0.58626866, 0.65492537,\n",
      "       0.66895522, 0.68268657, 0.72268657, 0.59940299, 0.64686567,\n",
      "       0.73641791, 0.80089552, 0.77671642, 0.6480597 , 0.81343284,\n",
      "       0.85462687, 0.78895522, 0.81641791, 0.82686567, 0.83761194,\n",
      "       0.8361194 , 0.82059701, 0.82865672, 0.82      , 0.83910448,\n",
      "       0.85850746, 0.82597015, 0.85552239, 0.85731343, 0.85910448,\n",
      "       0.89671642, 0.87044776, 0.90447761, 0.89134328, 0.90208955,\n",
      "       0.92358209, 0.92447761, 0.88268657, 0.92358209, 0.92328358,\n",
      "       0.82059701, 0.9638806 , 0.95164179, 0.9238806 , 0.95432836]), 'val_losses': array([ 9.00769368, 11.45687037,  5.23405524,  5.86437929,  4.38968565,\n",
      "        2.40292772,  3.29396173,  2.07483068,  1.77662876,  1.55255222,\n",
      "        1.23568212,  1.52904539,  1.3155688 ,  1.3873316 ,  1.53113944,\n",
      "        1.43325986,  1.04993402,  1.67149756,  1.29228921,  0.8152057 ,\n",
      "        0.97638203,  0.94839922,  0.80905927,  1.43271553,  0.97446339,\n",
      "        0.67114577,  0.56940981,  0.71205853,  0.9462845 ,  0.51478547,\n",
      "        0.40870721,  0.62304522,  0.69741819,  0.46521565,  0.41984822,\n",
      "        0.4969587 ,  0.46811132,  0.45156451,  0.46051968,  0.44620153,\n",
      "        0.3513792 ,  0.4163627 ,  0.39203877,  0.37618564,  0.35840082,\n",
      "        0.27635558,  0.30957148,  0.26871243,  0.26893555,  0.25350056,\n",
      "        0.21472408,  0.19940802,  0.32074889,  0.21420901,  0.22137205,\n",
      "        0.61084809,  0.1366735 ,  0.16441065,  0.24083993,  0.15212792])}, 'ATTACH::21::best_val_loss': 0.1366734961075569, 'ATTACH::22::history': {'train_accs': array([0.50399284, 0.61832973, 0.65512352, 0.66788566, 0.67490111,\n",
      "       0.68251362, 0.64691395, 0.64340622, 0.63922681, 0.6231062 ,\n",
      "       0.63385327, 0.6451974 , 0.64848123, 0.63086798, 0.63325621,\n",
      "       0.61310546, 0.61683708, 0.61638928]), 'train_losses': array([4.64720894, 0.81614292, 0.72193573, 0.69250557, 0.6882791 ,\n",
      "       0.67957098, 0.73503174, 0.73805913, 0.7446773 , 0.79007306,\n",
      "       0.77315487, 0.76294574, 0.7626793 , 0.77194995, 0.77322304,\n",
      "       0.79557656, 0.79829362, 0.78668877]), 'val_accs': array([0.60179104, 0.71731343, 0.72059701, 0.67164179, 0.60537313,\n",
      "       0.7038806 , 0.67014925, 0.72238806, 0.56567164, 0.68746269,\n",
      "       0.63104478, 0.69014925, 0.6719403 , 0.61910448, 0.63761194,\n",
      "       0.60716418, 0.63791045, 0.67671642]), 'val_losses': array([0.84676588, 0.7047388 , 0.62886473, 0.6707328 , 0.7282323 ,\n",
      "       0.61252094, 0.73500859, 0.61242652, 0.92517787, 0.70985597,\n",
      "       0.78629804, 0.75223708, 0.71917043, 0.7981074 , 0.70400969,\n",
      "       0.81328948, 0.79561426, 0.73686431])}, 'ATTACH::22::best_val_loss': 0.612426519358336, 'ATTACH::23::history': {'train_accs': array([0.44122696, 0.55690723, 0.62228525, 0.66355698, 0.69646989,\n",
      "       0.71624748, 0.73289051, 0.75416076, 0.7696843 , 0.77408762,\n",
      "       0.77699828, 0.79073065, 0.80513471, 0.80976192, 0.81125457,\n",
      "       0.82341966, 0.81655347, 0.82327039, 0.82543473, 0.82916636,\n",
      "       0.83259945, 0.82521084, 0.83394283, 0.83028584, 0.83580864,\n",
      "       0.83379357, 0.82655422, 0.7894619 , 0.82125532, 0.83230092,\n",
      "       0.83573401, 0.84797373, 0.84909322, 0.8451377 ]), 'train_losses': array([7.26132879, 1.1203019 , 0.86053255, 0.7498566 , 0.68007666,\n",
      "       0.65077755, 0.62155169, 0.58590362, 0.5614207 , 0.54977362,\n",
      "       0.53469365, 0.51207117, 0.48198768, 0.48431924, 0.47394251,\n",
      "       0.45125935, 0.46199658, 0.45057761, 0.43860141, 0.4404575 ,\n",
      "       0.42766939, 0.44623662, 0.43491668, 0.44938429, 0.44062774,\n",
      "       0.42736754, 0.45132538, 0.51362899, 0.44923809, 0.43460287,\n",
      "       0.41600837, 0.39667533, 0.40056965, 0.40892392]), 'val_accs': array([0.5241791 , 0.60268657, 0.69373134, 0.70477612, 0.66238806,\n",
      "       0.66238806, 0.74208955, 0.66      , 0.78      , 0.84358209,\n",
      "       0.74567164, 0.76656716, 0.74656716, 0.83761194, 0.74746269,\n",
      "       0.84328358, 0.82358209, 0.80686567, 0.74059701, 0.81552239,\n",
      "       0.84597015, 0.7558209 , 0.82597015, 0.8719403 , 0.7838806 ,\n",
      "       0.78477612, 0.71820896, 0.74119403, 0.71402985, 0.69850746,\n",
      "       0.88597015, 0.79910448, 0.78447761, 0.80179104]), 'val_losses': array([1.52051433, 0.84612772, 0.66052502, 0.64217365, 0.77878064,\n",
      "       0.67908696, 0.60306627, 0.67114013, 0.5067539 , 0.42221587,\n",
      "       0.50139974, 0.45476023, 0.63998691, 0.38040478, 0.63299021,\n",
      "       0.43966852, 0.42927439, 0.41254617, 0.60783922, 0.43294784,\n",
      "       0.36598818, 0.52975208, 0.39880826, 0.31810352, 0.5067542 ,\n",
      "       0.48404749, 0.81221819, 0.59022623, 0.65479394, 0.73980088,\n",
      "       0.32341516, 0.44482171, 0.42424321, 0.49184914])}, 'ATTACH::23::best_val_loss': 0.31810351638651607, 'ATTACH::24::history': {'train_accs': array([0.4489141 , 0.51041122, 0.54354803, 0.59534294, 0.63885365,\n",
      "       0.69311143, 0.73818942, 0.76214643, 0.80297037, 0.81841929,\n",
      "       0.83782372, 0.84476453, 0.86364654, 0.86260169, 0.8614822 ,\n",
      "       0.86946787, 0.86013882, 0.87812523, 0.87558773, 0.86558698,\n",
      "       0.88469289, 0.87722964, 0.87760281, 0.8867826 , 0.89573849,\n",
      "       0.87924472, 0.88558848, 0.84200313, 0.87737891, 0.87760281,\n",
      "       0.85267557, 0.59161131, 0.38614822, 0.40943354, 0.41458318,\n",
      "       0.42100157, 0.43145011]), 'train_losses': array([30.31618546,  1.07764129,  1.00336757,  0.93750699,  0.86321589,\n",
      "        0.77512324,  0.6879533 ,  0.62121747,  0.53862103,  0.49375247,\n",
      "        0.44595179,  0.42779328,  0.38023763,  0.3830398 ,  0.3686751 ,\n",
      "        0.35870634,  0.39545136,  0.33574677,  0.34469877,  0.37049713,\n",
      "        0.32365547,  0.32961816,  0.33686832,  0.31058102,  0.29193914,\n",
      "        0.33766433,  0.31055573,  0.42362464,  0.3338329 ,  0.3414911 ,\n",
      "        0.39506866,  0.97100451,  1.31686019,  1.28314284,  1.26985572,\n",
      "        1.27301115,  1.25567338]), 'val_accs': array([0.48925373, 0.52149254, 0.45671642, 0.62358209, 0.66149254,\n",
      "       0.70328358, 0.77164179, 0.74238806, 0.80089552, 0.82955224,\n",
      "       0.86626866, 0.85134328, 0.85552239, 0.86328358, 0.84597015,\n",
      "       0.86746269, 0.88507463, 0.87164179, 0.86656716, 0.83970149,\n",
      "       0.89552239, 0.89432836, 0.85671642, 0.88      , 0.89522388,\n",
      "       0.88149254, 0.89343284, 0.85074627, 0.85940299, 0.89731343,\n",
      "       0.86985075, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955]), 'val_losses': array([1.14007432, 1.00659167, 1.27754604, 0.91614574, 0.81960388,\n",
      "       0.78819638, 0.61765816, 0.71271688, 0.53624582, 0.4550012 ,\n",
      "       0.38561892, 0.42032842, 0.40764823, 0.39661149, 0.41010448,\n",
      "       0.37520142, 0.3219379 , 0.35406585, 0.35876509, 0.45050655,\n",
      "       0.29036009, 0.29523581, 0.38254804, 0.32469212, 0.29455404,\n",
      "       0.32754681, 0.28802594, 0.40441817, 0.39411786, 0.30007898,\n",
      "       0.36525471, 1.23996745, 1.31951388, 1.23587088, 1.45795491,\n",
      "       1.24274568, 1.24593389])}, 'ATTACH::24::best_val_loss': 0.2880259435479321, 'ATTACH::25::history': {'train_accs': array([0.33144264, 0.38652138, 0.40689604, 0.42450929, 0.43562952,\n",
      "       0.45204866, 0.46070602, 0.46600493, 0.47891634, 0.47996119,\n",
      "       0.48593179, 0.48630495, 0.49578327, 0.50757519, 0.50511232,\n",
      "       0.52869617, 0.53041272, 0.52824838, 0.53071125, 0.54511531,\n",
      "       0.5487723 , 0.55033958, 0.55713113, 0.56250466, 0.57019181,\n",
      "       0.57743115, 0.58183447, 0.59810434, 0.59989551, 0.59489514,\n",
      "       0.61944921, 0.61467274, 0.62765878, 0.6092992 , 0.62004627,\n",
      "       0.62571834, 0.62930069, 0.6313904 , 0.64057019, 0.64168968,\n",
      "       0.64295843, 0.64922755, 0.64669005, 0.65497425, 0.65989999,\n",
      "       0.66713934, 0.66504963, 0.67445332, 0.6843048 , 0.67609523,\n",
      "       0.68296142]), 'train_losses': array([101.75949575,  21.95061948,  13.47825561,   9.51889859,\n",
      "         7.2565713 ,   5.75664111,   4.79443792,   4.08548104,\n",
      "         3.51256833,   3.09593452,   2.78674682,   2.46318478,\n",
      "         2.2558206 ,   2.10405013,   1.92414786,   1.7283324 ,\n",
      "         1.61436948,   1.57887219,   1.50189761,   1.40496403,\n",
      "         1.32676788,   1.3178982 ,   1.19962678,   1.19438674,\n",
      "         1.13341641,   1.08114508,   1.04220606,   0.96465044,\n",
      "         0.96104821,   0.94708865,   0.90101579,   0.90178782,\n",
      "         0.86534936,   0.8945172 ,   0.87274436,   0.84707588,\n",
      "         0.82521879,   0.82755383,   0.80084269,   0.79516561,\n",
      "         0.79919285,   0.77238376,   0.78661567,   0.7529498 ,\n",
      "         0.7484243 ,   0.72304839,   0.73606065,   0.71295231,\n",
      "         0.69585224,   0.70546302,   0.70225521]), 'val_accs': array([0.4638806 , 0.54089552, 0.5       , 0.51492537, 0.49432836,\n",
      "       0.48746269, 0.51970149, 0.50059701, 0.54179104, 0.58029851,\n",
      "       0.61074627, 0.54835821, 0.61074627, 0.6158209 , 0.63552239,\n",
      "       0.54477612, 0.60507463, 0.57074627, 0.63373134, 0.60686567,\n",
      "       0.57880597, 0.52119403, 0.67223881, 0.63223881, 0.65313433,\n",
      "       0.66119403, 0.63014925, 0.69731343, 0.64447761, 0.6438806 ,\n",
      "       0.62119403, 0.69462687, 0.73313433, 0.64358209, 0.70835821,\n",
      "       0.68328358, 0.64268657, 0.70238806, 0.61552239, 0.67402985,\n",
      "       0.75223881, 0.63880597, 0.66746269, 0.68447761, 0.6761194 ,\n",
      "       0.67253731, 0.67462687, 0.69641791, 0.66298507, 0.73074627,\n",
      "       0.72298507]), 'val_losses': array([12.93677694,  4.94466948,  2.36800253,  3.3048491 ,  1.96651303,\n",
      "        2.103915  ,  1.60873763,  1.54138507,  1.33335797,  0.98324533,\n",
      "        0.96648873,  1.03879422,  0.97648087,  0.88026331,  0.78117606,\n",
      "        0.88154749,  0.81412879,  0.84403306,  0.75543685,  0.79797377,\n",
      "        0.90711176,  0.95435235,  0.69670165,  0.81471342,  0.68137532,\n",
      "        0.74141222,  0.69719639,  0.63071455,  0.76788842,  0.70318396,\n",
      "        0.7320493 ,  0.64922451,  0.60440594,  0.7256948 ,  0.60394964,\n",
      "        0.65900089,  0.69424992,  0.64049041,  0.78082175,  0.69927908,\n",
      "        0.56498084,  0.68587001,  0.64276211,  0.60271845,  0.61179839,\n",
      "        0.64013998,  0.64352268,  0.63314211,  0.59214365,  0.57868372,\n",
      "        0.56563974])}, 'ATTACH::25::best_val_loss': 0.564980840789738, 'ATTACH::26::history': {'train_accs': array([0.39667139, 0.44227181, 0.50839615, 0.56496753, 0.64004776,\n",
      "       0.68564818, 0.71930741, 0.7563251 , 0.78774535, 0.81326965,\n",
      "       0.83170386, 0.85588477, 0.87312486, 0.89275319, 0.90753041,\n",
      "       0.9137249 , 0.92678558, 0.92812896, 0.93768192, 0.94059258,\n",
      "       0.94701097, 0.94895141, 0.95723561, 0.95499664, 0.95910143,\n",
      "       0.96283305, 0.96447496, 0.96805732, 0.96626614, 0.972386  ,\n",
      "       0.97066945, 0.97529666, 0.97380402, 0.97529666, 0.9753713 ,\n",
      "       0.97805806, 0.97410254, 0.97902829, 0.97731174, 0.97992387,\n",
      "       0.97925218, 0.97962535, 0.98059557, 0.98231211, 0.9832077 ,\n",
      "       0.98410329, 0.98238675, 0.98686469, 0.9832077 ]), 'train_losses': array([20.03877867,  6.00373512,  2.63008287,  1.43826612,  0.89709285,\n",
      "        0.75696777,  0.69003798,  0.59687943,  0.53266732,  0.48308971,\n",
      "        0.43980723,  0.3853509 ,  0.35424342,  0.30255319,  0.27400426,\n",
      "        0.24837015,  0.21844468,  0.20761432,  0.18722474,  0.16671276,\n",
      "        0.1622536 ,  0.147896  ,  0.1303917 ,  0.13178517,  0.12125834,\n",
      "        0.11190672,  0.11101772,  0.09838723,  0.09589394,  0.08545953,\n",
      "        0.091649  ,  0.07633681,  0.08574435,  0.07410017,  0.07624301,\n",
      "        0.07037354,  0.08331125,  0.0636366 ,  0.06554887,  0.0622659 ,\n",
      "        0.06564443,  0.06371461,  0.05940159,  0.05626384,  0.04985934,\n",
      "        0.0518639 ,  0.05393959,  0.04151109,  0.05207055]), 'val_accs': array([0.29283582, 0.42537313, 0.50835821, 0.56865672, 0.60567164,\n",
      "       0.66358209, 0.68955224, 0.7280597 , 0.71462687, 0.81970149,\n",
      "       0.91761194, 0.92238806, 0.92567164, 0.94298507, 0.75522388,\n",
      "       0.82477612, 0.95492537, 0.97164179, 0.93641791, 0.97492537,\n",
      "       0.87761194, 0.96716418, 0.96537313, 0.95701493, 0.95074627,\n",
      "       0.94328358, 0.95164179, 0.96059701, 0.98119403, 0.96537313,\n",
      "       0.98029851, 0.97552239, 0.97313433, 0.96238806, 0.95791045,\n",
      "       0.80567164, 0.97164179, 0.97761194, 0.9838806 , 0.94238806,\n",
      "       0.97761194, 0.97761194, 0.98179104, 0.98179104, 0.94597015,\n",
      "       0.97462687, 0.97880597, 0.98328358, 0.97283582]), 'val_losses': array([14.72112382,  5.21521126,  3.95221365,  1.01781896,  1.75821283,\n",
      "        0.83571744,  0.91099039,  0.58917748,  0.58753397,  0.45227124,\n",
      "        0.25226176,  0.22479803,  0.20576369,  0.16461494,  0.87537249,\n",
      "        0.44379993,  0.12416691,  0.10225773,  0.17469959,  0.08223134,\n",
      "        0.3360333 ,  0.10775748,  0.11259334,  0.11673005,  0.15116016,\n",
      "        0.15026557,  0.13766655,  0.11805808,  0.06156096,  0.0934816 ,\n",
      "        0.06520476,  0.07893904,  0.08661233,  0.11033023,  0.12748855,\n",
      "        0.77374733,  0.07872964,  0.06890643,  0.05105056,  0.20921139,\n",
      "        0.06895094,  0.07784687,  0.05848542,  0.05610863,  0.2109313 ,\n",
      "        0.07786633,  0.06922133,  0.07185453,  0.09053586])}, 'ATTACH::26::best_val_loss': 0.0510505643709382, 'ATTACH::27::history': {'train_accs': array([0.41032913, 0.46122845, 0.49078289, 0.5204866 , 0.54608553,\n",
      "       0.58295395, 0.64303306, 0.71065005, 0.76401224, 0.81722517,\n",
      "       0.84409284, 0.88297634, 0.90603776, 0.92230763, 0.93036794,\n",
      "       0.94298082, 0.95230987, 0.95529517, 0.96141503, 0.96313158,\n",
      "       0.96738563, 0.97186357, 0.97410254, 0.97447571, 0.97910292,\n",
      "       0.97775953, 0.97843123, 0.9806702 , 0.98253601, 0.9829838 ,\n",
      "       0.98425256, 0.98552131, 0.98738712, 0.98559594, 0.98611837,\n",
      "       0.98835734, 0.98843197, 0.9885066 , 0.99044705, 0.98887977,\n",
      "       0.99052168, 0.99074558, 0.99014852, 0.98955146, 0.99343235]), 'train_losses': array([13.22890077,  4.45155096,  2.92318742,  2.06740292,  1.51826584,\n",
      "        1.15296317,  0.9110852 ,  0.71311548,  0.58304149,  0.48340718,\n",
      "        0.39649587,  0.32169423,  0.25861642,  0.21542989,  0.19308703,\n",
      "        0.16413037,  0.14292415,  0.13926142,  0.11589185,  0.11106271,\n",
      "        0.09811317,  0.09251938,  0.0785159 ,  0.07732429,  0.06710947,\n",
      "        0.06532231,  0.06107043,  0.06574766,  0.0527203 ,  0.05665524,\n",
      "        0.0489465 ,  0.04204888,  0.04070446,  0.04339807,  0.04002609,\n",
      "        0.03948707,  0.0362357 ,  0.03558409,  0.03058423,  0.03793145,\n",
      "        0.02829082,  0.02893216,  0.03411232,  0.03088843,  0.01981971]), 'val_accs': array([0.53880597, 0.4641791 , 0.56537313, 0.62537313, 0.56268657,\n",
      "       0.59283582, 0.71164179, 0.76059701, 0.65552239, 0.84179104,\n",
      "       0.84029851, 0.83462687, 0.89164179, 0.91283582, 0.90746269,\n",
      "       0.94626866, 0.95850746, 0.95910448, 0.95641791, 0.96089552,\n",
      "       0.89432836, 0.96447761, 0.9761194 , 0.97492537, 0.97164179,\n",
      "       0.96895522, 0.96268657, 0.96567164, 0.9761194 , 0.97462687,\n",
      "       0.95014925, 0.98059701, 0.97313433, 0.97970149, 0.98537313,\n",
      "       0.97641791, 0.96656716, 0.97462687, 0.97701493, 0.97820896,\n",
      "       0.96746269, 0.9838806 , 0.98626866, 0.95940299, 0.98716418]), 'val_losses': array([3.4125986 , 3.20821258, 1.14520662, 0.88451127, 2.13052894,\n",
      "       1.02723108, 0.69024483, 0.55313579, 0.88211366, 0.37804885,\n",
      "       0.42448238, 0.41875222, 0.27098098, 0.22164851, 0.23083301,\n",
      "       0.14613126, 0.11986729, 0.11314827, 0.12674783, 0.09724742,\n",
      "       0.31875585, 0.09476718, 0.07243925, 0.07608152, 0.07940112,\n",
      "       0.09083747, 0.10150572, 0.0938482 , 0.06624728, 0.07169846,\n",
      "       0.15444433, 0.06643069, 0.08728277, 0.06122757, 0.04432355,\n",
      "       0.07154815, 0.11128052, 0.0817606 , 0.07945174, 0.07526214,\n",
      "       0.101979  , 0.0503574 , 0.04743122, 0.1308452 , 0.05016154])}, 'ATTACH::27::best_val_loss': 0.044323551776026614, 'ATTACH::28::history': {'train_accs': array([0.46204941, 0.5566087 , 0.6369132 , 0.6949026 , 0.73818942,\n",
      "       0.77072916, 0.79229793, 0.80677663, 0.8143145 , 0.82722591,\n",
      "       0.83468916, 0.84461527, 0.84580939, 0.85536234, 0.85633256,\n",
      "       0.86237779, 0.86260169, 0.87036346, 0.86857228, 0.86849765,\n",
      "       0.86827375]), 'train_losses': array([9.00522286, 1.40333669, 0.8196019 , 0.67313555, 0.58892026,\n",
      "       0.53527242, 0.48697571, 0.45534504, 0.4388596 , 0.40891791,\n",
      "       0.39869029, 0.38210345, 0.38045637, 0.36842773, 0.36218945,\n",
      "       0.3450534 , 0.34810018, 0.33083889, 0.33215574, 0.33844208,\n",
      "       0.34027893]), 'val_accs': array([0.62686567, 0.61820896, 0.76268657, 0.75014925, 0.74089552,\n",
      "       0.75402985, 0.84746269, 0.85552239, 0.7838806 , 0.76447761,\n",
      "       0.85761194, 0.79462687, 0.80895522, 0.83104478, 0.82298507,\n",
      "       0.84119403, 0.85373134, 0.80059701, 0.80149254, 0.82149254,\n",
      "       0.82507463]), 'val_losses': array([1.35673816, 0.88835163, 0.57579801, 0.55650786, 0.62311324,\n",
      "       0.5302436 , 0.37196743, 0.37995782, 0.48199788, 0.61334695,\n",
      "       0.33807572, 0.51495461, 0.46185722, 0.44068335, 0.45135522,\n",
      "       0.45159735, 0.36676808, 0.46779249, 0.48455934, 0.47487058,\n",
      "       0.40890987])}, 'ATTACH::28::best_val_loss': 0.33807571674460796, 'ATTACH::29::history': {'train_accs': array([0.45085454, 0.56593776, 0.66385551, 0.70512725, 0.73438316,\n",
      "       0.75990746, 0.77461005, 0.77752071, 0.77647586, 0.8085678 ,\n",
      "       0.80662736, 0.79147698, 0.82446451, 0.82647959, 0.8285693 ,\n",
      "       0.83282335, 0.8368535 , 0.82438988, 0.82521084, 0.82625569,\n",
      "       0.84036122, 0.84394358, 0.84939175, 0.83804762]), 'train_losses': array([9.87139455, 1.18846137, 0.7643332 , 0.65506599, 0.60282032,\n",
      "       0.54436397, 0.52638038, 0.51509342, 0.5202636 , 0.44527236,\n",
      "       0.45068423, 0.48342789, 0.42034538, 0.41674796, 0.41659941,\n",
      "       0.40802994, 0.40052019, 0.41347317, 0.42228853, 0.43424535,\n",
      "       0.38850358, 0.39325323, 0.38368805, 0.39773148]), 'val_accs': array([0.59044776, 0.68089552, 0.67880597, 0.70477612, 0.69492537,\n",
      "       0.80089552, 0.77164179, 0.75820896, 0.7758209 , 0.79701493,\n",
      "       0.81074627, 0.86985075, 0.86895522, 0.81940299, 0.80358209,\n",
      "       0.74358209, 0.8358209 , 0.6758209 , 0.79910448, 0.80029851,\n",
      "       0.80089552, 0.77134328, 0.83522388, 0.82268657]), 'val_losses': array([1.07261035, 0.68805677, 0.69370161, 0.65799089, 0.70747179,\n",
      "       0.46941909, 0.50056525, 0.47255655, 0.51798822, 0.4519989 ,\n",
      "       0.51575558, 0.42581507, 0.38777487, 0.37671019, 0.50579245,\n",
      "       0.56159058, 0.45045636, 0.78768127, 0.44545967, 0.52375549,\n",
      "       0.54417768, 0.57188629, 0.4874887 , 0.57657136])}, 'ATTACH::29::best_val_loss': 0.3767101946161754}\n",
      "{'ATTACH::0::history': {'train_accs': array([0.34211508, 0.37995373, 0.40443317, 0.43152474, 0.45152623,\n",
      "       0.48279722, 0.51511307, 0.53414434, 0.55742966, 0.59034256,\n",
      "       0.61138891, 0.62235988, 0.65795955, 0.66810956, 0.68781252,\n",
      "       0.7061721 , 0.71871035, 0.73087544, 0.74371222, 0.75722069,\n",
      "       0.76796776, 0.78043138, 0.78378984, 0.79886559, 0.80506008,\n",
      "       0.81364281, 0.82065826, 0.8258079 , 0.83730129, 0.85051123,\n",
      "       0.85692962, 0.85394432, 0.867005  , 0.87185611, 0.87581163,\n",
      "       0.87939398, 0.88394656, 0.88939473, 0.89708187, 0.89999254,\n",
      "       0.90558997, 0.91521755, 0.90753041, 0.91603851, 0.92342712,\n",
      "       0.92238227, 0.92327786, 0.92999478, 0.92969625, 0.93566684,\n",
      "       0.93395029, 0.9390253 , 0.94387641, 0.94305545, 0.94790656,\n",
      "       0.94439884, 0.94484663, 0.949847  , 0.95402642, 0.95723561]), 'train_losses': array([59.43907496, 19.8097216 , 11.88254062,  8.45640275,  6.5087535 ,\n",
      "        5.35062708,  4.32276691,  3.74286838,  3.15081777,  2.76334554,\n",
      "        2.39388019,  2.16324623,  1.92817028,  1.74208118,  1.5559695 ,\n",
      "        1.42491725,  1.26600546,  1.17408407,  1.06535619,  0.97262459,\n",
      "        0.92152121,  0.82988328,  0.83558543,  0.74250539,  0.73211516,\n",
      "        0.69951955,  0.63924216,  0.62316298,  0.55531424,  0.52101639,\n",
      "        0.51149732,  0.49977229,  0.45815575,  0.43203453,  0.42517077,\n",
      "        0.38109838,  0.37671007,  0.35372024,  0.3348225 ,  0.32687008,\n",
      "        0.31344319,  0.26873447,  0.27864119,  0.27031292,  0.24291602,\n",
      "        0.24013667,  0.23878789,  0.21543548,  0.22058088,  0.20220411,\n",
      "        0.20820127,  0.1966185 ,  0.17816957,  0.17152574,  0.1614879 ,\n",
      "        0.16867769,  0.1716097 ,  0.1589806 ,  0.14352495,  0.13662807]), 'val_accs': array([0.42358209, 0.41074627, 0.5280597 , 0.49731343, 0.59134328,\n",
      "       0.59850746, 0.59253731, 0.71522388, 0.72447761, 0.67761194,\n",
      "       0.76895522, 0.79402985, 0.79671642, 0.80567164, 0.72865672,\n",
      "       0.82119403, 0.76179104, 0.84119403, 0.87253731, 0.83373134,\n",
      "       0.86358209, 0.87701493, 0.8519403 , 0.86626866, 0.86      ,\n",
      "       0.83343284, 0.87014925, 0.81910448, 0.90358209, 0.88895522,\n",
      "       0.89641791, 0.87791045, 0.89731343, 0.9       , 0.88238806,\n",
      "       0.91791045, 0.91552239, 0.92208955, 0.8919403 , 0.9319403 ,\n",
      "       0.91134328, 0.91164179, 0.91910448, 0.9161194 , 0.93134328,\n",
      "       0.93940299, 0.94089552, 0.94865672, 0.92238806, 0.91910448,\n",
      "       0.94238806, 0.94567164, 0.94925373, 0.93074627, 0.94686567,\n",
      "       0.90626866, 0.95731343, 0.93641791, 0.95074627, 0.94268657]), 'val_losses': array([9.38178196, 3.53777636, 3.28288355, 2.69506258, 1.69265756,\n",
      "       1.55446663, 1.49081291, 0.98805635, 0.87856192, 1.00470826,\n",
      "       0.83559341, 0.74694957, 0.74865652, 0.92876036, 0.76293906,\n",
      "       0.66587502, 0.74553943, 0.5536106 , 0.45223325, 0.59928486,\n",
      "       0.49380541, 0.45599977, 0.55681972, 0.45995632, 0.45932649,\n",
      "       0.49108348, 0.48729594, 0.51695755, 0.34447142, 0.3551916 ,\n",
      "       0.35843219, 0.38814878, 0.32179923, 0.34051928, 0.36521353,\n",
      "       0.26775586, 0.25291746, 0.26095369, 0.37875462, 0.21492694,\n",
      "       0.34282493, 0.25481397, 0.26614063, 0.25385486, 0.25717147,\n",
      "       0.23814081, 0.21795254, 0.18319347, 0.25079762, 0.25159802,\n",
      "       0.20644207, 0.19755199, 0.17831026, 0.24283663, 0.18863795,\n",
      "       0.29943231, 0.17473281, 0.21110569, 0.19453436, 0.20580952])}, 'ATTACH::0::best_val_loss': 0.1747328091468384, 'ATTACH::1::history': {'train_accs': array([0.48048362, 0.51593402, 0.51750131, 0.50839615, 0.51533697,\n",
      "       0.4958579 , 0.45510859, 0.46809463, 0.45107844, 0.47205015,\n",
      "       0.47294574, 0.44973505, 0.46540787, 0.46615419]), 'train_losses': array([4.84442226, 0.98287605, 0.95755315, 0.97399634, 0.97994596,\n",
      "       0.99345005, 0.99987686, 0.98937798, 1.10901342, 0.98031678,\n",
      "       1.00403658, 1.14423361, 1.01926369, 1.0482721 ]), 'val_accs': array([0.4919403 , 0.53104478, 0.51791045, 0.53492537, 0.48      ,\n",
      "       0.47014925, 0.52238806, 0.5119403 , 0.53880597, 0.46895522,\n",
      "       0.52      , 0.56089552, 0.46656716, 0.47910448]), 'val_losses': array([0.96477353, 0.87707304, 0.93658991, 0.87404983, 0.91497212,\n",
      "       0.92038852, 0.95282788, 0.91806502, 0.90105272, 0.92804692,\n",
      "       0.90845162, 0.94920597, 0.97632838, 0.97561084])}, 'ATTACH::1::best_val_loss': 0.8740498304367066, 'ATTACH::2::history': {'train_accs': array([0.40600045, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718]), 'train_losses': array([6.45497987, 1.30763215, 1.28582955, 1.27278569, 1.26385682,\n",
      "       1.25721938, 1.25296947, 1.24998741, 1.24871943, 1.24638083,\n",
      "       1.24576909, 1.24394444, 1.24320591, 1.24240141, 1.24220451,\n",
      "       1.24262013, 1.24251326, 1.24236362, 1.24169497, 1.24180858,\n",
      "       1.24161544, 1.24167991, 1.2413166 , 1.24086315, 1.24121906,\n",
      "       1.24118543, 1.24148203, 1.24055251, 1.24106485, 1.24159605,\n",
      "       1.24083852, 1.24150256, 1.24099041, 1.24060837, 1.24120527,\n",
      "       1.24086046, 1.24095816, 1.24076854, 1.24106529, 1.24072869,\n",
      "       1.24124046, 1.2414999 , 1.24093418, 1.24114923, 1.24105933,\n",
      "       1.24081129, 1.24067211, 1.24100027, 1.24096065, 1.24064934,\n",
      "       1.24120167, 1.24088364, 1.24153229, 1.24096308, 1.2411754 ,\n",
      "       1.24090039, 1.24110392, 1.24095891, 1.24066668, 1.24077148]), 'val_accs': array([0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955]), 'val_losses': array([1.33131663, 1.30221107, 1.28396002, 1.27194554, 1.26386131,\n",
      "       1.25834737, 1.25442566, 1.25157497, 1.24946637, 1.24789187,\n",
      "       1.24673899, 1.24583991, 1.24511448, 1.24453579, 1.24408926,\n",
      "       1.24372221, 1.24345056, 1.24318699, 1.24298787, 1.24282463,\n",
      "       1.24268966, 1.24256654, 1.24247551, 1.2423923 , 1.24232298,\n",
      "       1.24226831, 1.24221892, 1.2421727 , 1.24212919, 1.24209264,\n",
      "       1.24206435, 1.24204146, 1.24202118, 1.24200558, 1.2419915 ,\n",
      "       1.24197783, 1.24196427, 1.24195008, 1.24194632, 1.24193605,\n",
      "       1.24193118, 1.24191947, 1.24191083, 1.24190555, 1.24190132,\n",
      "       1.24189149, 1.24189009, 1.24188615, 1.24188363, 1.24187561,\n",
      "       1.24186996, 1.24186794, 1.24186252, 1.24186098, 1.24185631,\n",
      "       1.24185493, 1.24185491, 1.24184572, 1.24184475, 1.24184581])}, 'ATTACH::2::best_val_loss': 1.2418447478849497, 'ATTACH::3::history': {'train_accs': array([0.43182327, 0.46660199, 0.50503769, 0.55317561, 0.59832823,\n",
      "       0.61041869, 0.59541757, 0.57392343, 0.56601239, 0.57631167,\n",
      "       0.57071423, 0.57481902, 0.57720725, 0.58004329, 0.58399881]), 'train_losses': array([3.02872832, 1.062058  , 0.95639416, 0.87238017, 0.80866036,\n",
      "       0.80724751, 0.82197556, 0.83453062, 0.8434828 , 0.84370626,\n",
      "       0.84001117, 0.84849388, 0.84038802, 0.83194859, 0.8276948 ]), 'val_accs': array([0.46567164, 0.49820896, 0.51791045, 0.60955224, 0.69462687,\n",
      "       0.66119403, 0.56716418, 0.65402985, 0.54      , 0.63253731,\n",
      "       0.58835821, 0.63044776, 0.59462687, 0.64179104, 0.65820896]), 'val_losses': array([1.06133335, 0.98390065, 0.84481264, 0.73002112, 0.66336055,\n",
      "       0.69949758, 0.81883931, 0.75978838, 0.90053615, 0.76674625,\n",
      "       0.76063864, 0.77321849, 0.81595538, 0.77855279, 0.76532622])}, 'ATTACH::3::best_val_loss': 0.6633605489802005, 'ATTACH::4::history': {'train_accs': array([0.43495783, 0.44025674, 0.44831704, 0.48914098, 0.51041122,\n",
      "       0.51802373, 0.53548772, 0.54765281, 0.5816852 , 0.61847899,\n",
      "       0.65482499, 0.68497649, 0.72177028, 0.74923502, 0.77125159,\n",
      "       0.7839391 , 0.79267109, 0.81058288, 0.79916412, 0.82610643,\n",
      "       0.830659  , 0.83028584, 0.84073438, 0.84969028, 0.85282484,\n",
      "       0.86334801, 0.86745279, 0.86416897, 0.86558698, 0.87073662,\n",
      "       0.84588402, 0.85842227, 0.85409359, 0.87342339, 0.86655721,\n",
      "       0.85327263, 0.8644675 , 0.86155683]), 'train_losses': array([5.23721190e+02, 1.23664937e+00, 1.21593599e+00, 1.15229562e+00,\n",
      "       1.10198893e+00, 1.04562196e+00, 9.90370342e-01, 9.53021018e-01,\n",
      "       9.02825641e-01, 8.56526701e-01, 8.08977043e-01, 7.53590263e-01,\n",
      "       6.80936176e-01, 6.23210576e-01, 5.81033498e-01, 5.50600863e-01,\n",
      "       5.45351938e-01, 4.93945158e-01, 5.30743209e-01, 4.65953821e-01,\n",
      "       4.44586477e-01, 4.52212207e-01, 4.18364504e-01, 4.06439819e-01,\n",
      "       3.96692757e-01, 3.68817295e-01, 3.62780350e-01, 3.72475892e-01,\n",
      "       3.65159814e-01, 3.60877683e-01, 4.09770491e-01, 3.76319393e-01,\n",
      "       3.87360700e-01, 3.43498273e-01, 3.59276908e-01, 3.99152024e-01,\n",
      "       3.61215269e-01, 3.67162530e-01]), 'val_accs': array([0.44208955, 0.44208955, 0.47492537, 0.50119403, 0.48776119,\n",
      "       0.48537313, 0.51880597, 0.55641791, 0.61164179, 0.64328358,\n",
      "       0.67402985, 0.74029851, 0.77014925, 0.76238806, 0.82119403,\n",
      "       0.79910448, 0.83044776, 0.8480597 , 0.80597015, 0.76029851,\n",
      "       0.83701493, 0.83940299, 0.82268657, 0.84149254, 0.85970149,\n",
      "       0.84835821, 0.84179104, 0.86746269, 0.87014925, 0.85552239,\n",
      "       0.83253731, 0.84358209, 0.85910448, 0.85641791, 0.82597015,\n",
      "       0.85373134, 0.86447761, 0.81880597]), 'val_losses': array([1.23695163, 1.22352776, 1.19332653, 1.12889045, 1.11081328,\n",
      "       1.03033784, 1.0151751 , 0.9399912 , 0.83001453, 0.83926697,\n",
      "       0.8406951 , 0.66063864, 0.59502713, 0.70654068, 0.48080063,\n",
      "       0.5094686 , 0.45641127, 0.42898519, 0.5069744 , 0.63349574,\n",
      "       0.42392894, 0.43394437, 0.4892032 , 0.43202665, 0.36495486,\n",
      "       0.45161372, 0.42469567, 0.35623088, 0.35732037, 0.41841029,\n",
      "       0.45555149, 0.42198147, 0.376546  , 0.4092676 , 0.4798442 ,\n",
      "       0.41168927, 0.38519854, 0.49783042])}, 'ATTACH::4::best_val_loss': 0.356230883900799, 'ATTACH::5::history': {'train_accs': array([0.41704605, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.46764684, 0.51399358, 0.52757668, 0.52772595,\n",
      "       0.54347339, 0.55556385, 0.55556385, 0.56757967, 0.57452049,\n",
      "       0.58661094, 0.5952683 , 0.60474662, 0.61273229, 0.61944921,\n",
      "       0.62168819, 0.63004702, 0.63698783, 0.64146578, 0.65325771,\n",
      "       0.66430331, 0.67952832, 0.68594671, 0.69773864, 0.71132174,\n",
      "       0.71968057, 0.73848795, 0.74796627, 0.7561012 , 0.76513173,\n",
      "       0.77237107, 0.77125159, 0.78125233, 0.78714829, 0.78356594,\n",
      "       0.80274647, 0.80035824, 0.7947608 , 0.8087917 , 0.8115531 ,\n",
      "       0.81625494, 0.81976267, 0.82961415, 0.82625569, 0.83543548]), 'train_losses': array([5.81131876, 1.27744334, 1.25691674, 1.24825091, 1.24425603,\n",
      "       1.24219216, 1.24081659, 1.23933638, 1.23800778, 1.23640094,\n",
      "       1.23451838, 1.23176327, 1.22748391, 1.22176618, 1.21218148,\n",
      "       1.19393691, 1.16563726, 1.13705429, 1.11854912, 1.10922215,\n",
      "       1.08476716, 1.06564157, 1.05862076, 1.03682265, 1.01909401,\n",
      "       0.99803904, 0.98139952, 0.96397658, 0.94602571, 0.92617546,\n",
      "       0.90944311, 0.88867436, 0.86371437, 0.85197463, 0.82583154,\n",
      "       0.80610907, 0.78188513, 0.77083762, 0.75463391, 0.73268657,\n",
      "       0.71987157, 0.69561984, 0.66753726, 0.65058067, 0.63896805,\n",
      "       0.61784437, 0.6188545 , 0.59453396, 0.5790597 , 0.58387349,\n",
      "       0.53544032, 0.53738574, 0.55893353, 0.51520385, 0.50281635,\n",
      "       0.49642879, 0.48189225, 0.45854406, 0.46171311, 0.44089835]), 'val_accs': array([0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.50895522, 0.52716418, 0.53343284, 0.54358209,\n",
      "       0.55402985, 0.55313433, 0.53492537, 0.57104478, 0.5719403 ,\n",
      "       0.55283582, 0.61343284, 0.6158209 , 0.61791045, 0.60985075,\n",
      "       0.62746269, 0.6319403 , 0.64985075, 0.64089552, 0.64447761,\n",
      "       0.68626866, 0.70029851, 0.70955224, 0.71462687, 0.71104478,\n",
      "       0.71791045, 0.74865672, 0.74298507, 0.74716418, 0.76507463,\n",
      "       0.79373134, 0.76447761, 0.78626866, 0.78208955, 0.80626866,\n",
      "       0.8       , 0.78298507, 0.80298507, 0.78477612, 0.79701493,\n",
      "       0.82268657, 0.8158209 , 0.79910448, 0.82656716, 0.83373134]), 'val_losses': array([1.29584081, 1.26390506, 1.25102516, 1.24550202, 1.24284238,\n",
      "       1.24099968, 1.23977463, 1.23789512, 1.2363444 , 1.23467482,\n",
      "       1.23236743, 1.22905198, 1.2244551 , 1.21869178, 1.2062743 ,\n",
      "       1.18506334, 1.15335542, 1.12706291, 1.10963355, 1.090669  ,\n",
      "       1.0736993 , 1.06421009, 1.07840611, 1.0274179 , 1.02480867,\n",
      "       1.04344489, 0.9455381 , 0.94820906, 0.93260741, 0.939893  ,\n",
      "       0.90435865, 0.87238462, 0.83489137, 0.83381285, 0.85758325,\n",
      "       0.77486011, 0.76645831, 0.76151193, 0.75018482, 0.73042488,\n",
      "       0.7301813 , 0.70372111, 0.71150578, 0.68102885, 0.62818021,\n",
      "       0.58950749, 0.6236351 , 0.5826966 , 0.58321745, 0.54961057,\n",
      "       0.57979701, 0.5865469 , 0.53338494, 0.5757729 , 0.5680711 ,\n",
      "       0.52275103, 0.50563297, 0.53518976, 0.48384275, 0.46726343])}, 'ATTACH::5::best_val_loss': 0.4672634331503911, 'ATTACH::6::history': {'train_accs': array([0.39607433, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.46287036, 0.52085977, 0.53414434,\n",
      "       0.53414434, 0.54705575, 0.55556385, 0.56444511, 0.5708635 ,\n",
      "       0.58608851, 0.60138816, 0.61101575, 0.61825509, 0.6428838 ,\n",
      "       0.66385551, 0.67631913, 0.68415553, 0.69751474, 0.71027689,\n",
      "       0.72147175, 0.7337861 , 0.73871184, 0.7482648 , 0.76378834,\n",
      "       0.76677364, 0.77281887, 0.77162475, 0.77371446, 0.78916337,\n",
      "       0.79625345, 0.80998582, 0.79797   , 0.82685275, 0.83700276,\n",
      "       0.85737742, 0.87237854, 0.87999104, 0.88431973, 0.88991716,\n",
      "       0.89170834, 0.89954474, 0.90155982, 0.91230689, 0.91827748]), 'train_losses': array([6.48886288, 1.2855199 , 1.26305424, 1.2528272 , 1.24755523,\n",
      "       1.24465038, 1.24311988, 1.24212636, 1.24140409, 1.24098894,\n",
      "       1.24060178, 1.24012604, 1.2393465 , 1.23772999, 1.23507396,\n",
      "       1.23181133, 1.22570084, 1.19724219, 1.14191953, 1.09449501,\n",
      "       1.06827685, 1.02826236, 0.99951076, 0.96774455, 0.95013584,\n",
      "       0.9173962 , 0.89899094, 0.86620355, 0.85510829, 0.8285218 ,\n",
      "       0.79307378, 0.77877233, 0.76531684, 0.73717003, 0.71592594,\n",
      "       0.69353561, 0.67105401, 0.66305668, 0.64130339, 0.61328974,\n",
      "       0.60076758, 0.58676595, 0.5813183 , 0.57383542, 0.53592295,\n",
      "       0.51306533, 0.49356283, 0.52958163, 0.45382306, 0.43524481,\n",
      "       0.4041322 , 0.36707826, 0.35355841, 0.33715732, 0.31764008,\n",
      "       0.31049395, 0.29836509, 0.28553173, 0.26115822, 0.24697165]), 'val_accs': array([0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.49552239, 0.52059701, 0.50626866,\n",
      "       0.54298507, 0.55373134, 0.5561194 , 0.53283582, 0.55522388,\n",
      "       0.58597015, 0.56567164, 0.61880597, 0.63402985, 0.65044776,\n",
      "       0.68895522, 0.71462687, 0.71014925, 0.63014925, 0.72447761,\n",
      "       0.7558209 , 0.70925373, 0.76059701, 0.75880597, 0.76179104,\n",
      "       0.78328358, 0.78507463, 0.77880597, 0.78597015, 0.80955224,\n",
      "       0.7961194 , 0.81164179, 0.82985075, 0.84537313, 0.8519403 ,\n",
      "       0.87253731, 0.8761194 , 0.89343284, 0.88507463, 0.88059701,\n",
      "       0.8961194 , 0.8919403 , 0.90656716, 0.90955224, 0.91970149]), 'val_losses': array([1.30374133, 1.27080106, 1.25673547, 1.24938813, 1.24535847,\n",
      "       1.24327416, 1.24205239, 1.24126743, 1.24076502, 1.24032121,\n",
      "       1.23983564, 1.23940323, 1.2378874 , 1.23551572, 1.23250605,\n",
      "       1.22801655, 1.2171934 , 1.16836876, 1.11937951, 1.12134516,\n",
      "       1.04451538, 0.99379354, 0.97491291, 1.00545213, 0.95813041,\n",
      "       0.92328041, 0.94361526, 0.86723114, 0.85026988, 0.81170831,\n",
      "       0.76252068, 0.73412737, 0.74494236, 0.9369017 , 0.70416549,\n",
      "       0.65926385, 0.72897675, 0.62445912, 0.63237812, 0.64182473,\n",
      "       0.58305752, 0.58452175, 0.57671016, 0.58082713, 0.50371703,\n",
      "       0.53247538, 0.51657071, 0.47823941, 0.43410255, 0.4430209 ,\n",
      "       0.37679992, 0.36898248, 0.31444196, 0.33987243, 0.35113845,\n",
      "       0.32331039, 0.32681601, 0.29569693, 0.27445478, 0.25064537])}, 'ATTACH::6::best_val_loss': 0.25064537436214845, 'ATTACH::7::history': {'train_accs': array([0.34092096, 0.42353907, 0.44854094, 0.47458766, 0.47966266,\n",
      "       0.48608105, 0.52906933, 0.5676543 , 0.60601537, 0.66019852,\n",
      "       0.70378386, 0.73087544, 0.75751922, 0.79095455, 0.8283454 ,\n",
      "       0.85110829, 0.87013956, 0.89081275, 0.90924696, 0.91730726,\n",
      "       0.93335324, 0.93812971, 0.9415628 , 0.94954847, 0.95036943,\n",
      "       0.95522054, 0.95708635, 0.96096724, 0.96529592, 0.96790805,\n",
      "       0.97104262, 0.97484887, 0.97081872, 0.97283379, 0.97507277,\n",
      "       0.97932682, 0.97671468, 0.97917755, 0.98447645, 0.9857452 ,\n",
      "       0.9857452 , 0.98798418, 0.98820808, 0.98753638, 0.99067095,\n",
      "       0.98611837, 0.99231286, 0.98656616, 0.98940219, 0.99208896,\n",
      "       0.98656616, 0.98776028, 0.99320845, 0.99074558]), 'train_losses': array([21.90500237,  5.42244823,  3.44015851,  2.60786697,  2.15758174,\n",
      "        1.85533482,  1.54268918,  1.29863285,  1.17519574,  0.97861726,\n",
      "        0.83423417,  0.75683123,  0.6914933 ,  0.58295216,  0.49539772,\n",
      "        0.44001925,  0.37713706,  0.32964615,  0.27119615,  0.25017494,\n",
      "        0.20551066,  0.19251894,  0.1770866 ,  0.158887  ,  0.14916874,\n",
      "        0.13370815,  0.12995377,  0.11630213,  0.10811219,  0.09753748,\n",
      "        0.08898701,  0.07539473,  0.0909112 ,  0.08772513,  0.0746255 ,\n",
      "        0.06771493,  0.07039932,  0.06862167,  0.0543563 ,  0.04496736,\n",
      "        0.04352908,  0.03945107,  0.03969737,  0.037409  ,  0.02872995,\n",
      "        0.04468598,  0.02441962,  0.04357397,  0.03442134,  0.02343455,\n",
      "        0.03900156,  0.03949022,  0.02331835,  0.02753363]), 'val_accs': array([0.52776119, 0.50746269, 0.52298507, 0.54507463, 0.54776119,\n",
      "       0.55462687, 0.59044776, 0.60567164, 0.66298507, 0.7119403 ,\n",
      "       0.78149254, 0.79820896, 0.84179104, 0.87164179, 0.88925373,\n",
      "       0.89731343, 0.85910448, 0.90477612, 0.92776119, 0.94268657,\n",
      "       0.94686567, 0.93970149, 0.93910448, 0.94656716, 0.95343284,\n",
      "       0.94149254, 0.93671642, 0.92656716, 0.94447761, 0.95522388,\n",
      "       0.95343284, 0.95373134, 0.95104478, 0.95880597, 0.95522388,\n",
      "       0.96477612, 0.96746269, 0.9680597 , 0.96477612, 0.96955224,\n",
      "       0.96895522, 0.97402985, 0.97164179, 0.97432836, 0.96059701,\n",
      "       0.95850746, 0.97402985, 0.96328358, 0.96238806, 0.96776119,\n",
      "       0.97134328, 0.96746269, 0.97820896, 0.97223881]), 'val_losses': array([2.62627795, 1.93614579, 1.61638574, 1.4419546 , 1.32489665,\n",
      "       1.17867971, 0.9746896 , 0.90774022, 0.75113959, 0.67146373,\n",
      "       0.54478576, 0.45609008, 0.43207264, 0.37056448, 0.30468377,\n",
      "       0.28110529, 0.38734504, 0.26982339, 0.21115762, 0.1870217 ,\n",
      "       0.1711547 , 0.17521417, 0.21667131, 0.15862342, 0.1269917 ,\n",
      "       0.17489691, 0.20048185, 0.22480598, 0.18983645, 0.16476839,\n",
      "       0.13261022, 0.13829813, 0.15972277, 0.12388815, 0.12870213,\n",
      "       0.10673601, 0.12683102, 0.10668906, 0.11667385, 0.10073882,\n",
      "       0.11422281, 0.1074343 , 0.1176875 , 0.09610307, 0.16431494,\n",
      "       0.15359933, 0.10573589, 0.14764691, 0.1359742 , 0.12302803,\n",
      "       0.11001591, 0.11479948, 0.10422175, 0.12444322])}, 'ATTACH::7::best_val_loss': 0.09610306863909338, 'ATTACH::8::history': {'train_accs': array([0.4155534 , 0.4599597 , 0.48279722, 0.50197776, 0.52421823,\n",
      "       0.55242929, 0.57198298, 0.60034331, 0.63706247, 0.65989999,\n",
      "       0.67452795, 0.70027614, 0.71550116, 0.7422942 , 0.7673707 ,\n",
      "       0.78490932, 0.80177625, 0.81416524, 0.82162848, 0.8366296 ,\n",
      "       0.84998881, 0.8589447 , 0.86693037, 0.87536383, 0.87790134,\n",
      "       0.89036495, 0.89073811, 0.89954474, 0.90573923, 0.91491902,\n",
      "       0.91185909, 0.91641167, 0.92297933, 0.92559146, 0.93163669,\n",
      "       0.93186059, 0.9417867 , 0.94924994, 0.95044406, 0.95469811,\n",
      "       0.95738488, 0.96223599, 0.96574371, 0.96693783, 0.9666393 ,\n",
      "       0.96902754, 0.97171431, 0.97253526, 0.9726099 , 0.97455034,\n",
      "       0.97798343, 0.97484887, 0.97805806, 0.97999851, 0.97910292,\n",
      "       0.98156579, 0.98007314, 0.98178969, 0.98208822, 0.98440182]), 'train_losses': array([15.56019745,  5.63877226,  3.61135458,  2.55209085,  1.89652188,\n",
      "        1.45523859,  1.15691271,  0.95930014,  0.82638098,  0.75921338,\n",
      "        0.71251589,  0.66324914,  0.62868783,  0.59559701,  0.55261483,\n",
      "        0.53253518,  0.49507999,  0.4681559 ,  0.44597346,  0.43146163,\n",
      "        0.38562729,  0.3786327 ,  0.36097646,  0.33317613,  0.34261476,\n",
      "        0.30357683,  0.30198852,  0.27757101,  0.26837528,  0.24738543,\n",
      "        0.25143459,  0.23861635,  0.22325381,  0.21005578,  0.1934842 ,\n",
      "        0.19589419,  0.16738621,  0.15361675,  0.15301559,  0.13350068,\n",
      "        0.12352541,  0.11639846,  0.11391352,  0.10544121,  0.09989642,\n",
      "        0.09416088,  0.08280141,  0.0845979 ,  0.085397  ,  0.07980232,\n",
      "        0.07151149,  0.07379098,  0.0708816 ,  0.06199092,  0.06583104,\n",
      "        0.05919061,  0.06111236,  0.05861144,  0.05496107,  0.05495003]), 'val_accs': array([0.51552239, 0.54328358, 0.53820896, 0.49462687, 0.61910448,\n",
      "       0.66865672, 0.69402985, 0.5841791 , 0.67671642, 0.67253731,\n",
      "       0.72179104, 0.70059701, 0.61731343, 0.79283582, 0.85164179,\n",
      "       0.75671642, 0.75940299, 0.74985075, 0.84716418, 0.8241791 ,\n",
      "       0.82746269, 0.89343284, 0.88716418, 0.90865672, 0.88626866,\n",
      "       0.84298507, 0.90179104, 0.89880597, 0.84328358, 0.90895522,\n",
      "       0.89104478, 0.8961194 , 0.91701493, 0.91402985, 0.93791045,\n",
      "       0.92567164, 0.92656716, 0.93850746, 0.93970149, 0.96268657,\n",
      "       0.88925373, 0.95552239, 0.96208955, 0.96328358, 0.94208955,\n",
      "       0.97432836, 0.94895522, 0.93462687, 0.94298507, 0.96985075,\n",
      "       0.97104478, 0.97462687, 0.97104478, 0.97164179, 0.98268657,\n",
      "       0.95552239, 0.97522388, 0.98149254, 0.97373134, 0.98238806]), 'val_losses': array([6.08323533, 2.55522095, 2.15494939, 1.19760446, 1.11068715,\n",
      "       0.91674906, 0.69714467, 1.27176077, 0.6651568 , 0.73430884,\n",
      "       0.63877258, 0.58863835, 0.79508592, 0.48209434, 0.40930161,\n",
      "       0.53631427, 0.55831734, 0.68297256, 0.38499887, 0.44395795,\n",
      "       0.43909434, 0.28804635, 0.29878143, 0.25734841, 0.27120176,\n",
      "       0.3749127 , 0.23986978, 0.28440871, 0.38317678, 0.21483055,\n",
      "       0.27393985, 0.23697954, 0.18415655, 0.22310345, 0.16460119,\n",
      "       0.18446862, 0.22248949, 0.18326837, 0.15346152, 0.11479726,\n",
      "       0.37391172, 0.12987424, 0.11046704, 0.1132737 , 0.17166952,\n",
      "       0.08005891, 0.15152088, 0.18418315, 0.16146247, 0.08540844,\n",
      "       0.08780935, 0.07436552, 0.0834742 , 0.08282112, 0.05196687,\n",
      "       0.1323201 , 0.06868762, 0.06118215, 0.07771591, 0.05789202])}, 'ATTACH::8::best_val_loss': 0.051966866325976246, 'ATTACH::9::history': {'train_accs': array([0.29815658, 0.4433913 , 0.44607807, 0.44451078, 0.44197328,\n",
      "       0.44309277, 0.44286887, 0.44286887, 0.44286887, 0.44271961,\n",
      "       0.4429435 , 0.44301814, 0.44563027, 0.44279424, 0.4429435 ,\n",
      "       0.4429435 , 0.44249571, 0.4429435 , 0.44309277, 0.4433913 ,\n",
      "       0.44279424, 0.44346593, 0.44510784, 0.44466005, 0.4429435 ,\n",
      "       0.44182402, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718]), 'train_losses': array([7.39615874, 1.3379579 , 1.3106011 , 1.29155224, 1.2845611 ,\n",
      "       1.27281371, 1.26543527, 1.26005522, 1.25656187, 1.25197078,\n",
      "       1.24943809, 1.2472805 , 1.24057022, 1.24540974, 1.24374114,\n",
      "       1.24162792, 1.24074592, 1.23954785, 1.2376873 , 1.23624366,\n",
      "       1.23486825, 1.22825969, 1.21797111, 1.20597099, 1.21947265,\n",
      "       1.23871359, 1.23710978, 1.23676896, 1.23704566, 1.23667851,\n",
      "       1.23709975, 1.23673476, 1.2362489 , 1.23659806]), 'val_accs': array([0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44895522, 0.44895522, 0.44895522, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955]), 'val_losses': array([1.3632509 , 1.33365468, 1.31129911, 1.29524198, 1.28340521,\n",
      "       1.27421908, 1.26700023, 1.26132837, 1.25667595, 1.25313605,\n",
      "       1.24990592, 1.25117592, 1.24519066, 1.24716258, 1.2451231 ,\n",
      "       1.24248562, 1.24089315, 1.23985497, 1.23848987, 1.23736834,\n",
      "       1.2277938 , 1.21338055, 1.21150139, 1.1772072 , 1.23751408,\n",
      "       1.23712836, 1.23696575, 1.23687269, 1.23680797, 1.23674811,\n",
      "       1.23669305, 1.23664098, 1.23658761, 1.23653561])}, 'ATTACH::9::best_val_loss': 1.177207199779909, 'ATTACH::10::history': {'train_accs': array([0.33144264, 0.32860661, 0.33077095, 0.32539742, 0.34159266,\n",
      "       0.33024853, 0.32009851, 0.32285991, 0.34204045, 0.34674229,\n",
      "       0.35465333, 0.36480334, 0.37525188, 0.40570192, 0.41062766,\n",
      "       0.43794313, 0.44682439, 0.44995895, 0.46040749, 0.46899022,\n",
      "       0.48003582, 0.49414135, 0.50466453, 0.50839615, 0.51966565,\n",
      "       0.51682961, 0.52496455, 0.52571087, 0.53899545, 0.54586163,\n",
      "       0.5481006 , 0.54548847, 0.55623554, 0.55817598, 0.56160908,\n",
      "       0.57549071, 0.57265468, 0.58370028, 0.5901933 , 0.58907381,\n",
      "       0.58952161, 0.60153743, 0.61131428, 0.61034406, 0.60832898,\n",
      "       0.61168744, 0.6207926 , 0.623554  , 0.63236062, 0.63146503,\n",
      "       0.62915143, 0.64213747, 0.63922681, 0.65378013, 0.64863049,\n",
      "       0.66131801, 0.65870587, 0.66445257, 0.67109486, 0.67161728]), 'train_losses': array([100.32904766,  74.79595199,  57.96472179,  47.50920435,\n",
      "        36.76661945,  28.55366162,  23.41049845,  19.5500119 ,\n",
      "        16.4840204 ,  13.78327202,  11.48300632,   9.57310009,\n",
      "         8.25785286,   6.87447409,   6.30234266,   5.5909827 ,\n",
      "         5.06477252,   4.7009228 ,   4.28918497,   3.94071868,\n",
      "         3.80877561,   3.49393134,   3.23185561,   3.02983806,\n",
      "         2.87154385,   2.87164365,   2.6785384 ,   2.5123135 ,\n",
      "         2.41737984,   2.30818009,   2.28267103,   2.1522227 ,\n",
      "         2.07147568,   1.99416444,   1.91847713,   1.82186121,\n",
      "         1.79571531,   1.71019359,   1.66457551,   1.6634007 ,\n",
      "         1.60823014,   1.5356639 ,   1.49499695,   1.49827379,\n",
      "         1.4550441 ,   1.45605231,   1.36064701,   1.34211079,\n",
      "         1.30034876,   1.27350002,   1.28481183,   1.23228711,\n",
      "         1.24221529,   1.20317846,   1.21958962,   1.13810065,\n",
      "         1.11983274,   1.12553365,   1.08731241,   1.0796346 ]), 'val_accs': array([0.28716418, 0.27283582, 0.30716418, 0.32955224, 0.35223881,\n",
      "       0.22985075, 0.26656716, 0.2919403 , 0.29253731, 0.31373134,\n",
      "       0.34537313, 0.3480597 , 0.41552239, 0.45910448, 0.51671642,\n",
      "       0.57134328, 0.6161194 , 0.54865672, 0.56447761, 0.60268657,\n",
      "       0.62716418, 0.58298507, 0.6119403 , 0.60985075, 0.58776119,\n",
      "       0.57940299, 0.56029851, 0.57671642, 0.5838806 , 0.54955224,\n",
      "       0.57671642, 0.58328358, 0.58567164, 0.56119403, 0.60447761,\n",
      "       0.59373134, 0.58597015, 0.57313433, 0.61641791, 0.57671642,\n",
      "       0.58447761, 0.60268657, 0.61671642, 0.61104478, 0.66537313,\n",
      "       0.59940299, 0.58149254, 0.6438806 , 0.60328358, 0.62776119,\n",
      "       0.60477612, 0.66358209, 0.64268657, 0.57791045, 0.6838806 ,\n",
      "       0.61970149, 0.64208955, 0.62567164, 0.63134328, 0.64776119]), 'val_losses': array([62.44321089, 43.01537259, 30.60640973, 19.87630964, 10.92281581,\n",
      "        8.77206286,  8.03994663,  6.52622386,  6.33142146,  5.51281857,\n",
      "        4.19913864,  3.49386089,  2.09113262,  2.11713391,  1.81831395,\n",
      "        1.61924697,  1.39706367,  1.35288799,  1.37193763,  1.31481344,\n",
      "        1.31797997,  1.25409882,  1.2523848 ,  1.25136378,  1.13688759,\n",
      "        1.19666625,  1.35327502,  1.34239205,  1.15956017,  1.27706756,\n",
      "        1.41050618,  1.25236472,  1.16706653,  1.09931171,  1.04978087,\n",
      "        1.26708114,  1.3527949 ,  1.3502507 ,  1.1260841 ,  1.30087952,\n",
      "        1.08016135,  1.21014482,  0.93665236,  1.05965098,  0.93427551,\n",
      "        1.13510028,  0.97672436,  0.98311508,  1.16003012,  1.07090379,\n",
      "        1.13028893,  0.87568625,  0.96137071,  1.28568603,  0.70239762,\n",
      "        1.06820312,  0.86211766,  0.79470606,  0.97004913,  0.97732762])}, 'ATTACH::10::best_val_loss': 0.7023976150911245, 'ATTACH::11::history': {'train_accs': array([0.48011046, 0.59459661, 0.6534816 , 0.70624673, 0.73132323,\n",
      "       0.74572729, 0.75274274, 0.76811702, 0.7724457 , 0.78580491,\n",
      "       0.7869244 , 0.79707441, 0.80423912, 0.8032689 , 0.81043361,\n",
      "       0.81483693, 0.80401523, 0.80849317, 0.80946339, 0.80976192]), 'train_losses': array([7.42771658, 0.96876021, 0.79395675, 0.68569534, 0.63320868,\n",
      "       0.59408365, 0.57337455, 0.54309019, 0.53254165, 0.52226263,\n",
      "       0.5127312 , 0.49678592, 0.47077439, 0.47566581, 0.45632832,\n",
      "       0.45153142, 0.48721285, 0.4682996 , 0.46500734, 0.46717401]), 'val_accs': array([0.52059701, 0.67522388, 0.63164179, 0.75970149, 0.79850746,\n",
      "       0.76      , 0.81761194, 0.74925373, 0.67641791, 0.85940299,\n",
      "       0.77731343, 0.76686567, 0.69552239, 0.71373134, 0.81820896,\n",
      "       0.77731343, 0.76597015, 0.78716418, 0.68149254, 0.7719403 ]), 'val_losses': array([1.62847593, 0.83913289, 0.82028072, 0.57467726, 0.470003  ,\n",
      "       0.55097101, 0.46947671, 0.6969004 , 0.90298958, 0.4015928 ,\n",
      "       0.61499952, 0.61208323, 0.67638665, 0.72607193, 0.53436284,\n",
      "       0.52408091, 0.6840735 , 0.50668953, 1.01598405, 0.44958516])}, 'ATTACH::11::best_val_loss': 0.4015927958755351, 'ATTACH::12::history': {'train_accs': array([0.37958057, 0.42182252, 0.43771923, 0.44861557, 0.46025823,\n",
      "       0.48048362, 0.49376819, 0.50585865, 0.51175461, 0.52257631,\n",
      "       0.52996492, 0.54750355, 0.56243003, 0.56175834, 0.5676543 ,\n",
      "       0.58862602, 0.59982088, 0.60743339, 0.62295694, 0.62780805,\n",
      "       0.64064482, 0.64788417, 0.64967535, 0.67340846, 0.67982685,\n",
      "       0.6981118 , 0.70818718, 0.7287111 , 0.73751773, 0.7531159 ,\n",
      "       0.76871408, 0.78640197, 0.80446302, 0.82386745, 0.83558475,\n",
      "       0.84409284, 0.85924323, 0.87036346, 0.88051347, 0.88581237,\n",
      "       0.89596239, 0.90917233, 0.90947086, 0.92059109, 0.92678558,\n",
      "       0.92529293, 0.9364878 , 0.94126427, 0.94492126, 0.9475334 ,\n",
      "       0.95335473, 0.95208598, 0.95917606, 0.95708635, 0.96343011,\n",
      "       0.96492276, 0.96932607, 0.96902754, 0.97044556, 0.972386  ]), 'train_losses': array([35.25443268, 13.64808782,  8.96384449,  6.2490628 ,  4.64974666,\n",
      "        3.63705026,  3.00260227,  2.51065288,  2.22691926,  2.0449137 ,\n",
      "        1.80681446,  1.64662482,  1.49469723,  1.42820005,  1.34509518,\n",
      "        1.25636236,  1.19195572,  1.14090778,  1.04413527,  1.02397858,\n",
      "        0.96449536,  0.926838  ,  0.90983957,  0.83496   ,  0.82314739,\n",
      "        0.77643565,  0.73044448,  0.69399109,  0.66604382,  0.62097203,\n",
      "        0.58863766,  0.53202919,  0.50454115,  0.46333733,  0.42607021,\n",
      "        0.39864756,  0.37038579,  0.34050864,  0.31934735,  0.30135202,\n",
      "        0.27918734,  0.24939791,  0.24184913,  0.22255073,  0.20525614,\n",
      "        0.19676685,  0.17403873,  0.16500416,  0.15279475,  0.14703229,\n",
      "        0.12733785,  0.13146073,  0.11673628,  0.11645837,  0.1057342 ,\n",
      "        0.10338608,  0.09064058,  0.08587347,  0.08296951,  0.08000775]), 'val_accs': array([0.54268657, 0.5280597 , 0.47462687, 0.50238806, 0.58298507,\n",
      "       0.57940299, 0.64656716, 0.52447761, 0.47462687, 0.54119403,\n",
      "       0.55910448, 0.64955224, 0.63223881, 0.62507463, 0.68179104,\n",
      "       0.65641791, 0.55492537, 0.57671642, 0.58865672, 0.70477612,\n",
      "       0.73492537, 0.71850746, 0.63253731, 0.59761194, 0.70776119,\n",
      "       0.77492537, 0.74776119, 0.8161194 , 0.79313433, 0.81731343,\n",
      "       0.82716418, 0.84059701, 0.84955224, 0.80089552, 0.85940299,\n",
      "       0.86328358, 0.89791045, 0.9038806 , 0.90119403, 0.90119403,\n",
      "       0.89313433, 0.86507463, 0.9438806 , 0.93343284, 0.91462687,\n",
      "       0.93850746, 0.94029851, 0.9561194 , 0.94925373, 0.95373134,\n",
      "       0.95283582, 0.95134328, 0.96119403, 0.92686567, 0.96955224,\n",
      "       0.95402985, 0.97104478, 0.95462687, 0.95253731, 0.96567164]), 'val_losses': array([7.8199023 , 2.7659566 , 2.33932616, 2.40955694, 1.41244178,\n",
      "       0.95384787, 0.93671151, 1.15067914, 3.43799589, 1.02185956,\n",
      "       0.77568947, 0.7976183 , 0.94593301, 0.91729883, 0.73897686,\n",
      "       0.76161983, 1.45373146, 0.8422686 , 0.73472512, 0.76448336,\n",
      "       0.61042808, 0.6720133 , 0.76741293, 1.03271566, 0.6557694 ,\n",
      "       0.58383661, 0.61946722, 0.48422812, 0.46839145, 0.47151517,\n",
      "       0.42397259, 0.4093942 , 0.36155241, 0.43508379, 0.33506603,\n",
      "       0.32425908, 0.26349279, 0.26008801, 0.24918057, 0.25309107,\n",
      "       0.26710961, 0.34156535, 0.16386218, 0.18286679, 0.22200452,\n",
      "       0.16131328, 0.15575715, 0.12366299, 0.14187752, 0.12078977,\n",
      "       0.12151507, 0.13219415, 0.10265654, 0.21027358, 0.08599519,\n",
      "       0.13291238, 0.08145826, 0.1320507 , 0.12748906, 0.08969391])}, 'ATTACH::12::best_val_loss': 0.08145826371303246, 'ATTACH::13::history': {'train_accs': array([0.3768938 , 0.44943653, 0.48593179, 0.5096649 , 0.56093738,\n",
      "       0.61616539, 0.66094485, 0.69355922, 0.73751773, 0.78580491,\n",
      "       0.82655422, 0.86812449, 0.89633555, 0.92200911, 0.93559221,\n",
      "       0.93872677, 0.94962311, 0.95723561, 0.96596761, 0.97119188,\n",
      "       0.97163967, 0.97708784, 0.9751474 , 0.97805806, 0.98156579,\n",
      "       0.98119263, 0.9834316 , 0.98178969, 0.98395403, 0.98731249,\n",
      "       0.98581984, 0.98731249, 0.98910366, 0.98567057, 0.986193  ,\n",
      "       0.98940219, 0.99261139, 0.98940219, 0.9891783 , 0.98999925,\n",
      "       0.98970072, 0.98887977, 0.99216359, 0.99455183, 0.99440257,\n",
      "       0.99037242, 0.99320845, 0.9864169 , 0.99305918, 0.99395477,\n",
      "       0.99156653, 0.99134264, 0.99350698]), 'train_losses': array([2.17797007e+01, 4.11781364e+00, 2.50220342e+00, 1.78815151e+00,\n",
      "       1.45246734e+00, 1.19455981e+00, 9.80541046e-01, 9.00780436e-01,\n",
      "       7.48906256e-01, 5.90417740e-01, 4.94687018e-01, 3.84901883e-01,\n",
      "       2.94012191e-01, 2.24234957e-01, 1.90029286e-01, 1.79456149e-01,\n",
      "       1.51957013e-01, 1.27094885e-01, 1.08981015e-01, 9.01488362e-02,\n",
      "       8.52437233e-02, 7.14574387e-02, 7.91201258e-02, 6.66588943e-02,\n",
      "       5.59222769e-02, 6.09333857e-02, 5.15023663e-02, 5.64766423e-02,\n",
      "       5.21496802e-02, 3.77606796e-02, 4.08702096e-02, 3.92868986e-02,\n",
      "       3.39572925e-02, 4.56053933e-02, 4.23373714e-02, 3.50703017e-02,\n",
      "       2.33843083e-02, 3.38338476e-02, 3.36161722e-02, 3.18142095e-02,\n",
      "       3.27816747e-02, 3.28024370e-02, 2.63930175e-02, 1.73759368e-02,\n",
      "       1.70497833e-02, 3.17167523e-02, 2.60106088e-02, 4.39427847e-02,\n",
      "       2.11346171e-02, 1.83805040e-02, 2.59786238e-02, 2.81774177e-02,\n",
      "       2.20536985e-02]), 'val_accs': array([0.52238806, 0.49343284, 0.60955224, 0.53492537, 0.66955224,\n",
      "       0.67701493, 0.6361194 , 0.73283582, 0.73343284, 0.85641791,\n",
      "       0.86716418, 0.89492537, 0.93253731, 0.91313433, 0.94716418,\n",
      "       0.95701493, 0.95283582, 0.95671642, 0.96895522, 0.96567164,\n",
      "       0.9680597 , 0.97104478, 0.97492537, 0.96149254, 0.96985075,\n",
      "       0.96895522, 0.97701493, 0.97462687, 0.97462687, 0.96537313,\n",
      "       0.97223881, 0.97970149, 0.97492537, 0.9758209 , 0.97492537,\n",
      "       0.98238806, 0.9758209 , 0.97283582, 0.98119403, 0.96985075,\n",
      "       0.96716418, 0.96776119, 0.98328358, 0.97850746, 0.98089552,\n",
      "       0.97462687, 0.96895522, 0.97880597, 0.98537313, 0.98238806,\n",
      "       0.97910448, 0.98328358, 0.97910448]), 'val_losses': array([2.67465575, 1.87538217, 1.70139818, 1.59924652, 0.72498788,\n",
      "       0.82684891, 0.92963986, 0.61754478, 0.76419402, 0.37526165,\n",
      "       0.34691122, 0.30773126, 0.21014649, 0.27360402, 0.15690746,\n",
      "       0.13146783, 0.1425    , 0.13729399, 0.09738726, 0.10672635,\n",
      "       0.09852426, 0.09696999, 0.08847819, 0.12323101, 0.11159205,\n",
      "       0.09851881, 0.07996532, 0.08550171, 0.0943842 , 0.1453525 ,\n",
      "       0.09998069, 0.06499009, 0.07834237, 0.09474797, 0.10769213,\n",
      "       0.06125319, 0.1025232 , 0.10460196, 0.06825121, 0.10794697,\n",
      "       0.12629393, 0.12552672, 0.0526263 , 0.07048453, 0.07071432,\n",
      "       0.0904783 , 0.12558444, 0.06141445, 0.06176621, 0.05966632,\n",
      "       0.07591382, 0.05672864, 0.07266741])}, 'ATTACH::13::best_val_loss': 0.052626297697285884, 'ATTACH::14::history': {'train_accs': array([0.43309202, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44257034, 0.50996343, 0.5372789 ]), 'train_losses': array([5.6355074 , 1.2606748 , 1.24953822, 1.24534342, 1.2435822 ,\n",
      "       1.24263614, 1.24227911, 1.24207381, 1.24218139, 1.24212349,\n",
      "       1.24202334, 1.24213397, 1.24196841, 1.24205022, 1.24205788,\n",
      "       1.24207634, 1.24189865, 1.24193368, 1.24200005, 1.24190507,\n",
      "       1.24190848, 1.24195128, 1.24184225, 1.24196772, 1.24189307,\n",
      "       1.24177322, 1.24180008, 1.2417904 , 1.24170307, 1.24194176,\n",
      "       1.24179811, 1.24156546, 1.2417664 , 1.24162262, 1.24162851,\n",
      "       1.24141091, 1.24154713, 1.24135865, 1.24139055, 1.24146153,\n",
      "       1.24124014, 1.24114086, 1.24110384, 1.24088893, 1.24093393,\n",
      "       1.24082572, 1.24048995, 1.24026772, 1.2401228 , 1.23969159,\n",
      "       1.23935395, 1.23837783, 1.23773824, 1.23648423, 1.23412753,\n",
      "       1.23000347, 1.22155187, 1.19918082, 1.14541581, 1.09785671]), 'val_accs': array([0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.45791045, 0.53283582, 0.5319403 ]), 'val_losses': array([1.27129563, 1.25280561, 1.24674685, 1.24385902, 1.24269039,\n",
      "       1.24257857, 1.24201424, 1.24197998, 1.24196758, 1.24175137,\n",
      "       1.24167648, 1.24165714, 1.24199744, 1.24164623, 1.2416498 ,\n",
      "       1.24164232, 1.24160032, 1.24161034, 1.24157978, 1.24164032,\n",
      "       1.24155687, 1.24155175, 1.24157478, 1.24152708, 1.24150086,\n",
      "       1.24149804, 1.24145621, 1.24143802, 1.24178239, 1.2414345 ,\n",
      "       1.24137546, 1.24143626, 1.24133656, 1.24128472, 1.24172389,\n",
      "       1.24124521, 1.24148635, 1.24136708, 1.24148056, 1.24138708,\n",
      "       1.24103153, 1.24090281, 1.24079947, 1.24066085, 1.24055286,\n",
      "       1.24038424, 1.24035407, 1.24075658, 1.23967907, 1.23936071,\n",
      "       1.23886008, 1.23806821, 1.23748624, 1.2354168 , 1.2328537 ,\n",
      "       1.22836794, 1.22258311, 1.17757568, 1.1222927 , 1.09363194])}, 'ATTACH::14::best_val_loss': 1.0936319438735052, 'ATTACH::15::history': {'train_accs': array([0.43316665, 0.48839466, 0.53130831, 0.59810434, 0.6428838 ,\n",
      "       0.67818494, 0.70982909, 0.73878648, 0.76147474, 0.77707292,\n",
      "       0.79304426, 0.81274722, 0.82170311, 0.83252482, 0.84692888,\n",
      "       0.85103366, 0.85819837, 0.85610866, 0.86327338, 0.86573625]), 'train_losses': array([12.57454349,  3.47421135,  1.80755154,  1.12684619,  0.85751496,\n",
      "        0.73047137,  0.6587508 ,  0.59743181,  0.54231262,  0.52215595,\n",
      "        0.49291539,  0.45686524,  0.43411169,  0.41195628,  0.40222722,\n",
      "        0.394864  ,  0.37199952,  0.36630801,  0.35524581,  0.34782184]), 'val_accs': array([0.51671642, 0.63641791, 0.60835821, 0.71671642, 0.76567164,\n",
      "       0.73850746, 0.76537313, 0.65791045, 0.81910448, 0.81761194,\n",
      "       0.81940299, 0.76477612, 0.75044776, 0.83522388, 0.74119403,\n",
      "       0.80268657, 0.72447761, 0.82925373, 0.73701493, 0.70059701]), 'val_losses': array([2.57779028, 1.22786034, 1.16670016, 0.70758718, 0.63053589,\n",
      "       0.59307172, 0.54642575, 0.71781154, 0.46370212, 0.43433849,\n",
      "       0.43489794, 0.55923305, 0.50797829, 0.49174066, 0.49500584,\n",
      "       0.60888991, 0.49378219, 0.48833451, 0.52238449, 0.58777831])}, 'ATTACH::15::best_val_loss': 0.43433849146117026, 'ATTACH::16::history': {'train_accs': array([0.42689753, 0.44219718, 0.44227181, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44197328, 0.44219718, 0.44197328,\n",
      "       0.4433913 , 0.4705575 , 0.48802149, 0.51473991, 0.5400403 ,\n",
      "       0.55190686, 0.58332711, 0.60586611, 0.63153967, 0.64974998,\n",
      "       0.6651989 , 0.69087245, 0.70900813, 0.70296291, 0.71736697,\n",
      "       0.73968207, 0.74333906, 0.74975744, 0.75699679, 0.77886409,\n",
      "       0.79834316, 0.77766998, 0.75893723, 0.79371595, 0.78707366,\n",
      "       0.78796925, 0.79991044, 0.82088216, 0.82774834, 0.81610568,\n",
      "       0.81416524, 0.78774535, 0.79864169, 0.80065677, 0.80341817,\n",
      "       0.81006045, 0.82035973, 0.77975968, 0.73878648]), 'train_losses': array([8.76178783e+04, 1.24256041e+00, 1.24154502e+00, 1.24112267e+00,\n",
      "       1.24059592e+00, 1.24005983e+00, 1.23968936e+00, 1.23842936e+00,\n",
      "       1.23543157e+00, 1.23303824e+00, 1.22665244e+00, 1.19316540e+00,\n",
      "       1.16297339e+00, 1.10451405e+00, 1.03801336e+00, 9.86246541e-01,\n",
      "       9.47001381e-01, 9.00030451e-01, 8.71115400e-01, 8.39671285e-01,\n",
      "       8.24535785e-01, 7.70925712e-01, 7.42595933e-01, 7.41841232e-01,\n",
      "       6.97963082e-01, 6.64555999e-01, 6.50399255e-01, 6.31850264e-01,\n",
      "       6.09249901e-01, 5.63433114e-01, 5.30146427e-01, 5.68485919e-01,\n",
      "       6.07269499e-01, 5.40487915e-01, 5.57682254e-01, 5.45698704e-01,\n",
      "       5.18006158e-01, 4.79207423e-01, 4.61549640e-01, 4.86297719e-01,\n",
      "       4.93651905e-01, 5.58882724e-01, 5.21344855e-01, 5.18844270e-01,\n",
      "       5.09239943e-01, 4.94205196e-01, 4.82224001e-01, 5.64263165e-01,\n",
      "       6.53212906e-01]), 'val_accs': array([0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44238806,\n",
      "       0.45820896, 0.4841791 , 0.50865672, 0.53283582, 0.54149254,\n",
      "       0.47940299, 0.60985075, 0.66268657, 0.65283582, 0.64208955,\n",
      "       0.66      , 0.71880597, 0.69283582, 0.73373134, 0.73044776,\n",
      "       0.74328358, 0.76537313, 0.74      , 0.75970149, 0.79492537,\n",
      "       0.81492537, 0.82149254, 0.73104478, 0.83164179, 0.79970149,\n",
      "       0.77492537, 0.8280597 , 0.83940299, 0.85522388, 0.77253731,\n",
      "       0.80119403, 0.75074627, 0.82119403, 0.79970149, 0.83253731,\n",
      "       0.8280597 , 0.74059701, 0.77522388, 0.76925373]), 'val_losses': array([1.244478  , 1.24135507, 1.24113694, 1.23983506, 1.2393619 ,\n",
      "       1.23890203, 1.23820807, 1.23582706, 1.23230201, 1.22813187,\n",
      "       1.21258191, 1.16655065, 1.13444572, 1.06547559, 0.99693677,\n",
      "       1.1194871 , 0.91602029, 0.81962082, 0.84416799, 0.86110923,\n",
      "       0.91136011, 0.71047285, 0.77896106, 0.68767686, 0.68632971,\n",
      "       0.67226583, 0.6231171 , 0.64519884, 0.67512553, 0.52255844,\n",
      "       0.48526783, 0.48946801, 0.70699932, 0.45666269, 0.54826809,\n",
      "       0.64107102, 0.46972524, 0.43504816, 0.38935858, 0.60908492,\n",
      "       0.54436134, 0.66267832, 0.48339905, 0.49849954, 0.46056559,\n",
      "       0.46667864, 0.7834479 , 0.5865192 , 0.5926079 ])}, 'ATTACH::16::best_val_loss': 0.38935857877802493, 'ATTACH::17::history': {'train_accs': array([0.39398463, 0.40719457, 0.43294276, 0.45615344, 0.46137771,\n",
      "       0.4816031 , 0.49018583, 0.50839615, 0.50929174, 0.51809837,\n",
      "       0.52429286, 0.53048735, 0.52809911, 0.53489066, 0.54407045,\n",
      "       0.54772744, 0.56541533, 0.56205687, 0.56175834, 0.57474438,\n",
      "       0.57347563, 0.57093813, 0.57720725, 0.5899694 , 0.58899918,\n",
      "       0.59168595, 0.59190984, 0.5929547 , 0.58967087, 0.601015  ,\n",
      "       0.59855213, 0.60691096, 0.60116427, 0.60982163, 0.6037764 ,\n",
      "       0.60526905, 0.61303082, 0.61444884, 0.614822  , 0.61429957,\n",
      "       0.61982237, 0.62019554, 0.62698709, 0.61967311, 0.62489738,\n",
      "       0.6284051 , 0.63303232, 0.63370401, 0.63668931, 0.64131652,\n",
      "       0.63840585, 0.64251064, 0.64489887, 0.64101799, 0.64915292,\n",
      "       0.64907829, 0.64392865, 0.64997388, 0.65288454, 0.65139189]), 'train_losses': array([11.74614193,  1.21352467,  1.13207928,  1.0852665 ,  1.05484232,\n",
      "        1.00122893,  0.97668671,  0.95606976,  0.93605844,  0.92105969,\n",
      "        0.91465428,  0.91303687,  0.91187559,  0.89696159,  0.89438537,\n",
      "        0.87962549,  0.86623192,  0.86918724,  0.86388129,  0.85502932,\n",
      "        0.84994469,  0.85131957,  0.84301792,  0.83103454,  0.82762618,\n",
      "        0.82325047,  0.80756248,  0.8142288 ,  0.81177521,  0.80488166,\n",
      "        0.79854148,  0.79701573,  0.79835144,  0.78334403,  0.79562925,\n",
      "        0.79214814,  0.78108064,  0.78450709,  0.77037973,  0.7764851 ,\n",
      "        0.77756935,  0.76678332,  0.75947431,  0.76996587,  0.77267883,\n",
      "        0.76314562,  0.75088897,  0.76477839,  0.75557082,  0.74153165,\n",
      "        0.76296141,  0.74966316,  0.74560095,  0.74599083,  0.73708871,\n",
      "        0.73623263,  0.73931364,  0.73925598,  0.73387607,  0.72895517]), 'val_accs': array([0.41164179, 0.52149254, 0.4961194 , 0.47373134, 0.48746269,\n",
      "       0.53104478, 0.50089552, 0.53223881, 0.59164179, 0.57492537,\n",
      "       0.55701493, 0.58955224, 0.58298507, 0.51492537, 0.57970149,\n",
      "       0.56      , 0.57701493, 0.57850746, 0.58537313, 0.53880597,\n",
      "       0.57820896, 0.58089552, 0.60597015, 0.56238806, 0.59462687,\n",
      "       0.62955224, 0.60567164, 0.6041791 , 0.61164179, 0.63343284,\n",
      "       0.63940299, 0.62238806, 0.59880597, 0.6280597 , 0.59223881,\n",
      "       0.64149254, 0.64895522, 0.66149254, 0.69492537, 0.63552239,\n",
      "       0.70955224, 0.65104478, 0.68238806, 0.68328358, 0.68865672,\n",
      "       0.69343284, 0.67134328, 0.71791045, 0.66835821, 0.74597015,\n",
      "       0.7280597 , 0.7080597 , 0.69014925, 0.69104478, 0.69253731,\n",
      "       0.68507463, 0.70626866, 0.70149254, 0.65970149, 0.72298507]), 'val_losses': array([1.13971291, 1.09635016, 1.06682133, 1.02241308, 0.96677907,\n",
      "       0.92010144, 0.89261083, 0.88686082, 0.86984625, 0.85978256,\n",
      "       0.840765  , 0.83776688, 0.82908479, 0.83473877, 0.82452867,\n",
      "       0.82216572, 0.79722548, 0.81328139, 0.80356136, 0.80286815,\n",
      "       0.81048785, 0.78799691, 0.78394244, 0.79421489, 0.77880335,\n",
      "       0.77100036, 0.76917247, 0.74881434, 0.75974938, 0.75709262,\n",
      "       0.75039852, 0.74377191, 0.75245366, 0.75854453, 0.76514623,\n",
      "       0.73320574, 0.71761831, 0.72344695, 0.72427664, 0.71561385,\n",
      "       0.70018834, 0.71762614, 0.70101036, 0.71433364, 0.71998424,\n",
      "       0.72279931, 0.69634064, 0.67527411, 0.69977181, 0.67138391,\n",
      "       0.68656054, 0.68003758, 0.68017974, 0.67603341, 0.67565914,\n",
      "       0.6744086 , 0.66618554, 0.66346566, 0.66900061, 0.66040307])}, 'ATTACH::17::best_val_loss': 0.6604030704142443, 'ATTACH::18::history': {'train_accs': array([0.3959997 , 0.44324203, 0.44264497, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718]), 'train_losses': array([2.03966982, 1.24876868, 1.27176529, 1.27004205, 1.26012582,\n",
      "       1.25585686, 1.25146641, 1.25037598, 1.24821962, 1.24597967,\n",
      "       1.24675603, 1.24521046]), 'val_accs': array([0.44208955, 0.47492537, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955]), 'val_losses': array([1.26890401, 1.20307276, 1.2916506 , 1.27466851, 1.26489007,\n",
      "       1.25871302, 1.25466931, 1.25175635, 1.24966198, 1.24807233,\n",
      "       1.24691944, 1.24599264])}, 'ATTACH::18::best_val_loss': 1.2030727599983784, 'ATTACH::19::history': {'train_accs': array([0.42779312, 0.50093291, 0.59511904, 0.67781178, 0.73401   ,\n",
      "       0.76923651, 0.81506083, 0.84580939, 0.87096052, 0.89118591,\n",
      "       0.90969475, 0.91753116, 0.92835286, 0.93760728, 0.94529442,\n",
      "       0.94827972, 0.95372789, 0.96066871, 0.96081797, 0.96537055,\n",
      "       0.96693783, 0.97089335, 0.97134114, 0.97208747, 0.97581909,\n",
      "       0.97783417, 0.97664005, 0.97753564, 0.98186432, 0.98134189,\n",
      "       0.97917755, 0.98261064, 0.98261064, 0.98193895, 0.9829838 ,\n",
      "       0.98402866, 0.98507351, 0.98477498, 0.98679006, 0.98955146,\n",
      "       0.98514815, 0.98656616, 0.98671543, 0.98761102, 0.98940219,\n",
      "       0.98589447, 0.99029778, 0.98835734, 0.98902903, 0.99022315,\n",
      "       0.98880513, 0.98992462, 0.99082021, 0.99022315, 0.98828271,\n",
      "       0.99037242, 0.99082021, 0.99096948, 0.99141727, 0.98805881]), 'train_losses': array([9.31535829, 2.83988056, 1.56170965, 1.04522062, 0.76165678,\n",
      "       0.60945229, 0.49211225, 0.416546  , 0.35019097, 0.30228597,\n",
      "       0.25209869, 0.22796805, 0.20742035, 0.17521061, 0.15428434,\n",
      "       0.14891545, 0.13828068, 0.11963719, 0.11606191, 0.10124153,\n",
      "       0.10065537, 0.08855043, 0.08538804, 0.08546324, 0.07641444,\n",
      "       0.07202405, 0.07054431, 0.06584577, 0.05799747, 0.05881727,\n",
      "       0.06283999, 0.05390227, 0.0555752 , 0.05675308, 0.05159611,\n",
      "       0.04849468, 0.04868667, 0.04921378, 0.04878864, 0.0339652 ,\n",
      "       0.04801873, 0.04281982, 0.04142677, 0.03783521, 0.03453262,\n",
      "       0.0455665 , 0.03506259, 0.03383383, 0.037896  , 0.03278186,\n",
      "       0.0350771 , 0.03490949, 0.02968978, 0.02915299, 0.03282789,\n",
      "       0.03028449, 0.02949657, 0.02799374, 0.02813   , 0.03290993]), 'val_accs': array([0.63313433, 0.60268657, 0.67671642, 0.7838806 , 0.80567164,\n",
      "       0.85134328, 0.75701493, 0.89880597, 0.89223881, 0.91761194,\n",
      "       0.91552239, 0.94149254, 0.94835821, 0.94      , 0.95761194,\n",
      "       0.96567164, 0.96686567, 0.97432836, 0.95731343, 0.95522388,\n",
      "       0.95791045, 0.93701493, 0.97253731, 0.97432836, 0.97671642,\n",
      "       0.98238806, 0.9838806 , 0.97402985, 0.98029851, 0.97761194,\n",
      "       0.97432836, 0.97313433, 0.95283582, 0.9841791 , 0.98119403,\n",
      "       0.9761194 , 0.98208955, 0.95044776, 0.98119403, 0.9758209 ,\n",
      "       0.98746269, 0.98149254, 0.97970149, 0.97671642, 0.98238806,\n",
      "       0.97940299, 0.9761194 , 0.98716418, 0.98119403, 0.98447761,\n",
      "       0.99044776, 0.97731343, 0.97791045, 0.98119403, 0.98716418,\n",
      "       0.98477612, 0.98746269, 0.98865672, 0.98089552, 0.97283582]), 'val_losses': array([1.41201058, 0.99172122, 1.3426789 , 0.53663059, 0.50165319,\n",
      "       0.39410484, 0.54940732, 0.26116011, 0.26172072, 0.20301975,\n",
      "       0.21818436, 0.15085743, 0.14246396, 0.1717771 , 0.13172825,\n",
      "       0.10202343, 0.11144085, 0.09217817, 0.11795479, 0.14211621,\n",
      "       0.13412786, 0.18354431, 0.08506742, 0.09360184, 0.07618489,\n",
      "       0.06367857, 0.06074906, 0.0858147 , 0.06713548, 0.07070948,\n",
      "       0.07484196, 0.08838209, 0.16356085, 0.05197762, 0.067878  ,\n",
      "       0.08171685, 0.06440432, 0.14613668, 0.06759694, 0.08888088,\n",
      "       0.05021031, 0.07273581, 0.07831209, 0.0878989 , 0.05436368,\n",
      "       0.06800863, 0.08391883, 0.05386664, 0.06403489, 0.05482415,\n",
      "       0.03237665, 0.07494735, 0.07687363, 0.07353974, 0.04802585,\n",
      "       0.06049243, 0.04365725, 0.03549885, 0.06555865, 0.08844273])}, 'ATTACH::19::best_val_loss': 0.03237664831468641, 'ATTACH::20::history': {'train_accs': array([0.25457124, 0.31853123, 0.33412941, 0.35748936, 0.37338607,\n",
      "       0.36868423, 0.37726696, 0.38181954, 0.39592507, 0.39405926,\n",
      "       0.40301515, 0.4125681 , 0.41741921, 0.42652437, 0.44137622,\n",
      "       0.45212329, 0.46204941, 0.47421449, 0.47719979, 0.48309575,\n",
      "       0.49496231, 0.50600791, 0.51765057, 0.5179491 , 0.53108441,\n",
      "       0.52981566, 0.55019031, 0.5563848 , 0.55145906, 0.5566087 ,\n",
      "       0.56556459, 0.57116203, 0.5786999 , 0.57929696, 0.59034256,\n",
      "       0.59011867]), 'train_losses': array([96.50690632, 34.2242772 , 24.68807035, 18.83698541, 13.99734438,\n",
      "       11.48116102,  9.60876264,  8.25716642,  6.91664676,  6.3090638 ,\n",
      "        5.53167234,  4.81623198,  4.33028735,  4.23297388,  3.71407255,\n",
      "        3.35917681,  3.11197279,  2.84681262,  2.71533416,  2.56193876,\n",
      "        2.37698304,  2.23227129,  2.09645612,  1.96496872,  1.89833455,\n",
      "        1.84624851,  1.7052682 ,  1.64520237,  1.55957175,  1.5101586 ,\n",
      "        1.44328607,  1.36373479,  1.3227618 ,  1.25583044,  1.21602734,\n",
      "        1.22754083]), 'val_accs': array([0.38895522, 0.44208955, 0.44119403, 0.45432836, 0.44059701,\n",
      "       0.44626866, 0.41701493, 0.40507463, 0.41343284, 0.41044776,\n",
      "       0.38447761, 0.43283582, 0.49432836, 0.43134328, 0.47253731,\n",
      "       0.51074627, 0.52238806, 0.53492537, 0.57791045, 0.58507463,\n",
      "       0.51552239, 0.54477612, 0.55253731, 0.58208955, 0.57492537,\n",
      "       0.58      , 0.54865672, 0.58059701, 0.5880597 , 0.64238806,\n",
      "       0.54686567, 0.60656716, 0.66268657, 0.63164179, 0.6441791 ,\n",
      "       0.59253731]), 'val_losses': array([22.9670912 , 17.16823208, 10.0475773 ,  7.54546695,  5.38413621,\n",
      "        4.62239197,  3.47430004,  3.50404317,  2.35123077,  2.05858358,\n",
      "        1.78001115,  2.17504484,  1.92062712,  1.55584895,  1.39918326,\n",
      "        1.20502569,  1.19026825,  1.14428868,  1.08735334,  1.14277292,\n",
      "        1.11837065,  1.43684751,  1.29564995,  1.10614232,  1.03374911,\n",
      "        0.90964183,  0.92750267,  1.24005167,  1.0431586 ,  0.92120861,\n",
      "        1.07551221,  1.14716093,  0.96102754,  0.91974339,  0.92647617,\n",
      "        0.91790726])}, 'ATTACH::20::best_val_loss': 0.909641828892836, 'ATTACH::21::history': {'train_accs': array([0.48518546, 0.58414807, 0.60004478, 0.61944921, 0.62780805,\n",
      "       0.64071946, 0.66512426, 0.67714009, 0.69430555, 0.67893126,\n",
      "       0.68863348, 0.70788865, 0.72199418, 0.71176954, 0.70012688,\n",
      "       0.70773938, 0.70229122, 0.68564818, 0.65430256, 0.66385551,\n",
      "       0.66587059, 0.66967684, 0.67452795, 0.67094559, 0.67743861,\n",
      "       0.67415479, 0.66818419, 0.67236361]), 'train_losses': array([9.6787277 , 0.99842778, 0.8910912 , 0.81464679, 0.77214712,\n",
      "       0.7638196 , 0.71912806, 0.69849231, 0.67481919, 0.70139727,\n",
      "       0.68767424, 0.65307081, 0.64541577, 0.6462681 , 0.66834393,\n",
      "       0.65997326, 0.65424887, 0.68708771, 0.7336771 , 0.71767684,\n",
      "       0.70849102, 0.70416091, 0.69744971, 0.69805326, 0.69626176,\n",
      "       0.7010433 , 0.71142685, 0.71043031]), 'val_accs': array([0.6158209 , 0.5638806 , 0.62656716, 0.53253731, 0.59164179,\n",
      "       0.71910448, 0.65373134, 0.60029851, 0.56895522, 0.61701493,\n",
      "       0.63373134, 0.71074627, 0.72985075, 0.63462687, 0.62179104,\n",
      "       0.62208955, 0.6680597 , 0.70119403, 0.57761194, 0.66208955,\n",
      "       0.55880597, 0.68179104, 0.67820896, 0.61820896, 0.61343284,\n",
      "       0.64656716, 0.46059701, 0.61970149]), 'val_losses': array([1.27679648, 0.90917673, 0.77805869, 1.03715769, 1.17758843,\n",
      "       0.61483442, 0.71816638, 0.92552946, 0.94533214, 0.77782557,\n",
      "       0.73996783, 0.66271688, 0.60094005, 0.83211091, 0.84814146,\n",
      "       0.77722108, 0.66073995, 0.59468013, 0.96115394, 0.7175452 ,\n",
      "       0.76448156, 0.66956553, 0.8084675 , 0.89600661, 0.74608587,\n",
      "       0.83109966, 2.34215373, 0.81672708])}, 'ATTACH::21::best_val_loss': 0.5946801322609631, 'ATTACH::22::history': {'train_accs': array([0.34293604, 0.35450407, 0.36487798, 0.37181879, 0.38375998,\n",
      "       0.39055153, 0.40114934, 0.41443391, 0.4238376 , 0.42921114,\n",
      "       0.45010822, 0.45182476, 0.46861706, 0.47093067, 0.4790656 ,\n",
      "       0.49145459, 0.50264945, 0.51347115, 0.53436824, 0.54063736,\n",
      "       0.55048884, 0.55444436, 0.5653407 , 0.56869916, 0.57929696,\n",
      "       0.58653631, 0.59399955, 0.59594   , 0.60265691]), 'train_losses': array([43.49329258, 23.93799175, 18.28738931, 14.7854365 , 12.26553064,\n",
      "       10.80691462,  9.37693194,  8.13817064,  7.28222864,  6.40169435,\n",
      "        5.793832  ,  5.30931253,  4.67990291,  4.23233002,  3.92547839,\n",
      "        3.55861568,  3.23830771,  3.00480731,  2.7469704 ,  2.5284592 ,\n",
      "        2.4018159 ,  2.2344759 ,  2.11119376,  2.02624416,  1.89335201,\n",
      "        1.80222662,  1.75942457,  1.65702041,  1.58904216]), 'val_accs': array([0.42298507, 0.39492537, 0.39791045, 0.34597015, 0.34895522,\n",
      "       0.37283582, 0.36776119, 0.45850746, 0.43074627, 0.47164179,\n",
      "       0.53014925, 0.57701493, 0.5438806 , 0.61701493, 0.58328358,\n",
      "       0.57283582, 0.59402985, 0.6119403 , 0.62865672, 0.67462687,\n",
      "       0.68776119, 0.63134328, 0.63731343, 0.58507463, 0.65283582,\n",
      "       0.6880597 , 0.66925373, 0.6719403 , 0.6438806 ]), 'val_losses': array([11.10052133, 10.13995357,  7.54142635,  6.67681478,  4.73098805,\n",
      "        3.93002184,  2.92132467,  2.23651831,  2.12413445,  1.76447088,\n",
      "        1.36176587,  1.24893915,  1.17941669,  1.23786994,  1.26139758,\n",
      "        1.44173975,  1.19652188,  1.22669496,  0.95981714,  1.19937338,\n",
      "        1.11341857,  1.29536337,  1.08887848,  1.41622808,  1.40947585,\n",
      "        1.09926798,  1.21565639,  1.23958516,  1.32629168])}, 'ATTACH::22::best_val_loss': 0.9598171365794851, 'ATTACH::23::history': {'train_accs': array([0.40226883, 0.44242108, 0.4795134 , 0.51473991, 0.54063736,\n",
      "       0.57646093, 0.61191134, 0.64004776, 0.65758639, 0.68370774,\n",
      "       0.70930666, 0.73371147, 0.75572804, 0.77393835, 0.78662587,\n",
      "       0.80558251, 0.82297186, 0.83319651, 0.85125756, 0.85692962,\n",
      "       0.87334876, 0.88543921, 0.89096201, 0.90402269, 0.90708262,\n",
      "       0.91566535, 0.91775506, 0.9254422 , 0.92962161, 0.93253228,\n",
      "       0.93992089, 0.93820434, 0.94126427, 0.94521979, 0.94924994,\n",
      "       0.95156355, 0.95193671, 0.95634003, 0.95775804, 0.9588029 ,\n",
      "       0.96126577, 0.95962385, 0.96484812, 0.96611687, 0.96611687,\n",
      "       0.96551981, 0.96910217, 0.9696246 , 0.97268453, 0.97208747,\n",
      "       0.97499813, 0.97343085, 0.97559519, 0.97290843, 0.97716247,\n",
      "       0.97380402, 0.97664005, 0.97850586, 0.97850586, 0.97843123]), 'train_losses': array([10.03245879,  4.21734484,  2.85252234,  2.0702418 ,  1.65379293,\n",
      "        1.33485157,  1.09549875,  0.9781292 ,  0.90190565,  0.81222379,\n",
      "        0.73072788,  0.69203272,  0.61715011,  0.58941315,  0.5461605 ,\n",
      "        0.50690124,  0.46863204,  0.44366431,  0.3997599 ,  0.3850281 ,\n",
      "        0.35307011,  0.33027787,  0.30139604,  0.27842616,  0.27720275,\n",
      "        0.25577431,  0.23796345,  0.22214171,  0.20907068,  0.19518467,\n",
      "        0.17959763,  0.17890045,  0.17153788,  0.15905275,  0.15246793,\n",
      "        0.14737873,  0.14395728,  0.13143055,  0.12619049,  0.12204267,\n",
      "        0.11941194,  0.11785529,  0.10658081,  0.10425596,  0.10178066,\n",
      "        0.0995005 ,  0.09484639,  0.09278088,  0.08951123,  0.0855587 ,\n",
      "        0.08030674,  0.07956627,  0.08136333,  0.08046982,  0.07162228,\n",
      "        0.07663332,  0.07125823,  0.07053061,  0.06672926,  0.06653145]), 'val_accs': array([0.38208955, 0.44776119, 0.54567164, 0.63343284, 0.63044776,\n",
      "       0.6358209 , 0.58029851, 0.57492537, 0.72149254, 0.64626866,\n",
      "       0.77343284, 0.72238806, 0.76567164, 0.72835821, 0.65910448,\n",
      "       0.78089552, 0.86626866, 0.70925373, 0.68985075, 0.79283582,\n",
      "       0.90597015, 0.77641791, 0.90955224, 0.94537313, 0.88358209,\n",
      "       0.86925373, 0.88119403, 0.93223881, 0.91313433, 0.93701493,\n",
      "       0.95402985, 0.91164179, 0.93462687, 0.77313433, 0.95432836,\n",
      "       0.93970149, 0.96537313, 0.91343284, 0.93223881, 0.95791045,\n",
      "       0.96328358, 0.96746269, 0.96776119, 0.97462687, 0.97104478,\n",
      "       0.93671642, 0.90298507, 0.94985075, 0.9761194 , 0.88686567,\n",
      "       0.96537313, 0.95432836, 0.97432836, 0.98298507, 0.98268657,\n",
      "       0.9758209 , 0.97164179, 0.98358209, 0.96567164, 0.9758209 ]), 'val_losses': array([3.46481685, 1.82036875, 1.18598265, 0.99615399, 1.0393756 ,\n",
      "       0.78425899, 1.04678117, 2.17528553, 0.61348146, 0.96806972,\n",
      "       0.5005719 , 0.65260525, 0.58274495, 0.63890731, 1.01180139,\n",
      "       0.55056852, 0.34573777, 0.85138517, 1.20744836, 0.60524475,\n",
      "       0.26259194, 0.59655138, 0.24450922, 0.1741895 , 0.31741419,\n",
      "       0.35183156, 0.31858728, 0.1920912 , 0.24604603, 0.17688996,\n",
      "       0.13410245, 0.24269454, 0.19958005, 0.78669282, 0.13748754,\n",
      "       0.1774021 , 0.10775059, 0.23132891, 0.22047339, 0.13107038,\n",
      "       0.10612217, 0.1002991 , 0.10065135, 0.08306707, 0.08840051,\n",
      "       0.18408589, 0.31684792, 0.15881472, 0.07084747, 0.45608227,\n",
      "       0.10896432, 0.1435935 , 0.08783809, 0.06261189, 0.06010516,\n",
      "       0.08167624, 0.0850364 , 0.06271013, 0.12966255, 0.07937202])}, 'ATTACH::23::best_val_loss': 0.060105155411162486, 'ATTACH::24::history': {'train_accs': array([0.41473244, 0.45861631, 0.537055  , 0.63198746, 0.70497798,\n",
      "       0.7480409 , 0.77692365, 0.82207627, 0.8449138 , 0.87461751,\n",
      "       0.88730502, 0.90297783, 0.91111277, 0.9199194 , 0.92551683,\n",
      "       0.92805433, 0.93745802, 0.94589148, 0.95253377, 0.9505187 ,\n",
      "       0.95507127, 0.95865363, 0.95969848, 0.95731025, 0.96208672,\n",
      "       0.96447496, 0.96529592, 0.96551981, 0.96880364, 0.96649004,\n",
      "       0.97201284, 0.96649004, 0.97096798, 0.9746996 , 0.97156504,\n",
      "       0.96835585, 0.97380402, 0.97268453, 0.97529666, 0.97611762,\n",
      "       0.97649078, 0.97410254, 0.97686395, 0.97962535, 0.97977461]), 'train_losses': array([17.62751596,  3.6455475 ,  1.41519873,  0.84859247,  0.69275397,\n",
      "        0.59961339,  0.54958322,  0.45546831,  0.40698269,  0.33339152,\n",
      "        0.30940528,  0.26089331,  0.24940467,  0.22395898,  0.20445859,\n",
      "        0.20068732,  0.18069209,  0.15290102,  0.13898632,  0.14574538,\n",
      "        0.13681863,  0.12125513,  0.11708881,  0.12674646,  0.11079483,\n",
      "        0.10636936,  0.10885961,  0.09987109,  0.08994326,  0.10141695,\n",
      "        0.08374908,  0.10171974,  0.10179257,  0.07843797,  0.08641994,\n",
      "        0.09991553,  0.07595978,  0.07972464,  0.07424042,  0.07081222,\n",
      "        0.07283455,  0.07392931,  0.07389534,  0.06187933,  0.05977712]), 'val_accs': array([0.48268657, 0.56985075, 0.71701493, 0.64686567, 0.63253731,\n",
      "       0.77283582, 0.82      , 0.75492537, 0.88179104, 0.8438806 ,\n",
      "       0.9080597 , 0.88895522, 0.88895522, 0.83223881, 0.82      ,\n",
      "       0.87402985, 0.96746269, 0.96865672, 0.92686567, 0.80089552,\n",
      "       0.95313433, 0.95343284, 0.96925373, 0.89373134, 0.97164179,\n",
      "       0.94447761, 0.95850746, 0.98089552, 0.96835821, 0.9480597 ,\n",
      "       0.94477612, 0.95283582, 0.97074627, 0.97820896, 0.98686567,\n",
      "       0.98238806, 0.97910448, 0.96268657, 0.98      , 0.96985075,\n",
      "       0.94686567, 0.97880597, 0.96597015, 0.97850746, 0.98119403]), 'val_losses': array([2.99665745, 1.91019465, 0.7492887 , 0.68585875, 0.78278847,\n",
      "       0.52825144, 0.42105628, 0.58850453, 0.30578116, 0.45634473,\n",
      "       0.24498434, 0.28518053, 0.30942793, 0.5052073 , 0.6060022 ,\n",
      "       0.3225327 , 0.11656313, 0.10241002, 0.20804815, 0.79946618,\n",
      "       0.15460499, 0.14908942, 0.0949673 , 0.37601785, 0.08939034,\n",
      "       0.17017779, 0.15396583, 0.06374824, 0.10893663, 0.16369761,\n",
      "       0.18288509, 0.17242728, 0.08685429, 0.06720054, 0.04430249,\n",
      "       0.06045115, 0.08078267, 0.11829918, 0.07033952, 0.10978521,\n",
      "       0.16077019, 0.06430708, 0.08281018, 0.08301438, 0.05261758])}, 'ATTACH::24::best_val_loss': 0.04430249142780233, 'ATTACH::25::history': {'train_accs': array([0.42062841, 0.49182775, 0.52280021, 0.55444436, 0.58668557,\n",
      "       0.62930069, 0.69975371, 0.74460781, 0.78162549, 0.80207478,\n",
      "       0.82162848, 0.83536085, 0.85991492, 0.87372192, 0.88036421,\n",
      "       0.89051422, 0.89200687, 0.89805209, 0.91156056, 0.91268005,\n",
      "       0.91603851, 0.91887454, 0.92947235, 0.9254422 , 0.91708336,\n",
      "       0.92588999, 0.93148742, 0.92939772, 0.93126353, 0.93439809,\n",
      "       0.93633853, 0.94021942, 0.94029405, 0.93514441, 0.93618927,\n",
      "       0.94395104, 0.94148817, 0.93686096, 0.94395104, 0.94208523,\n",
      "       0.94044332, 0.94902605, 0.944772  , 0.94701097, 0.93111426,\n",
      "       0.94619001]), 'train_losses': array([15.74532809,  2.7968588 ,  1.86322448,  1.2625447 ,  0.9717878 ,\n",
      "        0.85611967,  0.70384611,  0.60344136,  0.53413537,  0.49085169,\n",
      "        0.45534059,  0.41963232,  0.36750227,  0.34170215,  0.31782032,\n",
      "        0.29507201,  0.28748104,  0.27723317,  0.25015352,  0.24086026,\n",
      "        0.23364378,  0.22138089,  0.19891383,  0.20381662,  0.22827241,\n",
      "        0.20515139,  0.19202096,  0.20239684,  0.19508049,  0.18506342,\n",
      "        0.17959958,  0.17268931,  0.17078058,  0.17933903,  0.17704043,\n",
      "        0.15727568,  0.16124721,  0.18128262,  0.15797165,  0.16158357,\n",
      "        0.16896152,  0.1438187 ,  0.15837536,  0.15069346,  0.19277581,\n",
      "        0.15218337]), 'val_accs': array([0.56955224, 0.59134328, 0.59552239, 0.65343284, 0.67492537,\n",
      "       0.7119403 , 0.67731343, 0.79552239, 0.8158209 , 0.83791045,\n",
      "       0.82865672, 0.89134328, 0.86567164, 0.85940299, 0.87910448,\n",
      "       0.87343284, 0.91641791, 0.87552239, 0.92298507, 0.90268657,\n",
      "       0.92477612, 0.93014925, 0.93970149, 0.89820896, 0.91492537,\n",
      "       0.9358209 , 0.91701493, 0.93820896, 0.94      , 0.91253731,\n",
      "       0.93253731, 0.91940299, 0.93850746, 0.95253731, 0.94955224,\n",
      "       0.95014925, 0.93701493, 0.94865672, 0.9161194 , 0.9358209 ,\n",
      "       0.94537313, 0.92507463, 0.93044776, 0.94776119, 0.94179104,\n",
      "       0.95343284]), 'val_losses': array([2.21364941, 1.20898856, 1.12710582, 0.79106679, 0.69542053,\n",
      "       0.66150033, 0.64641841, 0.51087416, 0.46711507, 0.38946685,\n",
      "       0.45593105, 0.27579906, 0.33767992, 0.34835535, 0.31385473,\n",
      "       0.34517034, 0.22742144, 0.35363725, 0.21508542, 0.2759887 ,\n",
      "       0.1947245 , 0.18844521, 0.16303083, 0.26701532, 0.21564251,\n",
      "       0.15521599, 0.25268442, 0.17624255, 0.18009006, 0.22573368,\n",
      "       0.19466946, 0.21419634, 0.17968881, 0.15042451, 0.14861901,\n",
      "       0.1456607 , 0.18205239, 0.1707375 , 0.24117413, 0.20416179,\n",
      "       0.19045898, 0.23537019, 0.21973983, 0.18098579, 0.16439174,\n",
      "       0.15351375])}, 'ATTACH::25::best_val_loss': 0.14566069651450683, 'ATTACH::26::history': {'train_accs': array([0.46891559, 0.57765505, 0.64833197, 0.68467796, 0.69632062,\n",
      "       0.71535189, 0.71796403, 0.7257258 , 0.72766624, 0.72833794,\n",
      "       0.72968132, 0.7282633 , 0.73289051, 0.74669751, 0.73192029,\n",
      "       0.73781625, 0.74281663, 0.74177177, 0.73923427, 0.73251735,\n",
      "       0.73296515, 0.73512949, 0.75020524, 0.73908501, 0.74371222,\n",
      "       0.7503545 , 0.72691992, 0.73266662, 0.72938279, 0.72184491,\n",
      "       0.73639824, 0.74348832, 0.74438391, 0.72512874, 0.7397567 ,\n",
      "       0.74326442, 0.7448317 , 0.74393611, 0.7422942 , 0.75319054,\n",
      "       0.73841331, 0.71565042, 0.73341294]), 'train_losses': array([6.66903016, 0.96376349, 0.77514324, 0.70395718, 0.67333941,\n",
      "       0.64723767, 0.63424037, 0.60829763, 0.60830527, 0.60212875,\n",
      "       0.60126931, 0.60573134, 0.59050907, 0.56837937, 0.60135182,\n",
      "       0.57833242, 0.58900843, 0.5629989 , 0.58190876, 0.59884282,\n",
      "       0.59466332, 0.60415237, 0.56849504, 0.58796281, 0.57338059,\n",
      "       0.59259547, 0.61063634, 0.59058952, 0.60301746, 0.59171668,\n",
      "       0.57347245, 0.58234823, 0.56867895, 0.60554315, 0.58218928,\n",
      "       0.567837  , 0.57296645, 0.59031432, 0.58383014, 0.54718351,\n",
      "       0.58293178, 0.61464775, 0.58122725]), 'val_accs': array([0.48597015, 0.61492537, 0.70835821, 0.73462687, 0.62179104,\n",
      "       0.61641791, 0.74119403, 0.71641791, 0.75492537, 0.74507463,\n",
      "       0.71373134, 0.76      , 0.75552239, 0.69402985, 0.74537313,\n",
      "       0.67880597, 0.76328358, 0.71761194, 0.70507463, 0.7119403 ,\n",
      "       0.78238806, 0.76507463, 0.75761194, 0.69283582, 0.70985075,\n",
      "       0.72149254, 0.77940299, 0.78119403, 0.77462687, 0.76716418,\n",
      "       0.79373134, 0.71880597, 0.80358209, 0.75164179, 0.74179104,\n",
      "       0.75343284, 0.67641791, 0.66537313, 0.75343284, 0.64208955,\n",
      "       0.66      , 0.67970149, 0.73791045]), 'val_losses': array([1.44513005, 0.85283512, 0.648437  , 0.60649015, 0.74847735,\n",
      "       1.03660107, 0.60875851, 0.62919266, 0.63394359, 0.71855024,\n",
      "       0.59456282, 0.52580051, 0.59818708, 0.68796952, 0.7098758 ,\n",
      "       0.67163456, 0.69913714, 0.62833247, 0.68522602, 0.61288522,\n",
      "       0.49293541, 0.61604484, 0.53951296, 0.72626252, 0.69120243,\n",
      "       0.60648634, 0.58904671, 0.49695694, 0.5012951 , 0.4911169 ,\n",
      "       0.53166937, 0.65137302, 0.48717684, 0.53984207, 0.5860353 ,\n",
      "       0.54521961, 0.76425077, 0.74145319, 0.52885271, 0.87858576,\n",
      "       0.70127967, 0.71180781, 0.54911245])}, 'ATTACH::26::best_val_loss': 0.4871768400206495, 'ATTACH::27::history': {'train_accs': array([0.46190014, 0.58019255, 0.66990074, 0.71885962, 0.74393611,\n",
      "       0.76229569, 0.78214792, 0.79737294, 0.80550787, 0.82215091,\n",
      "       0.83349504, 0.84312262, 0.85558624, 0.85931786, 0.86431823,\n",
      "       0.86454213, 0.87857303, 0.88790208, 0.88812598, 0.89290246,\n",
      "       0.90514217, 0.90506754, 0.90976939, 0.91708336, 0.91768042,\n",
      "       0.91782969, 0.92506904, 0.92096425, 0.92805433, 0.84551086,\n",
      "       0.84013732, 0.84282409, 0.85207851, 0.86178073, 0.89305172,\n",
      "       0.91820285, 0.92290469]), 'train_losses': array([6.74668637, 1.17675994, 0.76292457, 0.65852912, 0.59670024,\n",
      "       0.55156979, 0.52326279, 0.48646847, 0.46791238, 0.42625396,\n",
      "       0.41567891, 0.39723832, 0.37029466, 0.36769147, 0.34815492,\n",
      "       0.35081839, 0.32147427, 0.30636183, 0.30292214, 0.29645096,\n",
      "       0.2727136 , 0.27218041, 0.25927619, 0.25177849, 0.24018676,\n",
      "       0.25161509, 0.22094864, 0.22979534, 0.21392127, 0.38864564,\n",
      "       0.37289914, 0.37317824, 0.36044378, 0.34628377, 0.29643243,\n",
      "       0.23586848, 0.23031029]), 'val_accs': array([0.52895522, 0.68238806, 0.71791045, 0.81940299, 0.77104478,\n",
      "       0.79671642, 0.86776119, 0.78298507, 0.78955224, 0.83761194,\n",
      "       0.81074627, 0.84985075, 0.82895522, 0.75940299, 0.91164179,\n",
      "       0.86328358, 0.86447761, 0.81641791, 0.86149254, 0.85492537,\n",
      "       0.91671642, 0.82567164, 0.90059701, 0.88985075, 0.83731343,\n",
      "       0.89970149, 0.92029851, 0.89104478, 0.91223881, 0.7758209 ,\n",
      "       0.71522388, 0.76955224, 0.8161194 , 0.82358209, 0.85641791,\n",
      "       0.86      , 0.91641791]), 'val_losses': array([1.90625212, 0.7151213 , 0.61453124, 0.42643058, 0.5049465 ,\n",
      "       0.46356353, 0.40861866, 0.50545425, 0.43550302, 0.40163588,\n",
      "       0.40961037, 0.37877845, 0.46391821, 0.63188574, 0.32705878,\n",
      "       0.3537548 , 0.40098955, 0.35095215, 0.45091857, 0.36687053,\n",
      "       0.26196935, 0.54006997, 0.32089921, 0.35181031, 0.38055205,\n",
      "       0.29580757, 0.2569872 , 0.47686691, 0.30416917, 0.52160995,\n",
      "       0.62171705, 0.43526357, 0.43849303, 0.4133866 , 0.33663086,\n",
      "       0.41311514, 0.28457425])}, 'ATTACH::27::best_val_loss': 0.2569872013757478, 'ATTACH::28::history': {'train_accs': array([0.49705202, 0.62512128, 0.64377939, 0.65251138, 0.63668931,\n",
      "       0.637361  , 0.64915292, 0.66594522, 0.65482499, 0.63758489,\n",
      "       0.60646317, 0.59071573, 0.61556833]), 'train_losses': array([6.9853368 , 0.8237748 , 0.77616394, 0.74916569, 0.76302958,\n",
      "       0.7611551 , 0.74083803, 0.71735909, 0.74069936, 0.764591  ,\n",
      "       0.8401143 , 0.86140158, 0.82744363]), 'val_accs': array([0.66865672, 0.63343284, 0.71044776, 0.6238806 , 0.6758209 ,\n",
      "       0.59940299, 0.67552239, 0.69970149, 0.64208955, 0.67462687,\n",
      "       0.63462687, 0.72835821, 0.66507463]), 'val_losses': array([0.78619814, 0.73999159, 0.68326139, 0.77326816, 0.75866661,\n",
      "       0.73217396, 0.71074801, 0.73254203, 0.70392298, 0.77055693,\n",
      "       0.8240677 , 0.70166094, 0.74238723])}, 'ATTACH::28::best_val_loss': 0.6832613887359847, 'ATTACH::29::history': {'train_accs': array([0.37152026, 0.4183148 , 0.44286887, 0.45518322, 0.48025972,\n",
      "       0.50578401, 0.52847227, 0.54959325, 0.56922158, 0.58944697,\n",
      "       0.59340249, 0.63452496, 0.65534741, 0.67527427, 0.69385775,\n",
      "       0.71594895, 0.73266662, 0.7450556 , 0.76154937, 0.78744682,\n",
      "       0.79737294, 0.80834391, 0.82774834, 0.84036122, 0.85819837,\n",
      "       0.86327338, 0.88073737, 0.88782745, 0.89252929, 0.90297783,\n",
      "       0.91111277, 0.91200836, 0.92305396, 0.92999478, 0.93245765,\n",
      "       0.93872677, 0.93723412, 0.94492126, 0.94783193, 0.9477573 ,\n",
      "       0.95059333, 0.95372789, 0.95566833, 0.95895216, 0.95954922,\n",
      "       0.96484812, 0.96440033, 0.96522129, 0.97007239, 0.97313232,\n",
      "       0.97119188, 0.97298306, 0.97484887, 0.9746996 , 0.97671468,\n",
      "       0.97455034, 0.97887902, 0.97671468, 0.98104336, 0.97820733]), 'train_losses': array([34.92931435, 13.0612441 ,  8.65746788,  6.69609971,  5.37788353,\n",
      "        4.44597535,  3.66019241,  3.16715903,  2.80431855,  2.47144832,\n",
      "        2.20556172,  1.86801926,  1.62717063,  1.42397363,  1.267847  ,\n",
      "        1.13964191,  1.02798263,  0.95182264,  0.87063921,  0.7547083 ,\n",
      "        0.67392151,  0.62869369,  0.57505587,  0.5274174 ,  0.45097098,\n",
      "        0.42444419,  0.38325747,  0.35323812,  0.33523119,  0.28964835,\n",
      "        0.27903104,  0.26398375,  0.24238548,  0.20941183,  0.20032092,\n",
      "        0.18428769,  0.18553973,  0.17061987,  0.15960252,  0.1546619 ,\n",
      "        0.14618287,  0.12783207,  0.13080984,  0.1308523 ,  0.11818238,\n",
      "        0.10661585,  0.10382997,  0.10478267,  0.09319687,  0.08572091,\n",
      "        0.08827907,  0.0843202 ,  0.08244961,  0.07600682,  0.07075821,\n",
      "        0.07800287,  0.06335946,  0.07638559,  0.05736603,  0.06432596]), 'val_accs': array([0.50835821, 0.49492537, 0.49373134, 0.52686567, 0.61104478,\n",
      "       0.64179104, 0.64447761, 0.60447761, 0.64865672, 0.69910448,\n",
      "       0.63761194, 0.76268657, 0.73880597, 0.7438806 , 0.75373134,\n",
      "       0.70238806, 0.79731343, 0.75313433, 0.65313433, 0.7719403 ,\n",
      "       0.67164179, 0.76626866, 0.88208955, 0.82567164, 0.89104478,\n",
      "       0.90268657, 0.90567164, 0.85492537, 0.90149254, 0.92447761,\n",
      "       0.9       , 0.92      , 0.94477612, 0.93940299, 0.93492537,\n",
      "       0.94686567, 0.90985075, 0.93701493, 0.91820896, 0.95940299,\n",
      "       0.95671642, 0.96865672, 0.94029851, 0.93641791, 0.95253731,\n",
      "       0.96835821, 0.96238806, 0.96955224, 0.94268657, 0.96537313,\n",
      "       0.96059701, 0.95731343, 0.96597015, 0.96925373, 0.97552239,\n",
      "       0.97283582, 0.97014925, 0.95791045, 0.97014925, 0.96716418]), 'val_losses': array([3.81020865, 4.53898142, 2.20317982, 3.85853316, 3.01001964,\n",
      "       1.49628514, 1.65080324, 1.98240127, 1.43422526, 1.14098359,\n",
      "       1.5223883 , 0.76702022, 0.90038791, 0.86759149, 0.93889018,\n",
      "       1.42127831, 0.60587296, 0.9105647 , 1.5160451 , 0.83578963,\n",
      "       1.46300842, 0.7169052 , 0.34328293, 0.51461198, 0.33535915,\n",
      "       0.30915897, 0.28217404, 0.42771382, 0.2887444 , 0.22579744,\n",
      "       0.29786663, 0.25697875, 0.16740401, 0.18635121, 0.20470402,\n",
      "       0.16837005, 0.30414349, 0.21457238, 0.22794847, 0.11859895,\n",
      "       0.13924311, 0.10835634, 0.19235694, 0.16784587, 0.13667811,\n",
      "       0.09571185, 0.12507174, 0.1105877 , 0.16035455, 0.09704815,\n",
      "       0.12621461, 0.13873734, 0.11009937, 0.10686293, 0.08745673,\n",
      "       0.08541362, 0.09886872, 0.13983837, 0.09083098, 0.10340989])}, 'ATTACH::29::best_val_loss': 0.08541361969353548}\n",
      "{'ATTACH::0::history': {'train_accs': array([0.34002538, 0.32703933, 0.31860587, 0.31912829, 0.32756176,\n",
      "       0.33420405, 0.34092096, 0.35151877, 0.35525039, 0.36107172,\n",
      "       0.3743563 , 0.38047615, 0.38689454, 0.39159639, 0.39905963,\n",
      "       0.3930144 , 0.40271662, 0.4183148 , 0.42100157, 0.42473319,\n",
      "       0.43316665, 0.43682364, 0.44003284, 0.440406  , 0.44257034,\n",
      "       0.45294425, 0.45085454, 0.45689977, 0.46376595, 0.47369207,\n",
      "       0.48040899, 0.47600567, 0.48234943, 0.48914098, 0.48943951,\n",
      "       0.48973804, 0.49742518, 0.5039182 , 0.50130607, 0.50682887,\n",
      "       0.51123218, 0.52317337, 0.52750205, 0.53093514, 0.5342936 ,\n",
      "       0.54272707, 0.54944399, 0.55884768, 0.55631017, 0.56011643,\n",
      "       0.57049034, 0.56817673, 0.57899843, 0.58623778, 0.58944697,\n",
      "       0.59444735, 0.5987014 , 0.59340249, 0.60407493, 0.61400104]), 'train_losses': array([65.99539886, 44.84893432, 33.71637043, 28.37971988, 24.04233567,\n",
      "       20.60005261, 18.17306162, 16.12266948, 14.06835015, 12.59116368,\n",
      "       11.50494261, 10.55380474,  9.7193452 ,  8.83897168,  8.28331258,\n",
      "        7.87679661,  7.51868559,  6.98608077,  6.61899382,  6.39811784,\n",
      "        5.86190638,  5.73655612,  5.58835601,  5.30152016,  5.1791974 ,\n",
      "        4.93317444,  4.71554398,  4.63761146,  4.54592654,  4.13811667,\n",
      "        4.15755417,  3.97241133,  3.88289619,  3.62211539,  3.63374351,\n",
      "        3.52401991,  3.40775793,  3.23665345,  3.14415564,  3.06126415,\n",
      "        2.99096822,  2.92098278,  2.81321102,  2.78987069,  2.6934992 ,\n",
      "        2.60889342,  2.52251756,  2.42642922,  2.3835956 ,  2.38240263,\n",
      "        2.25717179,  2.24962367,  2.16515851,  2.06682714,  2.10351604,\n",
      "        2.0414129 ,  2.01882178,  1.92144371,  1.91319333,  1.86699108]), 'val_accs': array([0.43731343, 0.44208955, 0.44208955, 0.44208955, 0.44835821,\n",
      "       0.43970149, 0.38537313, 0.36686567, 0.29910448, 0.34477612,\n",
      "       0.34238806, 0.37910448, 0.35164179, 0.36149254, 0.39791045,\n",
      "       0.37731343, 0.44119403, 0.45731343, 0.44179104, 0.46507463,\n",
      "       0.52059701, 0.53850746, 0.50955224, 0.58119403, 0.61671642,\n",
      "       0.57791045, 0.56149254, 0.57104478, 0.58985075, 0.60746269,\n",
      "       0.60537313, 0.5758209 , 0.59880597, 0.5680597 , 0.59164179,\n",
      "       0.62776119, 0.63402985, 0.58955224, 0.64447761, 0.63820896,\n",
      "       0.63402985, 0.68597015, 0.64985075, 0.65402985, 0.63283582,\n",
      "       0.6561194 , 0.6319403 , 0.64716418, 0.67104478, 0.68208955,\n",
      "       0.68328358, 0.70268657, 0.67313433, 0.70925373, 0.71731343,\n",
      "       0.69880597, 0.72358209, 0.72507463, 0.72895522, 0.72985075]), 'val_losses': array([40.3426474 , 26.62600197, 19.68838734, 14.2934375 ,  9.3155669 ,\n",
      "        7.43497191,  5.02400329,  4.10469371,  3.55139165,  3.41944242,\n",
      "        3.62689954,  3.55110759,  3.44318854,  2.75982936,  2.86521354,\n",
      "        2.29977806,  1.93287162,  1.97021978,  2.1500228 ,  1.89311084,\n",
      "        1.85423103,  1.59622911,  1.97122439,  1.59738817,  1.42814705,\n",
      "        1.50464088,  1.55777288,  1.6615486 ,  1.51018913,  1.30026978,\n",
      "        1.27778662,  1.42020174,  1.21757551,  1.42061592,  1.14813354,\n",
      "        1.14659542,  1.12323036,  1.21160499,  1.10846194,  1.11696499,\n",
      "        1.01686528,  0.99462062,  0.96562258,  1.03527902,  0.99634967,\n",
      "        0.97237863,  1.07831697,  1.17785538,  0.91727326,  0.94161539,\n",
      "        0.90952419,  0.88424767,  0.9669204 ,  0.79011641,  0.78737478,\n",
      "        0.8313061 ,  0.80892914,  0.92671295,  0.80038104,  0.7316136 ])}, 'ATTACH::0::best_val_loss': 0.731613599933795, 'ATTACH::1::history': {'train_accs': array([0.42316591, 0.46645272, 0.49735055, 0.52518845, 0.53309949,\n",
      "       0.53929398, 0.54377192, 0.56153444, 0.55825062, 0.57571461,\n",
      "       0.56884842, 0.58101351, 0.58631241, 0.5842227 , 0.5931786 ,\n",
      "       0.58735727, 0.58982014, 0.59735801, 0.59974625, 0.60489589,\n",
      "       0.60534368, 0.61325472, 0.61116501, 0.61012016, 0.61400104,\n",
      "       0.61303082, 0.62206135, 0.61743414, 0.61497127, 0.61415031,\n",
      "       0.62743488, 0.61691171, 0.62444959, 0.62624076, 0.62101649,\n",
      "       0.62646466, 0.63221136, 0.63176356, 0.64064482, 0.63586835,\n",
      "       0.63952534, 0.63773416, 0.62258377, 0.63743563, 0.64094335,\n",
      "       0.63362938, 0.63930144, 0.63825659, 0.64773491, 0.64191358,\n",
      "       0.64654079, 0.64251064, 0.64952608, 0.65161579, 0.65139189,\n",
      "       0.65251138, 0.65698933, 0.65460109, 0.64698858, 0.6534816 ]), 'train_losses': array([4.15538788, 1.00451414, 0.93836645, 0.9101276 , 0.88801193,\n",
      "       0.87631652, 0.87154297, 0.86105374, 0.85580794, 0.84990547,\n",
      "       0.84868654, 0.84230356, 0.8392233 , 0.83991326, 0.83377625,\n",
      "       0.83458504, 0.83016414, 0.82203207, 0.81959837, 0.8187676 ,\n",
      "       0.81310763, 0.8074451 , 0.80640639, 0.80622301, 0.79947264,\n",
      "       0.79576477, 0.79186777, 0.79366838, 0.784811  , 0.79249653,\n",
      "       0.78155369, 0.79201915, 0.77956236, 0.77849756, 0.77685135,\n",
      "       0.76859507, 0.77375053, 0.77532218, 0.76211282, 0.76040933,\n",
      "       0.76348904, 0.76799604, 0.7709058 , 0.75596017, 0.75655398,\n",
      "       0.75424518, 0.75062965, 0.75858386, 0.74307839, 0.75595905,\n",
      "       0.74820047, 0.74854758, 0.7357388 , 0.73311046, 0.74307078,\n",
      "       0.74327237, 0.7308741 , 0.73631382, 0.74183025, 0.73238314]), 'val_accs': array([0.5358209 , 0.50268657, 0.61104478, 0.56955224, 0.57373134,\n",
      "       0.61671642, 0.6480597 , 0.61492537, 0.66179104, 0.61731343,\n",
      "       0.64029851, 0.6519403 , 0.64238806, 0.64507463, 0.64567164,\n",
      "       0.63970149, 0.67164179, 0.60298507, 0.7038806 , 0.7041791 ,\n",
      "       0.68746269, 0.64238806, 0.64149254, 0.69761194, 0.69731343,\n",
      "       0.66358209, 0.73492537, 0.6680597 , 0.73373134, 0.7041791 ,\n",
      "       0.69223881, 0.70925373, 0.71641791, 0.7       , 0.66238806,\n",
      "       0.69791045, 0.68895522, 0.71462687, 0.72776119, 0.69373134,\n",
      "       0.71432836, 0.77313433, 0.72029851, 0.74567164, 0.72477612,\n",
      "       0.70149254, 0.70746269, 0.69940299, 0.7       , 0.74      ,\n",
      "       0.7241791 , 0.66149254, 0.67164179, 0.69074627, 0.73402985,\n",
      "       0.72149254, 0.75432836, 0.70149254, 0.72776119, 0.74328358]), 'val_losses': array([0.99086394, 0.89464038, 0.8606868 , 0.8406382 , 0.81632899,\n",
      "       0.79535766, 0.79882211, 0.78890992, 0.78289741, 0.77205097,\n",
      "       0.78454673, 0.74821541, 0.76376904, 0.75134127, 0.74099314,\n",
      "       0.74250345, 0.73142341, 0.76901103, 0.71328999, 0.71108129,\n",
      "       0.70773864, 0.73533551, 0.740758  , 0.68986872, 0.69341615,\n",
      "       0.68572699, 0.70147049, 0.70454464, 0.677655  , 0.67685262,\n",
      "       0.677232  , 0.66218391, 0.65465594, 0.65773218, 0.66852977,\n",
      "       0.64845591, 0.67695556, 0.63593675, 0.63385783, 0.67541692,\n",
      "       0.64316104, 0.62647397, 0.62278316, 0.63493046, 0.62066388,\n",
      "       0.64335601, 0.64323421, 0.6181879 , 0.62475456, 0.64533566,\n",
      "       0.64654826, 0.62316412, 0.6144667 , 0.59881527, 0.62517422,\n",
      "       0.60870363, 0.58779717, 0.60158723, 0.61270946, 0.59781573])}, 'ATTACH::1::best_val_loss': 0.5877971665894808, 'ATTACH::2::history': {'train_accs': array([0.37077394, 0.42338981, 0.46816927, 0.5011568 , 0.53354728,\n",
      "       0.56153444, 0.60086574, 0.63974924, 0.67146802, 0.70863497,\n",
      "       0.74363758, 0.78065527, 0.81565788, 0.83901784, 0.86058661,\n",
      "       0.8784984 , 0.89335025, 0.90238077, 0.91768042, 0.92865139,\n",
      "       0.93103963, 0.93798045, 0.94298082, 0.95313083, 0.95589223,\n",
      "       0.96186283, 0.96402717, 0.9691768 , 0.97037092, 0.97201284,\n",
      "       0.97529666, 0.97731174, 0.97992387, 0.98096873, 0.97977461,\n",
      "       0.98402866, 0.98529741, 0.98417792, 0.98731249, 0.9857452 ,\n",
      "       0.98671543, 0.98731249, 0.98738712, 0.98887977, 0.98925293,\n",
      "       0.99059631, 0.98925293, 0.98858124, 0.99126801, 0.99014852,\n",
      "       0.9914919 , 0.99201433, 0.99089484, 0.99305918, 0.99141727,\n",
      "       0.99238749, 0.99104411, 0.99179043, 0.99276065, 0.99261139]), 'train_losses': array([22.19551385,  6.46110808,  3.95696986,  2.6031481 ,  1.9526891 ,\n",
      "        1.54016723,  1.25501861,  1.07055795,  0.92112939,  0.8045766 ,\n",
      "        0.71021519,  0.61697287,  0.53665976,  0.48610187,  0.42396674,\n",
      "        0.3747267 ,  0.33735348,  0.30412742,  0.26749864,  0.23400739,\n",
      "        0.22058814,  0.19413615,  0.17781377,  0.15883276,  0.14058094,\n",
      "        0.12585506,  0.11550028,  0.10416816,  0.09615426,  0.0943781 ,\n",
      "        0.08024059,  0.0790588 ,  0.06944477,  0.06469401,  0.0653788 ,\n",
      "        0.0552821 ,  0.04726976,  0.05376046,  0.04566281,  0.04526945,\n",
      "        0.04713091,  0.04512381,  0.0418574 ,  0.03472654,  0.03450945,\n",
      "        0.03512526,  0.0376998 ,  0.03792981,  0.02958987,  0.0316212 ,\n",
      "        0.02929085,  0.02898286,  0.03227814,  0.02536859,  0.02642082,\n",
      "        0.02849335,  0.03122508,  0.02570706,  0.02473877,  0.0278682 ]), 'val_accs': array([0.53761194, 0.58029851, 0.56268657, 0.64      , 0.64447761,\n",
      "       0.55910448, 0.74238806, 0.67671642, 0.75820896, 0.65731343,\n",
      "       0.79253731, 0.7561194 , 0.84955224, 0.89134328, 0.89462687,\n",
      "       0.94059701, 0.87701493, 0.92238806, 0.9319403 , 0.91492537,\n",
      "       0.93104478, 0.94119403, 0.91104478, 0.94686567, 0.95343284,\n",
      "       0.94626866, 0.96835821, 0.95104478, 0.97492537, 0.96626866,\n",
      "       0.96985075, 0.97343284, 0.9761194 , 0.96059701, 0.96656716,\n",
      "       0.97940299, 0.94776119, 0.96895522, 0.98149254, 0.97014925,\n",
      "       0.97970149, 0.98      , 0.97671642, 0.98328358, 0.98059701,\n",
      "       0.97850746, 0.97820896, 0.97940299, 0.97044776, 0.98      ,\n",
      "       0.98208955, 0.98      , 0.98149254, 0.9880597 , 0.9838806 ,\n",
      "       0.97910448, 0.98059701, 0.9841791 , 0.9758209 , 0.98089552]), 'val_losses': array([4.38900254, 2.12088323, 1.54764039, 0.96985065, 0.85873985,\n",
      "       1.17921308, 0.64993402, 0.91528858, 0.58350726, 1.03704392,\n",
      "       0.51443691, 0.68450159, 0.39978978, 0.30301422, 0.30582573,\n",
      "       0.21813836, 0.33682959, 0.20282002, 0.18960912, 0.23257282,\n",
      "       0.18464585, 0.16575302, 0.24562601, 0.17623505, 0.14679471,\n",
      "       0.14661067, 0.1118332 , 0.15021657, 0.09682176, 0.12223463,\n",
      "       0.09269207, 0.09215521, 0.07740893, 0.11775892, 0.12889543,\n",
      "       0.07068214, 0.14681178, 0.1190886 , 0.07414126, 0.12306596,\n",
      "       0.07240196, 0.07961945, 0.08612522, 0.05709217, 0.07126703,\n",
      "       0.09632403, 0.09714249, 0.08767505, 0.0952582 , 0.07882215,\n",
      "       0.07324775, 0.08137327, 0.07436281, 0.0426415 , 0.06009297,\n",
      "       0.07107819, 0.08246434, 0.06226338, 0.1271557 , 0.07549139])}, 'ATTACH::2::best_val_loss': 0.04264150308764804, 'ATTACH::3::history': {'train_accs': array([0.26546757, 0.44697365, 0.44712292, 0.44697365, 0.44704829,\n",
      "       0.44682439, 0.44734682, 0.44734682, 0.44734682, 0.44734682,\n",
      "       0.44734682, 0.44727218, 0.44719755, 0.44719755, 0.44734682,\n",
      "       0.44719755, 0.44742145, 0.44727218, 0.44734682, 0.44727218,\n",
      "       0.44727218, 0.44727218, 0.44719755, 0.44719755, 0.44742145,\n",
      "       0.44727218, 0.44727218, 0.44727218, 0.44719755, 0.44734682,\n",
      "       0.44742145, 0.44742145, 0.44719755, 0.44727218, 0.44719755,\n",
      "       0.44734682, 0.44727218, 0.44734682, 0.44734682, 0.44742145,\n",
      "       0.44742145, 0.44727218, 0.44719755, 0.44734682, 0.44697365,\n",
      "       0.44719755, 0.44734682, 0.44734682, 0.44742145, 0.44734682,\n",
      "       0.44742145, 0.44727218, 0.44719755, 0.44727218, 0.44727218,\n",
      "       0.44742145, 0.44727218, 0.44719755, 0.44719755, 0.44734682]), 'train_losses': array([7.09822618, 1.33620714, 1.30652996, 1.28661734, 1.27303729,\n",
      "       1.26395657, 1.25678529, 1.25172556, 1.24775823, 1.24465522,\n",
      "       1.24211684, 1.2401879 , 1.23863977, 1.23723407, 1.23570655,\n",
      "       1.23508589, 1.2336352 , 1.23325334, 1.23247875, 1.23216108,\n",
      "       1.23165278, 1.23127322, 1.23115581, 1.23087854, 1.23014641,\n",
      "       1.2302318 , 1.23006174, 1.22991271, 1.23000343, 1.22946632,\n",
      "       1.22921433, 1.22914159, 1.22961435, 1.22939081, 1.22951183,\n",
      "       1.22906593, 1.22923329, 1.22898256, 1.22897655, 1.22874703,\n",
      "       1.2287071 , 1.22907146, 1.22926114, 1.22882277, 1.22979594,\n",
      "       1.22923145, 1.22882171, 1.22883141, 1.22865936, 1.22876736,\n",
      "       1.22866715, 1.22898011, 1.22922531, 1.22904194, 1.22900646,\n",
      "       1.22865383, 1.22895803, 1.22912042, 1.22914747, 1.22878852]), 'val_accs': array([0.44895522, 0.44895522, 0.44895522, 0.44895522, 0.44895522,\n",
      "       0.44895522, 0.44895522, 0.44895522, 0.44895522, 0.44895522,\n",
      "       0.44895522, 0.44895522, 0.44895522, 0.44895522, 0.44895522,\n",
      "       0.44895522, 0.44895522, 0.44895522, 0.44895522, 0.44895522,\n",
      "       0.44895522, 0.44895522, 0.44895522, 0.44895522, 0.44895522,\n",
      "       0.44895522, 0.44895522, 0.44895522, 0.44895522, 0.44895522,\n",
      "       0.44895522, 0.44895522, 0.44895522, 0.44895522, 0.44895522,\n",
      "       0.44895522, 0.44895522, 0.44895522, 0.44895522, 0.44895522,\n",
      "       0.44895522, 0.44895522, 0.44895522, 0.44895522, 0.44895522,\n",
      "       0.44895522, 0.44895522, 0.44895522, 0.44895522, 0.44895522,\n",
      "       0.44895522, 0.44895522, 0.44895522, 0.44895522, 0.44895522,\n",
      "       0.44895522, 0.44895522, 0.44895522, 0.44895522, 0.44895522]), 'val_losses': array([1.35287046, 1.31664021, 1.29192538, 1.27551023, 1.26437172,\n",
      "       1.25641311, 1.25063147, 1.24612356, 1.24258405, 1.23972612,\n",
      "       1.23734911, 1.23540466, 1.23379764, 1.23242086, 1.23126061,\n",
      "       1.23028111, 1.2294381 , 1.22871673, 1.22811093, 1.22759168,\n",
      "       1.22713275, 1.22675503, 1.22642661, 1.22613864, 1.22588973,\n",
      "       1.22569716, 1.22549325, 1.22533835, 1.22519197, 1.22505952,\n",
      "       1.22496344, 1.22486699, 1.22479092, 1.2247143 , 1.22466026,\n",
      "       1.22460417, 1.22455802, 1.22450993, 1.22448013, 1.22444107,\n",
      "       1.22441333, 1.22438975, 1.22436877, 1.22434809, 1.2243329 ,\n",
      "       1.22431965, 1.22430603, 1.2242946 , 1.22428123, 1.22427267,\n",
      "       1.22426227, 1.22425499, 1.22424677, 1.22424268, 1.22423782,\n",
      "       1.22423019, 1.2242278 , 1.22422275, 1.22422109, 1.2242141 ])}, 'ATTACH::3::best_val_loss': 1.2242140950729599, 'ATTACH::4::history': {'train_accs': array([0.25957161, 0.25591462, 0.28225987, 0.30964997, 0.32995   ,\n",
      "       0.34084633, 0.34756325, 0.35293679, 0.34756325, 0.35450407,\n",
      "       0.35420554, 0.35271289, 0.36062393, 0.3654004 , 0.36181805,\n",
      "       0.36681842, 0.36062393, 0.3688335 , 0.37099784, 0.36853497,\n",
      "       0.3709232 , 0.38025226, 0.37346071, 0.37592358, 0.37883424,\n",
      "       0.37935667, 0.38010299, 0.38667065, 0.38577506, 0.38883499,\n",
      "       0.38786477, 0.38950668, 0.38980521, 0.39450705, 0.39562654,\n",
      "       0.39570117, 0.39196955, 0.40100007, 0.39338757, 0.39338757,\n",
      "       0.40391074, 0.39749235, 0.39749235, 0.40316442, 0.40816479,\n",
      "       0.40950817, 0.41697142, 0.412792  , 0.41435928, 0.41010523,\n",
      "       0.41495634, 0.4180909 , 0.42137473, 0.42219569, 0.42025524,\n",
      "       0.42309128, 0.42137473, 0.42734532, 0.42861408, 0.43503247]), 'train_losses': array([334.82458575, 216.58256622, 130.72456293,  88.98158172,\n",
      "        76.71840712,  67.40968345,  59.25969375,  53.81744273,\n",
      "        48.94261583,  43.27294301,  39.37642488,  35.50839345,\n",
      "        32.84171967,  30.20415275,  28.5785712 ,  26.32447308,\n",
      "        25.23840104,  23.27799589,  22.04742946,  20.93015485,\n",
      "        19.93724123,  18.36442003,  18.01717132,  17.00351872,\n",
      "        16.39407865,  15.47115042,  14.80506693,  14.0246823 ,\n",
      "        13.30012436,  13.05616858,  12.52294647,  11.90295226,\n",
      "        11.43663812,  10.7544891 ,  10.40287354,  10.05684864,\n",
      "         9.79640954,   9.30264058,   9.17504245,   8.76657163,\n",
      "         8.33758002,   8.07265033,   7.92631853,   7.46800092,\n",
      "         7.12021125,   6.74258807,   6.68214653,   6.38163739,\n",
      "         6.18770269,   5.93625028,   5.9133512 ,   5.54384042,\n",
      "         5.4264928 ,   5.28876182,   5.07695273,   4.93469772,\n",
      "         4.76294796,   4.70245972,   4.5416912 ,   4.4059558 ]), 'val_accs': array([0.27432836, 0.27373134, 0.40268657, 0.39373134, 0.42835821,\n",
      "       0.41940299, 0.42567164, 0.43044776, 0.41820896, 0.39313433,\n",
      "       0.41462687, 0.40268657, 0.37492537, 0.36597015, 0.37820896,\n",
      "       0.34358209, 0.3719403 , 0.36089552, 0.37373134, 0.33910448,\n",
      "       0.37731343, 0.36746269, 0.35104478, 0.3838806 , 0.37820896,\n",
      "       0.32447761, 0.3841791 , 0.36835821, 0.42746269, 0.36656716,\n",
      "       0.39074627, 0.39761194, 0.43074627, 0.41731343, 0.4558209 ,\n",
      "       0.42507463, 0.41791045, 0.41402985, 0.46447761, 0.47731343,\n",
      "       0.44029851, 0.48089552, 0.4919403 , 0.50119403, 0.45701493,\n",
      "       0.45283582, 0.53223881, 0.52208955, 0.48686567, 0.49373134,\n",
      "       0.50179104, 0.51820896, 0.48328358, 0.50477612, 0.47432836,\n",
      "       0.48119403, 0.52059701, 0.53283582, 0.58298507, 0.48238806]), 'val_losses': array([200.89944039, 100.66060868,  36.2564081 ,  41.78261674,\n",
      "        41.00355008,  36.08536137,  31.65605718,  26.91763293,\n",
      "        22.97303839,  19.16352156,  16.54578453,  14.28568368,\n",
      "        13.12533447,  11.35200699,  10.58530178,   9.63138349,\n",
      "         8.91890518,   8.20643065,   7.73360167,   7.27584128,\n",
      "         6.60927533,   6.22600914,   6.20209149,   5.14290536,\n",
      "         4.98064178,   5.16350343,   4.82495699,   4.24261243,\n",
      "         3.69763016,   3.94628133,   3.89475435,   3.71661802,\n",
      "         3.27994119,   3.19614258,   2.98680705,   2.96376343,\n",
      "         2.77821533,   2.71915849,   2.79912409,   2.4295252 ,\n",
      "         2.49861574,   2.29100574,   2.26408756,   2.19428368,\n",
      "         2.17237905,   2.1145103 ,   2.00357475,   1.90999433,\n",
      "         1.88712884,   1.88465324,   1.82847713,   1.71978024,\n",
      "         1.6864575 ,   1.62660168,   1.65530192,   1.63593796,\n",
      "         1.47201764,   1.53531647,   1.45190445,   1.62947407])}, 'ATTACH::4::best_val_loss': 1.4519044466872713, 'ATTACH::5::history': {'train_accs': array([0.27912531, 0.31576983, 0.33024853, 0.33166654, 0.34450332,\n",
      "       0.34211508, 0.34756325, 0.35525039, 0.35256362, 0.3548026 ,\n",
      "       0.35748936, 0.35831032, 0.37166953, 0.36853497, 0.37652064,\n",
      "       0.38353608, 0.38741697, 0.38704381, 0.38838719, 0.39682066,\n",
      "       0.40346294, 0.40622435, 0.41607583, 0.41995671, 0.41906112,\n",
      "       0.42182252, 0.42316591, 0.4263751 , 0.43965968, 0.43271886,\n",
      "       0.42988283, 0.43316665, 0.43712217, 0.43174864, 0.4463766 ,\n",
      "       0.44518248, 0.45428763, 0.45652661, 0.45451153, 0.45578028,\n",
      "       0.45130234, 0.45913874, 0.46316889, 0.46369132, 0.46802   ,\n",
      "       0.47294574, 0.4707814 , 0.46749757, 0.48294649, 0.4707814 ,\n",
      "       0.4739906 , 0.47891634, 0.48122994, 0.48623032, 0.4848123 ,\n",
      "       0.49503694, 0.49578327, 0.49899246, 0.50585865, 0.50324651]), 'train_losses': array([104.41889155,  76.29002366,  63.58378694,  53.9392665 ,\n",
      "        47.84274724,  42.68632736,  38.31004682,  32.96270358,\n",
      "        30.31161941,  26.99300988,  24.31203616,  22.07741347,\n",
      "        20.01108292,  17.99088648,  16.8930574 ,  15.38582811,\n",
      "        14.38386807,  13.02293029,  12.16931316,  11.40832184,\n",
      "        10.53929128,   9.80172165,   9.32571162,   8.8838959 ,\n",
      "         8.47286812,   8.19585241,   7.63169264,   7.38306655,\n",
      "         6.93882502,   6.68076327,   6.58043248,   6.21744527,\n",
      "         6.08404246,   6.09486868,   5.68180298,   5.43800149,\n",
      "         5.13849176,   4.99806937,   4.97338822,   4.84514086,\n",
      "         4.79360872,   4.49318843,   4.46413407,   4.24793221,\n",
      "         4.2099574 ,   4.00928137,   3.9415192 ,   3.90089921,\n",
      "         3.74632474,   3.62108007,   3.54483589,   3.46715425,\n",
      "         3.43766622,   3.22906894,   3.22851787,   3.08681371,\n",
      "         3.07728621,   3.00682705,   2.83185128,   2.8086293 ]), 'val_accs': array([0.44208955, 0.44208955, 0.43641791, 0.43641791, 0.44298507,\n",
      "       0.43761194, 0.43761194, 0.43761194, 0.43761194, 0.43134328,\n",
      "       0.43134328, 0.43761194, 0.43761194, 0.43761194, 0.43761194,\n",
      "       0.43552239, 0.45074627, 0.43552239, 0.43552239, 0.43253731,\n",
      "       0.44149254, 0.44149254, 0.45134328, 0.45343284, 0.45223881,\n",
      "       0.45970149, 0.46507463, 0.45731343, 0.4558209 , 0.48268657,\n",
      "       0.51492537, 0.46447761, 0.53223881, 0.46865672, 0.5319403 ,\n",
      "       0.4958209 , 0.50059701, 0.47910448, 0.5       , 0.50089552,\n",
      "       0.53283582, 0.50328358, 0.52029851, 0.48597015, 0.53014925,\n",
      "       0.54716418, 0.53134328, 0.53820896, 0.56865672, 0.55910448,\n",
      "       0.5319403 , 0.54328358, 0.5438806 , 0.54477612, 0.56298507,\n",
      "       0.56179104, 0.55701493, 0.56089552, 0.57880597, 0.57074627]), 'val_losses': array([55.07558285, 48.68145291, 40.13067046, 32.80797442, 29.52406529,\n",
      "       25.98896671, 24.7704101 , 22.49807282, 20.85069296, 18.75647847,\n",
      "       16.90992335, 14.61662281, 12.66456191, 11.9651791 , 10.46223244,\n",
      "        9.22637273,  8.45343279,  7.88757691,  6.37541609,  5.78531004,\n",
      "        4.78781233,  4.55910667,  4.62986048,  4.16209004,  3.8747128 ,\n",
      "        3.2142674 ,  3.00762874,  2.91652747,  3.21201175,  2.71571438,\n",
      "        2.33932163,  2.52388322,  2.2264073 ,  2.48395277,  2.04126499,\n",
      "        2.16924713,  1.94977792,  1.73567981,  1.99085363,  1.83993732,\n",
      "        1.80120709,  1.60497536,  1.78720698,  1.59786647,  1.55123994,\n",
      "        1.47757226,  1.43599191,  1.37686519,  1.33188388,  1.32779611,\n",
      "        1.31354647,  1.23220575,  1.25792665,  1.25725151,  1.14798527,\n",
      "        1.13912368,  1.12799352,  1.11108422,  1.07187911,  1.01947094])}, 'ATTACH::5::best_val_loss': 1.0194709412019645, 'ATTACH::6::history': {'train_accs': array([0.29800731, 0.30584372, 0.32151653, 0.35196656, 0.35383238,\n",
      "       0.36928129, 0.38211807, 0.39734309, 0.39988059, 0.39017837,\n",
      "       0.40100007, 0.40614971, 0.41607583, 0.41943429, 0.42234495,\n",
      "       0.43219643, 0.43525636, 0.44921263, 0.45667587, 0.45510859,\n",
      "       0.45869095, 0.47145309, 0.47622957, 0.48727517, 0.49832077,\n",
      "       0.49944026, 0.51488917, 0.52056124, 0.52615867, 0.53533846,\n",
      "       0.54205538, 0.5428017 , 0.54847377, 0.56287783, 0.55884768,\n",
      "       0.57108739, 0.573401  , 0.58414807, 0.5897455 , 0.59482051,\n",
      "       0.60549295, 0.60601537, 0.60877677, 0.6175834 , 0.614822  ,\n",
      "       0.62191208, 0.6284051 , 0.62773341, 0.63825659, 0.64258527,\n",
      "       0.64258527, 0.63795806, 0.64780954, 0.65355624, 0.66213897,\n",
      "       0.65788492, 0.66258676, 0.66676618, 0.67437868, 0.67102023]), 'train_losses': array([61.87947229, 36.53494912, 27.14402392, 21.23312886, 17.33349762,\n",
      "       13.7272449 , 11.59668872,  9.89142383,  8.66184181,  7.88503119,\n",
      "        7.03439409,  6.58481606,  6.01011327,  5.45171114,  4.98509288,\n",
      "        4.6527923 ,  4.30413325,  3.89213669,  3.74022835,  3.55198605,\n",
      "        3.29500246,  3.05348052,  2.88697398,  2.70303399,  2.60374366,\n",
      "        2.46415194,  2.31774729,  2.19015904,  2.10597813,  2.02372718,\n",
      "        1.9323884 ,  1.94052749,  1.84599799,  1.79276249,  1.69519223,\n",
      "        1.63695186,  1.63383389,  1.53803218,  1.48438044,  1.4606206 ,\n",
      "        1.35916342,  1.35407623,  1.35139064,  1.29867132,  1.31353439,\n",
      "        1.23362418,  1.18835505,  1.21121697,  1.17838663,  1.09941849,\n",
      "        1.09714244,  1.09287949,  1.05746695,  1.02891428,  0.99921077,\n",
      "        1.00370476,  0.97228872,  0.96981229,  0.91185815,  0.94625261]), 'val_accs': array([0.37253731, 0.38149254, 0.32029851, 0.29731343, 0.28089552,\n",
      "       0.30298507, 0.36865672, 0.39253731, 0.42298507, 0.42119403,\n",
      "       0.46358209, 0.44985075, 0.47910448, 0.49552239, 0.52029851,\n",
      "       0.51014925, 0.51014925, 0.52537313, 0.54447761, 0.54507463,\n",
      "       0.57044776, 0.56179104, 0.55373134, 0.56925373, 0.58686567,\n",
      "       0.61044776, 0.62      , 0.61134328, 0.62686567, 0.62238806,\n",
      "       0.62      , 0.66656716, 0.61343284, 0.62179104, 0.66925373,\n",
      "       0.63253731, 0.65104478, 0.65492537, 0.67223881, 0.64089552,\n",
      "       0.65671642, 0.66059701, 0.6558209 , 0.69402985, 0.70029851,\n",
      "       0.65223881, 0.67850746, 0.68626866, 0.69104478, 0.66537313,\n",
      "       0.71850746, 0.72716418, 0.70537313, 0.68925373, 0.72208955,\n",
      "       0.70238806, 0.73761194, 0.69373134, 0.72      , 0.71373134]), 'val_losses': array([24.80454658, 10.25651099,  7.64296407,  6.2972224 ,  5.23255566,\n",
      "        4.06835752,  3.09620612,  2.7855832 ,  2.70590729,  2.59703242,\n",
      "        2.3972544 ,  2.45800485,  2.14365205,  1.95620327,  1.75124576,\n",
      "        1.55794775,  1.43314368,  1.36878833,  1.38605219,  1.31882164,\n",
      "        1.24320142,  1.31033999,  1.29139551,  1.2462061 ,  1.07265879,\n",
      "        1.04748037,  1.02676967,  0.99943948,  1.04165824,  1.08359498,\n",
      "        1.07895491,  0.87350527,  1.12721563,  1.14714625,  0.84799259,\n",
      "        1.06182181,  0.90276684,  0.97390653,  0.95177104,  1.0724162 ,\n",
      "        0.9405281 ,  0.83978979,  0.7926514 ,  0.83423328,  0.74572088,\n",
      "        0.72422632,  0.86624679,  0.8728636 ,  0.72218861,  0.75864113,\n",
      "        0.70926623,  0.69624402,  0.85171814,  0.67444037,  0.70404452,\n",
      "        0.69174616,  0.58995568,  0.91713946,  0.6813599 ,  0.63686552])}, 'ATTACH::6::best_val_loss': 0.5899556777014662, 'ATTACH::7::history': {'train_accs': array([0.35577282, 0.40286589, 0.43271886, 0.46682588, 0.48496156,\n",
      "       0.51160534, 0.52817374, 0.53645794, 0.56795283, 0.58653631,\n",
      "       0.59444735, 0.60601537, 0.62877827, 0.63333085, 0.64698858,\n",
      "       0.65743712, 0.66885588, 0.68027465, 0.69841033, 0.70258975,\n",
      "       0.71990447, 0.73781625, 0.75729532, 0.76251959, 0.77826703,\n",
      "       0.78871558, 0.80043287, 0.82341966, 0.8283454 , 0.83259945,\n",
      "       0.84886932, 0.85625793, 0.86707963, 0.87745354, 0.88200612,\n",
      "       0.8943951 , 0.88790208, 0.90126129, 0.9029032 , 0.90999328,\n",
      "       0.91633704, 0.91872528, 0.92402418, 0.92253153, 0.93081573,\n",
      "       0.93365177, 0.93230838, 0.93611464, 0.93939846, 0.94275692,\n",
      "       0.94387641, 0.94678707, 0.95193671, 0.94999627, 0.94924994,\n",
      "       0.95193671, 0.9558176 , 0.95678782, 0.95641466, 0.96014628]), 'train_losses': array([28.70719305, 13.24507283,  9.22846283,  6.94695994,  5.6267892 ,\n",
      "        4.74735075,  4.05571317,  3.52351691,  3.09868473,  2.74931424,\n",
      "        2.46614251,  2.20799472,  1.96014045,  1.8602235 ,  1.66070148,\n",
      "        1.54295655,  1.43330247,  1.36266805,  1.25473726,  1.16452999,\n",
      "        1.09747844,  0.9706505 ,  0.90596673,  0.8446486 ,  0.7820702 ,\n",
      "        0.72724899,  0.698341  ,  0.58834575,  0.5937061 ,  0.56596649,\n",
      "        0.49464323,  0.46170082,  0.44096524,  0.39509947,  0.3869527 ,\n",
      "        0.35294124,  0.35454238,  0.31847044,  0.29928844,  0.28859544,\n",
      "        0.26978523,  0.25282468,  0.24597928,  0.25335956,  0.21575484,\n",
      "        0.21390953,  0.21540298,  0.21555959,  0.18252788,  0.18527439,\n",
      "        0.18253375,  0.16572774,  0.15553535,  0.15659478,  0.16190318,\n",
      "        0.15599479,  0.14483399,  0.14136586,  0.13987   ,  0.12900775]), 'val_accs': array([0.46328358, 0.5438806 , 0.55343284, 0.57970149, 0.56865672,\n",
      "       0.61641791, 0.62328358, 0.63850746, 0.67343284, 0.71462687,\n",
      "       0.70358209, 0.71552239, 0.71641791, 0.74119403, 0.68179104,\n",
      "       0.71253731, 0.77014925, 0.71850746, 0.75134328, 0.75432836,\n",
      "       0.75283582, 0.72895522, 0.80268657, 0.84537313, 0.79940299,\n",
      "       0.88059701, 0.82298507, 0.86089552, 0.83701493, 0.84507463,\n",
      "       0.84328358, 0.86179104, 0.89552239, 0.85313433, 0.88835821,\n",
      "       0.92358209, 0.9       , 0.89014925, 0.88119403, 0.91671642,\n",
      "       0.86835821, 0.92955224, 0.92328358, 0.92597015, 0.94298507,\n",
      "       0.92      , 0.94238806, 0.93820896, 0.92865672, 0.95402985,\n",
      "       0.94238806, 0.9       , 0.95552239, 0.91940299, 0.94059701,\n",
      "       0.96059701, 0.94955224, 0.96567164, 0.95761194, 0.95820896]), 'val_losses': array([6.85115888, 4.34670667, 4.29193163, 3.06901252, 2.95214386,\n",
      "       1.7900374 , 1.97678483, 1.93193622, 1.34386498, 1.15328543,\n",
      "       1.27326485, 1.1521988 , 1.07333092, 0.95784125, 1.16804839,\n",
      "       0.79874321, 0.73071235, 1.03443359, 0.72020825, 0.91505896,\n",
      "       0.68022501, 0.96096692, 0.57596536, 0.42187469, 0.57422827,\n",
      "       0.32155148, 0.5302011 , 0.3808941 , 0.48960741, 0.46194728,\n",
      "       0.48469186, 0.39263307, 0.3127496 , 0.4545538 , 0.29248978,\n",
      "       0.21889557, 0.25357237, 0.32623966, 0.37220291, 0.23141532,\n",
      "       0.40901182, 0.20823273, 0.21174633, 0.20230387, 0.16183385,\n",
      "       0.27764053, 0.16946963, 0.18868177, 0.23120989, 0.12280401,\n",
      "       0.18130039, 0.28546353, 0.13529261, 0.27261761, 0.18073656,\n",
      "       0.11565796, 0.152745  , 0.10883532, 0.12950742, 0.13891784])}, 'ATTACH::7::best_val_loss': 0.10883532113103725, 'ATTACH::8::history': {'train_accs': array([0.4346593 , 0.45861631, 0.48003582, 0.48354355, 0.49570863,\n",
      "       0.51473991, 0.52586014, 0.52377043, 0.5264572 , 0.52996492,\n",
      "       0.5377267 , 0.54115979, 0.53757743, 0.54213001, 0.53436824,\n",
      "       0.53578625, 0.53996567, 0.55392193, 0.55339951, 0.54518994,\n",
      "       0.54653332, 0.55063811, 0.54698112, 0.56138518, 0.57131129,\n",
      "       0.52406896, 0.54936936, 0.55675797, 0.56728114, 0.56205687,\n",
      "       0.56131055, 0.56959475, 0.56011643, 0.56265393, 0.57384879,\n",
      "       0.56429584, 0.56071349, 0.55466826, 0.56437048, 0.56548996,\n",
      "       0.57302784, 0.57213225, 0.56631092, 0.57713262, 0.5704157 ,\n",
      "       0.57302784, 0.57295321, 0.53944324, 0.54511531, 0.53414434,\n",
      "       0.5432495 , 0.53615941, 0.55720576, 0.55511605]), 'train_losses': array([4.25963705, 1.11186778, 1.07574784, 1.01940192, 0.99820201,\n",
      "       0.96460062, 0.96569352, 0.95476898, 0.93876674, 0.94415747,\n",
      "       0.93452951, 0.9387573 , 0.93231862, 0.92122643, 0.92599209,\n",
      "       0.91478323, 0.91974192, 0.89885091, 0.89557292, 0.89155779,\n",
      "       0.90588446, 0.90469419, 0.89857562, 0.87648339, 0.86722873,\n",
      "       0.9331676 , 0.8982401 , 0.89499163, 0.88744888, 0.87793061,\n",
      "       0.87965506, 0.86744768, 0.8742638 , 0.87679524, 0.86474325,\n",
      "       0.86781497, 0.87037523, 0.87782559, 0.86670319, 0.85769067,\n",
      "       0.84602509, 0.85894955, 0.86555003, 0.84872773, 0.85402218,\n",
      "       0.84795577, 0.85293628, 0.91569581, 0.91325923, 0.92145613,\n",
      "       0.89549497, 0.90498185, 0.88304084, 0.87963847]), 'val_accs': array([0.45283582, 0.54059701, 0.51283582, 0.51402985, 0.5480597 ,\n",
      "       0.53014925, 0.54059701, 0.5961194 , 0.60268657, 0.57432836,\n",
      "       0.5319403 , 0.56925373, 0.59850746, 0.58537313, 0.62358209,\n",
      "       0.63402985, 0.58507463, 0.62626866, 0.62179104, 0.60865672,\n",
      "       0.62029851, 0.60865672, 0.64597015, 0.69313433, 0.67671642,\n",
      "       0.61104478, 0.61253731, 0.59014925, 0.63402985, 0.63910448,\n",
      "       0.62746269, 0.6838806 , 0.6441791 , 0.63253731, 0.58597015,\n",
      "       0.65850746, 0.63641791, 0.62776119, 0.66925373, 0.65462687,\n",
      "       0.64776119, 0.6280597 , 0.66955224, 0.67880597, 0.6558209 ,\n",
      "       0.63820896, 0.65014925, 0.57850746, 0.64507463, 0.62925373,\n",
      "       0.65761194, 0.64746269, 0.65940299, 0.57850746]), 'val_losses': array([1.15083051, 0.9698547 , 0.96190928, 1.03689461, 0.88624295,\n",
      "       0.90121118, 0.84458677, 0.83781918, 0.82821285, 0.84711175,\n",
      "       0.86924949, 0.82692555, 0.83088371, 0.80765697, 0.82023957,\n",
      "       0.81737299, 0.81983127, 0.8508419 , 0.77149938, 0.7984913 ,\n",
      "       0.79169516, 0.79612681, 0.77929151, 0.78075833, 0.82388433,\n",
      "       0.85948664, 0.78393689, 0.82078599, 0.76516121, 0.73623156,\n",
      "       0.765057  , 0.76443373, 0.781412  , 0.76006316, 0.7981865 ,\n",
      "       0.73777153, 0.75973904, 0.76320059, 0.74174962, 0.73607203,\n",
      "       0.74057077, 0.73750668, 0.74846613, 0.71396921, 0.73962918,\n",
      "       0.74381457, 0.80229685, 0.89085745, 0.7808503 , 0.78362973,\n",
      "       0.75624507, 0.79643403, 0.75638936, 0.83803243])}, 'ATTACH::8::best_val_loss': 0.7139692079487132, 'ATTACH::9::history': {'train_accs': array([0.42846481, 0.55071274, 0.62101649, 0.64982461, 0.67534891,\n",
      "       0.71184417, 0.73221882, 0.75207105, 0.75625047, 0.76662437,\n",
      "       0.77961042, 0.79535786, 0.78804388, 0.80804538, 0.80946339,\n",
      "       0.82394209, 0.82924099, 0.82886783, 0.83192776, 0.83789835,\n",
      "       0.83946563, 0.84125681, 0.84663035, 0.84304799, 0.85006344,\n",
      "       0.82633032, 0.85207851, 0.84916785, 0.85431749, 0.85260094,\n",
      "       0.85364579, 0.8587208 , 0.85588477, 0.83864467, 0.84148071,\n",
      "       0.83789835, 0.84409284]), 'train_losses': array([8.08126269, 1.27708419, 0.84548343, 0.77481786, 0.69475886,\n",
      "       0.63048908, 0.60613189, 0.57292539, 0.55414187, 0.5365999 ,\n",
      "       0.51159849, 0.47819135, 0.50550306, 0.46403942, 0.45064246,\n",
      "       0.43237222, 0.42574118, 0.41403282, 0.40476961, 0.40086673,\n",
      "       0.40102539, 0.39664789, 0.39204587, 0.38756594, 0.3713813 ,\n",
      "       0.41688715, 0.37335788, 0.38074481, 0.36458642, 0.36314939,\n",
      "       0.3624874 , 0.35589812, 0.35946836, 0.40239942, 0.38887762,\n",
      "       0.39564807, 0.37919412]), 'val_accs': array([0.50835821, 0.5838806 , 0.5441791 , 0.68895522, 0.79283582,\n",
      "       0.76865672, 0.73432836, 0.79223881, 0.78179104, 0.76895522,\n",
      "       0.71223881, 0.8280597 , 0.74089552, 0.71223881, 0.76268657,\n",
      "       0.85880597, 0.85552239, 0.84298507, 0.87432836, 0.83761194,\n",
      "       0.85880597, 0.81402985, 0.86656716, 0.83940299, 0.78179104,\n",
      "       0.85522388, 0.87791045, 0.81761194, 0.76985075, 0.83731343,\n",
      "       0.9119403 , 0.8080597 , 0.75283582, 0.83492537, 0.86358209,\n",
      "       0.80507463, 0.81820896]), 'val_losses': array([3.7315483 , 0.92030498, 1.55215791, 0.69964544, 0.51423816,\n",
      "       0.55297079, 0.57392472, 0.49601799, 0.50496227, 0.54764776,\n",
      "       0.7142397 , 0.4194843 , 0.59879654, 0.57111715, 0.51376339,\n",
      "       0.31801085, 0.37183584, 0.39808094, 0.3084194 , 0.45762297,\n",
      "       0.41179764, 0.38630247, 0.30872781, 0.38305595, 0.51708683,\n",
      "       0.31220363, 0.24564145, 0.34624074, 0.72170456, 0.35684129,\n",
      "       0.25519568, 0.49264289, 0.56076399, 0.35263868, 0.25772803,\n",
      "       0.43764668, 0.31969064])}, 'ATTACH::9::best_val_loss': 0.24564145072182614, 'ATTACH::10::history': {'train_accs': array([0.33024853, 0.3985372 , 0.41473244, 0.41331443, 0.41741921,\n",
      "       0.43376371, 0.44152549, 0.44548101, 0.44786924, 0.44854094,\n",
      "       0.45264572, 0.44816777, 0.46190014, 0.45578028, 0.45063064,\n",
      "       0.46137771, 0.44936189, 0.45503396, 0.46010896, 0.45712367,\n",
      "       0.46682588, 0.47190089, 0.46107919, 0.47324427, 0.46540787,\n",
      "       0.47182626, 0.46734831, 0.47145309, 0.4790656 , 0.47772222,\n",
      "       0.48391671, 0.48555862, 0.48809613, 0.49220091, 0.49996268,\n",
      "       0.504366  , 0.51167998, 0.51235167, 0.518173  , 0.51302336,\n",
      "       0.52026271, 0.51996418, 0.52197925, 0.53063661, 0.5322039 ,\n",
      "       0.53078588, 0.53892081, 0.52892007, 0.5425778 , 0.54295097,\n",
      "       0.54466751, 0.55205612, 0.55019031, 0.55444436, 0.54742891,\n",
      "       0.56160908, 0.56631092, 0.56332562, 0.56459437, 0.57138592]), 'train_losses': array([6.05282648, 1.2075928 , 1.14144748, 1.11674567, 1.08313799,\n",
      "       1.06036134, 1.03563894, 1.02898946, 0.99552326, 0.9954652 ,\n",
      "       0.9823555 , 0.97476258, 0.97627212, 0.97253092, 0.97536434,\n",
      "       0.96657755, 0.96612035, 0.96527173, 0.96068354, 0.93738103,\n",
      "       0.95136056, 0.93620856, 0.93847938, 0.92928694, 0.93264797,\n",
      "       0.91745443, 0.93277464, 0.92623505, 0.92637494, 0.91038882,\n",
      "       0.91640796, 0.92779936, 0.90291254, 0.90141827, 0.89171595,\n",
      "       0.8844804 , 0.8748494 , 0.87450741, 0.86925733, 0.85792008,\n",
      "       0.85421332, 0.858341  , 0.85799584, 0.84453726, 0.84743608,\n",
      "       0.84447852, 0.83270849, 0.83778456, 0.83625964, 0.83546841,\n",
      "       0.83009223, 0.83315364, 0.8312469 , 0.82405225, 0.82824995,\n",
      "       0.81095039, 0.81518366, 0.81576163, 0.81132719, 0.80893793]), 'val_accs': array([0.30179104, 0.38955224, 0.43343284, 0.41134328, 0.3241791 ,\n",
      "       0.38716418, 0.4358209 , 0.4358209 , 0.40597015, 0.42925373,\n",
      "       0.43402985, 0.44208955, 0.43014925, 0.44149254, 0.44567164,\n",
      "       0.45373134, 0.43402985, 0.43402985, 0.44447761, 0.43492537,\n",
      "       0.45343284, 0.41462687, 0.41373134, 0.45373134, 0.45164179,\n",
      "       0.43014925, 0.47283582, 0.47044776, 0.45731343, 0.41940299,\n",
      "       0.47283582, 0.53223881, 0.46656716, 0.46447761, 0.51820896,\n",
      "       0.59552239, 0.51910448, 0.53671642, 0.62746269, 0.48208955,\n",
      "       0.60238806, 0.57492537, 0.52208955, 0.59522388, 0.63014925,\n",
      "       0.6358209 , 0.63074627, 0.56119403, 0.56149254, 0.5638806 ,\n",
      "       0.57940299, 0.51731343, 0.63761194, 0.64029851, 0.63074627,\n",
      "       0.5719403 , 0.53910448, 0.57970149, 0.62      , 0.62447761]), 'val_losses': array([1.17136896, 1.08709381, 1.06259971, 1.07836674, 1.02528115,\n",
      "       0.99574803, 0.97174539, 0.95417372, 0.97865445, 0.92766332,\n",
      "       0.91716161, 0.94637263, 0.92560886, 0.90423925, 0.96717337,\n",
      "       0.91402422, 0.89375652, 0.9302638 , 1.01753961, 0.89096221,\n",
      "       0.89227892, 0.89139965, 0.8827725 , 0.88541645, 0.88124288,\n",
      "       0.87210332, 0.9103384 , 0.89619761, 0.86626088, 0.88347606,\n",
      "       0.91943066, 0.84774589, 0.84036281, 0.87373087, 0.82184175,\n",
      "       0.83392937, 0.82095911, 0.81575524, 0.82568795, 0.80089102,\n",
      "       0.83008345, 0.80994254, 0.79267201, 0.79428006, 0.81234403,\n",
      "       0.77595873, 0.79752969, 0.79043404, 0.78084839, 0.76818369,\n",
      "       0.76241528, 0.76325203, 0.78426406, 0.79616043, 0.78032082,\n",
      "       0.75686245, 0.76692977, 0.74336027, 0.75192509, 0.75304883])}, 'ATTACH::10::best_val_loss': 0.743360266898995, 'ATTACH::11::history': {'train_accs': array([0.30450034, 0.32360624, 0.33704008, 0.36062393, 0.37226659,\n",
      "       0.38420778, 0.38592432, 0.39883573, 0.39405926, 0.39547727,\n",
      "       0.4095828 , 0.41615046, 0.42279275, 0.42898724, 0.43988357,\n",
      "       0.45227256, 0.46339279, 0.4710053 , 0.47578177, 0.48443914,\n",
      "       0.48264796, 0.49660422, 0.50063438, 0.51399358, 0.5128741 ,\n",
      "       0.53205463, 0.53339801, 0.54071199, 0.54489141, 0.55317561,\n",
      "       0.56138518, 0.56466901, 0.58631241, 0.58011792, 0.58862602,\n",
      "       0.59131278, 0.59997015, 0.60855288, 0.60765729, 0.61698634,\n",
      "       0.62086723, 0.62407642, 0.63265915, 0.6341518 , 0.64631689,\n",
      "       0.64460034, 0.65258601, 0.65840734, 0.66266139, 0.66504963,\n",
      "       0.66393014, 0.67109486, 0.67639376, 0.67811031, 0.68184193,\n",
      "       0.67251287]), 'train_losses': array([48.81020657, 26.16547057, 16.71021662, 12.20262772,  9.5822618 ,\n",
      "        7.8654461 ,  6.93249602,  5.86759464,  5.22353253,  4.78456025,\n",
      "        4.27941106,  3.85882304,  3.52365424,  3.182401  ,  2.99526074,\n",
      "        2.71948237,  2.4954727 ,  2.3713211 ,  2.27568583,  2.06937114,\n",
      "        1.98208656,  1.88787134,  1.69323536,  1.63090176,  1.55341764,\n",
      "        1.48123457,  1.45480977,  1.37297877,  1.30754831,  1.24344355,\n",
      "        1.18270557,  1.18517792,  1.12315241,  1.12676925,  1.06462586,\n",
      "        1.04985464,  1.02415988,  1.00594896,  0.98497838,  0.97419227,\n",
      "        0.9501483 ,  0.95387858,  0.92265209,  0.91622767,  0.88318856,\n",
      "        0.88774456,  0.85513471,  0.83764484,  0.82664837,  0.82757537,\n",
      "        0.81581071,  0.80541773,  0.78841617,  0.78477637,  0.76935322,\n",
      "        0.77504936]), 'val_accs': array([0.28477612, 0.40686567, 0.53940299, 0.54119403, 0.51850746,\n",
      "       0.5358209 , 0.45970149, 0.46208955, 0.50895522, 0.5119403 ,\n",
      "       0.48119403, 0.55402985, 0.48      , 0.58238806, 0.5880597 ,\n",
      "       0.59343284, 0.56477612, 0.5719403 , 0.60955224, 0.59791045,\n",
      "       0.60895522, 0.56835821, 0.63253731, 0.59164179, 0.62059701,\n",
      "       0.60626866, 0.63671642, 0.63910448, 0.62447761, 0.61044776,\n",
      "       0.61343284, 0.6558209 , 0.64089552, 0.61940299, 0.64238806,\n",
      "       0.61432836, 0.62925373, 0.63552239, 0.65014925, 0.61313433,\n",
      "       0.6241791 , 0.66447761, 0.63074627, 0.66686567, 0.67910448,\n",
      "       0.69701493, 0.65462687, 0.64865672, 0.66447761, 0.68208955,\n",
      "       0.68059701, 0.66985075, 0.63373134, 0.67820896, 0.67910448,\n",
      "       0.67552239]), 'val_losses': array([14.86128056,  8.14062352,  5.06456773,  3.85852086,  2.67115689,\n",
      "        2.28485374,  2.09773538,  2.02625555,  1.86623801,  1.74684432,\n",
      "        1.71204571,  1.52124972,  1.41387062,  1.37131746,  1.27266442,\n",
      "        1.15776172,  1.20070592,  1.08922559,  1.13551347,  1.0817093 ,\n",
      "        0.94044309,  1.17968884,  0.96818731,  1.09771619,  0.92958217,\n",
      "        0.95922661,  0.82264345,  0.87603815,  0.85546539,  0.90888399,\n",
      "        0.88986567,  0.76546685,  0.83231809,  0.85524918,  0.82939972,\n",
      "        0.89462338,  0.83848798,  0.82974685,  0.76860173,  0.81840339,\n",
      "        0.91060802,  0.72765343,  0.75879861,  0.76038963,  0.73581944,\n",
      "        0.67532725,  0.80287143,  0.78027205,  0.75341429,  0.69962948,\n",
      "        0.72634925,  0.76114928,  0.86004868,  0.74374307,  0.71756176,\n",
      "        0.71604094])}, 'ATTACH::11::best_val_loss': 0.675327247327833, 'ATTACH::12::history': {'train_accs': array([0.42577804, 0.51182924, 0.53839839, 0.56377342, 0.5954922 ,\n",
      "       0.63004702, 0.6647511 , 0.68393164, 0.71826256, 0.7588626 ,\n",
      "       0.77707292, 0.81394134, 0.82827077, 0.84819763, 0.85976565,\n",
      "       0.87245317, 0.88111053, 0.89118591, 0.89297709, 0.91133667,\n",
      "       0.90454512, 0.90850063, 0.91036645, 0.91708336, 0.92409881,\n",
      "       0.92372565, 0.93342787, 0.93126353, 0.92842749, 0.93156206,\n",
      "       0.91909844, 0.92566609, 0.92156131, 0.92365102, 0.9390253 ,\n",
      "       0.93424882, 0.93872677, 0.93454735, 0.93790581, 0.94111501,\n",
      "       0.94604075, 0.94596612, 0.93477125, 0.94439884, 0.9422345 ,\n",
      "       0.94738413, 0.94492126, 0.9502948 ]), 'train_losses': array([11.42187382,  2.32088722,  1.43690028,  1.16951737,  0.93747939,\n",
      "        0.83046232,  0.73677062,  0.68826857,  0.64575859,  0.56757878,\n",
      "        0.53634071,  0.45613221,  0.41973699,  0.38576178,  0.36280083,\n",
      "        0.3386194 ,  0.31172203,  0.29098921,  0.28943223,  0.23759651,\n",
      "        0.25162975,  0.24366309,  0.24879376,  0.22728466,  0.2167204 ,\n",
      "        0.21583548,  0.19113006,  0.19239519,  0.19747026,  0.19197933,\n",
      "        0.21952834,  0.2061838 ,  0.22274436,  0.21547058,  0.17424773,\n",
      "        0.19183125,  0.1664921 ,  0.18742342,  0.17663735,  0.15867108,\n",
      "        0.15453071,  0.15461388,  0.19076685,  0.15250013,  0.16474541,\n",
      "        0.14565446,  0.16152392,  0.14565142]), 'val_accs': array([0.49044776, 0.59462687, 0.59164179, 0.59791045, 0.61820896,\n",
      "       0.73731343, 0.67462687, 0.71343284, 0.78238806, 0.78149254,\n",
      "       0.80358209, 0.8638806 , 0.78179104, 0.86746269, 0.90179104,\n",
      "       0.84686567, 0.90029851, 0.84179104, 0.8961194 , 0.87313433,\n",
      "       0.85402985, 0.92      , 0.89761194, 0.91164179, 0.91731343,\n",
      "       0.9241791 , 0.9119403 , 0.92537313, 0.91552239, 0.89791045,\n",
      "       0.89552239, 0.88149254, 0.92179104, 0.90865672, 0.90985075,\n",
      "       0.94029851, 0.92716418, 0.96119403, 0.92238806, 0.94298507,\n",
      "       0.90059701, 0.92567164, 0.92537313, 0.91552239, 0.94358209,\n",
      "       0.91164179, 0.91940299, 0.93522388]), 'val_losses': array([2.965224  , 1.0541778 , 0.92426738, 0.89413547, 0.87162101,\n",
      "       0.62114306, 0.66427791, 0.63061946, 0.53962291, 0.48245438,\n",
      "       0.48399127, 0.38423868, 0.50261989, 0.33902643, 0.26463213,\n",
      "       0.37832578, 0.24892577, 0.39179366, 0.28639496, 0.32491366,\n",
      "       0.36987762, 0.21641816, 0.26895487, 0.23733418, 0.25795314,\n",
      "       0.20430433, 0.24505989, 0.19984766, 0.19451693, 0.26976377,\n",
      "       0.28909841, 0.29059141, 0.21851651, 0.2335057 , 0.21403789,\n",
      "       0.16141794, 0.18323556, 0.12285439, 0.21216846, 0.17887433,\n",
      "       0.22789091, 0.23530631, 0.1838201 , 0.21594454, 0.13436623,\n",
      "       0.21550122, 0.21365412, 0.19482005])}, 'ATTACH::12::best_val_loss': 0.12285438824945422, 'ATTACH::13::history': {'train_accs': array([0.48511083, 0.59959698, 0.64049556, 0.67952832, 0.69408165,\n",
      "       0.71385924, 0.72953205, 0.72087469, 0.68714083, 0.69639525,\n",
      "       0.70512725, 0.69460407, 0.66116874, 0.66997537, 0.68751399]), 'train_losses': array([7.44220459, 0.88696551, 0.77677857, 0.70974836, 0.68577836,\n",
      "       0.64670979, 0.61184469, 0.6122258 , 0.68188823, 0.64720792,\n",
      "       0.64552824, 0.64246039, 0.71739402, 0.69416989, 0.66286397]), 'val_accs': array([0.59731343, 0.60567164, 0.5719403 , 0.69223881, 0.7080597 ,\n",
      "       0.62865672, 0.70835821, 0.67432836, 0.71134328, 0.68776119,\n",
      "       0.66597015, 0.69104478, 0.72776119, 0.6719403 , 0.71850746]), 'val_losses': array([0.97804678, 0.76931716, 0.77328461, 0.66684573, 0.62155728,\n",
      "       0.74617464, 0.65799944, 0.65751564, 0.62799585, 0.65269272,\n",
      "       0.65339662, 0.69207387, 0.66324181, 0.73022354, 0.65977621])}, 'ATTACH::13::best_val_loss': 0.6215572802344365, 'ATTACH::14::history': {'train_accs': array([0.32681543, 0.3792074 , 0.41040376, 0.4213001 , 0.42644974,\n",
      "       0.44413762, 0.45607881, 0.48048362, 0.4875737 , 0.50518695,\n",
      "       0.52317337, 0.52862154, 0.55302635, 0.57407269, 0.58608851,\n",
      "       0.614822  , 0.63146503, 0.6534816 , 0.67460258, 0.68990223,\n",
      "       0.70236585, 0.72356146, 0.7397567 , 0.75259348, 0.76498246,\n",
      "       0.77356519, 0.79020822, 0.79535786, 0.80610493, 0.8251362 ,\n",
      "       0.83297261, 0.84088365, 0.84379431, 0.85543697, 0.85558624,\n",
      "       0.87066199, 0.87185611, 0.87879693, 0.88946936, 0.89133517,\n",
      "       0.90357489, 0.90432122, 0.91320248, 0.92014329, 0.92185984,\n",
      "       0.91827748, 0.92924845, 0.93111426, 0.93514441, 0.93857751,\n",
      "       0.93730875, 0.9390253 , 0.94671244, 0.94813046, 0.94947384,\n",
      "       0.9502948 , 0.95596686, 0.95708635, 0.95902679, 0.96634077]), 'train_losses': array([100.95837957,  29.60712513,  17.76093316,  13.46748335,\n",
      "        11.02170484,   8.89253183,   7.72860451,   6.40441042,\n",
      "         5.55797573,   4.97893751,   4.47425372,   4.03352415,\n",
      "         3.5623406 ,   3.06747837,   2.77803627,   2.50161176,\n",
      "         2.29156934,   2.03483297,   1.80736556,   1.59110906,\n",
      "         1.50276641,   1.31813473,   1.26040409,   1.09283935,\n",
      "         1.03348046,   0.94686875,   0.85065534,   0.8259358 ,\n",
      "         0.76363462,   0.65700125,   0.61505003,   0.58269604,\n",
      "         0.55417842,   0.51526846,   0.4887398 ,   0.44364013,\n",
      "         0.42628561,   0.40362084,   0.37339228,   0.34828972,\n",
      "         0.32097604,   0.32035412,   0.29086334,   0.26136006,\n",
      "         0.26720118,   0.26452673,   0.23075232,   0.22945153,\n",
      "         0.21652727,   0.20064309,   0.20315862,   0.2015982 ,\n",
      "         0.17264743,   0.17574029,   0.17818971,   0.17622996,\n",
      "         0.14793237,   0.14731109,   0.14580439,   0.11864453]), 'val_accs': array([0.4641791 , 0.50477612, 0.4638806 , 0.48268657, 0.49104478,\n",
      "       0.54      , 0.55880597, 0.55552239, 0.57432836, 0.60268657,\n",
      "       0.64746269, 0.66268657, 0.68298507, 0.71313433, 0.68656716,\n",
      "       0.72208955, 0.73970149, 0.76      , 0.77820896, 0.77791045,\n",
      "       0.75313433, 0.80567164, 0.8361194 , 0.81791045, 0.83253731,\n",
      "       0.85910448, 0.80686567, 0.86716418, 0.85074627, 0.80895522,\n",
      "       0.84746269, 0.83701493, 0.89402985, 0.8519403 , 0.88895522,\n",
      "       0.88955224, 0.89164179, 0.87552239, 0.88626866, 0.88746269,\n",
      "       0.89701493, 0.90656716, 0.90567164, 0.87701493, 0.86328358,\n",
      "       0.90328358, 0.90208955, 0.84597015, 0.8638806 , 0.89402985,\n",
      "       0.90597015, 0.89671642, 0.92507463, 0.91492537, 0.87671642,\n",
      "       0.9161194 , 0.85791045, 0.89283582, 0.89014925, 0.91432836]), 'val_losses': array([16.85018942,  7.97354038,  5.11646864,  4.12061108,  3.62405873,\n",
      "        2.94056788,  2.35459897,  2.23403671,  1.75899331,  1.75768797,\n",
      "        1.7207961 ,  1.70327184,  1.36546646,  1.40621216,  1.32776816,\n",
      "        1.16394485,  1.15280067,  0.88540568,  0.77763125,  0.76617262,\n",
      "        0.94663507,  0.69540105,  0.56111001,  0.63698328,  0.58864061,\n",
      "        0.53305502,  0.60492015,  0.52141438,  0.45577037,  0.64440286,\n",
      "        0.50237199,  0.49589034,  0.36262961,  0.43038823,  0.33050738,\n",
      "        0.40223798,  0.34060798,  0.37994175,  0.32985767,  0.31733637,\n",
      "        0.29442612,  0.33704866,  0.27937771,  0.35287898,  0.44142852,\n",
      "        0.31063259,  0.3045707 ,  0.52654972,  0.38746668,  0.38210109,\n",
      "        0.31045199,  0.36395271,  0.25265198,  0.27029538,  0.44696793,\n",
      "        0.25480279,  0.46303459,  0.35355068,  0.35478593,  0.30631363])}, 'ATTACH::14::best_val_loss': 0.2526519840399721, 'ATTACH::15::history': {'train_accs': array([0.44839167, 0.48302112, 0.5013807 , 0.51914322, 0.54929472,\n",
      "       0.61691171, 0.63877901, 0.68878274, 0.73617434, 0.76819166,\n",
      "       0.79102918, 0.81580715, 0.83991343, 0.85349653, 0.87499067,\n",
      "       0.87797597, 0.8923054 , 0.90327636, 0.91268005, 0.91984476,\n",
      "       0.92603926, 0.92178521, 0.93521905, 0.93895067, 0.93753265,\n",
      "       0.93760728, 0.94290619, 0.94954847, 0.94798119, 0.95574297,\n",
      "       0.9581312 , 0.96074334, 0.96507202, 0.96462423, 0.96753489,\n",
      "       0.96910217, 0.9664154 , 0.97044556, 0.97208747, 0.97081872,\n",
      "       0.97022166, 0.97522203, 0.972386  , 0.97104262, 0.97372938,\n",
      "       0.97447571, 0.9749235 , 0.97671468, 0.97611762, 0.9776849 ,\n",
      "       0.97589372, 0.95947459, 0.9751474 , 0.97738637, 0.97664005,\n",
      "       0.98216285]), 'train_losses': array([5.9511244 , 1.17923907, 1.13917431, 1.09473535, 1.01393395,\n",
      "       0.9155687 , 0.86066542, 0.77455591, 0.6770464 , 0.59829501,\n",
      "       0.54383034, 0.49348334, 0.43502176, 0.39953908, 0.35573253,\n",
      "       0.3302904 , 0.30288611, 0.26728673, 0.24730973, 0.22847879,\n",
      "       0.21509585, 0.21509718, 0.17724723, 0.17796736, 0.176326  ,\n",
      "       0.17608041, 0.15706556, 0.14251983, 0.14775284, 0.13006796,\n",
      "       0.12052752, 0.11314344, 0.10042113, 0.09818773, 0.09669948,\n",
      "       0.08957833, 0.09551746, 0.08331685, 0.08213914, 0.08658452,\n",
      "       0.0860919 , 0.07421041, 0.07800296, 0.08032031, 0.07829779,\n",
      "       0.07065912, 0.07204307, 0.06811411, 0.07114772, 0.06532849,\n",
      "       0.06632112, 0.11824342, 0.06956452, 0.06127503, 0.06321932,\n",
      "       0.05284452]), 'val_accs': array([0.45910448, 0.5080597 , 0.51552239, 0.52089552, 0.5758209 ,\n",
      "       0.63313433, 0.67074627, 0.72208955, 0.75492537, 0.79074627,\n",
      "       0.7958209 , 0.84089552, 0.84746269, 0.83761194, 0.84895522,\n",
      "       0.86298507, 0.8838806 , 0.90268657, 0.91074627, 0.90776119,\n",
      "       0.91283582, 0.88149254, 0.90328358, 0.91522388, 0.9238806 ,\n",
      "       0.92567164, 0.93044776, 0.92895522, 0.91283582, 0.92835821,\n",
      "       0.89731343, 0.92865672, 0.94059701, 0.92895522, 0.92179104,\n",
      "       0.92895522, 0.93343284, 0.93970149, 0.93343284, 0.94328358,\n",
      "       0.94567164, 0.91671642, 0.9119403 , 0.94925373, 0.93671642,\n",
      "       0.95462687, 0.93492537, 0.90029851, 0.93791045, 0.93910448,\n",
      "       0.92238806, 0.91791045, 0.95313433, 0.95343284, 0.94597015,\n",
      "       0.94149254]), 'val_losses': array([1.20798076, 1.14851141, 1.12164094, 1.06854909, 0.9827265 ,\n",
      "       0.89688291, 0.80773396, 0.6957827 , 0.62880781, 0.56500844,\n",
      "       0.54187086, 0.43307914, 0.42493251, 0.45174126, 0.43394816,\n",
      "       0.38348625, 0.3254343 , 0.27965062, 0.25241539, 0.28572472,\n",
      "       0.25430056, 0.35763738, 0.29060677, 0.25036003, 0.23389321,\n",
      "       0.22413312, 0.20984602, 0.21195732, 0.27623139, 0.23289742,\n",
      "       0.320044  , 0.22240621, 0.17791214, 0.22370285, 0.25003048,\n",
      "       0.24084316, 0.20842913, 0.19586774, 0.21739796, 0.18666499,\n",
      "       0.16432733, 0.26867586, 0.27237262, 0.17639421, 0.20958456,\n",
      "       0.15092583, 0.20907073, 0.32061291, 0.20510897, 0.20906683,\n",
      "       0.26581814, 0.25881836, 0.15738576, 0.16303595, 0.17718932,\n",
      "       0.190695  ])}, 'ATTACH::15::best_val_loss': 0.15092583111084218, 'ATTACH::16::history': {'train_accs': array([0.36607209, 0.41921039, 0.45063064, 0.48048362, 0.52235241,\n",
      "       0.54220464, 0.56660945, 0.59049183, 0.60437346, 0.63325621,\n",
      "       0.65318307, 0.67273677, 0.692589  , 0.7093813 , 0.72781551,\n",
      "       0.74557803, 0.76057915, 0.77647586, 0.79334279, 0.80476155,\n",
      "       0.81715053, 0.83312187, 0.83483842, 0.84842152, 0.86103441,\n",
      "       0.86581088, 0.87469214, 0.8778267 , 0.8812598 , 0.89170834,\n",
      "       0.89872379, 0.90514217, 0.90976939, 0.91424733, 0.91872528,\n",
      "       0.92641242, 0.92693485, 0.93641317, 0.93536831, 0.93589074,\n",
      "       0.9390253 , 0.94230913, 0.94402567, 0.94783193, 0.94842899,\n",
      "       0.95022017, 0.95439958, 0.95357863, 0.95783267, 0.9555937 ,\n",
      "       0.9608926 , 0.96119113, 0.96216136, 0.9611165 , 0.96223599,\n",
      "       0.96581834, 0.96746026, 0.96820658, 0.96462423, 0.97111725]), 'train_losses': array([15.36250225,  5.63943901,  3.4544558 ,  2.53823369,  1.93464527,\n",
      "        1.54948475,  1.32225928,  1.15587422,  1.02764252,  0.90687816,\n",
      "        0.85476133,  0.78898759,  0.7353974 ,  0.6900362 ,  0.64693542,\n",
      "        0.60559625,  0.57273199,  0.54011946,  0.50964325,  0.48381518,\n",
      "        0.45540398,  0.42571556,  0.41667077,  0.38974988,  0.36012688,\n",
      "        0.35334823,  0.33501902,  0.32251441,  0.30961107,  0.28990203,\n",
      "        0.2796783 ,  0.25655535,  0.24702927,  0.23933487,  0.22258111,\n",
      "        0.21135756,  0.20411902,  0.18381536,  0.18134534,  0.18263311,\n",
      "        0.17269081,  0.16233773,  0.16196694,  0.14915631,  0.14885959,\n",
      "        0.14591714,  0.13487087,  0.13729088,  0.12523248,  0.13276776,\n",
      "        0.11615954,  0.11601428,  0.11203075,  0.11343542,  0.11281348,\n",
      "        0.10137573,  0.09781352,  0.09658648,  0.10192992,  0.08930127]), 'val_accs': array([0.41432836, 0.49910448, 0.47044776, 0.61820896, 0.60179104,\n",
      "       0.54776119, 0.59044776, 0.56208955, 0.54179104, 0.66119403,\n",
      "       0.64447761, 0.6758209 , 0.59791045, 0.62746269, 0.69462687,\n",
      "       0.68477612, 0.79522388, 0.70925373, 0.7758209 , 0.81850746,\n",
      "       0.85134328, 0.84      , 0.88089552, 0.85641791, 0.8761194 ,\n",
      "       0.85164179, 0.88597015, 0.91283582, 0.89731343, 0.90238806,\n",
      "       0.93283582, 0.8958209 , 0.91462687, 0.91522388, 0.92507463,\n",
      "       0.92716418, 0.93791045, 0.93820896, 0.9080597 , 0.93432836,\n",
      "       0.95074627, 0.94895522, 0.96059701, 0.94776119, 0.93850746,\n",
      "       0.95552239, 0.9519403 , 0.96328358, 0.95850746, 0.96447761,\n",
      "       0.93940299, 0.96447761, 0.96865672, 0.93731343, 0.96776119,\n",
      "       0.96477612, 0.93283582, 0.96149254, 0.97731343, 0.97522388]), 'val_losses': array([5.6544136 , 2.10339849, 2.31105219, 1.29088125, 1.11806877,\n",
      "       1.6171074 , 1.18018591, 1.08407406, 1.1622306 , 0.87128199,\n",
      "       1.01633022, 0.72516426, 0.9157844 , 0.71357523, 0.66094989,\n",
      "       0.79401228, 0.4840408 , 0.61583763, 0.50963912, 0.44599414,\n",
      "       0.3999893 , 0.39496529, 0.31731882, 0.34483344, 0.31383763,\n",
      "       0.35356799, 0.2848052 , 0.23086978, 0.26143247, 0.24547949,\n",
      "       0.18156295, 0.25983893, 0.21602354, 0.21684009, 0.19719246,\n",
      "       0.18740879, 0.16138733, 0.15978813, 0.22778025, 0.1808434 ,\n",
      "       0.131269  , 0.13341068, 0.10977452, 0.14576788, 0.15737921,\n",
      "       0.11466511, 0.12802606, 0.09636847, 0.11681647, 0.09283133,\n",
      "       0.1535986 , 0.09734654, 0.0830522 , 0.16547531, 0.08671319,\n",
      "       0.09145338, 0.1737981 , 0.09062428, 0.06436455, 0.06785258])}, 'ATTACH::16::best_val_loss': 0.06436455024267311, 'ATTACH::17::history': {'train_accs': array([0.28539443, 0.33674155, 0.34771252, 0.35592208, 0.36681842,\n",
      "       0.37107247, 0.37950593, 0.38681991, 0.39592507, 0.40122397,\n",
      "       0.40495559, 0.4070453 , 0.41197104, 0.42585268, 0.42070304,\n",
      "       0.42309128, 0.43704754, 0.43951041, 0.44451078, 0.4574222 ,\n",
      "       0.45010822, 0.45779536, 0.45660124, 0.45981043, 0.47413986,\n",
      "       0.47234868, 0.47048287, 0.4763042 , 0.476752  , 0.48623032,\n",
      "       0.49018583, 0.49488768, 0.49429062, 0.49899246, 0.50787372,\n",
      "       0.50578401, 0.50324651, 0.51391895, 0.51697888, 0.5177252 ,\n",
      "       0.51929248, 0.53406971, 0.52466602, 0.53227853, 0.53272632,\n",
      "       0.53048735, 0.53966714, 0.54063736, 0.54981715, 0.54772744,\n",
      "       0.55459363, 0.55504142, 0.56123591, 0.55713113, 0.55832525,\n",
      "       0.55944473, 0.56914695, 0.56675871, 0.56952011, 0.5786999 ]), 'train_losses': array([110.94709511,  67.15007448,  52.44350617,  40.88502389,\n",
      "        31.69453918,  25.15786535,  21.15683564,  17.96153772,\n",
      "        15.37873354,  13.73981855,  12.40718246,  11.57110489,\n",
      "        10.20340462,   9.23830135,   8.69688474,   8.0389724 ,\n",
      "         7.67604852,   6.91184693,   6.28481659,   5.95256763,\n",
      "         5.47916406,   5.26557365,   4.9360849 ,   4.61275604,\n",
      "         4.26858168,   4.05595915,   3.95425098,   3.69247444,\n",
      "         3.5278643 ,   3.30549184,   3.18566424,   3.05203538,\n",
      "         2.88985176,   2.73685845,   2.65939246,   2.56676931,\n",
      "         2.52164636,   2.37071345,   2.3460767 ,   2.14049828,\n",
      "         2.125109  ,   1.95930233,   2.02596701,   1.93895339,\n",
      "         1.85262965,   1.80468975,   1.76892783,   1.72164289,\n",
      "         1.66845336,   1.63177227,   1.54365339,   1.50325054,\n",
      "         1.49689435,   1.47988407,   1.45079915,   1.39524528,\n",
      "         1.37678303,   1.38550217,   1.31136925,   1.28561603]), 'val_accs': array([0.45014925, 0.48746269, 0.45850746, 0.49492537, 0.46537313,\n",
      "       0.46358209, 0.48447761, 0.50656716, 0.49701493, 0.48746269,\n",
      "       0.51462687, 0.52149254, 0.53253731, 0.54507463, 0.55791045,\n",
      "       0.55492537, 0.5080597 , 0.56119403, 0.56955224, 0.5841791 ,\n",
      "       0.53940299, 0.5680597 , 0.58268657, 0.57253731, 0.57313433,\n",
      "       0.58029851, 0.62179104, 0.62328358, 0.60656716, 0.59134328,\n",
      "       0.64268657, 0.65253731, 0.64985075, 0.66328358, 0.68179104,\n",
      "       0.65432836, 0.65253731, 0.61134328, 0.63701493, 0.65014925,\n",
      "       0.63402985, 0.65492537, 0.66119403, 0.64358209, 0.68537313,\n",
      "       0.67940299, 0.65402985, 0.67223881, 0.67671642, 0.66238806,\n",
      "       0.66686567, 0.69910448, 0.69791045, 0.71910448, 0.71701493,\n",
      "       0.72447761, 0.70895522, 0.71970149, 0.66716418, 0.64835821]), 'val_losses': array([14.12118818,  9.33921239,  5.78754589,  4.26618717,  4.19734965,\n",
      "        4.78380393,  4.22099737,  3.53890109,  4.30146514,  4.57636679,\n",
      "        3.26314265,  3.44243739,  3.43126807,  3.12829656,  2.59976868,\n",
      "        2.35917517,  2.37741829,  2.25609899,  2.35105813,  2.07027187,\n",
      "        2.01793725,  1.89211248,  1.62356483,  1.51926876,  1.39762436,\n",
      "        1.33666969,  1.18659889,  1.1394521 ,  1.15682529,  1.18084636,\n",
      "        1.00132529,  1.00498594,  1.02527244,  0.90866333,  0.90260221,\n",
      "        0.86976612,  0.85013571,  0.8961054 ,  0.83566429,  0.82472984,\n",
      "        0.81891142,  0.80717505,  0.82763514,  0.79930909,  0.78297927,\n",
      "        0.78188118,  0.81119038,  0.76030781,  0.77432928,  0.81487911,\n",
      "        0.80844381,  0.75717142,  0.78278834,  0.72818208,  0.72679619,\n",
      "        0.70674601,  0.72185348,  0.73604295,  0.82984496,  0.75461362])}, 'ATTACH::17::best_val_loss': 0.7067460149081786, 'ATTACH::18::history': {'train_accs': array([0.49220091, 0.5980297 , 0.66676618, 0.69915665, 0.71065005,\n",
      "       0.72169565, 0.72751698, 0.69975371, 0.69438018, 0.68094634,\n",
      "       0.6955743 , 0.70512725, 0.70990372, 0.68363311, 0.69400702,\n",
      "       0.69393238]), 'train_losses': array([5.18064854, 0.9134529 , 0.70717758, 0.65030548, 0.62816944,\n",
      "       0.60968014, 0.59302985, 0.66641567, 0.65378502, 0.69860825,\n",
      "       0.66229545, 0.63815488, 0.63343906, 0.67693042, 0.67021022,\n",
      "       0.66359568]), 'val_accs': array([0.61104478, 0.70686567, 0.71373134, 0.7038806 , 0.64089552,\n",
      "       0.74985075, 0.63014925, 0.63462687, 0.57044776, 0.6280597 ,\n",
      "       0.68268657, 0.70716418, 0.61940299, 0.54656716, 0.68746269,\n",
      "       0.62567164]), 'val_losses': array([0.79725817, 0.61618864, 0.60654994, 0.60811368, 0.68848503,\n",
      "       0.54028579, 0.66780237, 0.77071848, 0.74156097, 0.75993194,\n",
      "       0.67800399, 0.73368211, 0.85349702, 0.90579709, 0.72488253,\n",
      "       0.76836234])}, 'ATTACH::18::best_val_loss': 0.5402857912832232, 'ATTACH::19::history': {'train_accs': array([0.35047392, 0.398985  , 0.42868871, 0.46010896, 0.47921487,\n",
      "       0.5066796 , 0.51914322, 0.53317412, 0.54847377, 0.56757967,\n",
      "       0.58265542, 0.5897455 , 0.60668707, 0.61101575, 0.62825584,\n",
      "       0.63609225, 0.64721248, 0.66019852, 0.66975147, 0.68893201,\n",
      "       0.69878349, 0.71072468, 0.72080006, 0.72699455, 0.74214494,\n",
      "       0.74632435, 0.7620718 , 0.76774386, 0.77431152, 0.78528248,\n",
      "       0.79670125, 0.81438913, 0.81841929, 0.83185312, 0.84640645,\n",
      "       0.85476528, 0.86655721, 0.87924472, 0.88670796, 0.89603702,\n",
      "       0.90200761, 0.90857527, 0.91327711, 0.92305396, 0.92357639,\n",
      "       0.92723338, 0.93492052, 0.93656243, 0.94081648, 0.94484663,\n",
      "       0.9470856 , 0.94947384, 0.95372789, 0.95425032, 0.95850437,\n",
      "       0.95939996, 0.95984775, 0.96395253, 0.9638779 , 0.96828122]), 'train_losses': array([23.08652297,  8.56791577,  5.49779879,  3.83598282,  2.90502787,\n",
      "        2.24615411,  1.83330201,  1.58420989,  1.31755256,  1.23124313,\n",
      "        1.08350878,  1.03806734,  0.96582945,  0.9289773 ,  0.86430169,\n",
      "        0.8535281 ,  0.80755995,  0.76988046,  0.74916651,  0.71724331,\n",
      "        0.6910873 ,  0.66288175,  0.64330001,  0.63000733,  0.60910312,\n",
      "        0.5996015 ,  0.56475488,  0.55405231,  0.54560646,  0.52677462,\n",
      "        0.50161983,  0.46937654,  0.45251211,  0.4268135 ,  0.39523107,\n",
      "        0.37679852,  0.34665164,  0.32534168,  0.30510594,  0.28208048,\n",
      "        0.26724526,  0.24942721,  0.23074621,  0.21565018,  0.20680964,\n",
      "        0.19798965,  0.18392287,  0.17556377,  0.16785778,  0.1562203 ,\n",
      "        0.15107322,  0.14033488,  0.13242724,  0.1304632 ,  0.11685176,\n",
      "        0.11657693,  0.11400222,  0.10427891,  0.10350121,  0.09265474]), 'val_accs': array([0.3680597 , 0.46955224, 0.55701493, 0.66149254, 0.62238806,\n",
      "       0.65074627, 0.60328358, 0.63283582, 0.63671642, 0.65701493,\n",
      "       0.65253731, 0.73044776, 0.73104478, 0.69492537, 0.69940299,\n",
      "       0.75970149, 0.74895522, 0.77432836, 0.75791045, 0.71671642,\n",
      "       0.78208955, 0.75701493, 0.80238806, 0.78149254, 0.8       ,\n",
      "       0.80029851, 0.7958209 , 0.79014925, 0.80328358, 0.7438806 ,\n",
      "       0.8241791 , 0.8       , 0.83343284, 0.86835821, 0.86268657,\n",
      "       0.89044776, 0.89880597, 0.88298507, 0.91850746, 0.90656716,\n",
      "       0.9280597 , 0.92238806, 0.92358209, 0.93402985, 0.93910448,\n",
      "       0.94119403, 0.91462687, 0.9438806 , 0.9358209 , 0.95014925,\n",
      "       0.94447761, 0.9558209 , 0.95313433, 0.9519403 , 0.96358209,\n",
      "       0.96059701, 0.96358209, 0.95880597, 0.96507463, 0.96477612]), 'val_losses': array([5.80300982, 3.21004436, 1.83672188, 1.12600411, 1.13351109,\n",
      "       0.88291949, 1.0560002 , 0.92912732, 0.800308  , 0.71787115,\n",
      "       0.81026949, 0.62948055, 0.62793257, 0.64854984, 0.65486907,\n",
      "       0.61065277, 0.5613343 , 0.56157538, 0.55332972, 0.60733005,\n",
      "       0.53270477, 0.52463337, 0.48470725, 0.51477929, 0.47262759,\n",
      "       0.46972774, 0.49356402, 0.46105055, 0.4653542 , 0.56270458,\n",
      "       0.44088879, 0.44388671, 0.40036464, 0.32727446, 0.34875858,\n",
      "       0.28497313, 0.2644911 , 0.29156139, 0.22178416, 0.23855996,\n",
      "       0.2035935 , 0.20900623, 0.20346041, 0.17802099, 0.1717089 ,\n",
      "       0.16521642, 0.22770131, 0.15814906, 0.18459212, 0.14525769,\n",
      "       0.15299784, 0.13125456, 0.13318116, 0.13180431, 0.10729926,\n",
      "       0.11110903, 0.09883307, 0.11400985, 0.10039348, 0.09406233])}, 'ATTACH::19::best_val_loss': 0.09406232631250994, 'ATTACH::20::history': {'train_accs': array([0.37069931, 0.41122472, 0.45092917, 0.48630495, 0.51720278,\n",
      "       0.55138443, 0.57877454, 0.6205687 , 0.64474961, 0.68520039,\n",
      "       0.71953131, 0.75759385, 0.78722293, 0.81379207, 0.83886857,\n",
      "       0.86290022, 0.87648332, 0.8861109 , 0.8923054 , 0.89805209,\n",
      "       0.90782894, 0.91618778, 0.91939697, 0.92402418, 0.93208448,\n",
      "       0.93253228, 0.93969699, 0.94171207, 0.94574222, 0.9500709 ,\n",
      "       0.95089186, 0.95313083, 0.95544444, 0.95648929, 0.95962385,\n",
      "       0.96245989, 0.96380327, 0.96447496, 0.96402717, 0.96753489,\n",
      "       0.96798269, 0.9696246 , 0.97029629, 0.97029629, 0.96947533,\n",
      "       0.97141578, 0.97574446, 0.97335622, 0.97365475, 0.97410254,\n",
      "       0.97686395, 0.97656542, 0.97783417, 0.97678931, 0.98014777,\n",
      "       0.97850586, 0.97775953, 0.98014777, 0.98193895, 0.98126726]), 'train_losses': array([30.24064837,  9.2922113 ,  5.15281234,  3.21025086,  2.21653696,\n",
      "        1.7010888 ,  1.37232807,  1.1353567 ,  0.98229856,  0.82958819,\n",
      "        0.72506043,  0.62975459,  0.55993401,  0.4975414 ,  0.43968668,\n",
      "        0.38157606,  0.35061376,  0.32824919,  0.31115902,  0.28691894,\n",
      "        0.25992841,  0.24032097,  0.2288109 ,  0.2226091 ,  0.19884889,\n",
      "        0.19581384,  0.1823191 ,  0.17737068,  0.16521249,  0.15656427,\n",
      "        0.14854117,  0.13841372,  0.13820061,  0.13506119,  0.12671303,\n",
      "        0.11808831,  0.11309247,  0.1161428 ,  0.11168207,  0.10614402,\n",
      "        0.09629667,  0.09481579,  0.09306593,  0.09376831,  0.09684264,\n",
      "        0.08972476,  0.08149807,  0.08205812,  0.08242509,  0.07915656,\n",
      "        0.0764572 ,  0.07175072,  0.0734774 ,  0.07376716,  0.06842286,\n",
      "        0.07112765,  0.07117959,  0.06404656,  0.05561717,  0.06067184]), 'val_accs': array([0.51492537, 0.47641791, 0.51074627, 0.57074627, 0.54746269,\n",
      "       0.66179104, 0.72686567, 0.68089552, 0.66238806, 0.67671642,\n",
      "       0.77104478, 0.77970149, 0.82955224, 0.83223881, 0.82626866,\n",
      "       0.90059701, 0.90477612, 0.8880597 , 0.91134328, 0.88865672,\n",
      "       0.9358209 , 0.90865672, 0.91641791, 0.95223881, 0.93910448,\n",
      "       0.9519403 , 0.95074627, 0.95373134, 0.95074627, 0.94865672,\n",
      "       0.96895522, 0.90119403, 0.96507463, 0.9561194 , 0.93940299,\n",
      "       0.95044776, 0.95940299, 0.96925373, 0.94925373, 0.96208955,\n",
      "       0.96985075, 0.95671642, 0.95880597, 0.94835821, 0.95104478,\n",
      "       0.94686567, 0.9641791 , 0.96179104, 0.97313433, 0.96238806,\n",
      "       0.96567164, 0.97641791, 0.94656716, 0.97373134, 0.97850746,\n",
      "       0.97671642, 0.97432836, 0.96626866, 0.97074627, 0.97313433]), 'val_losses': array([4.97231424, 2.76032333, 2.21820737, 1.7349562 , 1.32973064,\n",
      "       0.88791262, 0.68904857, 1.14670513, 0.71692996, 0.7562496 ,\n",
      "       0.55817257, 0.54978416, 0.43856079, 0.44496033, 0.43523782,\n",
      "       0.27841607, 0.27095707, 0.30087708, 0.24911454, 0.2845752 ,\n",
      "       0.21003418, 0.24093034, 0.23016166, 0.16320543, 0.18485873,\n",
      "       0.14592211, 0.14989279, 0.13522515, 0.1479328 , 0.14003037,\n",
      "       0.10551654, 0.29477082, 0.11711659, 0.10915693, 0.16069352,\n",
      "       0.1268999 , 0.10650476, 0.09054461, 0.13292208, 0.09734584,\n",
      "       0.11138393, 0.11268872, 0.10679475, 0.13251114, 0.12725579,\n",
      "       0.13836044, 0.09034638, 0.08273869, 0.09402976, 0.09558713,\n",
      "       0.11527466, 0.09061283, 0.14934337, 0.09123526, 0.08174561,\n",
      "       0.09152116, 0.08542225, 0.08751021, 0.08862368, 0.08293479])}, 'ATTACH::20::best_val_loss': 0.08174560543872528, 'ATTACH::21::history': {'train_accs': array([0.39017837, 0.44436152, 0.51145608, 0.55466826, 0.60131353,\n",
      "       0.64654079, 0.68714083, 0.71885962, 0.73945817, 0.75423539,\n",
      "       0.78050601, 0.79729831, 0.80677663, 0.82230017, 0.83267408,\n",
      "       0.84006269, 0.85543697, 0.85976565, 0.87379655, 0.88118516,\n",
      "       0.89170834, 0.89626091, 0.9005896 , 0.90894843, 0.91365027,\n",
      "       0.91939697, 0.92365102, 0.9305172 , 0.93536831, 0.93529368,\n",
      "       0.93753265, 0.94081648, 0.94410031, 0.94760803, 0.94962311,\n",
      "       0.949847  , 0.95148892, 0.95596686, 0.9553698 , 0.95969848,\n",
      "       0.96268378, 0.96141503, 0.96290768, 0.96514665, 0.96313158,\n",
      "       0.96604224]), 'train_losses': array([13.22237125,  4.63911391,  2.72459121,  1.91613733,  1.42206036,\n",
      "        1.08636796,  0.92595577,  0.77760982,  0.70761194,  0.64837877,\n",
      "        0.5976814 ,  0.53975517,  0.50901544,  0.48240322,  0.44716228,\n",
      "        0.42578493,  0.39018829,  0.38175082,  0.35194214,  0.33003271,\n",
      "        0.30071838,  0.29075906,  0.26943002,  0.25501838,  0.24338715,\n",
      "        0.2242236 ,  0.21947441,  0.19969203,  0.18796657,  0.18903686,\n",
      "        0.17633209,  0.17068736,  0.1619064 ,  0.15192203,  0.14891435,\n",
      "        0.14403372,  0.14548426,  0.1275334 ,  0.13125996,  0.12526808,\n",
      "        0.11324387,  0.11714381,  0.10834252,  0.1025218 ,  0.11546069,\n",
      "        0.09766315]), 'val_accs': array([0.48925373, 0.56567164, 0.62985075, 0.70447761, 0.67223881,\n",
      "       0.64029851, 0.76208955, 0.78328358, 0.79641791, 0.81164179,\n",
      "       0.83522388, 0.79104478, 0.79432836, 0.76955224, 0.84656716,\n",
      "       0.87253731, 0.85164179, 0.90268657, 0.90298507, 0.90895522,\n",
      "       0.88358209, 0.9161194 , 0.90716418, 0.91432836, 0.93880597,\n",
      "       0.94149254, 0.92567164, 0.94149254, 0.92955224, 0.94089552,\n",
      "       0.93731343, 0.92865672, 0.95253731, 0.95462687, 0.95134328,\n",
      "       0.96597015, 0.94656716, 0.95791045, 0.95432836, 0.9480597 ,\n",
      "       0.93731343, 0.95791045, 0.95522388, 0.95343284, 0.92955224,\n",
      "       0.96716418]), 'val_losses': array([2.42015002, 1.61545805, 1.12114605, 0.78048494, 0.98510767,\n",
      "       0.78289756, 0.61993824, 0.61895665, 0.46626432, 0.50588537,\n",
      "       0.40722994, 0.55116276, 0.68307397, 0.64195002, 0.3921287 ,\n",
      "       0.33975834, 0.39368642, 0.25996784, 0.27152366, 0.26552961,\n",
      "       0.31262291, 0.23093414, 0.26792884, 0.23329198, 0.17066443,\n",
      "       0.16594884, 0.19951192, 0.17950894, 0.17403026, 0.1604963 ,\n",
      "       0.16500473, 0.19547044, 0.1336684 , 0.13342448, 0.1330812 ,\n",
      "       0.11637391, 0.12953706, 0.12029719, 0.13115578, 0.1346166 ,\n",
      "       0.15316276, 0.12393714, 0.11884729, 0.13240893, 0.18278065,\n",
      "       0.12470723])}, 'ATTACH::21::best_val_loss': 0.11637391205154247, 'ATTACH::22::history': {'train_accs': array([0.39696992, 0.41659825, 0.4597358 , 0.49847004, 0.54951862,\n",
      "       0.59004403, 0.61982237, 0.64004776, 0.6617658 , 0.70587357,\n",
      "       0.72721845, 0.74565266, 0.76863945, 0.78468542, 0.80789611,\n",
      "       0.82409135, 0.83573401, 0.84364505, 0.86342264, 0.86790059,\n",
      "       0.87775207, 0.89096201, 0.89118591, 0.90044033, 0.90573923,\n",
      "       0.91305321, 0.91753116, 0.92357639, 0.92790507, 0.93230838,\n",
      "       0.93208448, 0.93939846, 0.94745877, 0.94619001, 0.95104112,\n",
      "       0.95596686, 0.95686245, 0.95708635, 0.95962385, 0.96104187,\n",
      "       0.96350474, 0.96238525, 0.96313158, 0.96425106, 0.96678857,\n",
      "       0.97014703, 0.96723636, 0.96850511, 0.96738563, 0.97313232,\n",
      "       0.97283379, 0.96932607, 0.97358012, 0.97649078, 0.97544593]), 'train_losses': array([20.40797127,  6.32680798,  3.38481567,  2.0536466 ,  1.37158089,\n",
      "        1.05745695,  0.89853695,  0.82792308,  0.7766616 ,  0.69044006,\n",
      "        0.65555863,  0.62462417,  0.56027505,  0.53543133,  0.49444215,\n",
      "        0.46272137,  0.44193748,  0.41395733,  0.37634123,  0.36126205,\n",
      "        0.34478641,  0.30895728,  0.30574634,  0.28402231,  0.26949682,\n",
      "        0.25578927,  0.23842353,  0.22235671,  0.21437013,  0.19215686,\n",
      "        0.19656073,  0.1763229 ,  0.1591793 ,  0.1646221 ,  0.14833634,\n",
      "        0.13393653,  0.12681317,  0.12711076,  0.13129297,  0.11393197,\n",
      "        0.11017224,  0.11292763,  0.11240075,  0.10740226,  0.09941757,\n",
      "        0.09021221,  0.09919999,  0.09393299,  0.09767358,  0.08208498,\n",
      "        0.082267  ,  0.09572669,  0.08189452,  0.07237295,  0.07338473]), 'val_accs': array([0.3480597 , 0.48567164, 0.5241791 , 0.58149254, 0.58776119,\n",
      "       0.6119403 , 0.61044776, 0.62716418, 0.64238806, 0.75402985,\n",
      "       0.71253731, 0.74358209, 0.79373134, 0.84567164, 0.83164179,\n",
      "       0.75492537, 0.75910448, 0.82119403, 0.87104478, 0.81880597,\n",
      "       0.85940299, 0.92447761, 0.92835821, 0.85402985, 0.92567164,\n",
      "       0.87134328, 0.92746269, 0.94567164, 0.94686567, 0.94955224,\n",
      "       0.87164179, 0.95402985, 0.83701493, 0.96925373, 0.92626866,\n",
      "       0.92149254, 0.95313433, 0.96985075, 0.93044776, 0.96298507,\n",
      "       0.9558209 , 0.96149254, 0.97671642, 0.97104478, 0.97970149,\n",
      "       0.97492537, 0.9680597 , 0.97014925, 0.98119403, 0.97910448,\n",
      "       0.97880597, 0.97402985, 0.97820896, 0.97522388, 0.95910448]), 'val_losses': array([8.04313748, 2.58307001, 1.54681809, 1.28274937, 1.01996272,\n",
      "       1.07409335, 1.1149603 , 0.85720447, 0.72778545, 0.59290314,\n",
      "       0.66438975, 0.59523614, 0.47733729, 0.40974044, 0.44478923,\n",
      "       0.57666135, 0.56629973, 0.50138434, 0.34116859, 0.41552633,\n",
      "       0.33950221, 0.23625144, 0.2139672 , 0.3569999 , 0.21589396,\n",
      "       0.38061992, 0.20818114, 0.16859358, 0.15552989, 0.17961841,\n",
      "       0.36878071, 0.14257563, 0.4829643 , 0.10416546, 0.22666219,\n",
      "       0.23000079, 0.13444823, 0.09359088, 0.17485233, 0.13864697,\n",
      "       0.11053619, 0.10965391, 0.07739118, 0.08850123, 0.06049074,\n",
      "       0.07415969, 0.10353722, 0.08830446, 0.06876455, 0.06787264,\n",
      "       0.07670547, 0.0740068 , 0.06702925, 0.08799301, 0.12291358])}, 'ATTACH::22::best_val_loss': 0.06049074366021512, 'ATTACH::23::history': {'train_accs': array([0.39823867, 0.43003209, 0.46398985, 0.49712665, 0.53675647,\n",
      "       0.58399881, 0.62109113, 0.65825808, 0.70736622, 0.74154788,\n",
      "       0.77849093, 0.79341742, 0.8175237 , 0.84252556, 0.85745205,\n",
      "       0.88006568, 0.88708113, 0.89558922, 0.91044108, 0.91581461,\n",
      "       0.92768117, 0.93417419, 0.93454735, 0.9443242 , 0.94320472,\n",
      "       0.95701172, 0.95686245, 0.95820584, 0.95275767, 0.96186283,\n",
      "       0.96507202, 0.96865438, 0.96611687, 0.9696246 , 0.96723636,\n",
      "       0.96872901, 0.96857974, 0.96820658, 0.97037092, 0.9719382 ,\n",
      "       0.97372938, 0.97455034, 0.97589372]), 'train_losses': array([20.63626203,  7.29947023,  3.9167631 ,  2.40883412,  1.53783976,\n",
      "        1.0666516 ,  0.86999465,  0.75380444,  0.66880423,  0.61663394,\n",
      "        0.54541975,  0.51516831,  0.46499201,  0.4189703 ,  0.37955294,\n",
      "        0.33597649,  0.31562973,  0.28985924,  0.2545082 ,  0.23767697,\n",
      "        0.20747294,  0.18665582,  0.19048655,  0.16304906,  0.1617702 ,\n",
      "        0.13098262,  0.13113121,  0.11870408,  0.13577073,  0.11074145,\n",
      "        0.10214826,  0.09287599,  0.10175369,  0.09181957,  0.09556216,\n",
      "        0.08819742,  0.09179666,  0.09373877,  0.07949993,  0.0785837 ,\n",
      "        0.07928753,  0.07409626,  0.07222705]), 'val_accs': array([0.45104478, 0.57641791, 0.59940299, 0.58268657, 0.54746269,\n",
      "       0.66746269, 0.65910448, 0.6838806 , 0.60119403, 0.68925373,\n",
      "       0.83134328, 0.84835821, 0.88686567, 0.87044776, 0.75432836,\n",
      "       0.85134328, 0.91014925, 0.89462687, 0.94328358, 0.93522388,\n",
      "       0.8880597 , 0.95940299, 0.96597015, 0.97253731, 0.97223881,\n",
      "       0.9758209 , 0.95074627, 0.94477612, 0.97492537, 0.9758209 ,\n",
      "       0.97731343, 0.96895522, 0.98238806, 0.97671642, 0.97820896,\n",
      "       0.96507463, 0.97850746, 0.97044776, 0.98089552, 0.96776119,\n",
      "       0.98059701, 0.96865672, 0.97164179]), 'val_losses': array([8.50127249, 1.80263113, 1.77686855, 1.61242689, 1.03051406,\n",
      "       0.72057772, 0.90338647, 0.74313362, 0.92273865, 0.6432361 ,\n",
      "       0.47185019, 0.36900403, 0.32067414, 0.33034417, 0.63790908,\n",
      "       0.3760452 , 0.23193018, 0.28809064, 0.17270144, 0.18012457,\n",
      "       0.3383236 , 0.12197959, 0.09744393, 0.08193203, 0.09258799,\n",
      "       0.06742197, 0.12886533, 0.14012423, 0.07278233, 0.06641714,\n",
      "       0.0705106 , 0.09817314, 0.04942148, 0.07527785, 0.06121531,\n",
      "       0.11177396, 0.05784591, 0.08884768, 0.05003752, 0.09039301,\n",
      "       0.06375819, 0.10082359, 0.07545685])}, 'ATTACH::23::best_val_loss': 0.04942148016459906, 'ATTACH::24::history': {'train_accs': array([0.40114934, 0.44645123, 0.45481006, 0.46242257, 0.44801851,\n",
      "       0.45152623, 0.44480931, 0.44771998, 0.44264497, 0.4431674 ,\n",
      "       0.43876409, 0.43965968, 0.44122696, 0.44204791]), 'train_losses': array([36.88536569,  1.26544302,  1.18124853,  1.16972456,  1.19112545,\n",
      "        1.18850559,  1.22249578,  1.26221509,  1.22105683,  1.2602883 ,\n",
      "        1.35979273,  1.52185822,  1.3051566 ,  1.31808997]), 'val_accs': array([0.44208955, 0.44208955, 0.44208955, 0.45313433, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955]), 'val_losses': array([1.24400251, 1.24089449, 1.24236377, 1.2144761 , 1.24380655,\n",
      "       1.24347814, 1.24377378, 1.24613248, 1.24382451, 1.24416514,\n",
      "       1.2274126 , 1.24291497, 1.24183473, 1.24202398])}, 'ATTACH::24::best_val_loss': 1.2144761046366905, 'ATTACH::25::history': {'train_accs': array([0.40010449, 0.43361445, 0.45592955, 0.46667662, 0.49384282,\n",
      "       0.52078513, 0.53414434, 0.57466975, 0.59877603, 0.63273379,\n",
      "       0.65482499, 0.67915516, 0.69079782, 0.70527651, 0.7174416 ,\n",
      "       0.72363609, 0.74542876, 0.77319203, 0.79625345, 0.81655347,\n",
      "       0.8311068 , 0.84707814, 0.85498918, 0.86573625, 0.88879767,\n",
      "       0.89603702, 0.91544145, 0.92178521, 0.9307411 , 0.94574222,\n",
      "       0.94887678, 0.95238451, 0.95335473, 0.96275841, 0.96619151,\n",
      "       0.96865438, 0.96753489, 0.97320696, 0.97417718, 0.97246063,\n",
      "       0.9781327 , 0.9808941 , 0.97843123, 0.98373013, 0.98193895,\n",
      "       0.98074483, 0.98671543, 0.98275991, 0.98731249, 0.98701396,\n",
      "       0.98947683, 0.98611837, 0.98686469, 0.98798418, 0.9891783 ,\n",
      "       0.99044705, 0.98813344, 0.99029778, 0.98940219, 0.9914919 ]), 'train_losses': array([22.69141254,  7.78376962,  4.83173281,  3.45790708,  2.42696806,\n",
      "        1.80122498,  1.51931519,  1.12837556,  0.96655009,  0.85648795,\n",
      "        0.79890941,  0.74902179,  0.71678027,  0.68512817,  0.65062242,\n",
      "        0.62648048,  0.58741192,  0.55555667,  0.52095837,  0.48325702,\n",
      "        0.44820961,  0.40305037,  0.38298965,  0.34932435,  0.30542007,\n",
      "        0.28596612,  0.23695368,  0.21925164,  0.19808123,  0.1593701 ,\n",
      "        0.15210937,  0.13926746,  0.14083241,  0.11223599,  0.1092402 ,\n",
      "        0.0966636 ,  0.09748003,  0.08434713,  0.07875075,  0.08142789,\n",
      "        0.0679668 ,  0.06106485,  0.06584173,  0.05094242,  0.05957587,\n",
      "        0.05765928,  0.04353241,  0.06245899,  0.0402087 ,  0.04466971,\n",
      "        0.03588157,  0.04260736,  0.03929773,  0.03924488,  0.03387225,\n",
      "        0.03287455,  0.03723175,  0.03060973,  0.03379355,  0.0263249 ]), 'val_accs': array([0.44208955, 0.59253731, 0.43223881, 0.60029851, 0.48238806,\n",
      "       0.47731343, 0.65940299, 0.59761194, 0.66865672, 0.67432836,\n",
      "       0.61223881, 0.66716418, 0.71313433, 0.80119403, 0.68      ,\n",
      "       0.67343284, 0.80985075, 0.75940299, 0.85283582, 0.84268657,\n",
      "       0.79074627, 0.84716418, 0.91701493, 0.8680597 , 0.9161194 ,\n",
      "       0.89492537, 0.92268657, 0.93253731, 0.95731343, 0.9480597 ,\n",
      "       0.95014925, 0.96089552, 0.89731343, 0.96447761, 0.96985075,\n",
      "       0.95223881, 0.97253731, 0.92298507, 0.97343284, 0.97731343,\n",
      "       0.96447761, 0.97641791, 0.96656716, 0.95970149, 0.97880597,\n",
      "       0.96835821, 0.98059701, 0.98358209, 0.97940299, 0.96895522,\n",
      "       0.9761194 , 0.9838806 , 0.98268657, 0.98179104, 0.97074627,\n",
      "       0.98507463, 0.96029851, 0.91910448, 0.98537313, 0.97731343]), 'val_losses': array([20.20641097,  1.65935855,  6.34607263,  1.94907599,  2.32828056,\n",
      "        1.46612157,  0.90520489,  1.05628224,  0.71299746,  0.69148553,\n",
      "        0.80820881,  0.7705392 ,  0.63510324,  0.52868872,  0.67180237,\n",
      "        0.76924898,  0.47179   ,  0.57889704,  0.40579299,  0.43693987,\n",
      "        0.47179633,  0.38339295,  0.23802843,  0.34118176,  0.23258046,\n",
      "        0.2904615 ,  0.22361346,  0.17831548,  0.13512301,  0.15094149,\n",
      "        0.15527587,  0.11208615,  0.31150306,  0.10513121,  0.08689773,\n",
      "        0.13643618,  0.07454132,  0.24905835,  0.07266023,  0.07949979,\n",
      "        0.10894671,  0.07238686,  0.12139359,  0.12686003,  0.06278083,\n",
      "        0.10692231,  0.05965435,  0.0585943 ,  0.07373142,  0.09519051,\n",
      "        0.07282194,  0.06011394,  0.05820413,  0.06078103,  0.10435141,\n",
      "        0.05305416,  0.1308889 ,  0.33364964,  0.05638488,  0.09585324])}, 'ATTACH::25::best_val_loss': 0.05305416186465256, 'ATTACH::26::history': {'train_accs': array([0.41585193, 0.48078215, 0.53324875, 0.587208  , 0.63594298,\n",
      "       0.67139339, 0.69975371, 0.73371147, 0.75147399, 0.78289425,\n",
      "       0.81229943, 0.84521233, 0.87387118, 0.88894694, 0.90454512,\n",
      "       0.91738189, 0.92454661, 0.93506978, 0.94290619, 0.95320546,\n",
      "       0.95014553, 0.95514591, 0.96574371, 0.96581834, 0.97119188,\n",
      "       0.97328159, 0.97365475, 0.97447571, 0.97649078, 0.97969998,\n",
      "       0.98052093, 0.98111799, 0.98164042, 0.97835659, 0.98425256,\n",
      "       0.98656616, 0.98290917, 0.98410329, 0.98813344, 0.9887305 ,\n",
      "       0.98447645, 0.98992462, 0.98962609, 0.99044705, 0.98649153,\n",
      "       0.98947683, 0.99134264, 0.99022315, 0.98932756, 0.98970072,\n",
      "       0.99052168, 0.99141727, 0.99156653, 0.98984999, 0.99082021]), 'train_losses': array([11.70728639,  2.72237347,  1.47264445,  1.03214427,  0.83194546,\n",
      "        0.73589349,  0.67767907,  0.62649653,  0.58596597,  0.51964472,\n",
      "        0.47016759,  0.41131164,  0.3578433 ,  0.32186369,  0.28780127,\n",
      "        0.25500769,  0.22942473,  0.19444229,  0.18054602,  0.14178717,\n",
      "        0.14690787,  0.14221455,  0.10681367,  0.10555115,  0.0854586 ,\n",
      "        0.08691249,  0.08320642,  0.07422361,  0.07606505,  0.06601164,\n",
      "        0.0629578 ,  0.06677597,  0.05497002,  0.06883575,  0.05070915,\n",
      "        0.04267094,  0.05145653,  0.05254726,  0.03970474,  0.03707693,\n",
      "        0.0506341 ,  0.03186107,  0.03602047,  0.03263293,  0.04517152,\n",
      "        0.03352347,  0.03038706,  0.03557728,  0.03419094,  0.03372218,\n",
      "        0.03087186,  0.02870517,  0.02861314,  0.03002213,  0.02783591]), 'val_accs': array([0.48626866, 0.5758209 , 0.65253731, 0.64447761, 0.7480597 ,\n",
      "       0.67910448, 0.8480597 , 0.75402985, 0.82656716, 0.71880597,\n",
      "       0.92149254, 0.82358209, 0.81552239, 0.86507463, 0.93104478,\n",
      "       0.96298507, 0.94507463, 0.96895522, 0.95283582, 0.98      ,\n",
      "       0.94955224, 0.97313433, 0.97343284, 0.96507463, 0.9841791 ,\n",
      "       0.98089552, 0.98238806, 0.9761194 , 0.97970149, 0.95910448,\n",
      "       0.97492537, 0.96447761, 0.97164179, 0.97044776, 0.98059701,\n",
      "       0.98716418, 0.97223881, 0.98328358, 0.97343284, 0.98567164,\n",
      "       0.9838806 , 0.98447761, 0.97462687, 0.98537313, 0.98537313,\n",
      "       0.84716418, 0.98656716, 0.96746269, 0.98268657, 0.98567164,\n",
      "       0.97850746, 0.98507463, 0.98686567, 0.98537313, 0.98268657]), 'val_losses': array([2.84355581, 1.14883017, 0.92348726, 0.82075318, 0.60284341,\n",
      "       0.67853886, 0.47974424, 0.58977234, 0.41086215, 0.62955526,\n",
      "       0.29214259, 0.38884752, 0.51166566, 0.33259931, 0.20016904,\n",
      "       0.13753152, 0.17087556, 0.10853828, 0.13847207, 0.08289269,\n",
      "       0.13100231, 0.09144554, 0.09165953, 0.0930853 , 0.05774118,\n",
      "       0.06135303, 0.06715835, 0.07567009, 0.0637345 , 0.13923466,\n",
      "       0.06973282, 0.11103542, 0.08558865, 0.08481345, 0.05451994,\n",
      "       0.04795304, 0.09540288, 0.05593015, 0.0798636 , 0.0497469 ,\n",
      "       0.05285462, 0.05103668, 0.08725417, 0.05338972, 0.04127694,\n",
      "       0.60174901, 0.05138284, 0.09794779, 0.06050571, 0.05565954,\n",
      "       0.07774917, 0.04838325, 0.04994413, 0.05412396, 0.06549648])}, 'ATTACH::26::best_val_loss': 0.04127694398387154, 'ATTACH::27::history': {'train_accs': array([0.42898724, 0.52892007, 0.60086574, 0.62646466, 0.64691395,\n",
      "       0.65684006, 0.6592283 , 0.66870662, 0.65273528, 0.65042167,\n",
      "       0.66990074, 0.68370774, 0.68624524, 0.69505187, 0.69699231,\n",
      "       0.68900664, 0.69915665, 0.71400851, 0.69960445, 0.69482797,\n",
      "       0.69997761]), 'train_losses': array([16.19326667,  1.25652286,  0.88171553,  0.81966182,  0.75721157,\n",
      "        0.73374335,  0.73786238,  0.71955452,  0.73840384,  0.76607723,\n",
      "        0.73453563,  0.68731686,  0.69008193,  0.67418734,  0.67142007,\n",
      "        0.68530411,  0.66908192,  0.63895049,  0.6614446 ,  0.6856246 ,\n",
      "        0.6791273 ]), 'val_accs': array([0.53074627, 0.54985075, 0.65761194, 0.61910448, 0.57552239,\n",
      "       0.68835821, 0.66179104, 0.69761194, 0.72029851, 0.72328358,\n",
      "       0.7958209 , 0.69014925, 0.64328358, 0.67940299, 0.59044776,\n",
      "       0.6       , 0.64328358, 0.70208955, 0.63850746, 0.67253731,\n",
      "       0.67492537]), 'val_losses': array([1.02593474, 1.08262073, 0.71839647, 0.80183939, 0.90678592,\n",
      "       0.63484557, 0.68681175, 0.69885112, 0.70142578, 0.66421838,\n",
      "       0.58261684, 0.69151319, 0.7090422 , 0.62888814, 0.94347551,\n",
      "       0.67379823, 0.77889484, 0.65910626, 0.73244099, 0.73112984,\n",
      "       0.75338331])}, 'ATTACH::27::best_val_loss': 0.5826168389818561, 'ATTACH::28::history': {'train_accs': array([0.41980745, 0.4571983 , 0.47272184, 0.4988432 , 0.52287484,\n",
      "       0.54265244, 0.57205762, 0.61332935, 0.63683857, 0.67818494,\n",
      "       0.69482797, 0.72445705, 0.74304053, 0.76565415, 0.79028286,\n",
      "       0.80826927, 0.8334204 , 0.85969102, 0.87319949, 0.88297634,\n",
      "       0.89693261, 0.90700799, 0.91730726, 0.92312859, 0.92909919,\n",
      "       0.93559221, 0.93992089, 0.94104038, 0.94992164, 0.95268304,\n",
      "       0.95208598, 0.95708635, 0.95663856, 0.96231062, 0.96305694,\n",
      "       0.96634077, 0.96813195, 0.96925144, 0.97126651, 0.97290843,\n",
      "       0.97096798, 0.97634152, 0.977461  , 0.97701321, 0.97858049,\n",
      "       0.97895365, 0.97775953, 0.97902829, 0.98193895, 0.97984924,\n",
      "       0.98216285, 0.98201358, 0.98171505, 0.98581984, 0.98440182,\n",
      "       0.98462572, 0.98440182, 0.98679006, 0.98716322, 0.98887977]), 'train_losses': array([17.64039182,  5.1806563 ,  3.4217362 ,  2.43965703,  1.88852384,\n",
      "        1.56165675,  1.33811995,  1.13069074,  1.03383104,  0.89050801,\n",
      "        0.82161545,  0.72949053,  0.67943326,  0.62257291,  0.56324219,\n",
      "        0.50855413,  0.46594298,  0.39556615,  0.36317676,  0.33248738,\n",
      "        0.30178638,  0.28664494,  0.24397871,  0.22971887,  0.21383323,\n",
      "        0.19129083,  0.18184832,  0.17866119,  0.15761917,  0.13947775,\n",
      "        0.15069779,  0.1289243 ,  0.12465255,  0.11194138,  0.11201769,\n",
      "        0.10252094,  0.09391438,  0.09581507,  0.09437515,  0.08330416,\n",
      "        0.08910555,  0.06824258,  0.07338159,  0.0715111 ,  0.06163936,\n",
      "        0.06141905,  0.06935347,  0.05819191,  0.0568435 ,  0.05545703,\n",
      "        0.05403562,  0.05583066,  0.05164792,  0.0418608 ,  0.04299919,\n",
      "        0.04832139,  0.04538195,  0.04273446,  0.04000433,  0.03584947]), 'val_accs': array([0.45850746, 0.56985075, 0.56746269, 0.57761194, 0.58208955,\n",
      "       0.63014925, 0.71910448, 0.64716418, 0.66328358, 0.75940299,\n",
      "       0.66865672, 0.68686567, 0.77432836, 0.83761194, 0.76179104,\n",
      "       0.82955224, 0.79731343, 0.87223881, 0.90149254, 0.90507463,\n",
      "       0.87402985, 0.84089552, 0.92567164, 0.91970149, 0.92716418,\n",
      "       0.93223881, 0.8919403 , 0.91940299, 0.96119403, 0.88746269,\n",
      "       0.95880597, 0.96268657, 0.9561194 , 0.95850746, 0.96358209,\n",
      "       0.94477612, 0.95910448, 0.96955224, 0.97253731, 0.9719403 ,\n",
      "       0.96567164, 0.96626866, 0.96208955, 0.97253731, 0.97671642,\n",
      "       0.97462687, 0.97462687, 0.97641791, 0.96865672, 0.96925373,\n",
      "       0.97671642, 0.97313433, 0.97522388, 0.97731343, 0.98179104,\n",
      "       0.9838806 , 0.98298507, 0.98268657, 0.96089552, 0.97432836]), 'val_losses': array([4.35666458, 1.53428175, 1.73913204, 1.71548324, 1.35230089,\n",
      "       0.87258189, 0.68560289, 1.05933287, 0.88189764, 0.6185644 ,\n",
      "       0.86707341, 0.76728504, 0.52935977, 0.45573179, 0.5858313 ,\n",
      "       0.44206522, 0.52267602, 0.37165605, 0.26793704, 0.27838891,\n",
      "       0.3526935 , 0.48373639, 0.20629271, 0.22108841, 0.2275825 ,\n",
      "       0.18960748, 0.28199166, 0.22542757, 0.11852707, 0.34488279,\n",
      "       0.1202384 , 0.12500093, 0.11770825, 0.12751132, 0.11856079,\n",
      "       0.16659598, 0.10738867, 0.1070389 , 0.08402554, 0.07725451,\n",
      "       0.11120883, 0.10552423, 0.11856798, 0.08682105, 0.07905258,\n",
      "       0.07611676, 0.08925629, 0.08217147, 0.10632395, 0.1049375 ,\n",
      "       0.07583942, 0.09971494, 0.08473937, 0.09340951, 0.0623573 ,\n",
      "       0.05869021, 0.06050728, 0.06438927, 0.1555706 , 0.11115066])}, 'ATTACH::28::best_val_loss': 0.05869021044571453, 'ATTACH::29::history': {'train_accs': array([0.42391223, 0.48943951, 0.54653332, 0.60840361, 0.64415255,\n",
      "       0.68609598, 0.70378386, 0.72430778, 0.73789089, 0.74132398,\n",
      "       0.75998209, 0.76729607, 0.77356519, 0.7839391 , 0.79789537,\n",
      "       0.78796925, 0.78744682, 0.79602955, 0.7975222 , 0.80677663,\n",
      "       0.81035898, 0.80871707, 0.80819464, 0.8170759 , 0.81655347,\n",
      "       0.82274797]), 'train_losses': array([14.36948111,  3.07287679,  1.37728237,  0.91595027,  0.80475871,\n",
      "        0.695326  ,  0.66027096,  0.62140574,  0.58047549,  0.58260394,\n",
      "        0.53226179,  0.52237964,  0.52403703,  0.48950973,  0.46184501,\n",
      "        0.47958335,  0.47921552,  0.46492879,  0.47236603,  0.44377995,\n",
      "        0.4371576 ,  0.43826698,  0.44015289,  0.41732719,  0.42221254,\n",
      "        0.40226304]), 'val_accs': array([0.5758209 , 0.62358209, 0.70656716, 0.59731343, 0.74656716,\n",
      "       0.74776119, 0.77462687, 0.81373134, 0.80328358, 0.76686567,\n",
      "       0.78776119, 0.75164179, 0.80119403, 0.78597015, 0.76567164,\n",
      "       0.82925373, 0.84179104, 0.81313433, 0.72537313, 0.81731343,\n",
      "       0.82119403, 0.71313433, 0.8038806 , 0.84865672, 0.81761194,\n",
      "       0.78149254]), 'val_losses': array([2.77056058, 1.51611729, 0.79079227, 0.93507916, 0.59212275,\n",
      "       0.54618033, 0.52887832, 0.49329667, 0.49100644, 0.56598239,\n",
      "       0.47896341, 0.57518975, 0.49000811, 0.5270943 , 0.4641333 ,\n",
      "       0.42488505, 0.50174726, 0.4808155 , 0.50517684, 0.45228213,\n",
      "       0.52932266, 0.53085472, 0.50922987, 0.47251807, 0.43939292,\n",
      "       0.57505615])}, 'ATTACH::29::best_val_loss': 0.42488505342113436}\n",
      "{'ATTACH::0::history': {'train_accs': array([0.44182402, 0.44988432, 0.45063064, 0.45063064, 0.45085454,\n",
      "       0.45092917, 0.45092917, 0.45152623, 0.4512277 , 0.45100381,\n",
      "       0.4512277 , 0.45100381, 0.45115307, 0.45152623, 0.4518994 ,\n",
      "       0.4512277 , 0.45077991, 0.4512277 , 0.4512277 , 0.45077991,\n",
      "       0.4516755 , 0.45025748, 0.45115307, 0.45077991, 0.4512277 ,\n",
      "       0.45100381, 0.4514516 , 0.45063064, 0.45092917, 0.45197403,\n",
      "       0.45085454, 0.45137697, 0.45152623, 0.45092917, 0.45085454,\n",
      "       0.45115307, 0.45160087, 0.4512277 , 0.45100381, 0.45107844,\n",
      "       0.45070528, 0.45100381]), 'train_losses': array([6.08171224, 1.29879437, 1.27806261, 1.26327058, 1.2522055 ,\n",
      "       1.24494688, 1.23889016, 1.23342462, 1.23025172, 1.22766332,\n",
      "       1.22511749, 1.2235858 , 1.22125747, 1.2195907 , 1.21796504,\n",
      "       1.21816524, 1.21880958, 1.21670664, 1.21657833, 1.21715984,\n",
      "       1.21476696, 1.21798147, 1.21516896, 1.21612392, 1.21516566,\n",
      "       1.21505025, 1.21375312, 1.21598271, 1.21479655, 1.21205646,\n",
      "       1.2146663 , 1.21327362, 1.21613817, 1.21699188, 1.21691357,\n",
      "       1.21593496, 1.21436607, 1.21560793, 1.21603448, 1.21569388,\n",
      "       1.2163495 , 1.2156575 ]), 'val_accs': array([0.45313433, 0.45313433, 0.45313433, 0.45313433, 0.45313433,\n",
      "       0.45313433, 0.45313433, 0.45313433, 0.45313433, 0.45313433,\n",
      "       0.45313433, 0.45313433, 0.45313433, 0.45313433, 0.45313433,\n",
      "       0.45313433, 0.45313433, 0.45313433, 0.45313433, 0.45313433,\n",
      "       0.45313433, 0.45313433, 0.45313433, 0.45313433, 0.45313433,\n",
      "       0.45313433, 0.45313433, 0.45313433, 0.45313433, 0.45313433,\n",
      "       0.45313433, 0.45313433, 0.45313433, 0.45313433, 0.45313433,\n",
      "       0.45313433, 0.45313433, 0.45313433, 0.45313433, 0.45313433,\n",
      "       0.45313433, 0.45313433]), 'val_losses': array([1.30761684, 1.28478402, 1.26686741, 1.25405815, 1.24418758,\n",
      "       1.23678355, 1.23146293, 1.22711211, 1.22368495, 1.22093599,\n",
      "       1.21889569, 1.21709308, 1.21566063, 1.21459468, 1.21349574,\n",
      "       1.21264687, 1.21197998, 1.21129817, 1.21086978, 1.21050563,\n",
      "       1.20996593, 1.20976209, 1.20960141, 1.20920533, 1.20911776,\n",
      "       1.20903718, 1.20863923, 1.20851297, 1.20827887, 1.20809491,\n",
      "       1.20825462, 1.20804294, 1.2108949 , 1.21061157, 1.21052219,\n",
      "       1.21010561, 1.2100435 , 1.21004457, 1.20978203, 1.20939635,\n",
      "       1.20925413, 1.20941642])}, 'ATTACH::0::best_val_loss': 1.2080429421609906, 'ATTACH::1::history': {'train_accs': array([0.39458169, 0.40279125, 0.45010822, 0.50473916, 0.50347041,\n",
      "       0.51488917, 0.51250093, 0.52914397, 0.50817225, 0.50533622,\n",
      "       0.4903351 , 0.50444063, 0.49272334, 0.4733189 , 0.47637883,\n",
      "       0.47175162, 0.46712441, 0.47817001, 0.48787223, 0.476752  ,\n",
      "       0.4845884 , 0.46264647]), 'train_losses': array([12.61266023,  1.21952962,  1.02174163,  0.95322152,  0.96272424,\n",
      "        0.93696217,  0.9459933 ,  0.90918158,  0.93619957,  0.95801956,\n",
      "        0.93078696,  1.07114761,  1.0516978 ,  1.04721672,  1.07644054,\n",
      "        1.12410947,  1.12973037,  1.10645162,  1.08898678,  1.13819625,\n",
      "        1.11466289,  1.12870266]), 'val_accs': array([0.46746269, 0.45283582, 0.53223881, 0.53761194, 0.55731343,\n",
      "       0.40955224, 0.5319403 , 0.56985075, 0.57940299, 0.54208955,\n",
      "       0.25283582, 0.62447761, 0.50119403, 0.44059701, 0.47432836,\n",
      "       0.47164179, 0.38447761, 0.51373134, 0.50358209, 0.48567164,\n",
      "       0.51791045, 0.46746269]), 'val_losses': array([1.01321942, 1.09957988, 0.88665279, 0.83835989, 0.91690115,\n",
      "       1.05351301, 0.86728017, 0.82175759, 0.81248006, 0.94097227,\n",
      "       3.03232862, 0.80511938, 1.06334691, 0.9845615 , 1.11750857,\n",
      "       1.11854853, 1.10926371, 1.01528256, 0.94586071, 1.0046117 ,\n",
      "       0.96993407, 1.06848896])}, 'ATTACH::1::best_val_loss': 0.8051193756843681, 'ATTACH::2::history': {'train_accs': array([0.53623405, 0.62213598, 0.60534368, 0.59153668, 0.58086424,\n",
      "       0.5759385 , 0.58093888, 0.58429734, 0.58937234, 0.57772968,\n",
      "       0.57728189]), 'train_losses': array([5.32018238, 0.83157002, 0.87606847, 0.87060509, 0.88449118,\n",
      "       0.87056805, 0.86439421, 0.86369658, 0.85994925, 0.86391827,\n",
      "       0.85684181]), 'val_accs': array([0.62      , 0.66      , 0.53373134, 0.60686567, 0.65820896,\n",
      "       0.61074627, 0.55910448, 0.5758209 , 0.64298507, 0.61074627,\n",
      "       0.61492537]), 'val_losses': array([0.77362019, 0.82042148, 0.90490568, 0.86529389, 0.77861365,\n",
      "       0.80022329, 0.82214547, 0.823844  , 0.79588231, 0.79304797,\n",
      "       0.78668201])}, 'ATTACH::2::best_val_loss': 0.773620186563748, 'ATTACH::3::history': {'train_accs': array([0.33129338, 0.3819688 , 0.40495559, 0.42003135, 0.43130084,\n",
      "       0.44234644, 0.46107919, 0.47503545, 0.4875737 , 0.51294873,\n",
      "       0.52765132, 0.5428017 , 0.55198149, 0.57422196, 0.5848944 ,\n",
      "       0.6042242 , 0.61795656, 0.63168893, 0.64482424, 0.6647511 ,\n",
      "       0.67139339, 0.69482797, 0.70236585, 0.71370998, 0.73468169,\n",
      "       0.74147324, 0.75602657, 0.76453467, 0.77438615, 0.78438689,\n",
      "       0.78513322, 0.79729831, 0.79670125, 0.8009553 , 0.81065751,\n",
      "       0.81013508, 0.82073289, 0.82274797, 0.82924099, 0.83491305,\n",
      "       0.83976416, 0.84065975, 0.85192925, 0.84834689, 0.85789984,\n",
      "       0.86267632, 0.8612583 , 0.8667811 , 0.86879618, 0.87685648,\n",
      "       0.88409583, 0.87737891, 0.88267781, 0.88790208, 0.889544  ,\n",
      "       0.89648481, 0.89663408, 0.8999179 , 0.903351  , 0.90491828]), 'train_losses': array([51.80927281, 18.31438876, 11.17556569,  8.72902045,  6.84260687,\n",
      "        5.76816121,  4.69397053,  4.17314317,  3.59303688,  3.08196229,\n",
      "        2.73790593,  2.45876304,  2.20455698,  2.06714035,  1.9146805 ,\n",
      "        1.69729981,  1.59790249,  1.45625894,  1.36767277,  1.19993111,\n",
      "        1.17326344,  1.09465396,  1.02204067,  0.93309187,  0.87313176,\n",
      "        0.82385653,  0.7789245 ,  0.74397445,  0.69904665,  0.67730912,\n",
      "        0.6499568 ,  0.60948897,  0.60747662,  0.58626834,  0.5516737 ,\n",
      "        0.54779193,  0.51421178,  0.50361718,  0.49037451,  0.47283065,\n",
      "        0.46454109,  0.45291542,  0.42273868,  0.42205631,  0.4091074 ,\n",
      "        0.39495391,  0.39044578,  0.3821457 ,  0.36755654,  0.35625207,\n",
      "        0.32677381,  0.35148445,  0.33480534,  0.3247949 ,  0.31833481,\n",
      "        0.29994643,  0.29639739,  0.29294258,  0.27920421,  0.28064845]), 'val_accs': array([0.4561194 , 0.48716418, 0.45761194, 0.4119403 , 0.47164179,\n",
      "       0.50597015, 0.54865672, 0.59223881, 0.61701493, 0.66686567,\n",
      "       0.69701493, 0.66119403, 0.65223881, 0.67970149, 0.72537313,\n",
      "       0.72298507, 0.72626866, 0.73462687, 0.75373134, 0.71223881,\n",
      "       0.75641791, 0.76477612, 0.79373134, 0.79880597, 0.77671642,\n",
      "       0.79850746, 0.79223881, 0.82059701, 0.82865672, 0.83820896,\n",
      "       0.79164179, 0.80776119, 0.8080597 , 0.84149254, 0.85253731,\n",
      "       0.83701493, 0.85761194, 0.81761194, 0.84925373, 0.81970149,\n",
      "       0.85731343, 0.85164179, 0.87402985, 0.8519403 , 0.84447761,\n",
      "       0.84597015, 0.85731343, 0.88985075, 0.89253731, 0.86955224,\n",
      "       0.87970149, 0.87432836, 0.90089552, 0.82626866, 0.88955224,\n",
      "       0.90447761, 0.89970149, 0.92268657, 0.88507463, 0.88567164]), 'val_losses': array([6.73026137, 3.76216596, 3.35665474, 2.60433682, 2.0496926 ,\n",
      "       1.492337  , 1.31429462, 1.34016566, 1.18281377, 1.08364199,\n",
      "       0.92152267, 0.9673752 , 0.88368324, 0.97355091, 0.75229288,\n",
      "       0.78590267, 0.72488789, 0.73388453, 0.64393159, 0.78784828,\n",
      "       0.71833233, 0.65172652, 0.57516624, 0.5761842 , 0.60766869,\n",
      "       0.56616456, 0.53340485, 0.45160217, 0.49139209, 0.46420709,\n",
      "       0.56550497, 0.52200806, 0.46611145, 0.4066216 , 0.39489977,\n",
      "       0.39348562, 0.3653058 , 0.46305457, 0.40719022, 0.47240877,\n",
      "       0.38862303, 0.3866601 , 0.3196222 , 0.38397135, 0.35277812,\n",
      "       0.36748034, 0.39138433, 0.30480177, 0.27894392, 0.32410599,\n",
      "       0.27529264, 0.31907848, 0.25010095, 0.44892056, 0.27778259,\n",
      "       0.26395588, 0.26246575, 0.21852296, 0.27448382, 0.30971772])}, 'ATTACH::3::best_val_loss': 0.21852295594428903, 'ATTACH::4::history': {'train_accs': array([0.4268229 , 0.53183073, 0.58623778, 0.62303157, 0.71609822,\n",
      "       0.73915964, 0.76856482, 0.7977461 , 0.81006045, 0.83073364,\n",
      "       0.82118069, 0.84125681, 0.84327189, 0.84730204, 0.85909396,\n",
      "       0.86946787, 0.87066199, 0.86737816, 0.88827524, 0.88364803,\n",
      "       0.88618554, 0.87864766, 0.88917083, 0.89215613, 0.88991716,\n",
      "       0.8948429 , 0.87752817, 0.89357415, 0.86857228, 0.88223002,\n",
      "       0.88648407, 0.89820136, 0.89252929, 0.88603627]), 'train_losses': array([9.41384656, 1.57933976, 0.97884279, 0.9046964 , 0.6444611 ,\n",
      "       0.62112284, 0.5642352 , 0.49269286, 0.47526202, 0.4262407 ,\n",
      "       0.44435488, 0.40143779, 0.40151769, 0.39503928, 0.36491244,\n",
      "       0.34385906, 0.33834748, 0.34241715, 0.29352087, 0.30435555,\n",
      "       0.29647091, 0.32280673, 0.29625741, 0.28538285, 0.28015108,\n",
      "       0.27954177, 0.31850885, 0.28668423, 0.34095349, 0.31182573,\n",
      "       0.29889361, 0.26512215, 0.28404983, 0.29577345]), 'val_accs': array([0.52626866, 0.62985075, 0.69910448, 0.64208955, 0.75820896,\n",
      "       0.78      , 0.81731343, 0.8361194 , 0.82149254, 0.85313433,\n",
      "       0.84656716, 0.8558209 , 0.83104478, 0.88447761, 0.86208955,\n",
      "       0.87044776, 0.88567164, 0.87462687, 0.88955224, 0.90597015,\n",
      "       0.89343284, 0.89522388, 0.89253731, 0.9241791 , 0.90238806,\n",
      "       0.85402985, 0.9161194 , 0.92119403, 0.86447761, 0.85104478,\n",
      "       0.88865672, 0.88238806, 0.87761194, 0.92507463]), 'val_losses': array([2.12625755, 0.85353248, 0.67884   , 0.70772928, 0.52604261,\n",
      "       0.5159394 , 0.45191069, 0.38037414, 0.42483362, 0.36204621,\n",
      "       0.35452201, 0.34040066, 0.39450117, 0.29534174, 0.32966696,\n",
      "       0.31035887, 0.28464485, 0.30368043, 0.27968536, 0.24251701,\n",
      "       0.27834706, 0.26352345, 0.27029778, 0.19863615, 0.2569771 ,\n",
      "       0.3868023 , 0.21425698, 0.20860315, 0.33306473, 0.34921801,\n",
      "       0.27872957, 0.32225173, 0.33411186, 0.21523505])}, 'ATTACH::4::best_val_loss': 0.19863614575186772, 'ATTACH::5::history': {'train_accs': array([0.49227554, 0.61012016, 0.60273155, 0.614822  , 0.56989328,\n",
      "       0.56272856, 0.5623554 , 0.55563848, 0.55175759, 0.56407195,\n",
      "       0.5515337 ]), 'train_losses': array([8.94969703, 0.8162483 , 0.81434996, 0.81100819, 0.88810575,\n",
      "       0.8947233 , 0.88047653, 0.92572597, 0.88880103, 0.89539189,\n",
      "       0.91292775]), 'val_accs': array([0.61074627, 0.59104478, 0.59343284, 0.67044776, 0.45970149,\n",
      "       0.56268657, 0.50298507, 0.53940299, 0.45074627, 0.60985075,\n",
      "       0.47850746]), 'val_losses': array([0.72826445, 1.17137953, 0.97897608, 0.74431986, 1.71826236,\n",
      "       0.8949149 , 0.99682682, 0.90358139, 1.04253227, 0.77267998,\n",
      "       1.00647275])}, 'ATTACH::5::best_val_loss': 0.7282644549412514, 'ATTACH::6::history': {'train_accs': array([0.42219569, 0.50481379, 0.55496679, 0.60900067, 0.65206359,\n",
      "       0.68460333, 0.71012762, 0.7259497 , 0.74408538, 0.76057915,\n",
      "       0.75520561, 0.76020599, 0.77192328, 0.7756549 , 0.7728935 ,\n",
      "       0.7756549 , 0.7786402 , 0.78423763, 0.78953653, 0.7786402 ,\n",
      "       0.78476006, 0.79595492, 0.79625345, 0.80267184, 0.79677588,\n",
      "       0.80005971, 0.80782148, 0.80408986, 0.79080528, 0.80200015,\n",
      "       0.79789537, 0.80386596, 0.8087917 , 0.80983655, 0.79856706,\n",
      "       0.80013434, 0.79916412, 0.80834391, 0.79797   , 0.81341891,\n",
      "       0.80476155, 0.81543399, 0.81662811, 0.81513546, 0.80826927,\n",
      "       0.81282185, 0.82110605, 0.8170759 , 0.82230017, 0.80102993,\n",
      "       0.79550713, 0.79543249]), 'train_losses': array([8.5023586 , 1.29631003, 0.98797353, 0.82089215, 0.72941895,\n",
      "       0.66908302, 0.63066512, 0.59796659, 0.57767975, 0.54386442,\n",
      "       0.54586761, 0.5393496 , 0.53438984, 0.51667131, 0.5124769 ,\n",
      "       0.53438984, 0.51363432, 0.49880871, 0.48222823, 0.52274037,\n",
      "       0.49190095, 0.48072829, 0.4696684 , 0.465781  , 0.47485563,\n",
      "       0.4497601 , 0.44604372, 0.46619707, 0.48258738, 0.46779134,\n",
      "       0.46661957, 0.4576555 , 0.44978481, 0.45275441, 0.48145378,\n",
      "       0.45680104, 0.47765009, 0.44886914, 0.46708572, 0.4412984 ,\n",
      "       0.47458956, 0.44396256, 0.4373625 , 0.43487627, 0.46172037,\n",
      "       0.44731684, 0.43780208, 0.43967203, 0.42697321, 0.47271544,\n",
      "       0.46866684, 0.47285248]), 'val_accs': array([0.63462687, 0.6238806 , 0.57641791, 0.68268657, 0.53820896,\n",
      "       0.6761194 , 0.61761194, 0.6680597 , 0.7319403 , 0.80895522,\n",
      "       0.76477612, 0.76208955, 0.73104478, 0.75910448, 0.80746269,\n",
      "       0.72656716, 0.76985075, 0.76089552, 0.62985075, 0.70567164,\n",
      "       0.7958209 , 0.78029851, 0.78447761, 0.77850746, 0.74955224,\n",
      "       0.73164179, 0.80298507, 0.81432836, 0.74238806, 0.70895522,\n",
      "       0.82029851, 0.69134328, 0.83164179, 0.78149254, 0.79432836,\n",
      "       0.65970149, 0.8441791 , 0.76298507, 0.81910448, 0.52447761,\n",
      "       0.85134328, 0.8438806 , 0.84179104, 0.79104478, 0.75761194,\n",
      "       0.72089552, 0.81253731, 0.73850746, 0.69223881, 0.82597015,\n",
      "       0.79462687, 0.7958209 ]), 'val_losses': array([0.85795809, 0.86205516, 1.01177352, 0.70562198, 0.82991572,\n",
      "       0.70002195, 0.82981874, 0.69060532, 0.65870577, 0.49212195,\n",
      "       0.60785956, 0.58823306, 0.60327175, 0.56353045, 0.48410884,\n",
      "       0.53964197, 0.52757651, 0.59324294, 0.91860259, 0.66764322,\n",
      "       0.43732721, 0.53726812, 0.53225554, 0.54819898, 0.66987214,\n",
      "       0.51508775, 0.48790319, 0.44793815, 0.64352775, 0.69032067,\n",
      "       0.42474198, 0.8644923 , 0.42672244, 0.59677297, 0.50462954,\n",
      "       0.85975283, 0.39221788, 0.50804187, 0.41513331, 1.41008614,\n",
      "       0.39078663, 0.36004783, 0.38561903, 0.48130628, 0.52625624,\n",
      "       0.56463113, 0.51765406, 0.57988857, 0.64601694, 0.40192304,\n",
      "       0.56309309, 0.49900621])}, 'ATTACH::6::best_val_loss': 0.3600478328163944, 'ATTACH::7::history': {'train_accs': array([0.49720128, 0.51332189, 0.46481081, 0.45346668, 0.45615344,\n",
      "       0.4463766 , 0.44585417, 0.44249571, 0.44122696, 0.44227181,\n",
      "       0.4406299 ]), 'train_losses': array([8.10445483, 0.99985926, 1.15443217, 1.18964984, 1.18729197,\n",
      "       1.21458939, 1.21647822, 1.26392311, 1.25216213, 1.2327853 ,\n",
      "       1.21901206]), 'val_accs': array([0.47343284, 0.4280597 , 0.5038806 , 0.46      , 0.46      ,\n",
      "       0.44895522, 0.44895522, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44238806]), 'val_losses': array([1.00149792, 1.17424776, 1.82119798, 1.17178603, 1.17224332,\n",
      "       1.21034431, 1.2037226 , 1.25551439, 1.22009605, 1.2173696 ,\n",
      "       1.22121515])}, 'ATTACH::7::best_val_loss': 1.0014979208761188, 'ATTACH::8::history': {'train_accs': array([0.44466005, 0.49675349, 0.54354803, 0.57817748, 0.62064333,\n",
      "       0.6401224 , 0.66945294, 0.68721546, 0.69423091, 0.70460482,\n",
      "       0.70900813, 0.70908277, 0.71856109, 0.71960594, 0.73348757,\n",
      "       0.7450556 , 0.74789163, 0.7531159 , 0.74960818, 0.75259348,\n",
      "       0.7508023 , 0.75595194, 0.76378834, 0.77550563, 0.77558027,\n",
      "       0.76513173, 0.77199791, 0.77423688]), 'train_losses': array([15.37058269,  2.82484769,  1.63253589,  1.16919792,  0.90683601,\n",
      "        0.82629483,  0.73622969,  0.70360744,  0.68337375,  0.66051398,\n",
      "        0.66791299,  0.65083545,  0.63585898,  0.61539407,  0.60557618,\n",
      "        0.58564017,  0.56513153,  0.54959442,  0.56414549,  0.55043793,\n",
      "        0.57605583,  0.56348427,  0.54394976,  0.51615898,  0.52132877,\n",
      "        0.53389174,  0.53243312,  0.52492599]), 'val_accs': array([0.52059701, 0.55134328, 0.63343284, 0.65343284, 0.74567164,\n",
      "       0.65970149, 0.78059701, 0.68507463, 0.77850746, 0.65253731,\n",
      "       0.70179104, 0.76119403, 0.74298507, 0.76507463, 0.78477612,\n",
      "       0.76776119, 0.82865672, 0.83492537, 0.81223881, 0.82238806,\n",
      "       0.77940299, 0.77641791, 0.84119403, 0.82716418, 0.77074627,\n",
      "       0.83044776, 0.78      , 0.70865672]), 'val_losses': array([2.5034637 , 2.17481186, 0.97862855, 0.82867582, 0.6116119 ,\n",
      "       0.71551521, 0.54384324, 0.60313033, 0.57536438, 0.6916676 ,\n",
      "       0.58909252, 0.57665294, 0.52421605, 0.54297753, 0.50093223,\n",
      "       0.51777944, 0.43825722, 0.4128302 , 0.47231807, 0.43004774,\n",
      "       0.52467639, 0.5123682 , 0.44626395, 0.4685588 , 0.49852166,\n",
      "       0.42311011, 0.4597674 , 0.56493005])}, 'ATTACH::8::best_val_loss': 0.4128301963165625, 'ATTACH::9::history': {'train_accs': array([0.38614822, 0.4318979 , 0.44324203, 0.46615419, 0.47742369,\n",
      "       0.48234943, 0.48943951, 0.47540861, 0.48667811, 0.49294723,\n",
      "       0.50197776, 0.50361967, 0.49555937, 0.50384357, 0.50108217,\n",
      "       0.49690275, 0.50332114, 0.50421673, 0.50585865, 0.50167923,\n",
      "       0.49593253, 0.49973879, 0.50996343, 0.50675424, 0.50645571,\n",
      "       0.51548623, 0.50929174, 0.50802299, 0.51018733, 0.49973879,\n",
      "       0.51630719, 0.48302112, 0.4486902 , 0.44563027, 0.44600343,\n",
      "       0.4457049 , 0.4459288 , 0.4457049 , 0.44555564, 0.44563027]), 'train_losses': array([8.32543664, 1.11004186, 1.07535431, 1.06929098, 1.05037377,\n",
      "       1.03181427, 1.02771312, 1.0320304 , 1.03196945, 1.03253244,\n",
      "       1.00627524, 1.00226941, 1.01389854, 1.00837396, 0.99362362,\n",
      "       0.99650582, 1.00883591, 0.99599693, 0.98184301, 0.9871668 ,\n",
      "       1.01478317, 0.99512375, 0.97299159, 0.97803272, 0.97289621,\n",
      "       0.96290486, 0.98182804, 0.96628921, 0.96733413, 0.9881693 ,\n",
      "       0.94798806, 1.06517234, 1.27412461, 1.2433517 , 1.24203937,\n",
      "       1.24036276, 1.2388853 , 1.23806841, 1.23700785, 1.23604885]), 'val_accs': array([0.41850746, 0.53313433, 0.53343284, 0.52537313, 0.53373134,\n",
      "       0.55761194, 0.54179104, 0.50865672, 0.56059701, 0.52029851,\n",
      "       0.53880597, 0.53462687, 0.55402985, 0.44746269, 0.52      ,\n",
      "       0.60328358, 0.56686567, 0.56      , 0.54477612, 0.46716418,\n",
      "       0.56895522, 0.55522388, 0.54597015, 0.57492537, 0.57731343,\n",
      "       0.59253731, 0.58567164, 0.52865672, 0.57104478, 0.59462687,\n",
      "       0.57373134, 0.48567164, 0.44895522, 0.44895522, 0.44895522,\n",
      "       0.44895522, 0.44895522, 0.44895522, 0.44895522, 0.44895522]), 'val_losses': array([1.09652108, 0.9449825 , 0.93375089, 0.93322296, 0.9471597 ,\n",
      "       0.90469273, 0.91526802, 0.8925001 , 0.87098826, 0.85579064,\n",
      "       0.86077747, 0.84607647, 0.85940293, 1.00101385, 0.94675388,\n",
      "       0.86268425, 0.8402717 , 0.85100147, 0.82402711, 0.98061435,\n",
      "       0.86110096, 0.85649219, 0.81222786, 0.83534145, 0.80357802,\n",
      "       0.81946947, 0.80382191, 0.85217029, 0.81427207, 0.79676845,\n",
      "       0.80487112, 1.17760206, 1.23975412, 1.23614277, 1.23444843,\n",
      "       1.23319994, 1.23100853, 1.23085793, 1.22838475, 1.22869776])}, 'ATTACH::9::best_val_loss': 0.7967684486019078, 'ATTACH::10::history': {'train_accs': array([0.34965296, 0.39905963, 0.41391149, 0.43682364, 0.44674976,\n",
      "       0.46458691, 0.46563176, 0.48921561, 0.51653108, 0.54220464,\n",
      "       0.56795283, 0.60392567, 0.63042018, 0.65504888, 0.69124562,\n",
      "       0.71385924, 0.74789163, 0.76348981, 0.78744682, 0.81006045,\n",
      "       0.83282335, 0.85566087, 0.8695425 , 0.87902082, 0.89133517,\n",
      "       0.90275394, 0.9088738 , 0.91775506, 0.92469587, 0.93253228,\n",
      "       0.93827898, 0.94402567, 0.94410031, 0.9500709 , 0.95148892,\n",
      "       0.95141428, 0.95962385, 0.96253452, 0.95925069, 0.96768416,\n",
      "       0.96865438, 0.97231137, 0.97380402, 0.97410254, 0.96895291,\n",
      "       0.97850586, 0.98171505, 0.98059557, 0.97664005, 0.97887902,\n",
      "       0.97887902, 0.97723711, 0.98350623, 0.98649153, 0.98522278,\n",
      "       0.97895365, 0.98141652, 0.98552131, 0.98373013, 0.98761102]), 'train_losses': array([29.86807868, 11.08416354,  7.21532016,  5.17589231,  4.10147476,\n",
      "        3.38376031,  2.9891762 ,  2.59636012,  2.24342297,  2.0293681 ,\n",
      "        1.81245176,  1.58195745,  1.419021  ,  1.26673852,  1.13257145,\n",
      "        1.00522319,  0.86808533,  0.80109438,  0.68988462,  0.63561824,\n",
      "        0.54807133,  0.47793202,  0.42663549,  0.39938737,  0.35506229,\n",
      "        0.3150661 ,  0.29306683,  0.27727297,  0.24471121,  0.21709528,\n",
      "        0.19659048,  0.17795102,  0.17812812,  0.16138262,  0.15396437,\n",
      "        0.15160083,  0.12633264,  0.12087798,  0.13388445,  0.10262034,\n",
      "        0.10215017,  0.08657799,  0.08386933,  0.08201724,  0.10168251,\n",
      "        0.06803596,  0.05657712,  0.0629451 ,  0.08344051,  0.07457554,\n",
      "        0.06773894,  0.07525157,  0.05296004,  0.04379349,  0.04700871,\n",
      "        0.06607081,  0.06411845,  0.04407669,  0.05541482,  0.03837582]), 'val_accs': array([0.42119403, 0.40477612, 0.51731343, 0.55253731, 0.57820896,\n",
      "       0.57104478, 0.61343284, 0.58955224, 0.56955224, 0.59850746,\n",
      "       0.68328358, 0.68835821, 0.73641791, 0.7119403 , 0.78208955,\n",
      "       0.82626866, 0.82746269, 0.83761194, 0.81134328, 0.8841791 ,\n",
      "       0.83134328, 0.88477612, 0.88776119, 0.92656716, 0.92507463,\n",
      "       0.92      , 0.91283582, 0.92268657, 0.93820896, 0.94089552,\n",
      "       0.9119403 , 0.95731343, 0.95313433, 0.94597015, 0.94776119,\n",
      "       0.96358209, 0.95791045, 0.95671642, 0.96597015, 0.95731343,\n",
      "       0.96776119, 0.96328358, 0.96626866, 0.96328358, 0.96358209,\n",
      "       0.95970149, 0.9719403 , 0.96537313, 0.95970149, 0.96358209,\n",
      "       0.96626866, 0.97074627, 0.96059701, 0.97462687, 0.96955224,\n",
      "       0.97104478, 0.96656716, 0.9680597 , 0.97164179, 0.97880597]), 'val_losses': array([7.42045669, 2.75032639, 2.19324369, 1.54821489, 1.45633821,\n",
      "       1.33034754, 1.03026091, 1.25011555, 1.05191589, 1.39578144,\n",
      "       0.96206504, 0.94675562, 0.8896308 , 0.75545623, 0.69964843,\n",
      "       0.5669685 , 0.53073591, 0.49431365, 0.61299969, 0.37093695,\n",
      "       0.50823586, 0.37545725, 0.31542701, 0.25169015, 0.24674777,\n",
      "       0.27672926, 0.33946908, 0.27394001, 0.20892444, 0.20364885,\n",
      "       0.30530596, 0.16312105, 0.16806236, 0.18649687, 0.16956146,\n",
      "       0.13656839, 0.14185596, 0.15122069, 0.12594666, 0.14914405,\n",
      "       0.12013004, 0.10976192, 0.1142604 , 0.12708171, 0.13555362,\n",
      "       0.14500313, 0.1002066 , 0.13345571, 0.15384189, 0.1368872 ,\n",
      "       0.13386607, 0.12352769, 0.16684148, 0.09211246, 0.12932668,\n",
      "       0.11308041, 0.13318616, 0.11901545, 0.09904277, 0.08822748])}, 'ATTACH::10::best_val_loss': 0.08822747526884969, 'ATTACH::11::history': {'train_accs': array([0.30979924, 0.33323382, 0.33166654, 0.3327114 , 0.34069707,\n",
      "       0.33786103, 0.3380103 , 0.3462945 , 0.35107098, 0.36017613,\n",
      "       0.37189342, 0.37749086, 0.38249123, 0.4012986 , 0.40771699,\n",
      "       0.42249422, 0.42615121, 0.43480857, 0.4457049 , 0.44257034,\n",
      "       0.45488469, 0.45854168, 0.47018434, 0.47705053, 0.4790656 ,\n",
      "       0.49705202, 0.49115606, 0.49630569, 0.49638033, 0.51414285,\n",
      "       0.51294873, 0.51571013, 0.52488992, 0.52123293, 0.52429286,\n",
      "       0.53347265, 0.53601015, 0.55026495, 0.53921934, 0.55825062,\n",
      "       0.5566087 , 0.56466901, 0.56026569, 0.56683335, 0.57541608,\n",
      "       0.58340175, 0.57653556, 0.58907381, 0.58780506, 0.58922308,\n",
      "       0.5844466 , 0.60198522, 0.59079036, 0.59728338, 0.60616464,\n",
      "       0.6067617 , 0.60571684, 0.61489663, 0.61452347, 0.61997164]), 'train_losses': array([71.88975471, 57.45021592, 50.03074565, 42.11692502, 34.71740918,\n",
      "       29.9633236 , 25.33248617, 22.26737716, 19.05279631, 16.77933488,\n",
      "       14.98719933, 13.57218525, 12.29976494, 11.25446769, 10.29249183,\n",
      "        9.41143382,  9.11717697,  8.43681367,  8.0749192 ,  7.70626253,\n",
      "        7.34845801,  6.99903818,  6.5315424 ,  6.33305432,  5.94153502,\n",
      "        5.66640012,  5.6120935 ,  5.38398465,  5.27702782,  4.94796464,\n",
      "        4.80336932,  4.76278691,  4.53781575,  4.45798375,  4.22992668,\n",
      "        4.10599056,  4.05016183,  3.8557567 ,  3.89046421,  3.57631464,\n",
      "        3.53627233,  3.39141667,  3.42460659,  3.28415419,  3.12666575,\n",
      "        3.08381309,  2.99761159,  2.9214148 ,  2.89704712,  2.85017312,\n",
      "        2.83139893,  2.66699702,  2.71425209,  2.64240005,  2.51989251,\n",
      "        2.51312589,  2.53591031,  2.39743929,  2.39860239,  2.32122818]), 'val_accs': array([0.42597015, 0.42567164, 0.44597015, 0.45134328, 0.42328358,\n",
      "       0.36985075, 0.3161194 , 0.30865672, 0.29104478, 0.30179104,\n",
      "       0.29940299, 0.33820896, 0.38179104, 0.39343284, 0.43402985,\n",
      "       0.49462687, 0.45253731, 0.43014925, 0.45492537, 0.48179104,\n",
      "       0.52238806, 0.49850746, 0.54298507, 0.52149254, 0.53522388,\n",
      "       0.56328358, 0.52029851, 0.57940299, 0.57791045, 0.5841791 ,\n",
      "       0.57850746, 0.59253731, 0.59761194, 0.60059701, 0.62      ,\n",
      "       0.62149254, 0.56656716, 0.62895522, 0.6558209 , 0.61761194,\n",
      "       0.6361194 , 0.6480597 , 0.63671642, 0.67492537, 0.65492537,\n",
      "       0.65134328, 0.63850746, 0.67044776, 0.66089552, 0.66029851,\n",
      "       0.65701493, 0.66208955, 0.68179104, 0.68119403, 0.66298507,\n",
      "       0.67671642, 0.64208955, 0.68925373, 0.66925373, 0.66089552]), 'val_losses': array([27.67753057, 23.05986047, 20.85250098, 19.25581793, 17.30984329,\n",
      "       15.47696721, 13.62142035, 11.85982358, 12.04014304, 10.23770119,\n",
      "       10.10960622,  8.34019693,  7.58571151,  7.41085108,  6.78040453,\n",
      "        5.04757479,  5.5268966 ,  5.51183609,  4.93036612,  4.52931716,\n",
      "        3.92357405,  4.01026634,  3.54661219,  3.82854955,  3.29177427,\n",
      "        2.95971918,  3.19132187,  2.80983578,  2.68804462,  2.53232482,\n",
      "        2.50652326,  2.60006568,  2.65484857,  2.48000039,  2.34937705,\n",
      "        2.18987569,  2.74275805,  2.33104875,  1.8933606 ,  2.31425519,\n",
      "        2.11348555,  2.22489043,  2.20795576,  1.84195727,  2.08146683,\n",
      "        1.98770603,  2.08970705,  1.82602595,  1.83221977,  1.86694326,\n",
      "        1.76197191,  1.93640473,  1.580439  ,  1.68160558,  1.77292685,\n",
      "        1.56978035,  1.72566509,  1.46122101,  1.73431309,  1.57838609])}, 'ATTACH::11::best_val_loss': 1.4612210056675015, 'ATTACH::12::history': {'train_accs': array([0.40786626, 0.45309351, 0.48697664, 0.52153146, 0.54959325,\n",
      "       0.55862378, 0.57713262, 0.61079185, 0.61362788, 0.65340697,\n",
      "       0.65915367, 0.67221434, 0.6728114 , 0.68445406, 0.69736548,\n",
      "       0.7038585 , 0.70818718, 0.71065005, 0.70982909, 0.71833719,\n",
      "       0.71811329, 0.72691992, 0.73281588, 0.72833794, 0.7342339 ,\n",
      "       0.74460781, 0.74572729, 0.75520561, 0.74975744, 0.75580267,\n",
      "       0.75520561, 0.75684753, 0.77371446, 0.78110307, 0.7614001 ,\n",
      "       0.77938652, 0.79274573, 0.77640122, 0.78207329, 0.78505859,\n",
      "       0.78819315, 0.7786402 , 0.78826778, 0.79237257, 0.78774535]), 'train_losses': array([30.49030431,  6.20233402,  3.07934495,  1.91820352,  1.47015225,\n",
      "        1.38458792,  1.23116494,  0.98842055,  0.93573561,  0.80836998,\n",
      "        0.78211621,  0.75579383,  0.74520265,  0.71488804,  0.67915817,\n",
      "        0.66368819,  0.66026043,  0.64018337,  0.65332382,  0.62531038,\n",
      "        0.6350349 ,  0.60895612,  0.6053735 ,  0.6234145 ,  0.59885394,\n",
      "        0.57495081,  0.56366576,  0.56618396,  0.57268745,  0.56721667,\n",
      "        0.55338813,  0.55970843,  0.52056985,  0.51421356,  0.53858332,\n",
      "        0.5145593 ,  0.49439302,  0.51286103,  0.50847472,  0.49728182,\n",
      "        0.49703378,  0.53581111,  0.50762991,  0.50039651,  0.5013233 ]), 'val_accs': array([0.54089552, 0.59970149, 0.6641791 , 0.57671642, 0.51880597,\n",
      "       0.64507463, 0.63641791, 0.69313433, 0.72656716, 0.69492537,\n",
      "       0.73522388, 0.62626866, 0.67731343, 0.70746269, 0.70597015,\n",
      "       0.76686567, 0.77313433, 0.75820896, 0.73373134, 0.76119403,\n",
      "       0.69373134, 0.80985075, 0.7680597 , 0.78358209, 0.77492537,\n",
      "       0.72      , 0.77820896, 0.77791045, 0.77044776, 0.80716418,\n",
      "       0.75522388, 0.74985075, 0.79940299, 0.77522388, 0.81373134,\n",
      "       0.78149254, 0.7919403 , 0.81253731, 0.82835821, 0.75343284,\n",
      "       0.80149254, 0.76507463, 0.79731343, 0.73791045, 0.82895522]), 'val_losses': array([3.99433362, 1.50907592, 1.02613109, 1.03811411, 1.39917794,\n",
      "       0.81459454, 0.79754823, 0.70419648, 0.65335822, 0.7087871 ,\n",
      "       0.6405507 , 0.75655389, 0.79110338, 0.67804608, 0.65594472,\n",
      "       0.5669137 , 0.53734405, 0.54642749, 0.57083679, 0.53056708,\n",
      "       0.71805823, 0.46476185, 0.55472153, 0.48623472, 0.50789235,\n",
      "       0.59706806, 0.48162485, 0.46853275, 0.51191173, 0.45235743,\n",
      "       0.52046745, 0.52163269, 0.42277628, 0.53421182, 0.42267509,\n",
      "       0.46106403, 0.47108573, 0.46520022, 0.42592466, 0.49858338,\n",
      "       0.49614381, 0.56930469, 0.51227056, 0.63685039, 0.44061064])}, 'ATTACH::12::best_val_loss': 0.4226750885550656, 'ATTACH::13::history': {'train_accs': array([0.32405403, 0.38017763, 0.40092544, 0.42003135, 0.43025599,\n",
      "       0.44271961, 0.43876409, 0.45451153, 0.46518397, 0.4765281 ,\n",
      "       0.49802224, 0.51317262, 0.5453392 , 0.56011643, 0.56720651,\n",
      "       0.59213374, 0.6095231 , 0.63430107, 0.64422718, 0.65773565,\n",
      "       0.67251287, 0.69109635, 0.70773938, 0.7199791 , 0.73371147,\n",
      "       0.75222031, 0.7558773 , 0.77513247, 0.77617733, 0.79259646,\n",
      "       0.79699978, 0.80714979, 0.81274722, 0.81976267, 0.83140533,\n",
      "       0.83827151, 0.84140608, 0.8476752 , 0.85819837, 0.86349728,\n",
      "       0.86887081, 0.87260243, 0.87752817, 0.88409583, 0.88648407,\n",
      "       0.89372341, 0.89297709, 0.89872379, 0.90432122, 0.90476901,\n",
      "       0.91036645, 0.91588925, 0.91656094, 0.92156131, 0.92305396,\n",
      "       0.92760654, 0.92962161, 0.93036794, 0.93395029, 0.93835361]), 'train_losses': array([54.65822069, 14.72423945,  9.30087683,  6.62769633,  5.3787362 ,\n",
      "        4.40942109,  3.85957448,  3.16338582,  2.7741726 ,  2.47887004,\n",
      "        2.20108459,  1.9969362 ,  1.72515117,  1.60896774,  1.48365107,\n",
      "        1.33515199,  1.19332884,  1.12906412,  1.04620663,  0.98317932,\n",
      "        0.91348829,  0.87306374,  0.80831639,  0.76367322,  0.7158152 ,\n",
      "        0.67529405,  0.66463123,  0.61858135,  0.59295267,  0.55800705,\n",
      "        0.54609309,  0.51585479,  0.50670751,  0.48209244,  0.4504663 ,\n",
      "        0.43632054,  0.42168688,  0.40418431,  0.38978066,  0.37841466,\n",
      "        0.35826803,  0.34483502,  0.33878652,  0.32772353,  0.31806411,\n",
      "        0.29523851,  0.29907889,  0.28252865,  0.26749862,  0.26886488,\n",
      "        0.25527953,  0.24253333,  0.23885655,  0.22198239,  0.2177648 ,\n",
      "        0.20847297,  0.20926015,  0.19873799,  0.19088925,  0.18141757]), 'val_accs': array([0.45313433, 0.50179104, 0.47940299, 0.48716418, 0.47492537,\n",
      "       0.50089552, 0.46298507, 0.33731343, 0.51970149, 0.52656716,\n",
      "       0.59940299, 0.65164179, 0.6080597 , 0.6480597 , 0.65044776,\n",
      "       0.69940299, 0.66447761, 0.55343284, 0.73164179, 0.68089552,\n",
      "       0.70119403, 0.66835821, 0.72686567, 0.75014925, 0.78059701,\n",
      "       0.78776119, 0.78746269, 0.75701493, 0.77313433, 0.66865672,\n",
      "       0.68955224, 0.82328358, 0.82477612, 0.77223881, 0.86865672,\n",
      "       0.85552239, 0.83701493, 0.87910448, 0.84268657, 0.82477612,\n",
      "       0.86179104, 0.86537313, 0.86686567, 0.88716418, 0.89850746,\n",
      "       0.66358209, 0.87970149, 0.87492537, 0.89641791, 0.78895522,\n",
      "       0.92507463, 0.89940299, 0.92626866, 0.9161194 , 0.93671642,\n",
      "       0.76268657, 0.90567164, 0.90626866, 0.86      , 0.92925373]), 'val_losses': array([15.94963629,  2.97682703,  3.10964572,  2.55427334,  2.24426213,\n",
      "        1.86990419,  2.63442391,  2.7856595 ,  1.93354004,  1.36206975,\n",
      "        1.36215648,  0.96571467,  0.86700316,  0.87633066,  0.88958836,\n",
      "        0.79535612,  0.78504098,  1.41002537,  0.61316143,  0.81487403,\n",
      "        0.73108271,  0.96816008,  0.74902652,  0.7071099 ,  0.61387423,\n",
      "        0.61246776,  0.59891154,  0.6804871 ,  0.65912044,  1.32175678,\n",
      "        1.22744344,  0.52201588,  0.45814424,  0.5711252 ,  0.38836081,\n",
      "        0.42733951,  0.4424304 ,  0.35222828,  0.42133549,  0.4583592 ,\n",
      "        0.3595269 ,  0.39937014,  0.36305798,  0.31535464,  0.30092912,\n",
      "        1.17290438,  0.32003642,  0.37245043,  0.30470541,  0.61601902,\n",
      "        0.23695853,  0.29770799,  0.21117043,  0.24273738,  0.18166765,\n",
      "        0.77357499,  0.28121652,  0.2710663 ,  0.38406791,  0.21422916])}, 'ATTACH::13::best_val_loss': 0.18166764944791794, 'ATTACH::14::history': {'train_accs': array([0.23934622, 0.26501978, 0.31517277, 0.3242033 , 0.33868199,\n",
      "       0.35241436, 0.36278827, 0.36674379, 0.38413314, 0.39958206,\n",
      "       0.39279051, 0.39480558, 0.40294052, 0.40391074, 0.41965818,\n",
      "       0.41361296, 0.42197179, 0.4268229 , 0.4268229 , 0.43577879,\n",
      "       0.43712217, 0.44786924, 0.45018285, 0.45264572, 0.46764684,\n",
      "       0.47130383, 0.48772296, 0.49167848, 0.48540936, 0.49966415,\n",
      "       0.49854467, 0.51153071, 0.50996343, 0.51473991, 0.51951638,\n",
      "       0.52138219, 0.52503918, 0.53414434, 0.53780133, 0.53071125,\n",
      "       0.54421972, 0.54018957, 0.55071274, 0.55750429, 0.56116128,\n",
      "       0.56907232, 0.57108739, 0.57534144, 0.57586387, 0.58758116,\n",
      "       0.58452123, 0.59071573, 0.59332786, 0.60474662, 0.60788119,\n",
      "       0.60892604, 0.61855362, 0.61340399, 0.62601687, 0.62683782]), 'train_losses': array([230.10492798,  73.01022284,  46.04944392,  34.68871749,\n",
      "        27.36107344,  21.10363413,  17.65841367,  15.19496252,\n",
      "        12.83669557,  10.74202268,   9.80213026,   8.9517406 ,\n",
      "         8.31746988,   7.64488901,   6.84660725,   6.28128218,\n",
      "         5.9452754 ,   5.44412826,   5.1750651 ,   4.73906313,\n",
      "         4.59284165,   4.0854801 ,   3.95811645,   3.82821984,\n",
      "         3.50688981,   3.35076493,   3.09704293,   2.93713852,\n",
      "         2.9192182 ,   2.79678343,   2.68281768,   2.53684168,\n",
      "         2.50074546,   2.32150547,   2.3169026 ,   2.19555715,\n",
      "         2.14943301,   2.03118749,   1.98428363,   1.96259019,\n",
      "         1.8811596 ,   1.83700522,   1.76527021,   1.70077231,\n",
      "         1.65996591,   1.61628496,   1.52198449,   1.48423685,\n",
      "         1.48441874,   1.40568402,   1.40079204,   1.32237717,\n",
      "         1.32080497,   1.24111392,   1.23218803,   1.23800096,\n",
      "         1.21445767,   1.19704588,   1.15072028,   1.13398009]), 'val_accs': array([0.25492537, 0.35820896, 0.36149254, 0.42985075, 0.46776119,\n",
      "       0.47791045, 0.50059701, 0.51104478, 0.47253731, 0.47731343,\n",
      "       0.49462687, 0.4919403 , 0.48208955, 0.40656716, 0.47014925,\n",
      "       0.42      , 0.43283582, 0.5158209 , 0.53044776, 0.55791045,\n",
      "       0.52      , 0.52029851, 0.57343284, 0.5638806 , 0.51850746,\n",
      "       0.56507463, 0.58925373, 0.60746269, 0.55074627, 0.59731343,\n",
      "       0.56656716, 0.58925373, 0.60119403, 0.6119403 , 0.57820896,\n",
      "       0.61462687, 0.59850746, 0.57731343, 0.5519403 , 0.59283582,\n",
      "       0.60626866, 0.65223881, 0.58656716, 0.66268657, 0.61373134,\n",
      "       0.61462687, 0.60746269, 0.60746269, 0.65373134, 0.61432836,\n",
      "       0.59820896, 0.65283582, 0.66686567, 0.63492537, 0.58268657,\n",
      "       0.5919403 , 0.64089552, 0.66358209, 0.66895522, 0.68089552]), 'val_losses': array([112.72586338,  29.77983957,  20.99292241,  13.27969457,\n",
      "        10.6829614 ,   8.11853345,   7.35970522,   5.78075996,\n",
      "         4.64496866,   4.13893294,   3.76343675,   3.14292448,\n",
      "         2.69206287,   2.53768479,   2.09451861,   2.95343267,\n",
      "         1.91317651,   1.63703303,   1.53500314,   1.49988948,\n",
      "         1.42525307,   1.33568581,   1.24755274,   1.25643068,\n",
      "         1.24990105,   1.21350136,   1.13673308,   1.12339084,\n",
      "         1.45666685,   1.1587168 ,   1.18518702,   0.957479  ,\n",
      "         1.06504723,   1.06154654,   1.14654897,   1.07241039,\n",
      "         1.00671551,   1.23194971,   1.2994043 ,   1.05782142,\n",
      "         1.04053505,   0.90936527,   0.91707872,   0.91782489,\n",
      "         1.05178061,   1.11448099,   0.9065819 ,   0.91652048,\n",
      "         0.92085783,   1.10960441,   0.84727402,   0.91958224,\n",
      "         0.78650044,   0.92379659,   1.09735987,   0.91855025,\n",
      "         0.81036492,   0.78091253,   0.83328119,   0.85627056])}, 'ATTACH::14::best_val_loss': 0.7809125311694928, 'ATTACH::15::history': {'train_accs': array([0.49421599, 0.61877752, 0.63534592, 0.66631838, 0.69452944,\n",
      "       0.70632137, 0.71318755, 0.73303978, 0.73520412, 0.74423464,\n",
      "       0.74162251, 0.75304127, 0.75244421, 0.7537876 , 0.75296664,\n",
      "       0.75774312, 0.71684454, 0.72736771]), 'train_losses': array([4.82469611, 0.88367875, 0.79484908, 0.72689549, 0.67588114,\n",
      "       0.65829739, 0.64909001, 0.61120745, 0.60339165, 0.5946097 ,\n",
      "       0.58214554, 0.56382713, 0.57329253, 0.56951214, 0.56613326,\n",
      "       0.57088906, 0.64120844, 0.61288696]), 'val_accs': array([0.6280597 , 0.70029851, 0.76955224, 0.61164179, 0.6041791 ,\n",
      "       0.7161194 , 0.77373134, 0.81820896, 0.74865672, 0.77820896,\n",
      "       0.7638806 , 0.81402985, 0.76179104, 0.75701493, 0.78059701,\n",
      "       0.75850746, 0.7680597 , 0.71820896]), 'val_losses': array([0.94795455, 0.68591052, 0.60707574, 0.82384623, 0.69237016,\n",
      "       0.62811138, 0.54205647, 0.48686181, 0.54678865, 0.59867811,\n",
      "       0.51430204, 0.48991156, 0.58930664, 0.5630383 , 0.52717126,\n",
      "       0.54866103, 0.58086941, 0.66976048])}, 'ATTACH::15::best_val_loss': 0.4868618080153394, 'ATTACH::16::history': {'train_accs': array([0.42077767, 0.44219718, 0.44167475, 0.45622808, 0.47339354,\n",
      "       0.49399209, 0.51108292, 0.51003806, 0.5264572 , 0.54272707,\n",
      "       0.5483245 , 0.56451974, 0.57586387, 0.5844466 , 0.59220837,\n",
      "       0.60414956, 0.62198672, 0.63683857, 0.65310844, 0.65818345,\n",
      "       0.67639376, 0.67452795, 0.68811105, 0.70303754, 0.71348608,\n",
      "       0.73281588, 0.7510262 , 0.76535562, 0.79087992, 0.80259721,\n",
      "       0.82438988, 0.82886783, 0.83125606, 0.8451377 , 0.85767595,\n",
      "       0.86073588, 0.86909471, 0.87633405, 0.880812  , 0.89551459,\n",
      "       0.89708187, 0.89909695, 0.90648556, 0.90282857, 0.90372416,\n",
      "       0.91424733, 0.91932234, 0.9114113 , 0.92462124, 0.92417345,\n",
      "       0.93081573, 0.93148742, 0.93827898, 0.93126353, 0.93954773,\n",
      "       0.94111501, 0.94118964, 0.94350325, 0.94902605, 0.94842899]), 'train_losses': array([8.86425346, 1.24219551, 1.22959199, 1.20396687, 1.17850973,\n",
      "       1.15169614, 1.12486175, 1.12570339, 1.09366196, 1.06343619,\n",
      "       1.04368358, 1.01185458, 0.97874529, 0.94590154, 0.92513537,\n",
      "       0.89037072, 0.86574913, 0.8399011 , 0.80846315, 0.80286445,\n",
      "       0.77821797, 0.76892307, 0.7501351 , 0.71344144, 0.70486103,\n",
      "       0.67371783, 0.64559244, 0.61357483, 0.56065745, 0.52872123,\n",
      "       0.46950544, 0.45536542, 0.44827251, 0.41494777, 0.38632698,\n",
      "       0.37922942, 0.36036889, 0.3502854 , 0.33559289, 0.30772142,\n",
      "       0.30450802, 0.29242261, 0.27605116, 0.28647806, 0.2899475 ,\n",
      "       0.25917512, 0.24958733, 0.26361924, 0.22256038, 0.23070452,\n",
      "       0.20819475, 0.21103367, 0.19251667, 0.20088031, 0.17945744,\n",
      "       0.17009725, 0.17358369, 0.16440885, 0.15087603, 0.15322237]), 'val_accs': array([0.44208955, 0.44208955, 0.44656716, 0.47791045, 0.4880597 ,\n",
      "       0.50985075, 0.52865672, 0.54298507, 0.54029851, 0.5561194 ,\n",
      "       0.5719403 , 0.57044776, 0.56328358, 0.59910448, 0.58835821,\n",
      "       0.65641791, 0.66089552, 0.64925373, 0.66029851, 0.69074627,\n",
      "       0.68      , 0.71044776, 0.66029851, 0.70835821, 0.73104478,\n",
      "       0.76955224, 0.76179104, 0.76208955, 0.78328358, 0.78835821,\n",
      "       0.81164179, 0.83880597, 0.85283582, 0.84597015, 0.83910448,\n",
      "       0.87253731, 0.87701493, 0.87044776, 0.89223881, 0.89134328,\n",
      "       0.9       , 0.88089552, 0.87283582, 0.91134328, 0.8638806 ,\n",
      "       0.90895522, 0.89373134, 0.91492537, 0.88776119, 0.92208955,\n",
      "       0.9038806 , 0.91850746, 0.93223881, 0.9238806 , 0.92119403,\n",
      "       0.91701493, 0.93313433, 0.92567164, 0.90985075, 0.91671642]), 'val_losses': array([1.25125143, 1.23089439, 1.21291337, 1.17995692, 1.15175699,\n",
      "       1.13628607, 1.12146741, 1.08113025, 1.07243471, 1.05157477,\n",
      "       0.99676648, 0.97733388, 0.96960377, 0.93705572, 0.89282507,\n",
      "       0.81991181, 0.8154778 , 0.85414618, 0.831729  , 0.78862422,\n",
      "       0.88154379, 0.731487  , 0.94058172, 0.8073978 , 0.69274082,\n",
      "       0.60974001, 0.65108208, 0.66887357, 0.63862427, 0.60106188,\n",
      "       0.52601114, 0.42225428, 0.391518  , 0.44873443, 0.47942068,\n",
      "       0.35504721, 0.33615177, 0.36922537, 0.32032309, 0.3218623 ,\n",
      "       0.31563803, 0.3814406 , 0.38483862, 0.29098509, 0.42194913,\n",
      "       0.30339692, 0.33651261, 0.27683295, 0.36192561, 0.2619724 ,\n",
      "       0.31198346, 0.25459632, 0.20007774, 0.2298219 , 0.25186838,\n",
      "       0.24795202, 0.2159705 , 0.23680035, 0.29478312, 0.27507643])}, 'ATTACH::16::best_val_loss': 0.2000777406567958, 'ATTACH::17::history': {'train_accs': array([0.3932383 , 0.42622584, 0.43406224, 0.43167401, 0.44242108,\n",
      "       0.45115307, 0.45354131, 0.46264647, 0.46175088, 0.46630346,\n",
      "       0.47190089, 0.47130383, 0.46854243, 0.47973729, 0.48652885,\n",
      "       0.48406598, 0.48645421, 0.49003657, 0.48891708, 0.49847004,\n",
      "       0.5041421 , 0.49682812, 0.50175386, 0.50227629, 0.51018733,\n",
      "       0.5011568 , 0.50675424, 0.51070975, 0.51981491, 0.52100903,\n",
      "       0.51653108, 0.5179491 , 0.52063587, 0.52638257, 0.52085977,\n",
      "       0.52459139, 0.53712964, 0.53168147, 0.54205538, 0.54093589,\n",
      "       0.53675647, 0.53638331, 0.53347265, 0.53503993, 0.55071274,\n",
      "       0.5432495 , 0.54586163, 0.5460109 , 0.55302635, 0.56101202,\n",
      "       0.55608628, 0.55466826, 0.56668408, 0.55601164, 0.56616165,\n",
      "       0.56228077, 0.56481827, 0.57422196, 0.57146056, 0.57160982]), 'train_losses': array([11.19434011,  1.47440206,  1.26465898,  1.2079138 ,  1.16419652,\n",
      "        1.13415188,  1.10141306,  1.08096246,  1.08063378,  1.06219237,\n",
      "        1.03916989,  1.03792737,  1.01929598,  1.01462869,  0.98724331,\n",
      "        0.9974562 ,  0.98622315,  0.98262896,  0.97983412,  0.97940093,\n",
      "        0.956771  ,  0.96695949,  0.96315335,  0.95430819,  0.94924137,\n",
      "        0.95403359,  0.955133  ,  0.93995558,  0.93193382,  0.92791713,\n",
      "        0.93453724,  0.92509684,  0.92847706,  0.91905322,  0.92170068,\n",
      "        0.9171858 ,  0.90719431,  0.90969158,  0.89922339,  0.90841129,\n",
      "        0.89968238,  0.89880599,  0.9042602 ,  0.90409946,  0.89032551,\n",
      "        0.89417054,  0.89133942,  0.88397811,  0.88054871,  0.86975785,\n",
      "        0.87643844,  0.87151444,  0.87136131,  0.87329946,  0.86792489,\n",
      "        0.8683498 ,  0.86146709,  0.85435338,  0.86004096,  0.85583778]), 'val_accs': array([0.40776119, 0.38298507, 0.42328358, 0.45343284, 0.43223881,\n",
      "       0.43492537, 0.53134328, 0.46955224, 0.47970149, 0.55044776,\n",
      "       0.52208955, 0.5641791 , 0.45791045, 0.45791045, 0.44835821,\n",
      "       0.47313433, 0.49014925, 0.46597015, 0.45343284, 0.53014925,\n",
      "       0.57671642, 0.53701493, 0.5638806 , 0.5241791 , 0.43402985,\n",
      "       0.54656716, 0.54507463, 0.52149254, 0.55432836, 0.46746269,\n",
      "       0.56268657, 0.51104478, 0.54656716, 0.56268657, 0.46119403,\n",
      "       0.51164179, 0.52865672, 0.55940299, 0.43223881, 0.49402985,\n",
      "       0.48238806, 0.49402985, 0.55850746, 0.50537313, 0.52358209,\n",
      "       0.58835821, 0.61820896, 0.61044776, 0.58776119, 0.6158209 ,\n",
      "       0.62179104, 0.55731343, 0.60597015, 0.65253731, 0.61402985,\n",
      "       0.60567164, 0.6280597 , 0.60268657, 0.64059701, 0.65791045]), 'val_losses': array([1.20876681, 1.22803143, 1.17834049, 1.11922006, 1.08306053,\n",
      "       1.0711579 , 1.02059891, 1.04662368, 1.00882699, 0.95148061,\n",
      "       0.99950408, 0.92686658, 0.93782347, 0.92507663, 0.93927623,\n",
      "       0.9676028 , 0.90003586, 0.89978552, 0.93121125, 0.90968082,\n",
      "       0.89126466, 0.92974832, 0.90009074, 0.89292405, 0.90024939,\n",
      "       0.87714095, 0.8734553 , 0.88130714, 0.86495279, 0.87425278,\n",
      "       0.87379253, 0.8759569 , 0.88077007, 0.85903776, 0.86841041,\n",
      "       0.87379503, 0.85548642, 0.86520915, 0.87424746, 0.85393766,\n",
      "       0.86253423, 0.92541134, 0.85526457, 0.85836465, 0.86713557,\n",
      "       0.82881765, 0.82398607, 0.83034145, 0.83478949, 0.81268221,\n",
      "       0.81214997, 0.82074604, 0.82848495, 0.79701375, 0.79637464,\n",
      "       0.8109428 , 0.80043669, 0.80574266, 0.80040476, 0.79315769])}, 'ATTACH::17::best_val_loss': 0.7931576893222866, 'ATTACH::18::history': {'train_accs': array([0.53847302, 0.637361  , 0.68527502, 0.69340996, 0.70475409,\n",
      "       0.69661915, 0.69527577, 0.6923651 , 0.68288678, 0.68303605,\n",
      "       0.68818569, 0.68139413, 0.68169266, 0.68117024]), 'train_losses': array([3.31279768, 0.78465097, 0.69047726, 0.65020688, 0.64896797,\n",
      "       0.64523536, 0.64247535, 0.64619627, 0.6701112 , 0.66978398,\n",
      "       0.65998095, 0.67928991, 0.68515116, 0.6717584 ]), 'val_accs': array([0.69044776, 0.71641791, 0.68358209, 0.72029851, 0.7719403 ,\n",
      "       0.68477612, 0.69223881, 0.74507463, 0.6119403 , 0.64865672,\n",
      "       0.63402985, 0.68985075, 0.63671642, 0.64119403]), 'val_losses': array([0.71330356, 0.60123863, 0.66805003, 0.56741497, 0.57718176,\n",
      "       0.69079417, 0.70818677, 0.61080981, 0.76049549, 0.72449421,\n",
      "       0.7725314 , 0.65687036, 0.69786372, 0.73505027])}, 'ATTACH::18::best_val_loss': 0.5674149666259538, 'ATTACH::19::history': {'train_accs': array([0.37637137, 0.45227256, 0.52056124, 0.56996791, 0.6313904 ,\n",
      "       0.68139413, 0.69296216, 0.72580043, 0.74542876, 0.78908874,\n",
      "       0.82118069, 0.8529741 , 0.86849765, 0.87887156, 0.90126129,\n",
      "       0.90521681, 0.91820285, 0.92656168, 0.9305172 , 0.93477125,\n",
      "       0.94372714, 0.94521979, 0.9502948 , 0.95723561, 0.95693708,\n",
      "       0.96298231, 0.96193746, 0.96313158, 0.96843048, 0.97178894,\n",
      "       0.97455034, 0.96275841, 0.97402791, 0.9749235 , 0.97932682,\n",
      "       0.98417792, 0.98029704, 0.977461  , 0.98261064, 0.97999851,\n",
      "       0.97962535, 0.97843123, 0.98693932, 0.98514815, 0.98664079,\n",
      "       0.98679006, 0.97783417, 0.98746175, 0.98783491, 0.98544668]), 'train_losses': array([23.17046467,  4.84268927,  2.97072714,  2.02972933,  1.45777423,\n",
      "        1.18098118,  1.09252257,  0.98723985,  0.8752827 ,  0.67040891,\n",
      "        0.55800811,  0.4527093 ,  0.39334934,  0.36167607,  0.29712636,\n",
      "        0.29038093,  0.25425479,  0.2302852 ,  0.21496817,  0.19847276,\n",
      "        0.17972227,  0.17125337,  0.15416236,  0.13404553,  0.13275341,\n",
      "        0.11130921,  0.1205975 ,  0.11286117,  0.10093132,  0.09988363,\n",
      "        0.08098431,  0.11844551,  0.08471676,  0.08299545,  0.06418021,\n",
      "        0.05322819,  0.0628961 ,  0.07037395,  0.05977298,  0.063348  ,\n",
      "        0.06505806,  0.0686214 ,  0.04176929,  0.05078804,  0.04427245,\n",
      "        0.04051614,  0.06920427,  0.04151767,  0.04153797,  0.04403489]), 'val_accs': array([0.51910448, 0.61343284, 0.67283582, 0.65044776, 0.74089552,\n",
      "       0.65731343, 0.75432836, 0.81731343, 0.77104478, 0.83432836,\n",
      "       0.83552239, 0.88567164, 0.89791045, 0.86776119, 0.88328358,\n",
      "       0.91014925, 0.90626866, 0.9080597 , 0.92865672, 0.93940299,\n",
      "       0.93223881, 0.90507463, 0.93044776, 0.94656716, 0.95014925,\n",
      "       0.96507463, 0.94626866, 0.9438806 , 0.96179104, 0.95343284,\n",
      "       0.97164179, 0.95373134, 0.96537313, 0.94507463, 0.97223881,\n",
      "       0.96835821, 0.9638806 , 0.96119403, 0.96119403, 0.97462687,\n",
      "       0.96567164, 0.95880597, 0.96835821, 0.95671642, 0.9638806 ,\n",
      "       0.96567164, 0.96119403, 0.96507463, 0.95641791, 0.97044776]), 'val_losses': array([3.10535603, 1.30221984, 1.05424082, 0.85068258, 0.84264155,\n",
      "       0.8631101 , 0.6685422 , 0.50882218, 0.69615175, 0.48954566,\n",
      "       0.45554949, 0.32844555, 0.26669245, 0.37908288, 0.32104162,\n",
      "       0.25315167, 0.29507801, 0.25345763, 0.20038875, 0.18846624,\n",
      "       0.17459991, 0.32602008, 0.24781228, 0.17024151, 0.15194543,\n",
      "       0.11499419, 0.17146857, 0.1855905 , 0.11825177, 0.19920874,\n",
      "       0.09593656, 0.17617585, 0.12200555, 0.20537792, 0.09613135,\n",
      "       0.11385928, 0.1299415 , 0.14685752, 0.15401106, 0.08802115,\n",
      "       0.13136378, 0.14841523, 0.10719662, 0.15147299, 0.1425901 ,\n",
      "       0.1222632 , 0.13682397, 0.12730003, 0.17942284, 0.11497635])}, 'ATTACH::19::best_val_loss': 0.0880211500623333, 'ATTACH::20::history': {'train_accs': array([0.38823793, 0.50675424, 0.55190686, 0.58914844, 0.59758191,\n",
      "       0.6065378 , 0.62556907, 0.64049556, 0.64251064, 0.6509441 ,\n",
      "       0.65579521, 0.66646765, 0.67818494, 0.67676692, 0.68012538,\n",
      "       0.68952907, 0.6815434 , 0.69452944, 0.71378461, 0.70408239,\n",
      "       0.7367714 , 0.74595119, 0.75460855, 0.76662437, 0.77923726,\n",
      "       0.78729756, 0.80700052, 0.8172998 , 0.83036047, 0.84200313,\n",
      "       0.86319875, 0.87222927, 0.88797671, 0.90723188, 0.91581461,\n",
      "       0.92350175, 0.93678633, 0.9388014 , 0.95223524, 0.95425032,\n",
      "       0.95238451, 0.95753414, 0.96298231, 0.96126577, 0.96634077,\n",
      "       0.96947533, 0.97022166, 0.97268453, 0.97208747, 0.97440107,\n",
      "       0.97395328, 0.9726099 , 0.97163967, 0.97671468]), 'train_losses': array([12.13452694,  2.00756781,  1.36637318,  1.08422089,  0.97624991,\n",
      "        0.92223415,  0.85899823,  0.79568671,  0.79341093,  0.75798464,\n",
      "        0.7603976 ,  0.72661552,  0.70309749,  0.70042582,  0.6924337 ,\n",
      "        0.67719821,  0.69299751,  0.65810853,  0.62346713,  0.64264805,\n",
      "        0.59272209,  0.57807794,  0.57718529,  0.55191055,  0.53025693,\n",
      "        0.50716576,  0.47961691,  0.47265773,  0.43377809,  0.41239639,\n",
      "        0.36671854,  0.335699  ,  0.30912765,  0.25804712,  0.23326469,\n",
      "        0.21431367,  0.18789041,  0.18136505,  0.1448176 ,  0.13699773,\n",
      "        0.13788902,  0.12395917,  0.11160797,  0.11844624,  0.098887  ,\n",
      "        0.0943185 ,  0.08997554,  0.0887743 ,  0.08772522,  0.07863833,\n",
      "        0.07938045,  0.08468069,  0.08993493,  0.07440607]), 'val_accs': array([0.58059701, 0.57402985, 0.69940299, 0.67164179, 0.70626866,\n",
      "       0.73492537, 0.71373134, 0.65373134, 0.7241791 , 0.70298507,\n",
      "       0.73283582, 0.72537313, 0.70686567, 0.76656716, 0.72477612,\n",
      "       0.75283582, 0.74328358, 0.75641791, 0.81164179, 0.78029851,\n",
      "       0.78656716, 0.72119403, 0.76      , 0.79731343, 0.78477612,\n",
      "       0.82716418, 0.83283582, 0.80985075, 0.83641791, 0.87283582,\n",
      "       0.85641791, 0.88059701, 0.90686567, 0.92776119, 0.87641791,\n",
      "       0.93462687, 0.90835821, 0.94477612, 0.93462687, 0.94835821,\n",
      "       0.93671642, 0.9519403 , 0.95701493, 0.95074627, 0.95701493,\n",
      "       0.94119403, 0.94835821, 0.93701493, 0.95462687, 0.94268657,\n",
      "       0.95014925, 0.9558209 , 0.95791045, 0.95164179]), 'val_losses': array([1.58736122, 1.30893908, 0.70607808, 0.73990324, 0.65029982,\n",
      "       0.60982831, 0.64884612, 0.73238063, 0.64040685, 0.64529256,\n",
      "       0.64136323, 0.59953286, 0.63819024, 0.58025914, 0.63389154,\n",
      "       0.53306814, 0.59008194, 0.54571223, 0.4862041 , 0.45636328,\n",
      "       0.52135022, 0.57972276, 0.50523778, 0.5017253 , 0.50059108,\n",
      "       0.4323415 , 0.40498221, 0.41599653, 0.40795069, 0.32159763,\n",
      "       0.35484237, 0.31361778, 0.24194231, 0.19648895, 0.33539773,\n",
      "       0.17115067, 0.26859876, 0.17256299, 0.1959457 , 0.16749435,\n",
      "       0.18727005, 0.17088284, 0.13916339, 0.13025328, 0.14413827,\n",
      "       0.1950355 , 0.17313099, 0.23353424, 0.14900004, 0.18818379,\n",
      "       0.17634683, 0.14621294, 0.14004678, 0.16124957])}, 'ATTACH::20::best_val_loss': 0.13025328075707848, 'ATTACH::21::history': {'train_accs': array([0.37532652, 0.46533323, 0.49787298, 0.51608329, 0.54339876,\n",
      "       0.59168595, 0.64027166, 0.68288678, 0.70438092, 0.74304053,\n",
      "       0.77535637, 0.80804538, 0.85252631, 0.87536383, 0.89021569,\n",
      "       0.90708262, 0.91768042, 0.92663632, 0.94163744, 0.94298082,\n",
      "       0.94790656, 0.95738488, 0.95783267, 0.96104187, 0.96611687,\n",
      "       0.96313158, 0.9691768 , 0.9694007 , 0.96723636, 0.97402791,\n",
      "       0.97134114, 0.97611762, 0.97738637, 0.97223673, 0.97313232,\n",
      "       0.97581909, 0.97925218, 0.9802224 , 0.98261064, 0.98223748,\n",
      "       0.98313307, 0.97731174, 0.98387939, 0.97761027, 0.98134189,\n",
      "       0.98604373, 0.98656616, 0.97783417]), 'train_losses': array([21.03141868,  3.32551274,  2.11998363,  1.62059284,  1.29688904,\n",
      "        1.04754711,  0.90514453,  0.7976186 ,  0.75185029,  0.66650601,\n",
      "        0.58212861,  0.50716665,  0.40691481,  0.3479036 ,  0.31207034,\n",
      "        0.27156661,  0.2454392 ,  0.2134766 ,  0.18048304,  0.1688866 ,\n",
      "        0.15469572,  0.13072895,  0.1310543 ,  0.11954556,  0.10374116,\n",
      "        0.10573282,  0.09247592,  0.09296385,  0.09118396,  0.07853569,\n",
      "        0.08448646,  0.0724112 ,  0.06962464,  0.08610616,  0.07911467,\n",
      "        0.07464637,  0.06332529,  0.05411079,  0.04755694,  0.05080408,\n",
      "        0.051394  ,  0.07311197,  0.047089  ,  0.07678356,  0.0578144 ,\n",
      "        0.04048875,  0.04286662,  0.07396257]), 'val_accs': array([0.45552239, 0.62686567, 0.58298507, 0.62358209, 0.57492537,\n",
      "       0.64925373, 0.73402985, 0.68298507, 0.76298507, 0.7761194 ,\n",
      "       0.84089552, 0.8758209 , 0.89223881, 0.89223881, 0.91552239,\n",
      "       0.92447761, 0.93402985, 0.92119403, 0.93462687, 0.93731343,\n",
      "       0.94059701, 0.94298507, 0.93552239, 0.93701493, 0.92119403,\n",
      "       0.95820896, 0.95671642, 0.95731343, 0.97253731, 0.96507463,\n",
      "       0.96149254, 0.96537313, 0.94208955, 0.96686567, 0.96298507,\n",
      "       0.97343284, 0.95671642, 0.97492537, 0.97552239, 0.95552239,\n",
      "       0.95223881, 0.97253731, 0.97462687, 0.96776119, 0.97044776,\n",
      "       0.96477612, 0.97761194, 0.97373134]), 'val_losses': array([2.30771268, 0.91595331, 1.20642782, 0.8963092 , 0.7602183 ,\n",
      "       0.72986068, 0.60052053, 0.69823359, 0.57673863, 0.54063611,\n",
      "       0.45049353, 0.33933469, 0.31878752, 0.28397183, 0.23109253,\n",
      "       0.20794806, 0.18492662, 0.20319214, 0.15945468, 0.15004954,\n",
      "       0.15793612, 0.13713975, 0.1709417 , 0.16658042, 0.18145566,\n",
      "       0.13091769, 0.12382375, 0.13538338, 0.10791927, 0.11802169,\n",
      "       0.12993381, 0.11253492, 0.19678936, 0.10261506, 0.10902374,\n",
      "       0.09414857, 0.13990317, 0.07387168, 0.08706953, 0.15477332,\n",
      "       0.17197655, 0.09770681, 0.095505  , 0.12841391, 0.10437092,\n",
      "       0.13934028, 0.08003688, 0.09674803])}, 'ATTACH::21::best_val_loss': 0.0738716826243187, 'ATTACH::22::history': {'train_accs': array([0.39286514, 0.50682887, 0.54944399, 0.56354952, 0.58138667,\n",
      "       0.59176058, 0.60138816, 0.62064333, 0.65154116, 0.67296067,\n",
      "       0.7174416 , 0.75050377, 0.79102918, 0.81162773, 0.84916785,\n",
      "       0.87424435, 0.89148444, 0.90327636, 0.90924696, 0.92394955,\n",
      "       0.93395029, 0.93447272, 0.94253302, 0.94656318, 0.95454885,\n",
      "       0.95828047, 0.95984775, 0.96328084, 0.96925144, 0.96656467,\n",
      "       0.97201284, 0.97380402, 0.97298306, 0.97566983, 0.98178969,\n",
      "       0.97641615, 0.97880439, 0.98052093, 0.97947608, 0.98156579,\n",
      "       0.98283454, 0.98201358, 0.9829838 , 0.98805881, 0.98246138,\n",
      "       0.98567057, 0.98417792, 0.98529741, 0.98447645, 0.98425256,\n",
      "       0.98313307, 0.9885066 , 0.98425256, 0.98932756, 0.98723785,\n",
      "       0.98910366, 0.98947683, 0.98783491, 0.99007389, 0.99231286]), 'train_losses': array([13.50885383,  2.22714324,  1.46353683,  1.20708423,  1.10313819,\n",
      "        1.01676488,  0.99478291,  0.91981152,  0.8713406 ,  0.80101494,\n",
      "        0.711705  ,  0.6339337 ,  0.54256165,  0.51013894,  0.42395862,\n",
      "        0.36444746,  0.32121654,  0.29268782,  0.26522003,  0.2296264 ,\n",
      "        0.19264768,  0.19020559,  0.16662486,  0.15902162,  0.13340444,\n",
      "        0.12381952,  0.11745828,  0.10921496,  0.094361  ,  0.1071291 ,\n",
      "        0.08167824,  0.08474988,  0.08779015,  0.07530078,  0.05606875,\n",
      "        0.07331751,  0.06275238,  0.06002301,  0.06273898,  0.05826289,\n",
      "        0.05090833,  0.05594087,  0.05325827,  0.04508419,  0.05280654,\n",
      "        0.04800536,  0.05051971,  0.04458827,  0.05282165,  0.0537308 ,\n",
      "        0.05298597,  0.03816499,  0.05372955,  0.03462543,  0.04101166,\n",
      "        0.0352859 ,  0.03190989,  0.03708274,  0.03261542,  0.02495464]), 'val_accs': array([0.52895522, 0.59074627, 0.6638806 , 0.67044776, 0.72567164,\n",
      "       0.67880597, 0.65014925, 0.67104478, 0.80328358, 0.73014925,\n",
      "       0.72179104, 0.80865672, 0.87641791, 0.80776119, 0.86238806,\n",
      "       0.89522388, 0.89313433, 0.85462687, 0.91641791, 0.94179104,\n",
      "       0.95820896, 0.94955224, 0.93820896, 0.96298507, 0.94597015,\n",
      "       0.96149254, 0.94925373, 0.95910448, 0.9641791 , 0.95880597,\n",
      "       0.97164179, 0.95731343, 0.96626866, 0.97313433, 0.97134328,\n",
      "       0.96656716, 0.96746269, 0.97791045, 0.9761194 , 0.97641791,\n",
      "       0.9758209 , 0.97134328, 0.97820896, 0.97910448, 0.97731343,\n",
      "       0.9719403 , 0.97910448, 0.97552239, 0.97940299, 0.97492537,\n",
      "       0.98      , 0.97731343, 0.9719403 , 0.9758209 , 0.97970149,\n",
      "       0.98119403, 0.98298507, 0.98328358, 0.98029851, 0.98119403]), 'val_losses': array([1.75856668, 1.05260637, 0.71370441, 0.83417522, 0.64143799,\n",
      "       0.79787093, 0.68168607, 0.68562019, 0.51669973, 0.64538611,\n",
      "       0.62526186, 0.47131754, 0.33597178, 0.50610204, 0.38509641,\n",
      "       0.30674367, 0.29234719, 0.44840912, 0.21975198, 0.1675792 ,\n",
      "       0.12757851, 0.14155632, 0.17935879, 0.10805478, 0.16453073,\n",
      "       0.1132936 , 0.14464657, 0.12000509, 0.11956648, 0.13267004,\n",
      "       0.10314703, 0.13418736, 0.11643114, 0.07668996, 0.10323865,\n",
      "       0.10098629, 0.10253528, 0.06463889, 0.07267585, 0.069131  ,\n",
      "       0.07959772, 0.10885434, 0.07977279, 0.07828216, 0.06950415,\n",
      "       0.09789223, 0.06258907, 0.08274627, 0.07925527, 0.09065644,\n",
      "       0.06765993, 0.07596146, 0.10774773, 0.07552363, 0.06518466,\n",
      "       0.06642937, 0.05597059, 0.06816091, 0.07079764, 0.06065401])}, 'ATTACH::22::best_val_loss': 0.05597059470726483, 'ATTACH::23::history': {'train_accs': array([0.37905814, 0.44757071, 0.47421449, 0.50123144, 0.54466751,\n",
      "       0.58347638, 0.62519591, 0.65407866, 0.6843048 , 0.69848496,\n",
      "       0.73139787, 0.76199716, 0.7924472 , 0.81610568, 0.83222629,\n",
      "       0.86028808, 0.87319949, 0.88536458, 0.90282857, 0.91365027,\n",
      "       0.92185984, 0.92603926, 0.93663706, 0.94492126, 0.94910068,\n",
      "       0.9475334 , 0.96051944, 0.9583551 , 0.96462423, 0.96828122,\n",
      "       0.97163967, 0.97201284, 0.97208747, 0.97350549, 0.97604299,\n",
      "       0.97880439, 0.97872976, 0.97693858, 0.97634152, 0.9832077 ,\n",
      "       0.98261064, 0.98380476, 0.98305844, 0.9832077 , 0.97828196,\n",
      "       0.98335697, 0.98350623, 0.98507351, 0.98283454]), 'train_losses': array([28.71541604,  8.59339245,  5.17065787,  3.66884032,  2.83443902,\n",
      "        2.19673091,  1.78124326,  1.5996376 ,  1.30819929,  1.18225559,\n",
      "        1.02376867,  0.87220275,  0.71519855,  0.62033238,  0.57025947,\n",
      "        0.45743009,  0.42289923,  0.38125381,  0.30722086,  0.27214992,\n",
      "        0.24540819,  0.2319121 ,  0.18550786,  0.17725674,  0.16202323,\n",
      "        0.1599456 ,  0.12767091,  0.13463116,  0.10778311,  0.09585586,\n",
      "        0.09064115,  0.09062675,  0.09229856,  0.08402819,  0.07134761,\n",
      "        0.06965997,  0.06193847,  0.07169666,  0.07263234,  0.05589937,\n",
      "        0.05016426,  0.04915672,  0.05547752,  0.05187101,  0.06790421,\n",
      "        0.04887185,  0.05186656,  0.04621536,  0.05964622]), 'val_accs': array([0.50507463, 0.5358209 , 0.59791045, 0.51134328, 0.60686567,\n",
      "       0.69313433, 0.69432836, 0.73701493, 0.7919403 , 0.74179104,\n",
      "       0.82      , 0.85402985, 0.85104478, 0.88447761, 0.8719403 ,\n",
      "       0.87880597, 0.91850746, 0.92686567, 0.93820896, 0.94358209,\n",
      "       0.92179104, 0.94895522, 0.95313433, 0.96447761, 0.96626866,\n",
      "       0.95074627, 0.97134328, 0.96835821, 0.97701493, 0.97373134,\n",
      "       0.96537313, 0.96776119, 0.97253731, 0.97880597, 0.97641791,\n",
      "       0.97761194, 0.97820896, 0.97164179, 0.98179104, 0.98059701,\n",
      "       0.98059701, 0.98149254, 0.97910448, 0.98179104, 0.98238806,\n",
      "       0.9841791 , 0.97910448, 0.98029851, 0.9641791 ]), 'val_losses': array([4.85743844, 2.12483406, 2.00674261, 1.85062891, 1.12166632,\n",
      "       1.27308952, 0.775546  , 0.86912894, 0.53805736, 0.6481852 ,\n",
      "       0.48910269, 0.40232108, 0.39537107, 0.31841159, 0.39449596,\n",
      "       0.35788172, 0.21110955, 0.20743744, 0.18237462, 0.1696013 ,\n",
      "       0.22232477, 0.16137827, 0.15056712, 0.11861704, 0.12011651,\n",
      "       0.14812907, 0.10166716, 0.1124081 , 0.08088992, 0.08299819,\n",
      "       0.13523185, 0.10408578, 0.1020861 , 0.08025052, 0.10580312,\n",
      "       0.09093445, 0.08330406, 0.09133488, 0.06212701, 0.06958449,\n",
      "       0.06844213, 0.09119161, 0.09735592, 0.06765274, 0.07613003,\n",
      "       0.06355016, 0.07448952, 0.06924793, 0.12827787])}, 'ATTACH::23::best_val_loss': 0.06212700821125685, 'ATTACH::24::history': {'train_accs': array([0.35293679, 0.40092544, 0.44518248, 0.46921412, 0.49369356,\n",
      "       0.52459139, 0.54414509, 0.55466826, 0.57295321, 0.59452198,\n",
      "       0.60713486, 0.6203448 , 0.63445033, 0.65273528, 0.67393089,\n",
      "       0.68497649, 0.70393313, 0.7172177 , 0.7344578 , 0.74475707,\n",
      "       0.76401224, 0.77655049, 0.79789537, 0.81938951, 0.82894246,\n",
      "       0.84401821, 0.85625793, 0.87379655, 0.88543921, 0.89469363,\n",
      "       0.90238077, 0.90962012, 0.90932159, 0.92320322, 0.92178521,\n",
      "       0.92753191, 0.93745802, 0.93835361, 0.94141354, 0.94768266,\n",
      "       0.95148892, 0.95753414, 0.95596686, 0.95992238, 0.95984775,\n",
      "       0.96417643, 0.96880364, 0.96857974, 0.97126651, 0.97022166,\n",
      "       0.97372938, 0.97372938, 0.9721621 , 0.97671468, 0.97581909,\n",
      "       0.97641615, 0.97678931, 0.98246138, 0.98358086, 0.98313307]), 'train_losses': array([40.78140581, 13.38846009,  8.1829619 ,  6.02999896,  4.60654777,\n",
      "        3.59459945,  3.02773717,  2.60762114,  2.30531753,  1.93674901,\n",
      "        1.73663878,  1.54067759,  1.38967667,  1.24950105,  1.10409171,\n",
      "        1.04853748,  0.97676466,  0.91333389,  0.8063772 ,  0.76865872,\n",
      "        0.70336313,  0.67871461,  0.59159696,  0.5294827 ,  0.49577409,\n",
      "        0.46150314,  0.41421272,  0.38214877,  0.33955773,  0.30920221,\n",
      "        0.29202833,  0.2720058 ,  0.2647946 ,  0.23477016,  0.23777044,\n",
      "        0.21683657,  0.19075155,  0.18090505,  0.1740108 ,  0.15541011,\n",
      "        0.14802088,  0.12791297,  0.13219007,  0.12036625,  0.11841013,\n",
      "        0.10899298,  0.10179923,  0.09650407,  0.09600171,  0.08912676,\n",
      "        0.08018791,  0.08429302,  0.08427921,  0.07135717,  0.0718122 ,\n",
      "        0.07251725,  0.07332258,  0.05319427,  0.05198406,  0.05135537]), 'val_accs': array([0.44298507, 0.45940299, 0.59134328, 0.61850746, 0.62208955,\n",
      "       0.65701493, 0.65313433, 0.67373134, 0.69164179, 0.65880597,\n",
      "       0.6561194 , 0.69850746, 0.69970149, 0.72298507, 0.70149254,\n",
      "       0.72656716, 0.77014925, 0.73373134, 0.77432836, 0.7761194 ,\n",
      "       0.79104478, 0.77552239, 0.85283582, 0.84179104, 0.85373134,\n",
      "       0.85223881, 0.86865672, 0.89910448, 0.90179104, 0.90268657,\n",
      "       0.89164179, 0.93014925, 0.91850746, 0.92835821, 0.91432836,\n",
      "       0.91522388, 0.93671642, 0.94477612, 0.91940299, 0.94      ,\n",
      "       0.94955224, 0.94358209, 0.9519403 , 0.9561194 , 0.95223881,\n",
      "       0.95880597, 0.95671642, 0.96298507, 0.95462687, 0.9480597 ,\n",
      "       0.96328358, 0.9680597 , 0.9480597 , 0.93492537, 0.97014925,\n",
      "       0.97104478, 0.96298507, 0.97791045, 0.94925373, 0.96865672]), 'val_losses': array([9.15113813, 3.03564755, 2.0018136 , 1.54871109, 1.36385669,\n",
      "       1.50896617, 1.30080343, 1.30373794, 1.10806818, 1.1760682 ,\n",
      "       1.22980024, 0.92576913, 1.08061157, 0.95174289, 0.9540089 ,\n",
      "       0.8235554 , 0.65591904, 0.72620569, 0.63382249, 0.55547052,\n",
      "       0.51645462, 0.59578808, 0.38940848, 0.4434191 , 0.40216696,\n",
      "       0.39814301, 0.3900194 , 0.27212918, 0.28561669, 0.26812814,\n",
      "       0.28953546, 0.18801325, 0.21751237, 0.19147882, 0.22838906,\n",
      "       0.22090538, 0.17729835, 0.15300723, 0.23142481, 0.15971807,\n",
      "       0.15081523, 0.15062134, 0.13844374, 0.13765098, 0.14898982,\n",
      "       0.10649924, 0.14016579, 0.09823836, 0.12734704, 0.14273045,\n",
      "       0.10706521, 0.09941953, 0.17554502, 0.19994296, 0.0736126 ,\n",
      "       0.07327241, 0.09655418, 0.06631302, 0.15825679, 0.10127619])}, 'ATTACH::24::best_val_loss': 0.06631301965397686, 'ATTACH::25::history': {'train_accs': array([0.38316292, 0.43854019, 0.45227256, 0.4763042 , 0.49361893,\n",
      "       0.50847078, 0.5292186 , 0.54384656, 0.54720502, 0.57078886,\n",
      "       0.59877603, 0.62235988, 0.65407866, 0.6815434 , 0.70132099,\n",
      "       0.72483021, 0.75125009, 0.79237257, 0.81737443, 0.83536085,\n",
      "       0.85775058, 0.87946862, 0.89514143, 0.90850063, 0.9226808 ,\n",
      "       0.92439734, 0.93163669, 0.94417494, 0.9390253 , 0.95245914,\n",
      "       0.9528323 , 0.95865363, 0.96029554, 0.96096724, 0.96596761,\n",
      "       0.9694007 , 0.96656467, 0.97178894, 0.97171431, 0.97156504,\n",
      "       0.97484887, 0.97417718, 0.97634152, 0.97619225, 0.97969998,\n",
      "       0.97955071, 0.97753564, 0.98074483, 0.98373013, 0.98014777,\n",
      "       0.98350623, 0.98261064, 0.98477498, 0.98671543, 0.98701396,\n",
      "       0.98708859, 0.98753638, 0.9832077 , 0.98664079, 0.98887977]), 'train_losses': array([23.08689512,  6.67912501,  4.37128854,  3.22893925,  2.54134868,\n",
      "        2.20372189,  1.82410152,  1.53850454,  1.49785438,  1.24423476,\n",
      "        1.14037661,  1.05911422,  0.91499702,  0.84495576,  0.77702362,\n",
      "        0.74387614,  0.65961987,  0.56263437,  0.50282849,  0.46471232,\n",
      "        0.39449174,  0.33499518,  0.298232  ,  0.26217264,  0.22571222,\n",
      "        0.22530045,  0.20155422,  0.17526408,  0.17856923,  0.14679922,\n",
      "        0.1399415 ,  0.1273363 ,  0.12347895,  0.11355317,  0.11066437,\n",
      "        0.09569118,  0.10282268,  0.08780722,  0.08418443,  0.08739701,\n",
      "        0.07766587,  0.07422448,  0.07160498,  0.06783753,  0.06175062,\n",
      "        0.06061641,  0.07030523,  0.06077131,  0.05226155,  0.06315783,\n",
      "        0.05168505,  0.05210262,  0.04640017,  0.04052514,  0.04000501,\n",
      "        0.04394028,  0.03796913,  0.05054475,  0.03968202,  0.03877281]), 'val_accs': array([0.48      , 0.53104478, 0.60865672, 0.59552239, 0.54955224,\n",
      "       0.62268657, 0.5719403 , 0.57223881, 0.62268657, 0.70208955,\n",
      "       0.67253731, 0.7119403 , 0.73014925, 0.7558209 , 0.67343284,\n",
      "       0.79820896, 0.83641791, 0.84      , 0.87552239, 0.86955224,\n",
      "       0.88358209, 0.89671642, 0.90716418, 0.92776119, 0.93731343,\n",
      "       0.93910448, 0.95044776, 0.95373134, 0.94567164, 0.95522388,\n",
      "       0.95014925, 0.94686567, 0.95462687, 0.95462687, 0.96925373,\n",
      "       0.96059701, 0.96865672, 0.9680597 , 0.95223881, 0.97343284,\n",
      "       0.97223881, 0.97134328, 0.97731343, 0.97552239, 0.97492537,\n",
      "       0.96597015, 0.97253731, 0.97522388, 0.97820896, 0.97820896,\n",
      "       0.97432836, 0.97552239, 0.97850746, 0.97731343, 0.97731343,\n",
      "       0.96776119, 0.96985075, 0.98      , 0.97820896, 0.98179104]), 'val_losses': array([2.94232478, 1.76053147, 1.36264642, 1.44272472, 1.34926492,\n",
      "       1.13533671, 1.05849193, 0.84977815, 0.96200784, 0.65386391,\n",
      "       0.70913271, 0.68079141, 0.6842246 , 0.57658919, 0.66924855,\n",
      "       0.47287004, 0.45156816, 0.42878415, 0.36218903, 0.3481684 ,\n",
      "       0.3205394 , 0.28389822, 0.26105376, 0.20895547, 0.19542744,\n",
      "       0.18307962, 0.15368918, 0.14471425, 0.16179423, 0.13694002,\n",
      "       0.14525237, 0.15890478, 0.14049625, 0.14093102, 0.10294731,\n",
      "       0.12064404, 0.09174536, 0.10062619, 0.15811057, 0.09039119,\n",
      "       0.09593561, 0.10048328, 0.07938965, 0.07885383, 0.08371454,\n",
      "       0.11718255, 0.08728413, 0.07918659, 0.0810153 , 0.07349849,\n",
      "       0.08504054, 0.08465481, 0.06591477, 0.08032128, 0.0707754 ,\n",
      "       0.10866975, 0.10120861, 0.06608194, 0.07803029, 0.07554762])}, 'ATTACH::25::best_val_loss': 0.06591477169696965, 'ATTACH::26::history': {'train_accs': array([0.41868796, 0.47555788, 0.50884394, 0.53302485, 0.56563923,\n",
      "       0.58310322, 0.59795507, 0.63549519, 0.65892977, 0.68460333,\n",
      "       0.69870886, 0.71602358, 0.73908501, 0.7452795 , 0.75580267,\n",
      "       0.77490858, 0.77296813, 0.78199866, 0.79341742, 0.80005971,\n",
      "       0.80341817, 0.81379207, 0.80797074, 0.81722517, 0.8230465 ,\n",
      "       0.83453989, 0.83289798, 0.84864542, 0.8502127 , 0.8534219 ,\n",
      "       0.86021345, 0.84998881, 0.86118367, 0.86245242]), 'train_losses': array([12.29360118,  3.24617678,  1.86890423,  1.38639854,  1.1309849 ,\n",
      "        0.94799345,  0.86278729,  0.76705032,  0.71465313,  0.66733531,\n",
      "        0.64310987,  0.60865376,  0.57492959,  0.56839671,  0.54486679,\n",
      "        0.5069426 ,  0.52184235,  0.4816886 ,  0.4729312 ,  0.45950001,\n",
      "        0.44468868,  0.41768941,  0.44045163,  0.42017875,  0.41386477,\n",
      "        0.3885076 ,  0.38853626,  0.35689975,  0.35858944,  0.35006106,\n",
      "        0.3313176 ,  0.36362112,  0.34091032,  0.33772142]), 'val_accs': array([0.54149254, 0.56597015, 0.55552239, 0.68776119, 0.60985075,\n",
      "       0.60447761, 0.62776119, 0.65940299, 0.72985075, 0.74328358,\n",
      "       0.74716418, 0.76865672, 0.71552239, 0.77552239, 0.7561194 ,\n",
      "       0.78925373, 0.80597015, 0.7241791 , 0.72358209, 0.78089552,\n",
      "       0.72925373, 0.69641791, 0.70268657, 0.83164179, 0.82208955,\n",
      "       0.75522388, 0.71223881, 0.78567164, 0.76149254, 0.80567164,\n",
      "       0.82447761, 0.74686567, 0.77223881, 0.78029851]), 'val_losses': array([1.77368084, 1.20941401, 0.92021558, 0.78211565, 1.06314148,\n",
      "       0.80262119, 0.84675084, 0.6547681 , 0.59288938, 0.51784313,\n",
      "       0.56562413, 0.50593944, 0.57226425, 0.50238398, 0.49885389,\n",
      "       0.50198762, 0.48161143, 0.55818606, 0.53710905, 0.52043597,\n",
      "       0.55405397, 0.5565772 , 0.61977318, 0.35575492, 0.51955091,\n",
      "       0.57867425, 0.74050103, 0.54622924, 0.5764577 , 0.43754721,\n",
      "       0.4262029 , 0.65779492, 0.59750166, 0.4993852 ])}, 'ATTACH::26::best_val_loss': 0.3557549183048419, 'ATTACH::27::history': {'train_accs': array([0.43413688, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718]), 'train_losses': array([12.85079769,  1.24379701,  1.24243652,  1.24208727,  1.24201412,\n",
      "        1.2419575 ,  1.24196577,  1.24201701,  1.24192373,  1.24198953,\n",
      "        1.24186578,  1.24197483,  1.24192444,  1.24196947,  1.24190003,\n",
      "        1.24183381,  1.24189921,  1.24195747,  1.24190899,  1.24191276,\n",
      "        1.24191877,  1.24196036,  1.2418894 ,  1.24189391,  1.2418152 ,\n",
      "        1.2418967 ,  1.24190753,  1.24190788,  1.24185926,  1.24188472,\n",
      "        1.24186018,  1.24181225,  1.24184191,  1.24200302,  1.24192337,\n",
      "        1.2418511 ,  1.24187103,  1.24181861,  1.2417962 ,  1.24191274,\n",
      "        1.24186479,  1.24186005,  1.24179654,  1.24187964,  1.24186151,\n",
      "        1.24187171,  1.24184717,  1.24190328,  1.24181021,  1.24179796,\n",
      "        1.24182313,  1.2418477 ,  1.24193339,  1.24177347,  1.24190795,\n",
      "        1.24177998,  1.24187134,  1.24185012,  1.24187031,  1.24179769]), 'val_accs': array([0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955]), 'val_losses': array([1.24575648, 1.24243572, 1.24191754, 1.24176798, 1.24174923,\n",
      "       1.24170101, 1.24168053, 1.24169057, 1.24168408, 1.24167178,\n",
      "       1.24166784, 1.24166462, 1.24165983, 1.24166457, 1.24165936,\n",
      "       1.24165574, 1.24165201, 1.24165363, 1.24164963, 1.24166024,\n",
      "       1.24165448, 1.24164328, 1.24163983, 1.24163869, 1.24165852,\n",
      "       1.24163295, 1.24163184, 1.24162856, 1.24163973, 1.24162451,\n",
      "       1.24162204, 1.24161998, 1.2416305 , 1.24164706, 1.24161434,\n",
      "       1.24161261, 1.24160988, 1.24160725, 1.24160853, 1.24160935,\n",
      "       1.24160236, 1.24164466, 1.24159705, 1.24159687, 1.2415928 ,\n",
      "       1.24160802, 1.24159544, 1.24158757, 1.24158781, 1.24158463,\n",
      "       1.24157836, 1.24157419, 1.24157689, 1.24159496, 1.24157219,\n",
      "       1.24156617, 1.24156496, 1.24156674, 1.24155978, 1.241556  ])}, 'ATTACH::27::best_val_loss': 1.2415559992861391, 'ATTACH::28::history': {'train_accs': array([0.31136652, 0.35532502, 0.39062617, 0.41644899, 0.43503247,\n",
      "       0.43861482, 0.45488469, 0.46339279, 0.46772147, 0.48279722,\n",
      "       0.50921711, 0.52854691, 0.53720427, 0.55504142, 0.5623554 ,\n",
      "       0.5816852 , 0.59190984, 0.6095231 , 0.62221061, 0.62198672,\n",
      "       0.62989775, 0.63795806, 0.65504888, 0.65989999, 0.66825883,\n",
      "       0.68199119, 0.69035003, 0.69863423, 0.70870961, 0.72117322,\n",
      "       0.72908426, 0.74908575, 0.75602657, 0.77729681, 0.79132771,\n",
      "       0.79998507, 0.81916561, 0.82961415, 0.84252556, 0.85640719,\n",
      "       0.86551235, 0.88066274, 0.87999104, 0.89693261, 0.90073886,\n",
      "       0.90603776, 0.91588925, 0.91939697, 0.92118815, 0.92663632,\n",
      "       0.93745802, 0.93909993, 0.94507053, 0.94589148, 0.9500709 ,\n",
      "       0.94589148, 0.95350399, 0.95111575, 0.95760878, 0.95820584]), 'train_losses': array([58.16102053, 25.02138186, 14.80362133, 10.78320184,  8.34783609,\n",
      "        7.08251651,  5.87426599,  5.01303614,  4.58326345,  4.06186038,\n",
      "        3.61061337,  3.18711441,  2.95830892,  2.60354302,  2.40167509,\n",
      "        2.16956467,  2.03950598,  1.84952475,  1.75453268,  1.66764281,\n",
      "        1.55396296,  1.54304028,  1.3726953 ,  1.32294054,  1.25203525,\n",
      "        1.12474719,  1.07184342,  1.02608675,  0.96313418,  0.94466344,\n",
      "        0.91092074,  0.81198418,  0.78139036,  0.72469165,  0.68871321,\n",
      "        0.6645774 ,  0.58328137,  0.5490938 ,  0.50268968,  0.4600253 ,\n",
      "        0.41837646,  0.38037528,  0.37537766,  0.33423162,  0.32411263,\n",
      "        0.29797003,  0.28343133,  0.25889535,  0.25132351,  0.23373316,\n",
      "        0.19826155,  0.19332396,  0.16993737,  0.17562928,  0.16048771,\n",
      "        0.16855139,  0.15210291,  0.15976675,  0.1394807 ,  0.1363313 ]), 'val_accs': array([0.45313433, 0.41044776, 0.30776119, 0.47910448, 0.5       ,\n",
      "       0.53492537, 0.52985075, 0.53701493, 0.56955224, 0.58238806,\n",
      "       0.60298507, 0.60865672, 0.60179104, 0.56238806, 0.61402985,\n",
      "       0.67074627, 0.58895522, 0.70537313, 0.72029851, 0.68686567,\n",
      "       0.70298507, 0.69313433, 0.67940299, 0.69761194, 0.77074627,\n",
      "       0.78895522, 0.75044776, 0.73522388, 0.74059701, 0.77820896,\n",
      "       0.77701493, 0.70537313, 0.79014925, 0.81850746, 0.78567164,\n",
      "       0.86567164, 0.82895522, 0.89313433, 0.8919403 , 0.86537313,\n",
      "       0.90328358, 0.91492537, 0.90716418, 0.90208955, 0.89910448,\n",
      "       0.92776119, 0.93432836, 0.94477612, 0.93402985, 0.92925373,\n",
      "       0.93761194, 0.93432836, 0.95074627, 0.95313433, 0.94119403,\n",
      "       0.96358209, 0.95552239, 0.96208955, 0.94208955, 0.95880597]), 'val_losses': array([16.16947106,  5.86735136,  3.68490498,  2.58883712,  2.50392303,\n",
      "        2.21195792,  2.5733538 ,  2.04388025,  1.84833872,  1.30527387,\n",
      "        1.40248717,  1.24596152,  1.34029421,  1.4231743 ,  1.37874649,\n",
      "        1.06237462,  1.13864464,  0.95444016,  0.84526257,  0.8523697 ,\n",
      "        0.82957906,  0.83830912,  0.92593196,  0.73378537,  0.69545723,\n",
      "        0.60313166,  0.60023395,  0.68908737,  0.64521441,  0.63420395,\n",
      "        0.63640974,  0.83816592,  0.58935595,  0.54268344,  0.64413526,\n",
      "        0.40859409,  0.56793038,  0.32246735,  0.33676139,  0.42471291,\n",
      "        0.29985648,  0.28606025,  0.30814849,  0.32930673,  0.30747126,\n",
      "        0.223038  ,  0.19388922,  0.17051138,  0.20185574,  0.21812965,\n",
      "        0.18967603,  0.19989005,  0.16865966,  0.14732588,  0.18255434,\n",
      "        0.13592961,  0.14053415,  0.12904996,  0.20017661,  0.14041   ])}, 'ATTACH::28::best_val_loss': 0.12904996206511313, 'ATTACH::29::history': {'train_accs': array([0.4378685 , 0.48652885, 0.51891932, 0.53593552, 0.55257855,\n",
      "       0.55735503, 0.55966863, 0.57153519, 0.57369953, 0.58817822,\n",
      "       0.57578924, 0.59623853, 0.60803045, 0.60265691, 0.61049332,\n",
      "       0.61452347, 0.61586686, 0.61691171, 0.62235988, 0.62071796,\n",
      "       0.6228823 , 0.62347936, 0.63325621, 0.61870289, 0.62474812,\n",
      "       0.63273379, 0.63631614, 0.63273379, 0.63467423, 0.64019703,\n",
      "       0.64071946, 0.64467498, 0.64064482, 0.6454213 , 0.64601836,\n",
      "       0.64236137, 0.66191507, 0.66251213, 0.65840734, 0.67109486,\n",
      "       0.66669154, 0.66348235, 0.66721397, 0.66900515, 0.67251287,\n",
      "       0.6757967 , 0.68027465, 0.67937906, 0.68191656, 0.67781178,\n",
      "       0.68102097, 0.67788641, 0.68169266, 0.6840809 , 0.69475334,\n",
      "       0.68885738, 0.67908053, 0.69072319, 0.69505187, 0.69266363]), 'train_losses': array([5.18691447, 1.00190756, 0.93008076, 0.87844697, 0.85038814,\n",
      "       0.84563633, 0.83129832, 0.81010557, 0.80508715, 0.7915251 ,\n",
      "       0.79482248, 0.78522073, 0.76735412, 0.77294721, 0.77696794,\n",
      "       0.77599294, 0.76407538, 0.76618013, 0.7477401 , 0.77045931,\n",
      "       0.75418385, 0.75266819, 0.73888994, 0.75847367, 0.74715422,\n",
      "       0.72938251, 0.73642919, 0.73733383, 0.72670137, 0.72453071,\n",
      "       0.7260874 , 0.72559568, 0.71718273, 0.7222062 , 0.72226158,\n",
      "       0.73494909, 0.71226563, 0.70433469, 0.7044813 , 0.69434856,\n",
      "       0.70492011, 0.70323024, 0.70397323, 0.70091019, 0.69419021,\n",
      "       0.68348731, 0.68771124, 0.68413292, 0.686503  , 0.68502959,\n",
      "       0.68347108, 0.68922068, 0.67872977, 0.67476285, 0.67161421,\n",
      "       0.66656841, 0.68798984, 0.6700876 , 0.67022819, 0.66872961]), 'val_accs': array([0.55701493, 0.58059701, 0.56567164, 0.63134328, 0.66029851,\n",
      "       0.66447761, 0.67641791, 0.65910448, 0.6680597 , 0.66985075,\n",
      "       0.69850746, 0.60985075, 0.66835821, 0.72865672, 0.66059701,\n",
      "       0.67731343, 0.63253731, 0.7241791 , 0.70746269, 0.72268657,\n",
      "       0.70089552, 0.70447761, 0.73492537, 0.70507463, 0.74626866,\n",
      "       0.74955224, 0.71522388, 0.68895522, 0.74059701, 0.70179104,\n",
      "       0.69074627, 0.75701493, 0.72925373, 0.73940299, 0.76089552,\n",
      "       0.74507463, 0.72895522, 0.78      , 0.78089552, 0.72537313,\n",
      "       0.72597015, 0.75492537, 0.72537313, 0.78149254, 0.74776119,\n",
      "       0.78089552, 0.78507463, 0.77641791, 0.79880597, 0.77701493,\n",
      "       0.77820896, 0.76059701, 0.76119403, 0.79701493, 0.80119403,\n",
      "       0.75313433, 0.78537313, 0.78089552, 0.78119403, 0.80567164]), 'val_losses': array([0.94736421, 0.85783433, 0.79094195, 0.74921062, 0.74874253,\n",
      "       0.739192  , 0.6986026 , 0.67651041, 0.67955343, 0.67339418,\n",
      "       0.66464193, 0.67257101, 0.63835711, 0.64692204, 0.63240711,\n",
      "       0.65207521, 0.65166438, 0.64421904, 0.66232609, 0.64347   ,\n",
      "       0.61865626, 0.62121732, 0.62047476, 0.61110836, 0.61400257,\n",
      "       0.59587877, 0.58291358, 0.5906281 , 0.60878161, 0.61502289,\n",
      "       0.59842823, 0.5890037 , 0.56590967, 0.56040795, 0.58413489,\n",
      "       0.58701325, 0.60487489, 0.54806562, 0.54106924, 0.56479181,\n",
      "       0.58482717, 0.53803726, 0.53282834, 0.53145459, 0.54137068,\n",
      "       0.53863874, 0.5472772 , 0.57273287, 0.55531416, 0.5122971 ,\n",
      "       0.53362437, 0.5524718 , 0.5280621 , 0.51536207, 0.51681469,\n",
      "       0.56138644, 0.51381577, 0.52627658, 0.50937756, 0.53079729])}, 'ATTACH::29::best_val_loss': 0.5093775605443698}\n",
      "{'ATTACH::0::history': {'train_accs': array([0.38353608, 0.41309053, 0.41353832, 0.42555415, 0.4539891 ,\n",
      "       0.48973804, 0.51205314, 0.52757668, 0.52638257, 0.52227778,\n",
      "       0.52832301, 0.53615941, 0.53489066, 0.52817374, 0.54056273,\n",
      "       0.54287633, 0.55310098, 0.54713038, 0.55459363, 0.56869916,\n",
      "       0.57571461, 0.57392343, 0.57952086, 0.5791477 , 0.57175909,\n",
      "       0.58004329, 0.56198224, 0.56675871, 0.56496753, 0.55601164,\n",
      "       0.5593701 , 0.57556534, 0.57160982, 0.57787895]), 'train_losses': array([3.61754116, 1.05234476, 1.02903649, 1.02836193, 0.98876198,\n",
      "       0.99317278, 0.94524507, 0.90336654, 0.91330849, 0.89497974,\n",
      "       0.88680927, 0.88115231, 0.87954461, 0.88440006, 0.87443474,\n",
      "       0.87389708, 0.86365447, 0.86209018, 0.85348141, 0.84054617,\n",
      "       0.82961943, 0.82559944, 0.82558856, 0.81876661, 0.83159078,\n",
      "       0.83277395, 0.86209246, 0.84856428, 0.85722289, 0.86444664,\n",
      "       0.85357138, 0.83972071, 0.847957  , 0.83419034]), 'val_accs': array([0.52507463, 0.50776119, 0.51731343, 0.48179104, 0.53791045,\n",
      "       0.56865672, 0.59731343, 0.64835821, 0.6041791 , 0.58835821,\n",
      "       0.67970149, 0.61014925, 0.67253731, 0.67104478, 0.65791045,\n",
      "       0.68477612, 0.68626866, 0.65761194, 0.65522388, 0.67134328,\n",
      "       0.65850746, 0.67134328, 0.68298507, 0.72477612, 0.70955224,\n",
      "       0.6519403 , 0.67104478, 0.68626866, 0.65462687, 0.57761194,\n",
      "       0.65014925, 0.68507463, 0.70238806, 0.69731343]), 'val_losses': array([0.95054452, 0.91986043, 0.90667448, 0.91344204, 0.96032712,\n",
      "       0.90060722, 0.79709963, 0.72501342, 0.80788382, 0.71892088,\n",
      "       0.71230253, 0.73493972, 0.73446967, 0.7051649 , 0.78955207,\n",
      "       0.70856086, 0.68768083, 0.67768089, 0.71784549, 0.66743919,\n",
      "       0.70006979, 0.6674263 , 0.68154348, 0.6065276 , 0.63387919,\n",
      "       0.76527268, 0.70894103, 0.71576041, 0.71288228, 0.74144136,\n",
      "       0.70703833, 0.66451954, 0.672956  , 0.64674354])}, 'ATTACH::0::best_val_loss': 0.6065276001104668, 'ATTACH::1::history': {'train_accs': array([0.28114038, 0.30435107, 0.35808642, 0.38226733, 0.39585044,\n",
      "       0.4074931 , 0.4213001 , 0.41182178, 0.4151056 , 0.43033062,\n",
      "       0.4180909 , 0.42324054, 0.42331517, 0.42047914, 0.42824091,\n",
      "       0.42995746, 0.43055452, 0.43167401, 0.44286887, 0.43846556,\n",
      "       0.43801776, 0.43406224, 0.43936115, 0.45376521, 0.4408538 ,\n",
      "       0.45018285, 0.45406374, 0.4512277 , 0.46115382, 0.46622882,\n",
      "       0.47130383, 0.47481155, 0.47018434, 0.47660273, 0.48406598,\n",
      "       0.49302187, 0.48555862, 0.49548474, 0.48615568, 0.48488693,\n",
      "       0.4958579 , 0.48667811, 0.48832002, 0.50018658, 0.49749981,\n",
      "       0.50152996, 0.50361967, 0.49682812, 0.50839615, 0.50033585,\n",
      "       0.4988432 , 0.52003881, 0.51011269, 0.51660572, 0.52548698,\n",
      "       0.5177252 , 0.51809837, 0.52414359, 0.51712814, 0.51959101]), 'train_losses': array([8.5105104 , 1.33153691, 1.20746924, 1.15119045, 1.1153624 ,\n",
      "       1.07647584, 1.06146577, 1.04212403, 1.03998628, 1.02101438,\n",
      "       1.01433443, 1.00808551, 0.99652683, 1.00011312, 0.99710966,\n",
      "       0.99666086, 0.99002365, 0.98418778, 0.98782887, 0.97522191,\n",
      "       0.97693151, 0.9798287 , 0.97014101, 0.96401964, 0.96249468,\n",
      "       0.95959964, 0.95017893, 0.95466394, 0.94769473, 0.94856542,\n",
      "       0.93984367, 0.93936974, 0.94451138, 0.94420284, 0.93278962,\n",
      "       0.92295425, 0.92686393, 0.92264774, 0.92872887, 0.92116483,\n",
      "       0.90898702, 0.9177027 , 0.90967803, 0.90408013, 0.90665421,\n",
      "       0.9048983 , 0.90146925, 0.90775998, 0.88583925, 0.89716971,\n",
      "       0.89782211, 0.88537882, 0.88472538, 0.88286312, 0.87523394,\n",
      "       0.87373101, 0.87585975, 0.86917847, 0.87845261, 0.86858873]), 'val_accs': array([0.20358209, 0.22895522, 0.37104478, 0.41791045, 0.39970149,\n",
      "       0.44208955, 0.43074627, 0.43910448, 0.4       , 0.43522388,\n",
      "       0.45014925, 0.45313433, 0.44059701, 0.39044776, 0.45014925,\n",
      "       0.45402985, 0.37014925, 0.45343284, 0.46447761, 0.43074627,\n",
      "       0.47134328, 0.45014925, 0.45402985, 0.39402985, 0.43432836,\n",
      "       0.49462687, 0.45074627, 0.49014925, 0.4441791 , 0.5041791 ,\n",
      "       0.42925373, 0.51253731, 0.54656716, 0.55283582, 0.48059701,\n",
      "       0.53164179, 0.51253731, 0.48895522, 0.49343284, 0.49940299,\n",
      "       0.5       , 0.55313433, 0.53432836, 0.50447761, 0.52746269,\n",
      "       0.5719403 , 0.59850746, 0.52208955, 0.53134328, 0.48238806,\n",
      "       0.5480597 , 0.57134328, 0.49552239, 0.50776119, 0.52716418,\n",
      "       0.55223881, 0.57283582, 0.63044776, 0.46268657, 0.56179104]), 'val_losses': array([1.39853904, 1.17516884, 1.09015238, 1.06503098, 1.03694041,\n",
      "       1.03145848, 1.00063041, 0.99437775, 0.9780919 , 0.96600166,\n",
      "       0.95936027, 0.96630548, 0.94728202, 0.96543872, 0.94413184,\n",
      "       0.950452  , 0.95521985, 0.9534064 , 0.9321876 , 0.94720286,\n",
      "       0.94240396, 0.93538947, 0.93547026, 0.92411054, 0.92860192,\n",
      "       0.91039788, 0.90904915, 0.90630039, 0.90344179, 0.90380036,\n",
      "       0.93481143, 0.90309853, 0.91069018, 0.88573683, 0.87667311,\n",
      "       0.87049722, 0.8624542 , 0.90282685, 0.8700728 , 0.84811605,\n",
      "       0.86067047, 0.84659094, 0.84133136, 0.86167344, 0.84438435,\n",
      "       0.85645409, 0.86201082, 0.84301158, 0.83368557, 0.89541968,\n",
      "       0.82266847, 0.82201157, 0.82195878, 0.83086347, 0.80564083,\n",
      "       0.8077829 , 0.79452121, 0.81339023, 0.86611704, 0.79800435])}, 'ATTACH::1::best_val_loss': 0.794521210976501, 'ATTACH::2::history': {'train_accs': array([0.44033137, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718]), 'train_losses': array([5.05666358, 1.25554481, 1.24856931, 1.24534593, 1.24371499,\n",
      "       1.24291935, 1.24248984, 1.2423114 , 1.24222569, 1.24210423,\n",
      "       1.24205257, 1.24206716, 1.24208133, 1.24202496, 1.24216185,\n",
      "       1.2419318 , 1.24204589, 1.24201752, 1.24202261, 1.2420608 ,\n",
      "       1.24203527, 1.24202727, 1.24199178, 1.24204086, 1.24185749,\n",
      "       1.2420862 , 1.24205649, 1.24200698, 1.24202332, 1.24202124,\n",
      "       1.24203539, 1.2420368 , 1.2419634 , 1.24205146, 1.24204027,\n",
      "       1.24202273, 1.24202204, 1.24203666, 1.24192476, 1.2420984 ,\n",
      "       1.24201782, 1.24205039, 1.24206398, 1.24204963, 1.24206546,\n",
      "       1.24198645, 1.24202551, 1.24204903, 1.24201651, 1.24205651,\n",
      "       1.2419859 , 1.24197847, 1.2419948 , 1.24195184, 1.24206822,\n",
      "       1.24198223, 1.2419629 , 1.24194192, 1.24201941, 1.24202585]), 'val_accs': array([0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955]), 'val_losses': array([1.26182877, 1.25085103, 1.24630097, 1.24406132, 1.24294612,\n",
      "       1.24243047, 1.24210117, 1.24201091, 1.24187387, 1.24183823,\n",
      "       1.24178671, 1.24178509, 1.24176546, 1.2417729 , 1.24176342,\n",
      "       1.24187323, 1.24180716, 1.24180339, 1.24178221, 1.24175961,\n",
      "       1.24175748, 1.24178162, 1.24197841, 1.24180385, 1.24203943,\n",
      "       1.24186805, 1.24176257, 1.241753  , 1.24177685, 1.2417858 ,\n",
      "       1.24177354, 1.24179206, 1.24182108, 1.2417496 , 1.24178607,\n",
      "       1.24175047, 1.24174827, 1.24175226, 1.24180985, 1.241744  ,\n",
      "       1.24175725, 1.24175553, 1.24174591, 1.24175671, 1.24175179,\n",
      "       1.24174414, 1.24175   , 1.24179453, 1.24173796, 1.24177852,\n",
      "       1.24173721, 1.24176518, 1.24176252, 1.24178299, 1.24174667,\n",
      "       1.24177013, 1.24175448, 1.24173153, 1.24174029, 1.24172806])}, 'ATTACH::2::best_val_loss': 1.2417280612063053, 'ATTACH::3::history': {'train_accs': array([0.43936115, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718]), 'train_losses': array([155.41820942,   1.244433  ,   1.24306078,   1.24308595,\n",
      "         1.24316035,   1.24282804,   1.24324691,   1.24295778,\n",
      "         1.24275651,   1.24304181,   1.24356067,   1.24342818,\n",
      "         1.24305625,   1.24292907,   1.24286459]), 'val_accs': array([0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955]), 'val_losses': array([1.24452581, 1.24208341, 1.24471837, 1.24186392, 1.24182088,\n",
      "       1.24183914, 1.24194857, 1.2419619 , 1.2436074 , 1.24182288,\n",
      "       1.24265756, 1.24197783, 1.24189087, 1.24254344, 1.24189113])}, 'ATTACH::3::best_val_loss': 1.2418208790537137, 'ATTACH::4::history': {'train_accs': array([0.38331219, 0.43271886, 0.4871259 , 0.54474214, 0.57429659,\n",
      "       0.63556982, 0.66803493, 0.70602284, 0.73811478, 0.76080305,\n",
      "       0.78528248, 0.80729905, 0.83334577, 0.85498918, 0.87193074,\n",
      "       0.88387193, 0.89932084, 0.91603851, 0.92596462, 0.93380103,\n",
      "       0.94313008, 0.94596612, 0.9502948 , 0.94969774, 0.95790731,\n",
      "       0.9611165 , 0.96417643, 0.96566908, 0.97074409, 0.96835585,\n",
      "       0.97440107, 0.9746996 , 0.97559519, 0.97656542, 0.97798343,\n",
      "       0.97969998, 0.97805806, 0.98119263, 0.98380476, 0.98171505,\n",
      "       0.98679006, 0.98559594, 0.98581984, 0.98664079, 0.98492425,\n",
      "       0.98611837, 0.9864169 , 0.98887977, 0.98723785, 0.9887305 ,\n",
      "       0.98768565, 0.99074558, 0.98977536, 0.98984999, 0.99014852,\n",
      "       0.99022315, 0.99074558, 0.98902903, 0.98977536, 0.99074558]), 'train_losses': array([16.6167755 ,  5.41158758,  3.42076659,  2.47484454,  1.91082025,\n",
      "        1.43949952,  1.17385521,  0.95209419,  0.80274687,  0.70647952,\n",
      "        0.59180218,  0.53498541,  0.46924288,  0.40781158,  0.37372914,\n",
      "        0.33862923,  0.28787253,  0.2493555 ,  0.2174654 ,  0.19468914,\n",
      "        0.17315637,  0.16423055,  0.14884487,  0.14449409,  0.12334994,\n",
      "        0.11887479,  0.1120456 ,  0.10210458,  0.09218586,  0.09403168,\n",
      "        0.08239708,  0.08085629,  0.07484993,  0.07280731,  0.07361398,\n",
      "        0.06316622,  0.06536538,  0.05917441,  0.05360623,  0.05573226,\n",
      "        0.04631797,  0.04795508,  0.04630132,  0.04180216,  0.04778229,\n",
      "        0.04329935,  0.04423659,  0.03725922,  0.04148447,  0.03833704,\n",
      "        0.03977413,  0.03052136,  0.03747536,  0.03131566,  0.0334222 ,\n",
      "        0.03018613,  0.02935129,  0.03537066,  0.03120819,  0.02944363]), 'val_accs': array([0.5280597 , 0.57970149, 0.53940299, 0.65283582, 0.60328358,\n",
      "       0.60268657, 0.8       , 0.75761194, 0.81910448, 0.79910448,\n",
      "       0.81462687, 0.87014925, 0.8441791 , 0.84985075, 0.88716418,\n",
      "       0.90358209, 0.93283582, 0.90238806, 0.8638806 , 0.92358209,\n",
      "       0.9358209 , 0.93791045, 0.93701493, 0.95164179, 0.94746269,\n",
      "       0.9358209 , 0.96268657, 0.93701493, 0.96597015, 0.95761194,\n",
      "       0.97164179, 0.96835821, 0.96985075, 0.97343284, 0.96656716,\n",
      "       0.97880597, 0.9680597 , 0.97492537, 0.9761194 , 0.97641791,\n",
      "       0.97164179, 0.97074627, 0.98059701, 0.97462687, 0.98626866,\n",
      "       0.97432836, 0.97731343, 0.97820896, 0.98089552, 0.98238806,\n",
      "       0.98      , 0.9761194 , 0.97283582, 0.97402985, 0.98567164,\n",
      "       0.98238806, 0.9719403 , 0.97970149, 0.98119403, 0.9719403 ]), 'val_losses': array([2.193906  , 1.4315328 , 1.95285638, 1.11967412, 1.08146702,\n",
      "       1.33929927, 0.60597407, 0.86481569, 0.60104859, 0.53043735,\n",
      "       0.59386046, 0.38101403, 0.45376444, 0.42342215, 0.3593895 ,\n",
      "       0.27837725, 0.22258125, 0.31055234, 0.37590298, 0.21235323,\n",
      "       0.19280424, 0.17745962, 0.16376104, 0.13312051, 0.13697347,\n",
      "       0.17799196, 0.09404637, 0.19004554, 0.12011498, 0.13144781,\n",
      "       0.08611413, 0.10322957, 0.09077049, 0.084156  , 0.08578275,\n",
      "       0.06810496, 0.09224413, 0.08934232, 0.07852436, 0.07005435,\n",
      "       0.08089157, 0.09197801, 0.05739321, 0.08522739, 0.04972606,\n",
      "       0.07283755, 0.0682993 , 0.07028456, 0.0735746 , 0.05588445,\n",
      "       0.06571896, 0.07763491, 0.0800548 , 0.10133109, 0.04575775,\n",
      "       0.07070967, 0.10377385, 0.05668269, 0.06326366, 0.10116973])}, 'ATTACH::4::best_val_loss': 0.04575775037269428, 'ATTACH::5::history': {'train_accs': array([0.31748638, 0.3380103 , 0.34151802, 0.3515934 , 0.35629525,\n",
      "       0.36875886, 0.37517725, 0.38965594, 0.3934622 , 0.41488171,\n",
      "       0.41995671, 0.43174864, 0.45346668, 0.4571983 , 0.46279573,\n",
      "       0.48145384, 0.48637958, 0.48966341, 0.51481454, 0.51750131,\n",
      "       0.52794985, 0.53272632, 0.53586089, 0.53847302, 0.5536234 ,\n",
      "       0.5651168 , 0.56414658, 0.56937085, 0.58116277, 0.58258079,\n",
      "       0.58780506, 0.58944697, 0.60295544, 0.60407493, 0.61161281,\n",
      "       0.61303082, 0.62847974, 0.64057019, 0.63803269, 0.63885365,\n",
      "       0.64482424, 0.65594447, 0.65818345, 0.66340772, 0.66490037,\n",
      "       0.68355847, 0.67937906, 0.68534965, 0.69430555, 0.69378312,\n",
      "       0.70557504, 0.70393313, 0.71259049, 0.71594895, 0.71841182,\n",
      "       0.72886036, 0.7344578 , 0.74035376, 0.74371222, 0.75147399]), 'train_losses': array([57.28541783, 44.63449178, 35.87293388, 28.89770869, 23.36836132,\n",
      "       19.41631938, 16.46173185, 13.98038821, 12.2012152 , 10.53948176,\n",
      "        9.58949749,  8.51539101,  7.6647778 ,  7.25196749,  6.76873405,\n",
      "        6.2705398 ,  5.86991858,  5.59951158,  5.05215729,  4.788081  ,\n",
      "        4.53814378,  4.31079754,  4.13260772,  3.82873437,  3.68209267,\n",
      "        3.53934938,  3.34742247,  3.21549473,  3.08285688,  2.94134531,\n",
      "        2.81761249,  2.68390303,  2.54929122,  2.45861765,  2.34431032,\n",
      "        2.3137812 ,  2.15420452,  2.06190935,  1.99155741,  1.94417592,\n",
      "        1.87476434,  1.72689094,  1.72085204,  1.70573982,  1.62198151,\n",
      "        1.5112631 ,  1.50982382,  1.43044033,  1.37538251,  1.35181257,\n",
      "        1.31637402,  1.28951687,  1.22534306,  1.17735216,  1.15275703,\n",
      "        1.14249438,  1.06266106,  1.02197199,  1.02301916,  0.99171419]), 'val_accs': array([0.39343284, 0.35880597, 0.35373134, 0.34716418, 0.38208955,\n",
      "       0.43701493, 0.49492537, 0.49343284, 0.49880597, 0.48895522,\n",
      "       0.51820896, 0.5238806 , 0.53791045, 0.54567164, 0.56059701,\n",
      "       0.57641791, 0.56835821, 0.59164179, 0.56537313, 0.63164179,\n",
      "       0.61671642, 0.64925373, 0.63074627, 0.66029851, 0.65253731,\n",
      "       0.67731343, 0.67074627, 0.67880597, 0.68895522, 0.6680597 ,\n",
      "       0.6958209 , 0.67641791, 0.69462687, 0.70477612, 0.69880597,\n",
      "       0.71940299, 0.71761194, 0.69880597, 0.73880597, 0.70119403,\n",
      "       0.73761194, 0.75731343, 0.76119403, 0.73402985, 0.69970149,\n",
      "       0.7361194 , 0.7761194 , 0.77701493, 0.79343284, 0.79910448,\n",
      "       0.79044776, 0.77910448, 0.78985075, 0.75552239, 0.80537313,\n",
      "       0.80626866, 0.81373134, 0.80029851, 0.82477612, 0.81313433]), 'val_losses': array([24.09110561, 20.23423081, 16.72058064, 12.62197134,  9.91692939,\n",
      "        7.57052414,  6.42614877,  5.48618258,  4.68036124,  4.6854545 ,\n",
      "        4.77016509,  3.49168998,  3.52155365,  3.25164425,  3.33530658,\n",
      "        2.79648791,  2.52931612,  2.00468579,  2.08165743,  1.81457837,\n",
      "        1.93695828,  1.73133445,  1.74842188,  1.64667839,  1.72437122,\n",
      "        1.48106083,  1.55766643,  1.27975392,  1.41351044,  1.4703884 ,\n",
      "        1.31907349,  1.50104806,  1.36417337,  1.24686474,  1.2063001 ,\n",
      "        1.08744733,  1.18772288,  1.3252181 ,  0.9548473 ,  1.21178903,\n",
      "        1.03684872,  0.89464184,  0.8110399 ,  0.97183792,  1.21594095,\n",
      "        1.02290826,  0.75347959,  0.70685523,  0.71045001,  0.67294346,\n",
      "        0.62615248,  0.78203594,  0.76476014,  0.85386427,  0.63449533,\n",
      "        0.58520208,  0.56496802,  0.60203195,  0.5617935 ,  0.55838374])}, 'ATTACH::5::best_val_loss': 0.5583837363079412, 'ATTACH::6::history': {'train_accs': array([0.35308605, 0.39017837, 0.41115009, 0.41712068, 0.43719681,\n",
      "       0.45212329, 0.45645197, 0.47787148, 0.50458989, 0.52429286,\n",
      "       0.54511531, 0.58907381, 0.60205986, 0.62959922, 0.65601911,\n",
      "       0.6928129 , 0.71087395, 0.74333906, 0.75983282, 0.77319203,\n",
      "       0.78819315, 0.80170162, 0.80886633, 0.83289798, 0.84812299,\n",
      "       0.86088514, 0.86760206, 0.87879693, 0.88939473, 0.8948429 ,\n",
      "       0.91327711, 0.91081424, 0.91685947, 0.92036719, 0.92574073,\n",
      "       0.93103963, 0.93753265, 0.94298082, 0.94230913, 0.94790656,\n",
      "       0.95044406, 0.9532801 , 0.95499664, 0.95731025, 0.96201209,\n",
      "       0.96231062, 0.96581834, 0.96537055, 0.96902754, 0.96969923,\n",
      "       0.9719382 , 0.97328159, 0.97380402, 0.97387865, 0.97693858,\n",
      "       0.97925218, 0.97917755, 0.98007314, 0.97910292, 0.98290917]), 'train_losses': array([22.10269504,  9.61272449,  6.47646762,  4.8825579 ,  3.84353303,\n",
      "        3.06958662,  2.47287875,  2.08185938,  1.78454295,  1.55367156,\n",
      "        1.39479325,  1.17144664,  1.09194674,  0.96340226,  0.89008503,\n",
      "        0.80480906,  0.75723726,  0.67477774,  0.63653314,  0.58592439,\n",
      "        0.55512004,  0.51926053,  0.50108053,  0.44664423,  0.4167335 ,\n",
      "        0.38518414,  0.36577694,  0.33591479,  0.31230575,  0.29154222,\n",
      "        0.26174244,  0.25738549,  0.2427533 ,  0.22623952,  0.21231935,\n",
      "        0.20041729,  0.18549017,  0.17556548,  0.17005566,  0.15889407,\n",
      "        0.14588445,  0.13564519,  0.13167784,  0.12618966,  0.11718549,\n",
      "        0.1144819 ,  0.10511729,  0.09827473,  0.09197976,  0.08861316,\n",
      "        0.08372244,  0.08116409,  0.07845742,  0.08095131,  0.06823106,\n",
      "        0.0643386 ,  0.06709045,  0.05883782,  0.06844471,  0.0524041 ]), 'val_accs': array([0.4480597 , 0.44895522, 0.49283582, 0.44208955, 0.51402985,\n",
      "       0.46895522, 0.43134328, 0.52179104, 0.54298507, 0.42149254,\n",
      "       0.60477612, 0.60268657, 0.56865672, 0.60746269, 0.6238806 ,\n",
      "       0.63970149, 0.64686567, 0.66537313, 0.72537313, 0.61223881,\n",
      "       0.68835821, 0.79432836, 0.85970149, 0.83164179, 0.91432836,\n",
      "       0.85313433, 0.91731343, 0.89701493, 0.86626866, 0.83820896,\n",
      "       0.87492537, 0.9238806 , 0.94208955, 0.91731343, 0.93432836,\n",
      "       0.90835821, 0.89462687, 0.94179104, 0.93880597, 0.89970149,\n",
      "       0.93552239, 0.95373134, 0.93492537, 0.9158209 , 0.91402985,\n",
      "       0.86507463, 0.95820896, 0.96776119, 0.96895522, 0.92746269,\n",
      "       0.95910448, 0.97940299, 0.96179104, 0.97104478, 0.97432836,\n",
      "       0.97970149, 0.9641791 , 0.97820896, 0.92208955, 0.97552239]), 'val_losses': array([8.44412844, 3.40383348, 3.62261552, 9.44210339, 2.32051373,\n",
      "       3.38207293, 4.30857607, 3.52368227, 1.34067819, 4.32184069,\n",
      "       1.36151131, 1.01142898, 1.2487669 , 0.82523702, 1.78662373,\n",
      "       0.74589332, 0.74686237, 0.84774821, 0.63664755, 1.30807354,\n",
      "       0.72910764, 0.56501302, 0.38080737, 0.40000456, 0.26695459,\n",
      "       0.34202138, 0.23873764, 0.25616167, 0.29426259, 0.39217614,\n",
      "       0.34341644, 0.22590707, 0.175464  , 0.22416432, 0.18151123,\n",
      "       0.2642008 , 0.28031528, 0.15879952, 0.16951227, 0.27252452,\n",
      "       0.18258788, 0.1198183 , 0.1867256 , 0.25403919, 0.25123165,\n",
      "       0.38956271, 0.11228617, 0.09881166, 0.09278603, 0.21413802,\n",
      "       0.11819309, 0.06836066, 0.1215789 , 0.0877214 , 0.074617  ,\n",
      "       0.06159587, 0.10289975, 0.05997869, 0.21908385, 0.07887859])}, 'ATTACH::6::best_val_loss': 0.05997868547021453, 'ATTACH::7::history': {'train_accs': array([0.3103963 , 0.36457945, 0.38599896, 0.3962236 , 0.39973132,\n",
      "       0.41197104, 0.41391149, 0.41913576, 0.43077842, 0.43652511,\n",
      "       0.44630196, 0.45876558, 0.47070677, 0.4903351 , 0.49466378,\n",
      "       0.51093365, 0.52906933, 0.54929472, 0.57459512, 0.5982536 ,\n",
      "       0.62109113, 0.64385402, 0.68333458, 0.71102321, 0.73065154,\n",
      "       0.7614001 , 0.77259497, 0.79080528, 0.80834391, 0.82043436,\n",
      "       0.8366296 , 0.84685424, 0.86327338, 0.87140831, 0.88118516,\n",
      "       0.88506605, 0.89902232, 0.90282857, 0.90894843, 0.91506829,\n",
      "       0.92051646, 0.92686021, 0.92753191, 0.9279797 , 0.92708411,\n",
      "       0.93574147, 0.94738413, 0.93887604, 0.95216061, 0.95089186,\n",
      "       0.95290693, 0.95589223, 0.9558176 , 0.96186283, 0.96119113,\n",
      "       0.95939996, 0.95410105, 0.96298231, 0.97328159, 0.97372938]), 'train_losses': array([46.95414323, 21.17569147, 13.90345583, 10.70170809,  9.19366042,\n",
      "        7.66370972,  6.69215623,  5.97470686,  5.10314942,  4.55545915,\n",
      "        4.10614287,  3.54919137,  3.17570687,  2.86024831,  2.61257362,\n",
      "        2.34300216,  2.15262234,  1.9487602 ,  1.74578867,  1.55448795,\n",
      "        1.44241168,  1.28011835,  1.18414194,  1.036592  ,  0.91980995,\n",
      "        0.82940967,  0.76411085,  0.69161334,  0.64500623,  0.58584131,\n",
      "        0.51782717,  0.47291423,  0.43311061,  0.3912315 ,  0.36753538,\n",
      "        0.35690916,  0.30884551,  0.30603205,  0.27903081,  0.26642011,\n",
      "        0.2525425 ,  0.22680577,  0.22253367,  0.22006205,  0.22130425,\n",
      "        0.19623146,  0.16406437,  0.19089054,  0.14613469,  0.15359457,\n",
      "        0.14296943,  0.13112124,  0.13565269,  0.12080703,  0.12301835,\n",
      "        0.1212106 ,  0.14267267,  0.11225943,  0.08175766,  0.0808004 ]), 'val_accs': array([0.4480597 , 0.41701493, 0.45970149, 0.48865672, 0.45820896,\n",
      "       0.46955224, 0.5       , 0.54865672, 0.4761194 , 0.54179104,\n",
      "       0.54776119, 0.59373134, 0.58656716, 0.58716418, 0.66925373,\n",
      "       0.60776119, 0.60895522, 0.6519403 , 0.68358209, 0.66268657,\n",
      "       0.73104478, 0.74985075, 0.78865672, 0.76089552, 0.80447761,\n",
      "       0.81970149, 0.83104478, 0.81492537, 0.83641791, 0.84746269,\n",
      "       0.86029851, 0.84597015, 0.86507463, 0.85701493, 0.87910448,\n",
      "       0.88597015, 0.90179104, 0.88746269, 0.90149254, 0.91104478,\n",
      "       0.89432836, 0.91014925, 0.91791045, 0.90776119, 0.90298507,\n",
      "       0.9238806 , 0.92716418, 0.91074627, 0.92179104, 0.92776119,\n",
      "       0.92089552, 0.91791045, 0.92686567, 0.92149254, 0.91641791,\n",
      "       0.92925373, 0.93671642, 0.93522388, 0.93820896, 0.93761194]), 'val_losses': array([6.95034716, 7.01978274, 3.72512618, 2.99173377, 2.27171007,\n",
      "       3.94793089, 1.80223797, 1.58551788, 1.57013653, 1.86959596,\n",
      "       1.45760671, 1.24760609, 1.30860462, 1.46495484, 0.93342579,\n",
      "       1.15496877, 0.98771879, 0.95435693, 0.9303768 , 0.95945768,\n",
      "       0.71944564, 0.70684662, 0.56065064, 0.76142476, 0.53880926,\n",
      "       0.49700524, 0.48686522, 0.53515858, 0.46944489, 0.42318431,\n",
      "       0.37254451, 0.46070591, 0.37978338, 0.38936027, 0.37226711,\n",
      "       0.34862224, 0.30067264, 0.35594543, 0.31138477, 0.27797989,\n",
      "       0.32874367, 0.27830848, 0.26940666, 0.31374243, 0.30125584,\n",
      "       0.25113555, 0.2376759 , 0.28575457, 0.25192468, 0.23362741,\n",
      "       0.26792894, 0.26036092, 0.24222223, 0.27141564, 0.27625097,\n",
      "       0.21999241, 0.20886151, 0.21514937, 0.22558729, 0.2041411 ])}, 'ATTACH::7::best_val_loss': 0.20414110421244777, 'ATTACH::8::history': {'train_accs': array([0.42227032, 0.46145235, 0.48488693, 0.50197776, 0.53100978,\n",
      "       0.54966789, 0.55011568, 0.5846705 , 0.58884991, 0.58937234,\n",
      "       0.60810508, 0.60004478, 0.62362863, 0.62109113, 0.63258452,\n",
      "       0.63094261, 0.63034555, 0.63810732, 0.63646541, 0.64534667,\n",
      "       0.64616762, 0.65206359, 0.64721248, 0.65557131, 0.65557131,\n",
      "       0.65109337, 0.64460034, 0.64071946, 0.63668931, 0.62847974,\n",
      "       0.64803344, 0.66079558, 0.66840809, 0.67475185, 0.66676618,\n",
      "       0.67758788, 0.67818494, 0.67646839, 0.67333383, 0.68221509,\n",
      "       0.6675125 , 0.67273677, 0.6762445 , 0.68243899, 0.68482723,\n",
      "       0.683857  , 0.65863124, 0.67034853, 0.66594522, 0.66624375,\n",
      "       0.67109486, 0.68034928]), 'train_losses': array([3.92049304, 1.07650105, 0.98804829, 0.94325109, 0.89905817,\n",
      "       0.87176223, 0.881638  , 0.83317285, 0.82661147, 0.8133193 ,\n",
      "       0.78677154, 0.78891664, 0.76466273, 0.76930295, 0.75250382,\n",
      "       0.75927791, 0.7571542 , 0.7424458 , 0.74647903, 0.74672002,\n",
      "       0.73013638, 0.72956369, 0.7207345 , 0.71196021, 0.70504698,\n",
      "       0.72923899, 0.75560174, 0.74759741, 0.76334983, 0.77722543,\n",
      "       0.7413402 , 0.72031453, 0.69466841, 0.69217852, 0.69264616,\n",
      "       0.67876994, 0.66640758, 0.6698322 , 0.6773569 , 0.66115265,\n",
      "       0.67836565, 0.666081  , 0.67178636, 0.66972633, 0.66255717,\n",
      "       0.66783143, 0.70576879, 0.68089708, 0.69038634, 0.69981896,\n",
      "       0.7003931 , 0.67627113]), 'val_accs': array([0.41134328, 0.55134328, 0.46835821, 0.52835821, 0.67283582,\n",
      "       0.63343284, 0.61283582, 0.68477612, 0.65432836, 0.65850746,\n",
      "       0.73492537, 0.61880597, 0.75791045, 0.71731343, 0.7441791 ,\n",
      "       0.69850746, 0.75641791, 0.72686567, 0.74985075, 0.68328358,\n",
      "       0.68089552, 0.75820896, 0.72656716, 0.76059701, 0.73910448,\n",
      "       0.75731343, 0.71134328, 0.73701493, 0.62686567, 0.7319403 ,\n",
      "       0.75044776, 0.73343284, 0.76089552, 0.71492537, 0.7761194 ,\n",
      "       0.73462687, 0.72925373, 0.75164179, 0.75283582, 0.75880597,\n",
      "       0.74238806, 0.77134328, 0.75671642, 0.73074627, 0.77223881,\n",
      "       0.67701493, 0.75462687, 0.72447761, 0.78746269, 0.72      ,\n",
      "       0.67850746, 0.78029851]), 'val_losses': array([1.10576737, 0.93573037, 0.87818781, 0.85927502, 0.7971303 ,\n",
      "       0.79987238, 0.75258842, 0.756102  , 0.72088524, 0.71226857,\n",
      "       0.6523884 , 0.69591102, 0.62584786, 0.65643785, 0.64455723,\n",
      "       0.6577349 , 0.65211002, 0.6247999 , 0.63619154, 0.65091789,\n",
      "       0.67556893, 0.6044698 , 0.63193694, 0.58657734, 0.59154102,\n",
      "       0.6136507 , 0.61989632, 0.67792854, 0.72799408, 0.63125895,\n",
      "       0.58758979, 0.57787158, 0.55537021, 0.60801448, 0.53088392,\n",
      "       0.58124468, 0.55595315, 0.56187708, 0.55488729, 0.56671615,\n",
      "       0.5666894 , 0.52565076, 0.57574378, 0.54765452, 0.53506299,\n",
      "       0.65502185, 0.63984345, 0.60585576, 0.56185826, 0.60930849,\n",
      "       0.66066819, 0.55643838])}, 'ATTACH::8::best_val_loss': 0.5256507618391691, 'ATTACH::9::history': {'train_accs': array([0.44048063, 0.53108441, 0.59101425, 0.64340622, 0.68243899,\n",
      "       0.70027614, 0.72012837, 0.73072617, 0.73751773, 0.75639973,\n",
      "       0.7646093 , 0.77020673, 0.79170087, 0.78438689, 0.79580566,\n",
      "       0.80364206, 0.79438764, 0.80252258, 0.80520934, 0.81767296,\n",
      "       0.82312113, 0.83021121, 0.83192776]), 'train_losses': array([8.4734564 , 1.53648864, 0.97692818, 0.8075514 , 0.71737616,\n",
      "       0.6710521 , 0.62983284, 0.61180377, 0.59385496, 0.55706948,\n",
      "       0.54555047, 0.51987855, 0.49472428, 0.50681402, 0.49968083,\n",
      "       0.47500727, 0.48964131, 0.47156257, 0.47772894, 0.4363175 ,\n",
      "       0.4283521 , 0.41981593, 0.41250479]), 'val_accs': array([0.52686567, 0.6119403 , 0.69104478, 0.68208955, 0.75313433,\n",
      "       0.75044776, 0.76447761, 0.79432836, 0.74298507, 0.76328358,\n",
      "       0.72656716, 0.79820896, 0.86149254, 0.73104478, 0.81014925,\n",
      "       0.77731343, 0.76865672, 0.80238806, 0.81820896, 0.78835821,\n",
      "       0.82119403, 0.79253731, 0.77373134]), 'val_losses': array([2.00346963, 0.87299014, 0.68565564, 0.6880489 , 0.60892755,\n",
      "       0.5236436 , 0.52886698, 0.51106561, 0.55623261, 0.53805891,\n",
      "       0.54808879, 0.49474335, 0.37618949, 0.54742521, 0.45487009,\n",
      "       0.49719245, 0.55329856, 0.45174664, 0.48024181, 0.47164068,\n",
      "       0.47007348, 0.5077299 , 0.56285616])}, 'ATTACH::9::best_val_loss': 0.37618948518340267, 'ATTACH::10::history': {'train_accs': array([0.36801254, 0.45555638, 0.48302112, 0.49809687, 0.48772296,\n",
      "       0.49578327, 0.49802224, 0.49302187, 0.50488842, 0.50996343,\n",
      "       0.51690425, 0.51332189, 0.51809837, 0.5239197 , 0.52593477,\n",
      "       0.5342936 , 0.53190537, 0.54071199, 0.537055  , 0.54742891,\n",
      "       0.53839839, 0.53951787, 0.5342936 , 0.53951787, 0.55063811,\n",
      "       0.54362266, 0.54638406, 0.54593626, 0.55354877, 0.55966863,\n",
      "       0.55944473, 0.56063885, 0.56131055, 0.573401  , 0.56459437,\n",
      "       0.57511755, 0.57563997, 0.5846705 , 0.58541682, 0.59183521,\n",
      "       0.59944772, 0.59638779, 0.59997015, 0.59691022, 0.60646317,\n",
      "       0.60041794, 0.60825435, 0.61258303, 0.61123964, 0.61400104,\n",
      "       0.61564296, 0.62698709, 0.63392791, 0.63318158, 0.63773416,\n",
      "       0.63885365, 0.6313904 , 0.64691395, 0.6426599 , 0.65042167]), 'train_losses': array([6.84587369, 1.07521578, 1.05376848, 1.01291649, 0.97994363,\n",
      "       0.95625899, 0.95870237, 0.94291777, 0.92725164, 0.92744952,\n",
      "       0.91690521, 0.91126464, 0.90515671, 0.89717927, 0.89532479,\n",
      "       0.89151709, 0.89192778, 0.87906629, 0.88360308, 0.87175238,\n",
      "       0.86114622, 0.86732386, 0.86747248, 0.8640887 , 0.84711654,\n",
      "       0.85898372, 0.85500238, 0.84212488, 0.84705219, 0.84811758,\n",
      "       0.83391069, 0.83653032, 0.84041454, 0.82654345, 0.83384227,\n",
      "       0.83013682, 0.82229255, 0.80910387, 0.81184172, 0.81609196,\n",
      "       0.80458446, 0.8024699 , 0.79171431, 0.80711583, 0.79246708,\n",
      "       0.79765769, 0.78400101, 0.7875315 , 0.7756903 , 0.78213632,\n",
      "       0.78107082, 0.76873343, 0.75934426, 0.76523183, 0.75555441,\n",
      "       0.75463921, 0.7538973 , 0.73626857, 0.74092426, 0.73849032]), 'val_accs': array([0.36507463, 0.49253731, 0.58268657, 0.49104478, 0.52477612,\n",
      "       0.56      , 0.49552239, 0.48626866, 0.53492537, 0.50179104,\n",
      "       0.56119403, 0.62447761, 0.5719403 , 0.54955224, 0.60268657,\n",
      "       0.60597015, 0.5719403 , 0.58      , 0.57104478, 0.60925373,\n",
      "       0.58      , 0.59044776, 0.59462687, 0.56835821, 0.60238806,\n",
      "       0.53761194, 0.59552239, 0.58626866, 0.55641791, 0.54985075,\n",
      "       0.51970149, 0.61014925, 0.62029851, 0.52985075, 0.63492537,\n",
      "       0.64089552, 0.6158209 , 0.65552239, 0.64208955, 0.62179104,\n",
      "       0.53641791, 0.6358209 , 0.63791045, 0.62835821, 0.69970149,\n",
      "       0.64656716, 0.66179104, 0.68029851, 0.6119403 , 0.66686567,\n",
      "       0.7080597 , 0.67970149, 0.66656716, 0.70835821, 0.72477612,\n",
      "       0.60835821, 0.74716418, 0.6958209 , 0.7319403 , 0.70985075]), 'val_losses': array([1.07665029, 1.05397396, 1.04496907, 0.97740672, 0.93022848,\n",
      "       0.93254832, 0.91006779, 0.93401375, 0.88013141, 0.87943509,\n",
      "       0.87518377, 0.86209514, 0.86169184, 0.84309762, 0.8850483 ,\n",
      "       0.83466267, 0.85161009, 0.8210135 , 0.82398814, 0.83769226,\n",
      "       0.79905975, 0.83914084, 0.82194453, 0.82450857, 0.81176635,\n",
      "       0.80879549, 0.81641404, 0.80683172, 0.77441147, 0.80487597,\n",
      "       0.78146148, 0.78713983, 0.77310374, 0.794616  , 0.81544255,\n",
      "       0.79368523, 0.82911335, 0.78942199, 0.79336388, 0.78363711,\n",
      "       0.79184598, 0.76875769, 0.73127967, 0.71912513, 0.70891828,\n",
      "       0.71781059, 0.7091865 , 0.70759415, 0.72883532, 0.76285783,\n",
      "       0.67096471, 0.70916365, 0.66535964, 0.68336557, 0.66469798,\n",
      "       0.80048389, 0.62901898, 0.62796969, 0.65280205, 0.65630403])}, 'ATTACH::10::best_val_loss': 0.6279696912907843, 'ATTACH::11::history': {'train_accs': array([0.43846556, 0.52742742, 0.58922308, 0.62362863, 0.64445108,\n",
      "       0.66281066, 0.67937906, 0.68378237, 0.70699306, 0.68527502,\n",
      "       0.68482723, 0.68572282, 0.71184417, 0.7174416 , 0.71684454,\n",
      "       0.71542652, 0.70244048, 0.68191656, 0.69012613, 0.69132025,\n",
      "       0.7006493 , 0.7031868 , 0.69445481]), 'train_losses': array([13.95780799,  1.07791203,  0.8376569 ,  0.76776631,  0.72778248,\n",
      "        0.69831848,  0.66505536,  0.65771934,  0.63296379,  0.65891555,\n",
      "        0.66428564,  0.65571419,  0.62727389,  0.61269475,  0.6235943 ,\n",
      "        0.64051472,  0.65174001,  0.6794685 ,  0.65834388,  0.64623036,\n",
      "        0.6371335 ,  0.63548236,  0.65162191]), 'val_accs': array([0.53014925, 0.59522388, 0.68895522, 0.71522388, 0.73671642,\n",
      "       0.68059701, 0.72059701, 0.75492537, 0.74985075, 0.70895522,\n",
      "       0.71253731, 0.70208955, 0.78268657, 0.68567164, 0.75223881,\n",
      "       0.72925373, 0.63791045, 0.62776119, 0.65373134, 0.64656716,\n",
      "       0.6841791 , 0.59641791, 0.67940299]), 'val_losses': array([0.9261369 , 0.82730471, 0.71400697, 0.66077708, 0.61563556,\n",
      "       0.63664757, 0.59548125, 0.57007767, 0.5718559 , 0.64130634,\n",
      "       0.62298388, 0.61505672, 0.5240482 , 0.67273511, 0.55369838,\n",
      "       0.63239442, 0.74311626, 0.73518444, 0.75826299, 0.80757851,\n",
      "       0.72208571, 0.77893602, 0.7691061 ])}, 'ATTACH::11::best_val_loss': 0.5240482026783388, 'ATTACH::12::history': {'train_accs': array([0.34136876, 0.34069707, 0.31136652, 0.29755952, 0.30435107,\n",
      "       0.31711322, 0.33151728, 0.35211583, 0.35398164, 0.36017613,\n",
      "       0.36771401, 0.37868498, 0.37831181, 0.38838719, 0.39831331,\n",
      "       0.40010449, 0.40085081, 0.3962236 , 0.40846332, 0.41346369,\n",
      "       0.41689678, 0.4213001 , 0.42652437, 0.43421151, 0.43115158,\n",
      "       0.42831555, 0.4408538 , 0.43973431, 0.43324129, 0.44951116,\n",
      "       0.4539891 , 0.44130159, 0.44712292, 0.44943653, 0.44652586,\n",
      "       0.45376521, 0.46152698, 0.45503396, 0.46712441, 0.46540787,\n",
      "       0.46033286, 0.47249795, 0.47167699, 0.47130383, 0.47600567,\n",
      "       0.47779685, 0.47682663, 0.47593104, 0.48548399, 0.4845884 ,\n",
      "       0.48839466, 0.49339503, 0.49406672, 0.50026121, 0.50451526,\n",
      "       0.50175386, 0.50279872, 0.51197851, 0.51638182, 0.51488917]), 'train_losses': array([143.0979074 ,  97.67724895,  69.00151603,  54.43135365,\n",
      "        44.11043831,  36.27206859,  30.50996827,  26.1994014 ,\n",
      "        22.53989131,  20.23325352,  17.61197382,  16.20165299,\n",
      "        14.7874703 ,  13.55035501,  12.6870218 ,  11.70911187,\n",
      "        10.9507311 ,  10.47909847,   9.7473287 ,   9.13353317,\n",
      "         8.81690648,   8.18909833,   7.79211671,   7.18979951,\n",
      "         6.94558432,   6.88268935,   6.36106644,   6.07530259,\n",
      "         5.95592624,   5.69727944,   5.23080001,   5.33539599,\n",
      "         4.90857747,   4.79508557,   4.63747283,   4.45778721,\n",
      "         4.38832612,   4.15467541,   3.94553303,   3.77465897,\n",
      "         3.65978792,   3.49888384,   3.5167411 ,   3.27582068,\n",
      "         3.12997217,   3.02205136,   2.97377951,   2.88621034,\n",
      "         2.74939182,   2.61403828,   2.54681088,   2.38587502,\n",
      "         2.41288795,   2.32090283,   2.24597804,   2.18485493,\n",
      "         2.12121721,   2.01830577,   1.90508966,   1.92876282]), 'val_accs': array([0.37522388, 0.42447761, 0.39283582, 0.37880597, 0.39761194,\n",
      "       0.38865672, 0.38925373, 0.42447761, 0.44955224, 0.38059701,\n",
      "       0.36925373, 0.38268657, 0.44358209, 0.47820896, 0.44567164,\n",
      "       0.40477612, 0.43164179, 0.46268657, 0.46537313, 0.42477612,\n",
      "       0.46089552, 0.47313433, 0.4719403 , 0.50507463, 0.50358209,\n",
      "       0.4919403 , 0.5238806 , 0.45343284, 0.47552239, 0.51074627,\n",
      "       0.52358209, 0.48358209, 0.50716418, 0.5158209 , 0.50149254,\n",
      "       0.52746269, 0.54      , 0.5238806 , 0.54298507, 0.53701493,\n",
      "       0.55462687, 0.57313433, 0.54746269, 0.58179104, 0.56298507,\n",
      "       0.61373134, 0.56835821, 0.58746269, 0.63014925, 0.57164179,\n",
      "       0.58059701, 0.61223881, 0.60895522, 0.59761194, 0.60029851,\n",
      "       0.6238806 , 0.61253731, 0.61940299, 0.61880597, 0.60626866]), 'val_losses': array([101.72208291,  61.92420197,  39.20355654,  27.51747564,\n",
      "        20.05668894,  12.8244892 ,   8.54969783,   6.60440911,\n",
      "         5.10916778,   4.1176792 ,   3.77522696,   3.59229994,\n",
      "         3.4617208 ,   3.25960703,   3.28432929,   3.0678871 ,\n",
      "         3.06014665,   3.27654651,   3.11195248,   2.83532997,\n",
      "         2.66968298,   2.78740855,   2.58241847,   2.62863747,\n",
      "         2.46272061,   2.33750425,   2.18762182,   2.22208482,\n",
      "         2.01580563,   2.19828296,   1.95407073,   1.92398514,\n",
      "         1.82990926,   1.70105144,   1.65614909,   1.7263642 ,\n",
      "         1.51803045,   1.54598005,   1.46077091,   1.46444036,\n",
      "         1.47181075,   1.41427366,   1.47943989,   1.35871815,\n",
      "         1.34607203,   1.45023487,   1.20598594,   1.17838528,\n",
      "         1.12660002,   1.29401422,   1.21786675,   1.20331929,\n",
      "         1.12984307,   1.13189302,   1.16687448,   1.09120728,\n",
      "         1.05379291,   1.03325145,   1.07210187,   1.03515953])}, 'ATTACH::12::best_val_loss': 1.0332514507379105, 'ATTACH::13::history': {'train_accs': array([0.4348832 , 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718]), 'train_losses': array([11.05208034,  1.31994989,  1.29533639,  1.27948993,  1.26883593,\n",
      "        1.26136428,  1.2564141 ,  1.25251277,  1.24991785,  1.24789096,\n",
      "        1.24645391,  1.24541427,  1.2445842 ,  1.24394159,  1.24348244,\n",
      "        1.24306158,  1.24291051,  1.24265999,  1.24244512,  1.24240124,\n",
      "        1.24227448,  1.24235476,  1.24209886,  1.24208568,  1.242025  ,\n",
      "        1.24209503,  1.24203802,  1.24198098,  1.24200282,  1.24197789,\n",
      "        1.2419803 ,  1.2421189 ,  1.24202605,  1.24191055,  1.24209898,\n",
      "        1.24205074,  1.24203117,  1.24200653,  1.24197613,  1.24208935,\n",
      "        1.24191476,  1.24198309,  1.24182492,  1.24205029,  1.24187838,\n",
      "        1.24213763,  1.24189776,  1.24193916,  1.24200511,  1.24199247,\n",
      "        1.2420345 ,  1.24184155,  1.24193791,  1.24198008,  1.24202005,\n",
      "        1.2419397 ]), 'val_accs': array([0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955]), 'val_losses': array([1.33671519, 1.30554897, 1.28610701, 1.27326946, 1.26443253,\n",
      "       1.25843931, 1.25403527, 1.25096201, 1.24865889, 1.24693044,\n",
      "       1.24565595, 1.2446886 , 1.24399641, 1.24345175, 1.24305264,\n",
      "       1.24274321, 1.2425018 , 1.24233269, 1.24219419, 1.24208871,\n",
      "       1.24224643, 1.24194345, 1.24188144, 1.24185355, 1.24184387,\n",
      "       1.2418295 , 1.24178267, 1.24180241, 1.24187012, 1.24177357,\n",
      "       1.24175055, 1.24176474, 1.24180651, 1.24173864, 1.24173509,\n",
      "       1.24174515, 1.2417634 , 1.24173127, 1.24177893, 1.24175381,\n",
      "       1.24174431, 1.24176258, 1.24172451, 1.24172464, 1.24178632,\n",
      "       1.24170179, 1.2417051 , 1.24171195, 1.24170884, 1.24171647,\n",
      "       1.2418227 , 1.2417102 , 1.24178647, 1.24172744, 1.24172285,\n",
      "       1.24172141])}, 'ATTACH::13::best_val_loss': 1.2417017926031084, 'ATTACH::14::history': {'train_accs': array([0.31173968, 0.32800955, 0.33659228, 0.34539891, 0.3543548 ,\n",
      "       0.36196731, 0.36532577, 0.37069931, 0.3711471 , 0.3909247 ,\n",
      "       0.38375998, 0.38547653, 0.3959997 , 0.39592507, 0.40264199,\n",
      "       0.40555265, 0.40017912, 0.40331368, 0.40100007, 0.40622435,\n",
      "       0.40629898, 0.41346369, 0.4125681 , 0.41286663, 0.41876259,\n",
      "       0.4123442 , 0.41794164, 0.41965818, 0.41973282, 0.42040451,\n",
      "       0.41838943, 0.42062841, 0.42010598, 0.4268229 , 0.42219569,\n",
      "       0.43346518, 0.42809165, 0.42398686, 0.4321218 , 0.43092768,\n",
      "       0.43712217, 0.43570416, 0.44406299, 0.45085454, 0.45227256,\n",
      "       0.45197403, 0.46197477, 0.46033286, 0.46652735, 0.46951265,\n",
      "       0.46913949, 0.4657064 , 0.48272259, 0.4763042 , 0.48749907,\n",
      "       0.49175312, 0.48555862, 0.49578327, 0.50369431, 0.50227629]), 'train_losses': array([134.99404564,  98.94442654,  79.76021612,  66.58632979,\n",
      "        55.5060958 ,  47.45726129,  40.6291769 ,  35.85911997,\n",
      "        32.21010517,  27.86380728,  25.65270908,  23.72880915,\n",
      "        21.48004683,  19.97773585,  18.79583003,  17.69452351,\n",
      "        16.7020224 ,  15.8457643 ,  15.21954686,  14.47984502,\n",
      "        13.85140346,  13.12337281,  12.84560116,  12.53666064,\n",
      "        11.62465705,  11.20232279,  10.52024131,  10.46193755,\n",
      "         9.88632894,   9.61511914,   9.26492859,   9.02517872,\n",
      "         8.76606053,   8.37939094,   7.95026457,   7.88241194,\n",
      "         7.69796728,   7.55085047,   7.30333674,   6.98284502,\n",
      "         6.75419489,   6.59298522,   6.2338291 ,   6.00962223,\n",
      "         5.72818425,   5.66866266,   5.49383828,   5.28997938,\n",
      "         5.23602326,   5.02466785,   4.98026794,   4.69629034,\n",
      "         4.63418938,   4.45363065,   4.35062586,   4.25462949,\n",
      "         4.19678043,   3.89226227,   3.91700787,   3.81364788]), 'val_accs': array([0.36029851, 0.32149254, 0.36955224, 0.36716418, 0.36238806,\n",
      "       0.34895522, 0.35791045, 0.36059701, 0.37283582, 0.37164179,\n",
      "       0.37850746, 0.39701493, 0.38895522, 0.38238806, 0.39074627,\n",
      "       0.41462687, 0.41164179, 0.40895522, 0.45164179, 0.46059701,\n",
      "       0.47671642, 0.5038806 , 0.48567164, 0.46925373, 0.4719403 ,\n",
      "       0.49641791, 0.50089552, 0.48776119, 0.51850746, 0.50238806,\n",
      "       0.50238806, 0.50238806, 0.51552239, 0.49552239, 0.50149254,\n",
      "       0.52328358, 0.51552239, 0.50507463, 0.48925373, 0.50985075,\n",
      "       0.53373134, 0.56268657, 0.50626866, 0.52746269, 0.51223881,\n",
      "       0.49910448, 0.52208955, 0.51552239, 0.55671642, 0.58567164,\n",
      "       0.54477612, 0.5361194 , 0.55402985, 0.58447761, 0.5       ,\n",
      "       0.57283582, 0.56895522, 0.59701493, 0.55014925, 0.56955224]), 'val_losses': array([21.40130026, 16.28340491, 14.03446388, 15.22008076, 16.13487582,\n",
      "       17.3601823 , 16.85098543, 13.64396119, 15.11242571, 13.95137639,\n",
      "       11.90351522,  9.88097092,  9.52405912,  8.80204855,  6.49800877,\n",
      "        5.79157533,  5.3549211 ,  5.17301867,  4.54267119,  4.41473606,\n",
      "        4.23080473,  4.33983372,  4.35797958,  4.10299433,  3.76748578,\n",
      "        3.85807506,  3.63749783,  3.66564002,  4.08164875,  3.37629764,\n",
      "        3.25143735,  3.05833227,  3.13190919,  2.75287597,  2.71525816,\n",
      "        2.58023272,  2.87929059,  2.39503269,  2.28872092,  2.26233298,\n",
      "        2.24199818,  2.16712818,  2.06306873,  1.97637342,  1.93300534,\n",
      "        2.18902419,  1.90914397,  1.76432745,  1.7447945 ,  1.63337034,\n",
      "        1.56622933,  1.62620834,  1.48836219,  1.44088991,  1.62199871,\n",
      "        1.5311304 ,  1.49502976,  1.42041597,  1.65852766,  1.37929366])}, 'ATTACH::14::best_val_loss': 1.379293655352806, 'ATTACH::15::history': {'train_accs': array([0.41615046, 0.43010672, 0.43652511, 0.43906262, 0.43868945,\n",
      "       0.43973431, 0.43861482, 0.44301814, 0.43854019, 0.44331667,\n",
      "       0.44025674, 0.44436152, 0.44145085, 0.44137622, 0.44137622,\n",
      "       0.44406299, 0.44376446, 0.44980969, 0.44809314, 0.44645123,\n",
      "       0.44704829, 0.45152623, 0.45451153, 0.45786999, 0.45563102,\n",
      "       0.45533249, 0.4484663 , 0.46234794, 0.45652661, 0.4567505 ,\n",
      "       0.45607881, 0.46070602, 0.46130308, 0.46757221, 0.46219867,\n",
      "       0.46861706, 0.46331816, 0.4627211 , 0.46869169, 0.47488619,\n",
      "       0.47712516, 0.47831928, 0.46951265, 0.47839391, 0.47600567,\n",
      "       0.47481155, 0.48033435, 0.48003582, 0.48421524, 0.48981267,\n",
      "       0.48727517, 0.4960818 , 0.49548474, 0.50197776, 0.50660497,\n",
      "       0.51302336, 0.50302261, 0.5069035 , 0.5128741 , 0.5149638 ]), 'train_losses': array([7.47675858, 1.25051363, 1.18837864, 1.15120142, 1.12386836,\n",
      "       1.11511865, 1.10425812, 1.0952823 , 1.08004312, 1.06914914,\n",
      "       1.05128264, 1.0400926 , 1.03398516, 1.0226939 , 1.00946875,\n",
      "       1.01015382, 1.0010733 , 0.99905368, 0.99285307, 0.99103828,\n",
      "       0.98196138, 0.9874487 , 0.9853806 , 0.97283543, 0.97010102,\n",
      "       0.97359787, 0.97049203, 0.96632432, 0.96515757, 0.96116742,\n",
      "       0.96435667, 0.96525845, 0.9620985 , 0.95622346, 0.95218953,\n",
      "       0.96201104, 0.95025617, 0.9573842 , 0.95477272, 0.95375789,\n",
      "       0.94793863, 0.9406222 , 0.94342369, 0.94481766, 0.937234  ,\n",
      "       0.93366397, 0.93282238, 0.93965381, 0.92818022, 0.93152868,\n",
      "       0.93032634, 0.92211285, 0.91716617, 0.92059408, 0.90927879,\n",
      "       0.91320255, 0.90922401, 0.89976477, 0.90911183, 0.90007721]), 'val_accs': array([0.44686567, 0.45313433, 0.38895522, 0.43910448, 0.44686567,\n",
      "       0.44686567, 0.44686567, 0.44686567, 0.44686567, 0.44686567,\n",
      "       0.44686567, 0.40208955, 0.42955224, 0.39223881, 0.42447761,\n",
      "       0.44686567, 0.37373134, 0.43880597, 0.4119403 , 0.45313433,\n",
      "       0.44686567, 0.43880597, 0.4519403 , 0.46149254, 0.45313433,\n",
      "       0.45313433, 0.45313433, 0.51731343, 0.42626866, 0.45223881,\n",
      "       0.46119403, 0.45313433, 0.4841791 , 0.45253731, 0.39044776,\n",
      "       0.46716418, 0.41223881, 0.47402985, 0.50835821, 0.46985075,\n",
      "       0.46149254, 0.45074627, 0.42298507, 0.50029851, 0.4919403 ,\n",
      "       0.50895522, 0.4558209 , 0.45880597, 0.52      , 0.51223881,\n",
      "       0.43164179, 0.49731343, 0.5361194 , 0.51791045, 0.50089552,\n",
      "       0.53253731, 0.54925373, 0.54268657, 0.4761194 , 0.53910448]), 'val_losses': array([1.10494949, 1.10863068, 1.09531702, 1.09831319, 1.06325349,\n",
      "       1.05490723, 1.03304122, 1.04653014, 1.04162069, 0.9983575 ,\n",
      "       0.98816136, 1.00135363, 0.99465875, 0.96113386, 0.99396048,\n",
      "       0.94351135, 0.95795647, 0.97272607, 0.94661162, 0.93260972,\n",
      "       0.94147167, 0.93222268, 0.9464584 , 0.95133189, 0.93035778,\n",
      "       0.92322527, 0.93842958, 0.93187533, 0.93160904, 0.91549831,\n",
      "       0.95273371, 0.92650479, 0.97705559, 0.90663275, 0.91325666,\n",
      "       0.90309204, 1.04768398, 0.90450391, 0.9190315 , 0.91897054,\n",
      "       0.89832408, 0.95033449, 0.97440864, 0.89126635, 0.89303523,\n",
      "       0.89226531, 0.90622916, 0.89852529, 0.88149954, 0.89305874,\n",
      "       0.89321644, 0.86836088, 0.8661537 , 0.86252821, 0.84184759,\n",
      "       0.87681068, 0.84669599, 0.85027719, 0.84029146, 0.82345076])}, 'ATTACH::15::best_val_loss': 0.8234507553613007, 'ATTACH::16::history': {'train_accs': array([0.3959997 , 0.4378685 , 0.47570714, 0.5294425 , 0.56690798,\n",
      "       0.59907456, 0.64512277, 0.70482872, 0.73811478, 0.78125233,\n",
      "       0.81229943, 0.83192776, 0.86872155, 0.88446899, 0.89611165,\n",
      "       0.90685872, 0.91551608, 0.92536756, 0.93118889, 0.93917457,\n",
      "       0.94492126, 0.94387641, 0.95671319, 0.95313083, 0.96298231,\n",
      "       0.9581312 , 0.96051944, 0.96551981, 0.9668632 , 0.97111725,\n",
      "       0.97283379, 0.97022166, 0.96820658, 0.97574446, 0.97619225,\n",
      "       0.97820733, 0.97887902, 0.97641615, 0.9776849 , 0.9776849 ,\n",
      "       0.98059557, 0.98275991, 0.98380476, 0.98492425, 0.98178969,\n",
      "       0.98559594, 0.98373013, 0.98693932, 0.98268528, 0.98522278,\n",
      "       0.98387939, 0.98559594, 0.98984999, 0.9887305 , 0.98970072,\n",
      "       0.98798418, 0.98910366, 0.9917158 , 0.98738712, 0.98947683]), 'train_losses': array([19.93799385,  5.58601251,  3.38011143,  2.2597936 ,  1.63342007,\n",
      "        1.35890442,  1.17151165,  0.92344711,  0.81286379,  0.64615366,\n",
      "        0.5781775 ,  0.53879011,  0.42051406,  0.36719932,  0.32729513,\n",
      "        0.2976155 ,  0.27874214,  0.24765283,  0.20445868,  0.1834837 ,\n",
      "        0.17454247,  0.18047225,  0.13922082,  0.14413195,  0.11463569,\n",
      "        0.1379346 ,  0.12497216,  0.10387928,  0.10735712,  0.09477433,\n",
      "        0.08411094,  0.09862773,  0.10541075,  0.07930628,  0.07457927,\n",
      "        0.077154  ,  0.06983052,  0.07453167,  0.06806122,  0.07563737,\n",
      "        0.06752448,  0.05800545,  0.05149322,  0.04758001,  0.05718302,\n",
      "        0.04905301,  0.05257394,  0.0440669 ,  0.06035769,  0.04922349,\n",
      "        0.05390901,  0.04169067,  0.03618364,  0.03526517,  0.0393835 ,\n",
      "        0.04137062,  0.03694905,  0.0295798 ,  0.04074231,  0.03571419]), 'val_accs': array([0.45970149, 0.54119403, 0.5919403 , 0.59552239, 0.65522388,\n",
      "       0.78746269, 0.62716418, 0.72925373, 0.79492537, 0.73313433,\n",
      "       0.86686567, 0.85761194, 0.92925373, 0.92716418, 0.94059701,\n",
      "       0.95074627, 0.93880597, 0.94716418, 0.92686567, 0.95313433,\n",
      "       0.9561194 , 0.95731343, 0.95671642, 0.93701493, 0.96746269,\n",
      "       0.9361194 , 0.95671642, 0.95940299, 0.96985075, 0.95671642,\n",
      "       0.97373134, 0.96865672, 0.95044776, 0.95462687, 0.95910448,\n",
      "       0.96477612, 0.95731343, 0.96656716, 0.96686567, 0.95343284,\n",
      "       0.97641791, 0.97970149, 0.97432836, 0.96686567, 0.97970149,\n",
      "       0.96119403, 0.97970149, 0.97641791, 0.97552239, 0.97850746,\n",
      "       0.98179104, 0.98358209, 0.96955224, 0.97552239, 0.97641791,\n",
      "       0.97492537, 0.98089552, 0.98716418, 0.97701493, 0.98328358]), 'val_losses': array([3.69863919, 1.65291232, 1.35824718, 0.90617697, 1.05630071,\n",
      "       0.61986962, 0.93805614, 0.93515854, 0.6345172 , 0.57817513,\n",
      "       0.35885873, 0.4195505 , 0.23710139, 0.19600588, 0.17936375,\n",
      "       0.16090131, 0.24168832, 0.16027325, 0.2088709 , 0.14300585,\n",
      "       0.14333405, 0.14538695, 0.13993508, 0.1984031 , 0.10993933,\n",
      "       0.22671857, 0.16142601, 0.12543824, 0.09898898, 0.1272914 ,\n",
      "       0.0967187 , 0.08700679, 0.1412982 , 0.13106893, 0.12608728,\n",
      "       0.09729097, 0.15184055, 0.12403532, 0.08243103, 0.18134463,\n",
      "       0.098333  , 0.07495564, 0.07901393, 0.10427035, 0.06779512,\n",
      "       0.15873599, 0.08207833, 0.08818017, 0.09272996, 0.08575487,\n",
      "       0.0722333 , 0.06188173, 0.11063045, 0.08841202, 0.08076004,\n",
      "       0.07744014, 0.06951417, 0.04973425, 0.0849661 , 0.06843384])}, 'ATTACH::16::best_val_loss': 0.04973425090533626, 'ATTACH::17::history': {'train_accs': array([0.40883648, 0.47891634, 0.53459213, 0.60094037, 0.65952683,\n",
      "       0.70721696, 0.74348832, 0.7701321 , 0.79811926, 0.82356892,\n",
      "       0.8368535 , 0.85409359, 0.87872229, 0.8973804 , 0.90663482,\n",
      "       0.92096425, 0.92984551, 0.93529368, 0.94007015, 0.94917531,\n",
      "       0.95477274, 0.9581312 , 0.96208672, 0.96544518, 0.96693783,\n",
      "       0.97126651, 0.97171431, 0.97372938, 0.9776849 , 0.97559519,\n",
      "       0.97962535, 0.98111799, 0.9804463 , 0.98171505, 0.98373013,\n",
      "       0.98358086, 0.98380476, 0.98484962, 0.98671543, 0.9859691 ,\n",
      "       0.98507351, 0.98865587, 0.98813344, 0.98805881, 0.98790955,\n",
      "       0.98910366, 0.98977536]), 'train_losses': array([7.53134544, 2.75178979, 1.88947994, 1.31921279, 1.11857443,\n",
      "       0.93485696, 0.76723727, 0.68491101, 0.60901012, 0.52199048,\n",
      "       0.49128107, 0.43164014, 0.35593718, 0.31643528, 0.28538621,\n",
      "       0.2441009 , 0.2126769 , 0.206207  , 0.18829731, 0.15807777,\n",
      "       0.15204708, 0.13686336, 0.12629575, 0.10974417, 0.10479509,\n",
      "       0.09273933, 0.09693579, 0.0857212 , 0.07384569, 0.07732679,\n",
      "       0.07110259, 0.06531388, 0.06226149, 0.06289255, 0.05213842,\n",
      "       0.05289969, 0.05430714, 0.05227781, 0.04417393, 0.04628347,\n",
      "       0.04598114, 0.03867912, 0.04049579, 0.03764439, 0.0383286 ,\n",
      "       0.03681026, 0.03257949]), 'val_accs': array([0.52835821, 0.5119403 , 0.55432836, 0.7080597 , 0.75343284,\n",
      "       0.78      , 0.85164179, 0.85701493, 0.88567164, 0.87402985,\n",
      "       0.8719403 , 0.85313433, 0.89791045, 0.8680597 , 0.91253731,\n",
      "       0.93343284, 0.92179104, 0.94537313, 0.94119403, 0.94776119,\n",
      "       0.94149254, 0.93402985, 0.94238806, 0.96059701, 0.96626866,\n",
      "       0.9761194 , 0.94567164, 0.96029851, 0.96298507, 0.96328358,\n",
      "       0.95910448, 0.98268657, 0.97731343, 0.96567164, 0.96955224,\n",
      "       0.97552239, 0.98298507, 0.98089552, 0.96626866, 0.97940299,\n",
      "       0.97223881, 0.97910448, 0.97970149, 0.97701493, 0.97701493,\n",
      "       0.97940299, 0.98149254]), 'val_losses': array([2.08011985, 1.76739614, 1.24196096, 0.82906287, 0.68330889,\n",
      "       0.63694871, 0.41836705, 0.44022934, 0.3054818 , 0.34938486,\n",
      "       0.38182086, 0.38169727, 0.29901992, 0.38748738, 0.22782336,\n",
      "       0.21469364, 0.23740354, 0.16844235, 0.17236249, 0.14272493,\n",
      "       0.15872464, 0.20497308, 0.1723183 , 0.11497785, 0.11097278,\n",
      "       0.08432246, 0.16458929, 0.12549529, 0.10762495, 0.11421394,\n",
      "       0.11657468, 0.0603428 , 0.0944246 , 0.11699587, 0.11039975,\n",
      "       0.08935367, 0.05754921, 0.06995958, 0.12074424, 0.06085586,\n",
      "       0.0970975 , 0.08121137, 0.07540936, 0.08016011, 0.07336652,\n",
      "       0.06036368, 0.07085584])}, 'ATTACH::17::best_val_loss': 0.057549208503550116, 'ATTACH::18::history': {'train_accs': array([0.43995821, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718]), 'train_losses': array([5.28708407, 1.24429423, 1.24240023, 1.24229736, 1.24199793,\n",
      "       1.24222812, 1.24227097, 1.24199958, 1.2421734 , 1.24224843,\n",
      "       1.24215924, 1.24212096, 1.24219821, 1.24217315, 1.2420662 ,\n",
      "       1.24230408, 1.24230299, 1.24238905, 1.24207265, 1.24235058,\n",
      "       1.24250444, 1.24216665, 1.2422719 , 1.24220266, 1.24209202,\n",
      "       1.24219546, 1.24218682, 1.24210553, 1.24216399, 1.24217335,\n",
      "       1.24226501, 1.2420774 , 1.24205972, 1.24210291]), 'val_accs': array([0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955]), 'val_losses': array([1.24699452, 1.24264504, 1.24270184, 1.24182464, 1.24233426,\n",
      "       1.24196826, 1.24176156, 1.24178358, 1.24197918, 1.24182536,\n",
      "       1.2417575 , 1.24201108, 1.24182372, 1.24183374, 1.24274591,\n",
      "       1.24178839, 1.24175014, 1.24175331, 1.24200772, 1.24181589,\n",
      "       1.24184381, 1.24184392, 1.24195478, 1.24173061, 1.24175903,\n",
      "       1.2421524 , 1.2418129 , 1.24174882, 1.24182601, 1.24189845,\n",
      "       1.24180365, 1.24192027, 1.24181835, 1.24221285])}, 'ATTACH::18::best_val_loss': 1.2417306122851015, 'ATTACH::19::history': {'train_accs': array([0.42809165, 0.43130084, 0.44219718, 0.43227107, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718]), 'train_losses': array([6.15246077e+03, 1.24841913e+00, 1.23951826e+00, 1.33529043e+00,\n",
      "       1.24693174e+00, 1.24432786e+00, 1.24356384e+00, 1.24539614e+00,\n",
      "       1.24625227e+00, 1.24446252e+00, 1.24470002e+00, 1.24665964e+00,\n",
      "       1.24392323e+00]), 'val_accs': array([0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955]), 'val_losses': array([1.25668579, 1.23855477, 1.236107  , 1.25919325, 1.2420646 ,\n",
      "       1.24404686, 1.24429859, 1.24262224, 1.24310542, 1.24209539,\n",
      "       1.24361639, 1.25135365, 1.241887  ])}, 'ATTACH::19::best_val_loss': 1.2361069987425164, 'ATTACH::20::history': {'train_accs': array([0.40271662, 0.47779685, 0.56571386, 0.61728487, 0.68214046,\n",
      "       0.73960743, 0.78744682, 0.82692738, 0.86006418, 0.88364803,\n",
      "       0.90558997, 0.9164863 , 0.92887529, 0.93738339, 0.94245839,\n",
      "       0.94887678, 0.95589223, 0.95828047, 0.963654  , 0.96649004,\n",
      "       0.96701246, 0.97029629, 0.97208747, 0.97290843, 0.97783417,\n",
      "       0.97865512, 0.97872976, 0.98052093, 0.9808941 , 0.9802224 ,\n",
      "       0.9804463 , 0.97999851, 0.98462572, 0.98275991, 0.98425256,\n",
      "       0.98537204, 0.98432719, 0.98447645, 0.9864169 , 0.98581984,\n",
      "       0.98753638, 0.98649153, 0.98701396, 0.98768565, 0.98790955,\n",
      "       0.98820808, 0.9887305 , 0.99059631, 0.98679006, 0.99052168,\n",
      "       0.99044705, 0.99052168, 0.98962609, 0.99141727]), 'train_losses': array([8.785632  , 2.90911936, 1.65152929, 1.12501714, 0.82766678,\n",
      "       0.65363238, 0.54012898, 0.46049886, 0.38324505, 0.32042391,\n",
      "       0.26284149, 0.23514192, 0.20122061, 0.17681333, 0.17186485,\n",
      "       0.14440883, 0.13210375, 0.11912653, 0.10219284, 0.09991785,\n",
      "       0.09823569, 0.08598677, 0.08908811, 0.08327158, 0.07041518,\n",
      "       0.0674896 , 0.0700843 , 0.06647628, 0.06430884, 0.05932323,\n",
      "       0.05951273, 0.05587237, 0.04890032, 0.05451129, 0.05110461,\n",
      "       0.04700469, 0.04978768, 0.04558332, 0.04278965, 0.04527567,\n",
      "       0.03792675, 0.04415022, 0.04171759, 0.04048119, 0.03902821,\n",
      "       0.03728009, 0.0340719 , 0.03243416, 0.0404001 , 0.03044041,\n",
      "       0.03050932, 0.03120325, 0.03511213, 0.02782379]), 'val_accs': array([0.52149254, 0.56567164, 0.64895522, 0.6441791 , 0.75014925,\n",
      "       0.81074627, 0.71044776, 0.87044776, 0.91820896, 0.90298507,\n",
      "       0.90059701, 0.93343284, 0.92626866, 0.93492537, 0.95343284,\n",
      "       0.93552239, 0.96      , 0.94059701, 0.95791045, 0.93731343,\n",
      "       0.97761194, 0.97343284, 0.96626866, 0.97552239, 0.96537313,\n",
      "       0.98208955, 0.98537313, 0.98119403, 0.98268657, 0.98298507,\n",
      "       0.98507463, 0.98268657, 0.96955224, 0.97432836, 0.98149254,\n",
      "       0.98567164, 0.98537313, 0.98626866, 0.98298507, 0.98597015,\n",
      "       0.98537313, 0.98567164, 0.98746269, 0.98955224, 0.98835821,\n",
      "       0.98776119, 0.98716418, 0.98507463, 0.98626866, 0.9838806 ,\n",
      "       0.9880597 , 0.98447761, 0.9838806 , 0.9719403 ]), 'val_losses': array([3.30176522, 1.67096495, 0.91292056, 0.82803942, 0.58963211,\n",
      "       0.45613595, 0.57844955, 0.35451689, 0.23095814, 0.23040447,\n",
      "       0.23196416, 0.16915039, 0.16304236, 0.15988728, 0.11204804,\n",
      "       0.14257901, 0.09791064, 0.16447852, 0.09916059, 0.16738804,\n",
      "       0.06295492, 0.0788171 , 0.08139043, 0.07705922, 0.10144622,\n",
      "       0.06491652, 0.04762619, 0.0638708 , 0.05362675, 0.04724984,\n",
      "       0.04487985, 0.05158967, 0.08472706, 0.08412107, 0.07138623,\n",
      "       0.04722373, 0.04641779, 0.03829615, 0.05323798, 0.04290736,\n",
      "       0.0525477 , 0.046622  , 0.03751581, 0.03113365, 0.03379312,\n",
      "       0.03623552, 0.04637845, 0.06085857, 0.04856998, 0.04782794,\n",
      "       0.03646526, 0.04507701, 0.05112526, 0.10951555])}, 'ATTACH::20::best_val_loss': 0.03113365327967192, 'ATTACH::21::history': {'train_accs': array([0.50973953, 0.62459885, 0.60407493, 0.61385178, 0.62534518,\n",
      "       0.62639003, 0.62571834, 0.60780655, 0.60646317, 0.59355176,\n",
      "       0.60019404, 0.60198522, 0.61094112, 0.60220912, 0.60706023]), 'train_losses': array([4.72517408, 0.80959362, 0.81076816, 0.79329178, 0.77274444,\n",
      "       0.77473996, 0.79094772, 0.81309078, 0.82758246, 0.8296336 ,\n",
      "       0.82687415, 0.83322086, 0.81443148, 0.81242384, 0.81219911]), 'val_accs': array([0.56985075, 0.67313433, 0.6241791 , 0.57671642, 0.70955224,\n",
      "       0.61641791, 0.66208955, 0.61791045, 0.58238806, 0.58179104,\n",
      "       0.57223881, 0.65731343, 0.63701493, 0.5880597 , 0.63223881]), 'val_losses': array([0.92049522, 0.73978836, 0.7576488 , 0.75688842, 0.68048826,\n",
      "       0.74447403, 0.73467125, 0.76159031, 0.86184534, 0.79392158,\n",
      "       0.84609444, 0.70908912, 0.77414138, 0.79611615, 0.74757874])}, 'ATTACH::21::best_val_loss': 0.6804882560915022, 'ATTACH::22::history': {'train_accs': array([0.42861408, 0.48428987, 0.52145683, 0.5536234 , 0.60728413,\n",
      "       0.6539294 , 0.69057392, 0.72221808, 0.74721994, 0.77326666,\n",
      "       0.78714829, 0.79252183, 0.80476155, 0.81326965, 0.81662811,\n",
      "       0.82797224, 0.83379357, 0.83648033, 0.8389432 , 0.84401821,\n",
      "       0.85177998, 0.85394432, 0.8584969 , 0.86319875, 0.86260169,\n",
      "       0.86902008, 0.86707963, 0.87260243, 0.87872229, 0.88096127,\n",
      "       0.88178222, 0.87939398, 0.88133443, 0.88648407, 0.88760355,\n",
      "       0.88760355]), 'train_losses': array([8.65658718, 2.9427281 , 1.76279503, 1.22206641, 0.89104405,\n",
      "       0.74830256, 0.66561363, 0.60817816, 0.5577108 , 0.52189293,\n",
      "       0.48512002, 0.47469455, 0.44893688, 0.43327015, 0.42453521,\n",
      "       0.39949223, 0.38240295, 0.386239  , 0.37617608, 0.35980337,\n",
      "       0.34948732, 0.34418191, 0.34655505, 0.33353837, 0.32736657,\n",
      "       0.31237367, 0.31237889, 0.30305531, 0.29383587, 0.28773857,\n",
      "       0.29101317, 0.29837927, 0.28963132, 0.28166775, 0.27739264,\n",
      "       0.27474023]), 'val_accs': array([0.4558209 , 0.55671642, 0.60626866, 0.54776119, 0.73552239,\n",
      "       0.70895522, 0.76208955, 0.74955224, 0.76238806, 0.78477612,\n",
      "       0.7758209 , 0.80925373, 0.81014925, 0.8161194 , 0.83880597,\n",
      "       0.80328358, 0.81253731, 0.8041791 , 0.81223881, 0.8       ,\n",
      "       0.83970149, 0.79253731, 0.85253731, 0.7961194 , 0.84358209,\n",
      "       0.87223881, 0.86895522, 0.80059701, 0.83283582, 0.86447761,\n",
      "       0.81462687, 0.80119403, 0.85820896, 0.81970149, 0.80268657,\n",
      "       0.82835821]), 'val_losses': array([4.16558264, 1.34669818, 0.88075415, 0.81511433, 0.62814981,\n",
      "       0.65255459, 0.49544903, 0.51727139, 0.51623991, 0.42562309,\n",
      "       0.46020851, 0.44915419, 0.42061751, 0.4195193 , 0.39407294,\n",
      "       0.46160383, 0.45173887, 0.51894819, 0.44196577, 0.51208275,\n",
      "       0.43408811, 0.48382944, 0.43983983, 0.48994219, 0.35981968,\n",
      "       0.32039574, 0.37046385, 0.44423112, 0.41195284, 0.38524427,\n",
      "       0.54047355, 0.51248932, 0.36026924, 0.35834346, 0.43940774,\n",
      "       0.43979824])}, 'ATTACH::22::best_val_loss': 0.3203957400748979, 'ATTACH::23::history': {'train_accs': array([0.49026047, 0.61004553, 0.67572207, 0.70870961, 0.72662139,\n",
      "       0.74893649, 0.76371371, 0.76281812, 0.7643854 , 0.77505784,\n",
      "       0.77140085, 0.77125159, 0.77192328, 0.76087768, 0.77207254,\n",
      "       0.78386447, 0.76811702, 0.77401299, 0.77453541, 0.76110157,\n",
      "       0.7503545 , 0.76953504, 0.76677364, 0.76557952, 0.77356519,\n",
      "       0.78438689, 0.76684827, 0.73236809, 0.73281588, 0.75528024,\n",
      "       0.75222031, 0.75348907]), 'train_losses': array([4.87903119, 0.84242615, 0.70253619, 0.64244487, 0.60864008,\n",
      "       0.56822877, 0.55394041, 0.55052316, 0.5527959 , 0.54096046,\n",
      "       0.52778408, 0.52880371, 0.51603136, 0.56524454, 0.51867268,\n",
      "       0.51315275, 0.54441506, 0.52087064, 0.52663547, 0.54918455,\n",
      "       0.58219703, 0.52142465, 0.56331564, 0.54012988, 0.52329647,\n",
      "       0.49919948, 0.52810383, 0.59688964, 0.61776431, 0.55861876,\n",
      "       0.57325247, 0.56182519]), 'val_accs': array([0.61701493, 0.62328358, 0.71044776, 0.73761194, 0.75940299,\n",
      "       0.80268657, 0.77014925, 0.78955224, 0.76447761, 0.78507463,\n",
      "       0.77492537, 0.80119403, 0.80835821, 0.74059701, 0.81970149,\n",
      "       0.78089552, 0.81492537, 0.7519403 , 0.76208955, 0.82      ,\n",
      "       0.77313433, 0.81402985, 0.80268657, 0.7958209 , 0.73970149,\n",
      "       0.8161194 , 0.74149254, 0.76179104, 0.73373134, 0.73074627,\n",
      "       0.78      , 0.77373134]), 'val_losses': array([0.7547635 , 0.81613852, 0.59808869, 0.56820962, 0.56273823,\n",
      "       0.47378355, 0.50416772, 0.49451094, 0.48879612, 0.4668393 ,\n",
      "       0.53199602, 0.55571086, 0.50157325, 0.63733049, 0.43090054,\n",
      "       0.48458731, 0.41950287, 0.61185846, 0.46053034, 0.47267032,\n",
      "       0.46054948, 0.40821424, 0.46386752, 0.48358707, 0.59024323,\n",
      "       0.43041709, 0.53661347, 0.54420745, 0.52941089, 0.54244628,\n",
      "       0.62407399, 0.4901609 ])}, 'ATTACH::23::best_val_loss': 0.4082142405723458, 'ATTACH::24::history': {'train_accs': array([0.34129413, 0.42144936, 0.45555638, 0.47085603, 0.47899097,\n",
      "       0.49951489, 0.53011419, 0.56302709, 0.59049183, 0.61109038,\n",
      "       0.63422643, 0.64661542, 0.67169192, 0.69072319, 0.70908277,\n",
      "       0.72512874, 0.72803941, 0.74117471, 0.75326517, 0.76901261,\n",
      "       0.77334129, 0.78722293, 0.79737294, 0.80983655, 0.81871781,\n",
      "       0.82901709, 0.84222703, 0.84715277, 0.85498918, 0.86379581,\n",
      "       0.87327412, 0.87633405, 0.88133443, 0.89081275, 0.89797746,\n",
      "       0.90350026, 0.90663482, 0.90879916, 0.91685947, 0.92036719,\n",
      "       0.92574073, 0.92999478, 0.9305172 , 0.93745802, 0.94163744,\n",
      "       0.93992089, 0.94633928, 0.94663781, 0.94969774, 0.95462348,\n",
      "       0.95462348, 0.95798194, 0.96007165, 0.96022091, 0.963654  ,\n",
      "       0.9668632 , 0.96566908, 0.96835585, 0.96895291, 0.9698485 ]), 'train_losses': array([22.97176893,  7.07703931,  4.16717593,  2.89647366,  2.33432379,\n",
      "        1.91358179,  1.62574731,  1.37788661,  1.23767977,  1.11633237,\n",
      "        0.98017333,  0.93197298,  0.86030027,  0.79510932,  0.73828166,\n",
      "        0.69588404,  0.68427425,  0.65761237,  0.62415484,  0.57861707,\n",
      "        0.5871944 ,  0.54666182,  0.51967361,  0.49804731,  0.47520694,\n",
      "        0.45510643,  0.42515492,  0.40679396,  0.39121431,  0.36930807,\n",
      "        0.35253868,  0.337564  ,  0.32229078,  0.30931783,  0.29044314,\n",
      "        0.27511995,  0.25981749,  0.25620234,  0.23256927,  0.22738699,\n",
      "        0.20867899,  0.20233758,  0.19765752,  0.18458251,  0.17265648,\n",
      "        0.17048041,  0.15419269,  0.15131081,  0.15072349,  0.13633334,\n",
      "        0.1409852 ,  0.12523213,  0.1202327 ,  0.12420169,  0.11008693,\n",
      "        0.10700789,  0.10563141,  0.09392042,  0.09568429,  0.09150658]), 'val_accs': array([0.45223881, 0.51910448, 0.58746269, 0.55074627, 0.59671642,\n",
      "       0.49462687, 0.53970149, 0.65850746, 0.57731343, 0.68626866,\n",
      "       0.57223881, 0.68477612, 0.78865672, 0.71014925, 0.76208955,\n",
      "       0.68985075, 0.78567164, 0.84208955, 0.77522388, 0.72298507,\n",
      "       0.81462687, 0.71671642, 0.84955224, 0.8041791 , 0.81910448,\n",
      "       0.84895522, 0.89343284, 0.87223881, 0.85373134, 0.89910448,\n",
      "       0.87462687, 0.87791045, 0.91850746, 0.87014925, 0.92268657,\n",
      "       0.93253731, 0.89791045, 0.92835821, 0.89343284, 0.91701493,\n",
      "       0.92776119, 0.94089552, 0.94119403, 0.93671642, 0.91134328,\n",
      "       0.95671642, 0.92567164, 0.95671642, 0.94865672, 0.95014925,\n",
      "       0.96119403, 0.89850746, 0.9441791 , 0.92298507, 0.96776119,\n",
      "       0.96686567, 0.96208955, 0.97074627, 0.97104478, 0.93791045]), 'val_losses': array([4.4690011 , 1.55773099, 1.32710702, 1.06110049, 1.25233568,\n",
      "       1.40853725, 1.34841059, 0.84974385, 1.02266463, 0.85062953,\n",
      "       1.19507845, 0.78343054, 0.52270551, 0.7952916 , 0.55780138,\n",
      "       0.63920484, 0.52991244, 0.41815643, 0.5099127 , 0.70054307,\n",
      "       0.50982537, 0.65564996, 0.39371602, 0.51473725, 0.46744045,\n",
      "       0.41639268, 0.29297346, 0.34792422, 0.40155439, 0.27901651,\n",
      "       0.34908508, 0.31201172, 0.22676865, 0.38236276, 0.23102911,\n",
      "       0.19599919, 0.29786943, 0.1991779 , 0.33147859, 0.2380529 ,\n",
      "       0.21830774, 0.17815156, 0.17131549, 0.1881464 , 0.26742308,\n",
      "       0.12092289, 0.21194274, 0.11284271, 0.14927015, 0.14669758,\n",
      "       0.09954033, 0.32847947, 0.15010365, 0.22757723, 0.08873575,\n",
      "       0.09143887, 0.11119593, 0.09434343, 0.08722602, 0.19749677])}, 'ATTACH::24::best_val_loss': 0.08722602104295546, 'ATTACH::25::history': {'train_accs': array([0.40353758, 0.48443914, 0.53862229, 0.59302933, 0.65430256,\n",
      "       0.71072468, 0.75774312, 0.80282111, 0.83304724, 0.86178073,\n",
      "       0.889544  , 0.90812747, 0.92648705, 0.93805508, 0.94447347,\n",
      "       0.95529517, 0.95977312, 0.96790805, 0.96716173, 0.97335622,\n",
      "       0.97671468, 0.97544593, 0.97955071, 0.97880439, 0.9806702 ,\n",
      "       0.9836555 , 0.98544668, 0.98611837, 0.98380476, 0.98693932,\n",
      "       0.98544668, 0.98671543, 0.98731249, 0.98746175, 0.98984999,\n",
      "       0.98753638, 0.98970072, 0.98984999, 0.98955146, 0.98984999,\n",
      "       0.98955146, 0.99231286, 0.98970072, 0.99037242, 0.9917158 ,\n",
      "       0.99276065, 0.98992462, 0.99052168, 0.99253676, 0.99208896,\n",
      "       0.99268602, 0.99298455]), 'train_losses': array([8.82881312, 2.95954413, 1.79391291, 1.28461802, 0.9488209 ,\n",
      "       0.74937051, 0.62759503, 0.52054463, 0.44537994, 0.37801359,\n",
      "       0.30939844, 0.25898739, 0.218448  , 0.18214028, 0.16028659,\n",
      "       0.12915616, 0.12118057, 0.10521454, 0.10329712, 0.08132517,\n",
      "       0.07790896, 0.07701785, 0.06890975, 0.06636406, 0.06502097,\n",
      "       0.05358384, 0.0511803 , 0.04636902, 0.05022812, 0.04388317,\n",
      "       0.04750464, 0.04279262, 0.04660903, 0.04100715, 0.03489356,\n",
      "       0.04067994, 0.0369943 , 0.03566032, 0.03613506, 0.03416029,\n",
      "       0.03356013, 0.02676462, 0.03254372, 0.03226064, 0.0284759 ,\n",
      "       0.02381134, 0.03006012, 0.02763238, 0.02292571, 0.02752705,\n",
      "       0.02356678, 0.02483432]), 'val_accs': array([0.50716418, 0.61761194, 0.70179104, 0.71432836, 0.67462687,\n",
      "       0.78865672, 0.85074627, 0.80029851, 0.89701493, 0.91104478,\n",
      "       0.91223881, 0.92686567, 0.92149254, 0.9158209 , 0.96716418,\n",
      "       0.95522388, 0.95253731, 0.94835821, 0.9438806 , 0.96328358,\n",
      "       0.95701493, 0.95373134, 0.9719403 , 0.96746269, 0.96656716,\n",
      "       0.97432836, 0.97880597, 0.97791045, 0.97701493, 0.96149254,\n",
      "       0.97552239, 0.97791045, 0.97671642, 0.98119403, 0.97522388,\n",
      "       0.96895522, 0.97462687, 0.97731343, 0.98119403, 0.96895522,\n",
      "       0.98328358, 0.98686567, 0.9761194 , 0.97044776, 0.96955224,\n",
      "       0.97313433, 0.96716418, 0.94626866, 0.96179104, 0.98      ,\n",
      "       0.98059701, 0.98208955]), 'val_losses': array([1.68545153, 1.34752247, 0.87520958, 0.69477226, 0.71276365,\n",
      "       0.53294743, 0.40419843, 0.53222979, 0.30787809, 0.24948298,\n",
      "       0.24246952, 0.21869208, 0.20411947, 0.19564462, 0.10876502,\n",
      "       0.10488933, 0.11550188, 0.13027489, 0.18256381, 0.09474984,\n",
      "       0.09968351, 0.10183765, 0.08925872, 0.08928391, 0.07790825,\n",
      "       0.07385564, 0.06679907, 0.06597623, 0.07217216, 0.09963984,\n",
      "       0.07622635, 0.07624277, 0.06752987, 0.0685845 , 0.05556802,\n",
      "       0.10801582, 0.08261466, 0.06513296, 0.05955373, 0.07880241,\n",
      "       0.05204164, 0.04707279, 0.0853708 , 0.09890291, 0.08144616,\n",
      "       0.08424702, 0.0819362 , 0.14828858, 0.10141691, 0.05079287,\n",
      "       0.06648819, 0.07173967])}, 'ATTACH::25::best_val_loss': 0.047072788767739016, 'ATTACH::26::history': {'train_accs': array([0.46533323, 0.55444436, 0.65251138, 0.72356146, 0.78035674,\n",
      "       0.81312038, 0.84342115, 0.86693037, 0.87678185, 0.88484215,\n",
      "       0.89103664, 0.89924621, 0.91111277, 0.91462049, 0.92365102,\n",
      "       0.92484514, 0.930965  , 0.93805508, 0.93044257, 0.94007015,\n",
      "       0.9477573 , 0.94850362, 0.95342936, 0.9500709 , 0.95626539,\n",
      "       0.95723561, 0.95447421, 0.96037018, 0.958579  , 0.963654  ,\n",
      "       0.9588029 , 0.9615643 , 0.96402717, 0.96760952, 0.96589298,\n",
      "       0.96738563, 0.96581834, 0.96857974]), 'train_losses': array([6.09028038, 1.41426511, 0.81929083, 0.65381654, 0.52782378,\n",
      "       0.46394518, 0.39620145, 0.35322164, 0.31951115, 0.30808498,\n",
      "       0.28691074, 0.26915073, 0.24343232, 0.23200086, 0.21663037,\n",
      "       0.21161213, 0.20400515, 0.17478565, 0.19622084, 0.16532468,\n",
      "       0.15161805, 0.15030917, 0.13392426, 0.14773849, 0.13119728,\n",
      "       0.13073357, 0.14084452, 0.11987137, 0.12529491, 0.1055558 ,\n",
      "       0.12399267, 0.1126254 , 0.11552633, 0.09963454, 0.1073409 ,\n",
      "       0.10031334, 0.10755786, 0.09577529]), 'val_accs': array([0.58567164, 0.62985075, 0.73313433, 0.76835821, 0.80597015,\n",
      "       0.85641791, 0.85761194, 0.85014925, 0.8958209 , 0.90925373,\n",
      "       0.9041791 , 0.92955224, 0.89402985, 0.94208955, 0.89910448,\n",
      "       0.88955224, 0.90298507, 0.92537313, 0.92059701, 0.9519403 ,\n",
      "       0.94119403, 0.94298507, 0.93014925, 0.93820896, 0.94507463,\n",
      "       0.96029851, 0.86686567, 0.98149254, 0.93701493, 0.94089552,\n",
      "       0.94567164, 0.93820896, 0.93970149, 0.97791045, 0.97074627,\n",
      "       0.96447761, 0.96716418, 0.95731343]), 'val_losses': array([1.05803644, 0.7633301 , 0.60120543, 0.51540134, 0.42591148,\n",
      "       0.3438942 , 0.35524595, 0.37641466, 0.23196117, 0.20640024,\n",
      "       0.22790956, 0.17626335, 0.23690527, 0.15958936, 0.22518163,\n",
      "       0.25093398, 0.25957833, 0.20416855, 0.19452558, 0.12824718,\n",
      "       0.13919018, 0.1315704 , 0.17631023, 0.17462576, 0.12182732,\n",
      "       0.13549212, 0.34202409, 0.07081268, 0.1706287 , 0.14918788,\n",
      "       0.16590039, 0.12015326, 0.16841429, 0.07724829, 0.079871  ,\n",
      "       0.10350169, 0.11142037, 0.13274567])}, 'ATTACH::26::best_val_loss': 0.07081267909772361, 'ATTACH::27::history': {'train_accs': array([0.27502052, 0.34793641, 0.38017763, 0.38823793, 0.40614971,\n",
      "       0.40413464, 0.4125681 , 0.43480857, 0.44801851, 0.45160087,\n",
      "       0.4737667 , 0.49026047, 0.48802149, 0.49787298, 0.51354579,\n",
      "       0.5177252 , 0.52309874, 0.53601015, 0.53884618, 0.55548922,\n",
      "       0.55869841, 0.57496828, 0.5899694 , 0.59168595, 0.59646242,\n",
      "       0.60840361, 0.62430032, 0.63422643, 0.65236212, 0.65795955,\n",
      "       0.67661766, 0.68064781, 0.69639525, 0.71050078, 0.7144563 ,\n",
      "       0.72632286, 0.7310247 , 0.7337861 , 0.75027987, 0.75572804]), 'train_losses': array([38.00309281, 20.00302339, 15.11910559, 12.31163586, 10.10450082,\n",
      "        8.80468043,  7.70410231,  6.5072161 ,  5.73441936,  5.3184431 ,\n",
      "        4.6028569 ,  4.22160792,  3.96540121,  3.64168631,  3.38749618,\n",
      "        3.12554811,  3.03390137,  2.80692185,  2.63053756,  2.4478564 ,\n",
      "        2.38873986,  2.20271411,  2.04388753,  1.99711694,  1.90913007,\n",
      "        1.81006628,  1.67798679,  1.64454902,  1.51464445,  1.45641583,\n",
      "        1.32594754,  1.26875064,  1.21164513,  1.18625769,  1.13723469,\n",
      "        1.10502195,  1.05058672,  0.99009314,  0.94266605,  0.92141856]), 'val_accs': array([0.5041791 , 0.41462687, 0.44656716, 0.50089552, 0.48985075,\n",
      "       0.52328358, 0.55641791, 0.51014925, 0.57014925, 0.56089552,\n",
      "       0.55731343, 0.59462687, 0.6280597 , 0.59970149, 0.64268657,\n",
      "       0.64925373, 0.60895522, 0.58567164, 0.60895522, 0.65253731,\n",
      "       0.63104478, 0.61164179, 0.61373134, 0.62268657, 0.71671642,\n",
      "       0.71104478, 0.73492537, 0.68328358, 0.72895522, 0.76985075,\n",
      "       0.7641791 , 0.78746269, 0.78895522, 0.72059701, 0.75970149,\n",
      "       0.81522388, 0.73164179, 0.71014925, 0.78298507, 0.80507463]), 'val_losses': array([ 8.54089663, 12.4135859 ,  8.7412353 ,  6.64375875,  5.45614415,\n",
      "        2.93816039,  2.2872832 ,  2.54717751,  2.38146415,  1.83350698,\n",
      "        2.06289986,  1.45201054,  1.46450971,  1.24752166,  1.09804143,\n",
      "        1.23887644,  1.47998983,  1.53409635,  1.32203797,  0.91765793,\n",
      "        1.33695892,  1.18955318,  1.36780702,  1.27864104,  0.79985546,\n",
      "        0.81977548,  0.69845696,  0.91519415,  0.79657555,  0.57949387,\n",
      "        0.65041335,  0.59662015,  0.58983495,  0.93599435,  0.8227144 ,\n",
      "        0.60243645,  1.02057535,  1.14176691,  0.76997723,  0.64080971])}, 'ATTACH::27::best_val_loss': 0.5794938695964529, 'ATTACH::28::history': {'train_accs': array([0.40040302, 0.46757221, 0.53123367, 0.57735652, 0.61922532,\n",
      "       0.65288454, 0.69408165, 0.7174416 , 0.74557803, 0.77065453,\n",
      "       0.7869244 , 0.80886633, 0.82991268, 0.8421524 , 0.86028808,\n",
      "       0.87170684, 0.88737966, 0.90656019, 0.91753116, 0.92021793,\n",
      "       0.92835286, 0.93753265, 0.93969699, 0.94439884, 0.95253377,\n",
      "       0.95387716, 0.9583551 , 0.95895216, 0.96417643, 0.96760952,\n",
      "       0.96850511, 0.96932607, 0.9696246 , 0.97581909, 0.97171431,\n",
      "       0.97626689, 0.97581909, 0.97626689, 0.97947608, 0.98029704,\n",
      "       0.97917755, 0.9806702 , 0.98275991, 0.9834316 , 0.98395403,\n",
      "       0.98410329, 0.98417792, 0.98559594, 0.98611837, 0.98604373,\n",
      "       0.98529741, 0.98649153, 0.98723785, 0.98746175, 0.98686469]), 'train_losses': array([12.28488131,  3.90635297,  2.28199311,  1.65086655,  1.29345431,\n",
      "        1.06815035,  0.88163226,  0.80663715,  0.7081342 ,  0.639544  ,\n",
      "        0.58164269,  0.51505715,  0.46512095,  0.43133075,  0.39314035,\n",
      "        0.35216969,  0.30908549,  0.27151138,  0.23930747,  0.22892729,\n",
      "        0.20025818,  0.18289786,  0.17131856,  0.15876272,  0.14085287,\n",
      "        0.13751503,  0.12611183,  0.11638648,  0.11234997,  0.09891075,\n",
      "        0.09600973,  0.09019969,  0.09273863,  0.08097646,  0.0862671 ,\n",
      "        0.07488303,  0.07207649,  0.0711895 ,  0.06395088,  0.06359218,\n",
      "        0.0652518 ,  0.05975881,  0.05412859,  0.05576338,  0.04931805,\n",
      "        0.04943514,  0.04999159,  0.04609474,  0.04405195,  0.04327995,\n",
      "        0.04477729,  0.04323587,  0.03903189,  0.03913765,  0.04032087]), 'val_accs': array([0.49761194, 0.58567164, 0.6761194 , 0.57850746, 0.58985075,\n",
      "       0.56895522, 0.71223881, 0.7319403 , 0.75701493, 0.7719403 ,\n",
      "       0.83253731, 0.80029851, 0.86955224, 0.89462687, 0.86447761,\n",
      "       0.90925373, 0.93820896, 0.89432836, 0.91283582, 0.94      ,\n",
      "       0.92925373, 0.92358209, 0.94567164, 0.94955224, 0.95402985,\n",
      "       0.9361194 , 0.94746269, 0.95223881, 0.95313433, 0.94597015,\n",
      "       0.97313433, 0.96268657, 0.96597015, 0.9638806 , 0.96686567,\n",
      "       0.96029851, 0.98059701, 0.97880597, 0.9758209 , 0.96925373,\n",
      "       0.97880597, 0.96537313, 0.97134328, 0.95492537, 0.97820896,\n",
      "       0.97462687, 0.97940299, 0.97373134, 0.9838806 , 0.97910448,\n",
      "       0.98119403, 0.96537313, 0.98149254, 0.98298507, 0.97552239]), 'val_losses': array([2.4285496 , 1.5060853 , 1.14304294, 2.44164271, 1.46807352,\n",
      "       1.22950912, 0.67020104, 0.76812864, 0.69284774, 0.68809637,\n",
      "       0.43517895, 0.49135254, 0.35974197, 0.28033407, 0.34562854,\n",
      "       0.27584507, 0.18785953, 0.33928521, 0.23569857, 0.1793673 ,\n",
      "       0.18813356, 0.21428782, 0.13030258, 0.12854914, 0.12278474,\n",
      "       0.18875251, 0.14795006, 0.13016646, 0.12763331, 0.15781325,\n",
      "       0.09684514, 0.0960311 , 0.08444556, 0.08890292, 0.09281414,\n",
      "       0.0971204 , 0.06737745, 0.06519955, 0.06134547, 0.07743891,\n",
      "       0.06822023, 0.10084196, 0.08025595, 0.12498167, 0.05490622,\n",
      "       0.06855445, 0.07295855, 0.06949322, 0.0559692 , 0.08025438,\n",
      "       0.06948401, 0.11874721, 0.06615786, 0.05610743, 0.08184091])}, 'ATTACH::28::best_val_loss': 0.054906215106151, 'ATTACH::29::history': {'train_accs': array([0.31935219, 0.37890887, 0.40406   , 0.43353982, 0.47152773,\n",
      "       0.5011568 , 0.51854616, 0.53966714, 0.56145981, 0.58608851,\n",
      "       0.60108963, 0.61616539, 0.63855512, 0.66146727, 0.68281215,\n",
      "       0.69923129, 0.71960594, 0.74445854, 0.76304202, 0.78774535,\n",
      "       0.80476155, 0.8230465 , 0.84110755, 0.85110829, 0.86200463,\n",
      "       0.87722964, 0.88551384, 0.89827599, 0.90551534, 0.91544145,\n",
      "       0.92260616, 0.92678558, 0.93208448, 0.93574147, 0.94036868,\n",
      "       0.94238376, 0.94910068, 0.94924994, 0.95656392, 0.95977312,\n",
      "       0.96395253, 0.96626614, 0.96798269, 0.96895291, 0.972386  ,\n",
      "       0.97440107, 0.97141578, 0.97566983, 0.97604299, 0.97977461,\n",
      "       0.97761027]), 'train_losses': array([27.43797277, 11.20998956,  7.84033116,  6.12951596,  4.51555095,\n",
      "        3.65469017,  3.10126033,  2.56786192,  2.27665554,  1.97352769,\n",
      "        1.7671265 ,  1.55919227,  1.39226726,  1.23072375,  1.08526252,\n",
      "        1.03793156,  0.91467696,  0.82242118,  0.74308332,  0.66701774,\n",
      "        0.60284762,  0.54150502,  0.4921638 ,  0.45052121,  0.4174401 ,\n",
      "        0.37021085,  0.34636763,  0.3044879 ,  0.28194885,  0.25863015,\n",
      "        0.23982481,  0.22220852,  0.21004809,  0.20115448,  0.1804904 ,\n",
      "        0.16808544,  0.14789281,  0.14559623,  0.13289228,  0.12297194,\n",
      "        0.11037445,  0.10613873,  0.09988603,  0.09095048,  0.0820953 ,\n",
      "        0.07966612,  0.08988711,  0.07392409,  0.07426601,  0.05845056,\n",
      "        0.06766443]), 'val_accs': array([0.44835821, 0.5041791 , 0.53910448, 0.53850746, 0.5841791 ,\n",
      "       0.64059701, 0.60328358, 0.65134328, 0.65343284, 0.68059701,\n",
      "       0.69641791, 0.61402985, 0.70925373, 0.74716418, 0.60179104,\n",
      "       0.76447761, 0.81731343, 0.81940299, 0.78746269, 0.85223881,\n",
      "       0.86746269, 0.8041791 , 0.88567164, 0.91134328, 0.89671642,\n",
      "       0.89940299, 0.87522388, 0.9080597 , 0.89164179, 0.93641791,\n",
      "       0.93492537, 0.91492537, 0.93014925, 0.90059701, 0.95910448,\n",
      "       0.93761194, 0.9119403 , 0.91761194, 0.95134328, 0.96119403,\n",
      "       0.97641791, 0.91701493, 0.96865672, 0.97134328, 0.95552239,\n",
      "       0.91164179, 0.95761194, 0.96985075, 0.97253731, 0.9638806 ,\n",
      "       0.97343284]), 'val_losses': array([7.24568969, 3.2732663 , 2.61944384, 2.77521018, 2.73021001,\n",
      "       1.21905429, 1.1242385 , 1.38506159, 1.19873612, 1.03427068,\n",
      "       0.73863898, 1.42027288, 0.80801057, 0.70044532, 1.35535369,\n",
      "       0.64169253, 0.52368208, 0.49389871, 0.68550913, 0.50579329,\n",
      "       0.38450244, 0.57794443, 0.34563341, 0.25619953, 0.32559157,\n",
      "       0.29251057, 0.38565191, 0.29623795, 0.35940018, 0.19530908,\n",
      "       0.19370587, 0.23535066, 0.21032978, 0.2878141 , 0.12287997,\n",
      "       0.1836412 , 0.24896872, 0.24463937, 0.14298466, 0.11468029,\n",
      "       0.07722549, 0.24690642, 0.10129474, 0.08886089, 0.14080413,\n",
      "       0.29993908, 0.13032524, 0.09339459, 0.08647668, 0.11927967,\n",
      "       0.09124073])}, 'ATTACH::29::best_val_loss': 0.07722549435807698}\n",
      "{'ATTACH::0::history': {'train_accs': array([0.4518994 , 0.51548623, 0.58131204, 0.6645272 , 0.73662214,\n",
      "       0.78856631, 0.82871856, 0.85760131, 0.88499142, 0.90163445,\n",
      "       0.91656094, 0.92790507, 0.93365177, 0.94350325, 0.94813046,\n",
      "       0.94857825, 0.95320546, 0.95977312, 0.963654  , 0.9666393 ,\n",
      "       0.96551981, 0.96902754, 0.96835585, 0.9696246 , 0.97096798,\n",
      "       0.97432644, 0.97417718, 0.9749235 , 0.97775953, 0.97708784,\n",
      "       0.97574446, 0.97820733, 0.97902829, 0.97656542, 0.97686395,\n",
      "       0.98052093, 0.98231211, 0.98149116, 0.97887902, 0.98037167,\n",
      "       0.98178969, 0.98164042, 0.97731174, 0.98186432, 0.98335697]), 'train_losses': array([10.04284371,  2.71682624,  1.2774641 ,  0.78583214,  0.61904869,\n",
      "        0.52669077,  0.44757689,  0.38584048,  0.32379684,  0.27753393,\n",
      "        0.24942398,  0.21220298,  0.19791574,  0.16977957,  0.16230817,\n",
      "        0.15508673,  0.13639289,  0.12508804,  0.11127144,  0.1068676 ,\n",
      "        0.11583294,  0.09715709,  0.09740251,  0.08909153,  0.0916541 ,\n",
      "        0.08261241,  0.07738615,  0.07992494,  0.07088772,  0.07107399,\n",
      "        0.07904949,  0.07098662,  0.06694293,  0.07364642,  0.07296886,\n",
      "        0.06197401,  0.05669513,  0.05633569,  0.0699252 ,  0.06286459,\n",
      "        0.05824107,  0.05976448,  0.07421324,  0.06015917,  0.0532728 ]), 'val_accs': array([0.58955224, 0.56447761, 0.64      , 0.80179104, 0.73492537,\n",
      "       0.7958209 , 0.86089552, 0.91014925, 0.8838806 , 0.88776119,\n",
      "       0.88955224, 0.94537313, 0.92865672, 0.94238806, 0.93104478,\n",
      "       0.96925373, 0.96328358, 0.96238806, 0.95910448, 0.93910448,\n",
      "       0.95432836, 0.97970149, 0.98298507, 0.97253731, 0.96656716,\n",
      "       0.97671642, 0.97731343, 0.96776119, 0.9838806 , 0.95671642,\n",
      "       0.97104478, 0.9758209 , 0.97910448, 0.95164179, 0.98208955,\n",
      "       0.96656716, 0.98179104, 0.93791045, 0.97373134, 0.97014925,\n",
      "       0.96925373, 0.97402985, 0.97373134, 0.96835821, 0.97850746]), 'val_losses': array([2.61059   , 1.52511318, 1.04284595, 0.53509804, 0.57166124,\n",
      "       0.46075789, 0.35841548, 0.25498852, 0.29311326, 0.28414617,\n",
      "       0.23009658, 0.16311511, 0.16822274, 0.16094405, 0.24788839,\n",
      "       0.09390897, 0.12020408, 0.11153982, 0.11122687, 0.1533014 ,\n",
      "       0.14568903, 0.06401595, 0.05996008, 0.09201587, 0.11876474,\n",
      "       0.07365443, 0.07297615, 0.11634912, 0.0627298 , 0.14396984,\n",
      "       0.08892854, 0.07656752, 0.05923762, 0.10992161, 0.05890393,\n",
      "       0.11640224, 0.07243637, 0.23659288, 0.09256909, 0.08592668,\n",
      "       0.10451924, 0.08219866, 0.08764731, 0.07968991, 0.069785  ])}, 'ATTACH::0::best_val_loss': 0.05890393359225188, 'ATTACH::1::history': {'train_accs': array([0.36741548, 0.43204717, 0.46555713, 0.46981118, 0.47779685,\n",
      "       0.49324576, 0.50541085, 0.5177252 , 0.53056198, 0.53690574,\n",
      "       0.53742817, 0.55242929, 0.5648929 , 0.573401  , 0.58526756,\n",
      "       0.58026718, 0.60392567, 0.63004702, 0.66952758, 0.69997761,\n",
      "       0.72139712, 0.75647436, 0.78543175, 0.80543324, 0.83983879,\n",
      "       0.85745205, 0.88342414, 0.8950668 , 0.90812747, 0.90775431,\n",
      "       0.92365102, 0.92924845, 0.93551758, 0.94439884, 0.94447347,\n",
      "       0.94723487, 0.94760803, 0.95910143, 0.96231062, 0.95828047,\n",
      "       0.96551981, 0.96477349, 0.96559445, 0.97066945, 0.96484812,\n",
      "       0.97544593, 0.97246063, 0.97522203, 0.97477424, 0.97641615,\n",
      "       0.97999851, 0.98104336, 0.97678931]), 'train_losses': array([41.45130826,  8.58134765,  5.26844911,  3.81136717,  3.01608075,\n",
      "        2.42917237,  2.22354322,  1.81252394,  1.66217882,  1.49236557,\n",
      "        1.45410547,  1.31931585,  1.14886229,  1.09214528,  1.05832689,\n",
      "        1.04959347,  0.98249183,  0.8901367 ,  0.78671977,  0.7549662 ,\n",
      "        0.69733172,  0.62025657,  0.55566775,  0.52849605,  0.43071228,\n",
      "        0.38261585,  0.32297503,  0.29707673,  0.26661528,  0.25645211,\n",
      "        0.21387653,  0.20054326,  0.18352381,  0.16372188,  0.15820577,\n",
      "        0.15857591,  0.1471323 ,  0.11904918,  0.11242248,  0.11356388,\n",
      "        0.10135146,  0.10335785,  0.09605121,  0.08797805,  0.09823577,\n",
      "        0.07263803,  0.08302541,  0.07598728,  0.07614658,  0.06987018,\n",
      "        0.06085361,  0.05976165,  0.06866907]), 'val_accs': array([0.44895522, 0.47402985, 0.5841791 , 0.63731343, 0.54328358,\n",
      "       0.56567164, 0.63641791, 0.62955224, 0.57492537, 0.57402985,\n",
      "       0.61432836, 0.55880597, 0.68985075, 0.67880597, 0.62567164,\n",
      "       0.63014925, 0.68567164, 0.70119403, 0.77104478, 0.78238806,\n",
      "       0.7480597 , 0.81910448, 0.78089552, 0.79492537, 0.89761194,\n",
      "       0.91134328, 0.89910448, 0.87044776, 0.85552239, 0.92985075,\n",
      "       0.92835821, 0.94358209, 0.94268657, 0.94179104, 0.9558209 ,\n",
      "       0.95164179, 0.95791045, 0.95014925, 0.95402985, 0.96507463,\n",
      "       0.95552239, 0.95671642, 0.97134328, 0.95044776, 0.96537313,\n",
      "       0.96328358, 0.95910448, 0.95552239, 0.96089552, 0.96238806,\n",
      "       0.95522388, 0.97313433, 0.95432836]), 'val_losses': array([3.36594628, 2.00730547, 1.69972558, 1.0971265 , 1.08393374,\n",
      "       1.40936323, 0.83369804, 0.91192769, 1.99137562, 1.07773   ,\n",
      "       0.93344134, 1.10088985, 0.71728186, 0.72804537, 0.79109527,\n",
      "       0.79876097, 0.79071109, 0.58136442, 0.5183043 , 0.48989397,\n",
      "       0.59371158, 0.43411434, 0.492746  , 0.47655857, 0.28311018,\n",
      "       0.238747  , 0.2586015 , 0.35814429, 0.38695702, 0.20099778,\n",
      "       0.20161685, 0.16284059, 0.17308764, 0.16344446, 0.1385552 ,\n",
      "       0.13892191, 0.12500581, 0.14538733, 0.14098029, 0.10135546,\n",
      "       0.13082347, 0.13084828, 0.08490137, 0.13790155, 0.10844794,\n",
      "       0.12797096, 0.12851432, 0.13456569, 0.11794252, 0.11399995,\n",
      "       0.1307727 , 0.09153092, 0.14478176])}, 'ATTACH::1::best_val_loss': 0.0849013720430545, 'ATTACH::2::history': {'train_accs': array([0.46854243, 0.64116725, 0.69169341, 0.7149041 , 0.69967908,\n",
      "       0.71759086, 0.7119188 , 0.72162102, 0.70139563, 0.67415479,\n",
      "       0.67542354, 0.69654452, 0.63654004, 0.62183745, 0.60556758,\n",
      "       0.61638928, 0.60273155]), 'train_losses': array([5.67793673, 0.77692424, 0.68443983, 0.64402836, 0.65088691,\n",
      "       0.61510287, 0.63541497, 0.61022702, 0.64916261, 0.68866948,\n",
      "       0.66694293, 0.63454549, 0.74920666, 0.77602723, 0.81064511,\n",
      "       0.78354188, 0.80205825]), 'val_accs': array([0.64447761, 0.76507463, 0.78955224, 0.75910448, 0.67701493,\n",
      "       0.82716418, 0.80268657, 0.78746269, 0.77432836, 0.67134328,\n",
      "       0.73552239, 0.72626866, 0.67014925, 0.59880597, 0.65283582,\n",
      "       0.57462687, 0.62029851]), 'val_losses': array([0.78132268, 0.60000823, 0.56297418, 0.55879775, 0.64153989,\n",
      "       0.4918419 , 0.49092003, 0.52683267, 0.5741148 , 0.65340645,\n",
      "       0.55861807, 0.56314144, 0.72493159, 0.78408449, 0.71937184,\n",
      "       0.79795998, 0.93106434])}, 'ATTACH::2::best_val_loss': 0.4909200292914661, 'ATTACH::3::history': {'train_accs': array([0.4095828 , 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718]), 'train_losses': array([8.98995996, 1.26237387, 1.24839871, 1.2443829 , 1.24300376,\n",
      "       1.24237886, 1.24217958, 1.24208797, 1.24219945, 1.24211007,\n",
      "       1.24215222, 1.24218181, 1.24202859, 1.24204835, 1.24216029,\n",
      "       1.24204631, 1.24219437, 1.24201711, 1.24206722, 1.24195064,\n",
      "       1.24209767, 1.24195589, 1.24208546, 1.24212673, 1.2420251 ,\n",
      "       1.24206142, 1.24202118, 1.24202639, 1.24196691, 1.24212945,\n",
      "       1.24194014, 1.24209525, 1.24214446, 1.24201231, 1.24213948,\n",
      "       1.2420221 , 1.2420383 , 1.24203958, 1.24197688, 1.24191549,\n",
      "       1.24188577, 1.24196911, 1.24185253, 1.24205613, 1.24173039,\n",
      "       1.24192719, 1.24193839, 1.2418914 , 1.24198337, 1.24177027,\n",
      "       1.24193944, 1.24189491, 1.2418648 , 1.24175638, 1.24181796,\n",
      "       1.24164093, 1.24165338, 1.24162637, 1.24148364, 1.24145334]), 'val_accs': array([0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955]), 'val_losses': array([1.27785941, 1.25245208, 1.2454337 , 1.24317161, 1.2423995 ,\n",
      "       1.24208726, 1.24206512, 1.24191761, 1.24180642, 1.24176935,\n",
      "       1.24184189, 1.24178857, 1.24194122, 1.2417584 , 1.24175269,\n",
      "       1.24175245, 1.24178515, 1.24178682, 1.24173891, 1.24220875,\n",
      "       1.24173238, 1.24200493, 1.2420039 , 1.24171997, 1.24173644,\n",
      "       1.24171452, 1.24182438, 1.24170961, 1.24232255, 1.24173153,\n",
      "       1.24189914, 1.24175455, 1.24171512, 1.24177589, 1.24167726,\n",
      "       1.24166973, 1.24166444, 1.24167923, 1.24166549, 1.24168328,\n",
      "       1.24184027, 1.24163991, 1.24241255, 1.24166791, 1.24195595,\n",
      "       1.24158521, 1.24155768, 1.24162789, 1.24149921, 1.24166766,\n",
      "       1.2414705 , 1.24147204, 1.24147799, 1.24151467, 1.24133801,\n",
      "       1.24152222, 1.24130657, 1.24131258, 1.24132568, 1.24112513])}, 'ATTACH::3::best_val_loss': 1.2411251327884731, 'ATTACH::4::history': {'train_accs': array([0.40592582, 0.44257034, 0.46809463, 0.48040899, 0.49190238,\n",
      "       0.50570938, 0.51272483, 0.51063512, 0.5204866 , 0.52563624,\n",
      "       0.53018882, 0.53780133, 0.54347339, 0.54541384, 0.55101127,\n",
      "       0.56168371, 0.56959475, 0.57011717, 0.57713262, 0.5763863 ,\n",
      "       0.57668483, 0.5842227 , 0.58347638, 0.59638779, 0.59989551,\n",
      "       0.59952235, 0.60258228, 0.61041869, 0.61295619, 0.61303082,\n",
      "       0.6150459 , 0.62041943, 0.61773267, 0.62071796, 0.63348011,\n",
      "       0.62639003, 0.63153967, 0.6371371 , 0.6288529 , 0.6345996 ,\n",
      "       0.64280916, 0.64303306, 0.64960072, 0.65258601, 0.65564594,\n",
      "       0.65721322, 0.64736174, 0.64937682, 0.65855661, 0.66952758,\n",
      "       0.66430331, 0.67146802, 0.67116949, 0.67146802, 0.67139339,\n",
      "       0.66863199, 0.67482648, 0.67430405, 0.67408016, 0.6840809 ]), 'train_losses': array([16.43934809,  3.21832513,  1.99528358,  1.47190114,  1.2750038 ,\n",
      "        1.14446583,  1.09424849,  1.05044173,  1.01738287,  0.99247821,\n",
      "        0.95754632,  0.94037349,  0.92796808,  0.945072  ,  0.9191511 ,\n",
      "        0.89602933,  0.89352055,  0.87974623,  0.86896295,  0.85873831,\n",
      "        0.85973812,  0.85588177,  0.86006358,  0.8370211 ,  0.83089772,\n",
      "        0.82891252,  0.8309297 ,  0.82400106,  0.81666981,  0.80981391,\n",
      "        0.81762752,  0.80034054,  0.8041684 ,  0.79578532,  0.77767984,\n",
      "        0.79371755,  0.78554991,  0.78206515,  0.79093671,  0.78029948,\n",
      "        0.77758301,  0.76807442,  0.7627848 ,  0.76346051,  0.75484839,\n",
      "        0.75933114,  0.7570938 ,  0.75600066,  0.74478044,  0.73998533,\n",
      "        0.74072457,  0.72634893,  0.73185495,  0.72319727,  0.73347972,\n",
      "        0.72944638,  0.7224091 ,  0.72175955,  0.71903111,  0.71031996]), 'val_accs': array([0.48059701, 0.59462687, 0.55910448, 0.5758209 , 0.61761194,\n",
      "       0.53164179, 0.5958209 , 0.67910448, 0.58925373, 0.66059701,\n",
      "       0.59791045, 0.52      , 0.62149254, 0.68      , 0.64089552,\n",
      "       0.64      , 0.62208955, 0.67014925, 0.68507463, 0.67850746,\n",
      "       0.62656716, 0.70179104, 0.6880597 , 0.66179104, 0.68149254,\n",
      "       0.7438806 , 0.71761194, 0.69850746, 0.70567164, 0.7038806 ,\n",
      "       0.7161194 , 0.7638806 , 0.73791045, 0.71462687, 0.70089552,\n",
      "       0.74149254, 0.78179104, 0.73671642, 0.72567164, 0.73850746,\n",
      "       0.72149254, 0.71880597, 0.74895522, 0.75850746, 0.8080597 ,\n",
      "       0.76149254, 0.75880597, 0.7761194 , 0.77850746, 0.7241791 ,\n",
      "       0.76955224, 0.75910448, 0.77313433, 0.77880597, 0.79462687,\n",
      "       0.79164179, 0.76149254, 0.78776119, 0.76268657, 0.80597015]), 'val_losses': array([1.76209336, 1.03458269, 0.87360442, 0.82556487, 0.84010182,\n",
      "       0.79821864, 0.86399387, 0.78154786, 0.77586082, 0.79353248,\n",
      "       0.76013456, 0.85124911, 0.75643906, 0.7671081 , 0.75337046,\n",
      "       0.74607033, 0.76145578, 0.74383986, 0.72088134, 0.72908537,\n",
      "       0.73778556, 0.69837377, 0.70612595, 0.71243606, 0.69296683,\n",
      "       0.68027384, 0.68308571, 0.66896613, 0.69241912, 0.66760802,\n",
      "       0.66184787, 0.66179975, 0.65987502, 0.6575281 , 0.67910957,\n",
      "       0.64713267, 0.64326151, 0.64600238, 0.65003827, 0.65162453,\n",
      "       0.65413843, 0.6331565 , 0.61696205, 0.6333204 , 0.61769725,\n",
      "       0.61788611, 0.62305958, 0.60977844, 0.60431482, 0.6069905 ,\n",
      "       0.60767868, 0.61216174, 0.59293127, 0.58619542, 0.58275121,\n",
      "       0.58361922, 0.59404444, 0.57860723, 0.57662645, 0.570457  ])}, 'ATTACH::4::best_val_loss': 0.570457001479704, 'ATTACH::5::history': {'train_accs': array([0.37025151, 0.43174864, 0.495634  , 0.53966714, 0.60385103,\n",
      "       0.65564594, 0.68788716, 0.72124785, 0.75319054, 0.78490932,\n",
      "       0.80632883, 0.85245168, 0.86260169, 0.87812523, 0.89745503,\n",
      "       0.90969475, 0.91678483, 0.93036794, 0.93141279, 0.94036868,\n",
      "       0.94857825, 0.9475334 , 0.94924994, 0.95768341, 0.95611613,\n",
      "       0.96395253, 0.96656467, 0.96925144, 0.96850511, 0.97126651,\n",
      "       0.96753489, 0.97350549, 0.9753713 , 0.97380402, 0.97641615,\n",
      "       0.9779088 , 0.97843123, 0.97671468, 0.98104336, 0.98104336,\n",
      "       0.98313307, 0.98096873, 0.98119263, 0.98484962, 0.98484962,\n",
      "       0.97947608]), 'train_losses': array([18.18269327,  4.11916575,  2.50733146,  1.81025815,  1.30361976,\n",
      "        1.05054221,  0.8945652 ,  0.76758326,  0.68574395,  0.58391114,\n",
      "        0.54270983,  0.40868024,  0.37723924,  0.33508625,  0.28941812,\n",
      "        0.25795765,  0.23499661,  0.20843764,  0.1981082 ,  0.16754565,\n",
      "        0.15212971,  0.15636161,  0.15428232,  0.12783073,  0.13109471,\n",
      "        0.11231748,  0.10487631,  0.09552096,  0.09437639,  0.08666376,\n",
      "        0.09502056,  0.08194222,  0.07232986,  0.07909208,  0.07276432,\n",
      "        0.06767733,  0.06652174,  0.06678414,  0.05936569,  0.0572171 ,\n",
      "        0.05299263,  0.05757243,  0.05816088,  0.05003941,  0.05024654,\n",
      "        0.05966475]), 'val_accs': array([0.51134328, 0.5441791 , 0.59044776, 0.59134328, 0.64567164,\n",
      "       0.72626866, 0.76208955, 0.73910448, 0.81014925, 0.85761194,\n",
      "       0.86776119, 0.86597015, 0.8719403 , 0.86656716, 0.90477612,\n",
      "       0.9161194 , 0.93074627, 0.93343284, 0.93552239, 0.94179104,\n",
      "       0.94686567, 0.95044776, 0.92835821, 0.93283582, 0.95701493,\n",
      "       0.94835821, 0.92776119, 0.95104478, 0.9519403 , 0.96746269,\n",
      "       0.94507463, 0.96298507, 0.95492537, 0.96029851, 0.96      ,\n",
      "       0.97462687, 0.97432836, 0.94447761, 0.9558209 , 0.97164179,\n",
      "       0.96985075, 0.96149254, 0.97313433, 0.97134328, 0.96029851,\n",
      "       0.96597015]), 'val_losses': array([3.12664627, 1.23997409, 1.09948153, 0.98024947, 0.89879357,\n",
      "       0.63699723, 0.62356035, 0.62297586, 0.55746072, 0.34731336,\n",
      "       0.35405416, 0.36081681, 0.31015374, 0.33906658, 0.2364392 ,\n",
      "       0.21274087, 0.20202481, 0.1954495 , 0.18098616, 0.17121886,\n",
      "       0.14274404, 0.1407018 , 0.2115832 , 0.18916852, 0.13509645,\n",
      "       0.16344722, 0.22850992, 0.15352954, 0.15363712, 0.09760969,\n",
      "       0.16324213, 0.11434829, 0.1434732 , 0.12527333, 0.12619597,\n",
      "       0.08235504, 0.08257002, 0.17844045, 0.14355493, 0.09103721,\n",
      "       0.10746061, 0.14112547, 0.0987836 , 0.10339073, 0.15195522,\n",
      "       0.10988857])}, 'ATTACH::5::best_val_loss': 0.08235503950225773, 'ATTACH::6::history': {'train_accs': array([0.4072692 , 0.46324353, 0.50354504, 0.51235167, 0.53145757,\n",
      "       0.57131129, 0.57586387, 0.59899993, 0.59616389, 0.59429808,\n",
      "       0.60609001, 0.587208  , 0.59997015, 0.59638779, 0.61444884,\n",
      "       0.60482126, 0.60862751, 0.58847675, 0.61064259, 0.59161131,\n",
      "       0.59511904, 0.59176058, 0.58064035, 0.59026793, 0.58235689,\n",
      "       0.57728189, 0.59056646, 0.59034256, 0.5814613 , 0.57929696,\n",
      "       0.58138667]), 'train_losses': array([5.35250864, 1.09308846, 0.94296465, 0.93262267, 0.89137977,\n",
      "       0.85373936, 0.82913707, 0.81144626, 0.81405595, 0.82091958,\n",
      "       0.8141217 , 0.84220373, 0.816431  , 0.8518872 , 0.79996211,\n",
      "       0.79652206, 0.80493763, 0.83705008, 0.80292279, 0.81924914,\n",
      "       0.83403469, 0.83965814, 0.85965885, 0.86694246, 0.84623228,\n",
      "       0.86332312, 0.84945838, 0.86077044, 0.8482788 , 0.85078417,\n",
      "       0.86023489]), 'val_accs': array([0.49014925, 0.53970149, 0.53402985, 0.51970149, 0.63731343,\n",
      "       0.5680597 , 0.56716418, 0.58507463, 0.66537313, 0.5961194 ,\n",
      "       0.48537313, 0.60477612, 0.63134328, 0.63522388, 0.65074627,\n",
      "       0.6161194 , 0.52656716, 0.59940299, 0.55373134, 0.60985075,\n",
      "       0.68925373, 0.62358209, 0.61701493, 0.63253731, 0.62925373,\n",
      "       0.63432836, 0.65940299, 0.50089552, 0.57343284, 0.60985075,\n",
      "       0.62447761]), 'val_losses': array([1.12282127, 0.90140096, 0.85754932, 0.83535812, 0.81593066,\n",
      "       0.78422043, 0.77687002, 0.79663553, 0.73458241, 0.77459283,\n",
      "       0.85279576, 0.73910416, 0.82168189, 0.76586331, 0.7185615 ,\n",
      "       0.79067617, 0.98602798, 0.83463359, 1.41305597, 0.77723504,\n",
      "       0.71695399, 0.79273472, 0.77257139, 0.75954242, 0.77182541,\n",
      "       0.83674595, 0.74881339, 1.06629067, 0.82447427, 0.75795948,\n",
      "       0.79620716])}, 'ATTACH::6::best_val_loss': 0.7169539915981578, 'ATTACH::7::history': {'train_accs': array([0.43727144, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.46063139, 0.49048436, 0.49526084, 0.50324651, 0.52548698,\n",
      "       0.54504067, 0.55854915, 0.56526606, 0.57690872, 0.57929696,\n",
      "       0.60086574, 0.60735876, 0.61974774, 0.63131577, 0.63430107,\n",
      "       0.64534667, 0.65840734, 0.65169042, 0.65818345, 0.66564669,\n",
      "       0.66960221, 0.68146877, 0.69729084, 0.70930666, 0.71968057,\n",
      "       0.69848496, 0.7006493 , 0.71923278, 0.74721994, 0.747817  ,\n",
      "       0.75639973, 0.76886335, 0.77169938, 0.77572953, 0.77684902,\n",
      "       0.76498246, 0.7503545 , 0.72169565, 0.75498172, 0.74214494]), 'train_losses': array([7.59384092, 1.25792899, 1.25028468, 1.24291046, 1.24303793,\n",
      "       1.24292178, 1.24307899, 1.2427586 , 1.24285953, 1.24236673,\n",
      "       1.24189869, 1.24197258, 1.24174701, 1.24028297, 1.23599005,\n",
      "       1.19092347, 1.12405703, 1.07772581, 1.03982222, 1.0187415 ,\n",
      "       0.99236661, 0.95798752, 0.94668732, 0.92556674, 0.91897138,\n",
      "       0.89562013, 0.88080115, 0.86145382, 0.84446932, 0.84019096,\n",
      "       0.82839548, 0.81007089, 0.81479414, 0.80863277, 0.80219586,\n",
      "       0.79795669, 0.76197554, 0.73217126, 0.69350387, 0.68478392,\n",
      "       0.72513888, 0.73499761, 0.70122663, 0.63838652, 0.63123346,\n",
      "       0.6200171 , 0.59614421, 0.59322943, 0.57325052, 0.56527387,\n",
      "       0.59983065, 0.63148567, 0.69452538, 0.63080346, 0.65706656]), 'val_accs': array([0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.51671642, 0.50477612, 0.51791045, 0.53164179, 0.54238806,\n",
      "       0.52597015, 0.53791045, 0.59940299, 0.59074627, 0.55044776,\n",
      "       0.58985075, 0.55373134, 0.53253731, 0.55134328, 0.65373134,\n",
      "       0.59343284, 0.55701493, 0.58477612, 0.67014925, 0.65761194,\n",
      "       0.68119403, 0.66746269, 0.71373134, 0.68955224, 0.73134328,\n",
      "       0.68597015, 0.71044776, 0.72328358, 0.71014925, 0.78835821,\n",
      "       0.73850746, 0.77671642, 0.79373134, 0.74149254, 0.7919403 ,\n",
      "       0.76567164, 0.77164179, 0.75791045, 0.73164179, 0.8041791 ]), 'val_losses': array([1.27979979, 1.29697754, 1.24249384, 1.24262641, 1.24274668,\n",
      "       1.24147626, 1.24147605, 1.24349129, 1.24186904, 1.24087736,\n",
      "       1.24069593, 1.24112411, 1.23984575, 1.23796865, 1.23806936,\n",
      "       1.09820784, 1.07767337, 1.09237038, 0.97341907, 0.95779985,\n",
      "       0.99081336, 1.12208677, 0.89274285, 0.93188755, 0.97703513,\n",
      "       0.93545481, 1.05951651, 1.32255351, 1.09898444, 0.85080142,\n",
      "       1.06459311, 1.12844782, 0.95319608, 0.77875071, 0.89485987,\n",
      "       0.78212594, 0.85402383, 0.72025353, 0.8536218 , 0.71428145,\n",
      "       0.85881735, 0.8373015 , 0.70525139, 0.76785696, 0.55061446,\n",
      "       0.80388486, 0.62107604, 0.61201269, 0.69795373, 0.60606364,\n",
      "       0.68198622, 0.70898863, 0.68606526, 0.74718981, 0.55113234])}, 'ATTACH::7::best_val_loss': 0.5506144617742567, 'ATTACH::8::history': {'train_accs': array([0.35248899, 0.43570416, 0.4652586 , 0.50458989, 0.5234719 ,\n",
      "       0.57922233, 0.62847974, 0.6675125 , 0.69714158, 0.73080081,\n",
      "       0.76035525, 0.7922233 , 0.82147921, 0.84364505, 0.85312337,\n",
      "       0.87625942, 0.8920815 , 0.89835062, 0.90745578, 0.9167102 ,\n",
      "       0.92715874, 0.93312934, 0.93775655, 0.93909993, 0.9445481 ,\n",
      "       0.95275767, 0.95216061, 0.95522054, 0.95648929, 0.96171356,\n",
      "       0.96372864, 0.96634077, 0.9664154 , 0.97111725, 0.97096798,\n",
      "       0.97410254, 0.97581909, 0.97552056, 0.9746996 , 0.97872976,\n",
      "       0.98104336, 0.98290917, 0.98186432, 0.98246138, 0.98477498,\n",
      "       0.98507351, 0.98693932, 0.98529741, 0.98604373, 0.98261064,\n",
      "       0.99104411, 0.98932756, 0.98940219, 0.98790955, 0.98544668,\n",
      "       0.98790955, 0.98947683, 0.98910366, 0.99201433, 0.99044705]), 'train_losses': array([19.21639613,  5.30520498,  3.21678753,  2.50032112,  1.99912404,\n",
      "        1.51135627,  1.27543709,  1.09461909,  0.94874043,  0.804836  ,\n",
      "        0.72215815,  0.59708031,  0.52575329,  0.45716041,  0.42219201,\n",
      "        0.35348743,  0.3226533 ,  0.30233844,  0.27271111,  0.24664185,\n",
      "        0.2231015 ,  0.20929671,  0.19515851,  0.18925583,  0.17971181,\n",
      "        0.1480725 ,  0.15139593,  0.13632158,  0.13394546,  0.11708475,\n",
      "        0.11185233,  0.10473465,  0.10121241,  0.08745143,  0.09231289,\n",
      "        0.08119545,  0.07531298,  0.07421984,  0.07852322,  0.06552104,\n",
      "        0.06343042,  0.05544684,  0.05829574,  0.05882689,  0.04636242,\n",
      "        0.0456924 ,  0.04507432,  0.04518432,  0.04674922,  0.05382744,\n",
      "        0.0300403 ,  0.03637397,  0.03541648,  0.03692506,  0.04807833,\n",
      "        0.03667396,  0.0330188 ,  0.03686808,  0.02720221,  0.03254225]), 'val_accs': array([0.52328358, 0.53970149, 0.60447761, 0.61641791, 0.65164179,\n",
      "       0.68955224, 0.72537313, 0.75343284, 0.76149254, 0.76358209,\n",
      "       0.84298507, 0.84328358, 0.85761194, 0.84626866, 0.8758209 ,\n",
      "       0.90656716, 0.92      , 0.91880597, 0.91044776, 0.90716418,\n",
      "       0.91701493, 0.93791045, 0.93701493, 0.88686567, 0.93820896,\n",
      "       0.93492537, 0.94626866, 0.92746269, 0.9361194 , 0.95522388,\n",
      "       0.95313433, 0.96626866, 0.9561194 , 0.96328358, 0.96985075,\n",
      "       0.96358209, 0.97283582, 0.97044776, 0.97074627, 0.96626866,\n",
      "       0.97910448, 0.97492537, 0.97671642, 0.9758209 , 0.97970149,\n",
      "       0.97014925, 0.97731343, 0.97522388, 0.98089552, 0.97910448,\n",
      "       0.98328358, 0.97641791, 0.98029851, 0.97134328, 0.98      ,\n",
      "       0.98089552, 0.97223881, 0.98059701, 0.97880597, 0.97820896]), 'val_losses': array([2.93548542, 1.33017462, 1.13939066, 1.07124435, 0.88389693,\n",
      "       0.77089143, 0.72162068, 0.68927971, 0.68588127, 0.59702095,\n",
      "       0.43730611, 0.43853953, 0.38524712, 0.42995406, 0.35664897,\n",
      "       0.28948647, 0.24760132, 0.24268853, 0.24284278, 0.28673717,\n",
      "       0.24721085, 0.18669573, 0.19021544, 0.3724169 , 0.17591646,\n",
      "       0.20991354, 0.15612525, 0.23041016, 0.20305077, 0.13138689,\n",
      "       0.13207233, 0.10590012, 0.13125238, 0.1213204 , 0.10781428,\n",
      "       0.11282512, 0.09791853, 0.10299471, 0.10296318, 0.12808585,\n",
      "       0.07758827, 0.09114553, 0.09219833, 0.08303322, 0.07993527,\n",
      "       0.10417007, 0.07841208, 0.08892471, 0.07052995, 0.06981825,\n",
      "       0.07381773, 0.07711005, 0.07885881, 0.10331891, 0.07175336,\n",
      "       0.07357031, 0.13293856, 0.07601559, 0.07960668, 0.07355109])}, 'ATTACH::8::best_val_loss': 0.06981824971540873, 'ATTACH::9::history': {'train_accs': array([0.38779013, 0.46593029, 0.49443988, 0.51332189, 0.53556236,\n",
      "       0.54227927, 0.56840063, 0.57765505, 0.5704157 , 0.5869841 ,\n",
      "       0.60683633, 0.59944772, 0.60937383, 0.61847899, 0.61101575,\n",
      "       0.64661542, 0.68631987, 0.71311292, 0.73311441, 0.76386297,\n",
      "       0.79140234, 0.80476155, 0.81976267, 0.83431599, 0.84080902,\n",
      "       0.86200463, 0.86775132, 0.87954325, 0.88969326, 0.90141055,\n",
      "       0.903351  , 0.90969475, 0.92021793, 0.92297933, 0.9277558 ,\n",
      "       0.93917457, 0.93484588, 0.94604075, 0.94850362, 0.95425032,\n",
      "       0.94999627, 0.95477274, 0.95671319, 0.96335547, 0.95969848,\n",
      "       0.96425106, 0.96268378, 0.96171356, 0.96693783, 0.97096798,\n",
      "       0.97320696, 0.97119188, 0.9664154 , 0.97559519, 0.97984924,\n",
      "       0.97775953, 0.98134189, 0.97962535, 0.97753564, 0.97969998]), 'train_losses': array([18.77538224,  4.03923929,  2.39767989,  1.89106986,  1.45623697,\n",
      "        1.30998193,  1.12572681,  1.03602593,  1.04483456,  0.96142056,\n",
      "        0.91086245,  0.94626515,  0.8953324 ,  0.85875531,  0.84980083,\n",
      "        0.7728869 ,  0.69888799,  0.66119705,  0.6331395 ,  0.57146696,\n",
      "        0.51010899,  0.48473258,  0.46024018,  0.42722711,  0.40927343,\n",
      "        0.35842788,  0.3493883 ,  0.32371278,  0.29620168,  0.27505209,\n",
      "        0.26279471,  0.24548371,  0.22464143,  0.21602324,  0.20224589,\n",
      "        0.17790336,  0.18430998,  0.16208505,  0.14648346,  0.13671935,\n",
      "        0.13974573,  0.12871627,  0.12396361,  0.11086609,  0.11116543,\n",
      "        0.10164417,  0.11204932,  0.11199491,  0.09096915,  0.08271556,\n",
      "        0.08440336,  0.08445498,  0.09620368,  0.07456295,  0.06237598,\n",
      "        0.06487335,  0.0533482 ,  0.06024716,  0.06593931,  0.05907169]), 'val_accs': array([0.54208955, 0.55104478, 0.64268657, 0.6       , 0.63552239,\n",
      "       0.58656716, 0.58149254, 0.71910448, 0.58029851, 0.66      ,\n",
      "       0.64567164, 0.71432836, 0.74895522, 0.63432836, 0.62925373,\n",
      "       0.68119403, 0.73343284, 0.75432836, 0.74835821, 0.81761194,\n",
      "       0.79880597, 0.85164179, 0.78835821, 0.84626866, 0.87074627,\n",
      "       0.89731343, 0.88447761, 0.84656716, 0.89522388, 0.88835821,\n",
      "       0.91641791, 0.95164179, 0.88746269, 0.94656716, 0.92298507,\n",
      "       0.9480597 , 0.92567164, 0.96179104, 0.9641791 , 0.9438806 ,\n",
      "       0.96089552, 0.94268657, 0.9680597 , 0.95671642, 0.96865672,\n",
      "       0.97522388, 0.96686567, 0.97104478, 0.96597015, 0.97432836,\n",
      "       0.97343284, 0.96955224, 0.97910448, 0.97522388, 0.97791045,\n",
      "       0.97522388, 0.97641791, 0.97671642, 0.97671642, 0.97880597]), 'val_losses': array([2.34164377, 1.50191144, 0.88142727, 0.98213261, 0.86177933,\n",
      "       1.03989727, 1.03731682, 0.71379587, 0.95334969, 0.72460148,\n",
      "       0.81736286, 0.66457808, 0.62983476, 0.72935422, 0.80741863,\n",
      "       0.75827703, 0.57838146, 0.53615083, 0.52483857, 0.44340395,\n",
      "       0.46739881, 0.34559518, 0.51702757, 0.32972938, 0.32302316,\n",
      "       0.26045002, 0.30507247, 0.38720038, 0.26890333, 0.29767299,\n",
      "       0.23635074, 0.14696701, 0.29832577, 0.15329699, 0.20039896,\n",
      "       0.15516182, 0.21477959, 0.108559  , 0.1011037 , 0.1496079 ,\n",
      "       0.11776411, 0.15283597, 0.09540135, 0.11302418, 0.0906275 ,\n",
      "       0.06738835, 0.08994964, 0.08523139, 0.09378245, 0.07324162,\n",
      "       0.07469636, 0.08199832, 0.06423787, 0.06652112, 0.07025378,\n",
      "       0.07847331, 0.07915593, 0.07317777, 0.07232092, 0.06157366])}, 'ATTACH::9::best_val_loss': 0.06157365797268254, 'ATTACH::10::history': {'train_accs': array([0.28920069, 0.29920143, 0.29345474, 0.30845585, 0.32069557,\n",
      "       0.33338309, 0.34427942, 0.35308605, 0.3573401 , 0.36084782,\n",
      "       0.36957982, 0.36629599, 0.37778939, 0.38704381, 0.38137174,\n",
      "       0.39794014, 0.40040302, 0.40256736, 0.40965744, 0.41682215,\n",
      "       0.41368759, 0.42831555, 0.42525562, 0.43174864, 0.43406224,\n",
      "       0.44234644, 0.4484663 , 0.44503321, 0.4569744 , 0.4624972 ,\n",
      "       0.4684678 , 0.46854243, 0.47637883, 0.48787223, 0.48488693,\n",
      "       0.49279797, 0.50227629, 0.50272408, 0.5066796 , 0.5151877 ,\n",
      "       0.51914322, 0.52981566, 0.53227853, 0.53914471, 0.54608553,\n",
      "       0.545787  , 0.55578775, 0.57093813, 0.57019181, 0.57504291,\n",
      "       0.57571461, 0.58825285, 0.58564072, 0.60317934, 0.5954922 ,\n",
      "       0.60176133, 0.61579222, 0.62027017, 0.62430032, 0.62161355]), 'train_losses': array([98.06741317, 72.68456826, 57.45552795, 47.65456203, 38.98044222,\n",
      "       32.67794542, 27.5953784 , 24.02281735, 21.28237503, 19.24641519,\n",
      "       17.42132321, 16.01527462, 14.41807776, 13.1352105 , 12.24281451,\n",
      "       11.16931841, 10.3601807 ,  9.62754856,  8.94120767,  8.23844816,\n",
      "        8.02260971,  7.17930853,  6.83925414,  6.43297443,  6.06602673,\n",
      "        5.65099043,  5.25573061,  5.10837278,  4.73850212,  4.42784645,\n",
      "        4.30057452,  4.14024459,  3.91345757,  3.64228841,  3.49788861,\n",
      "        3.36377352,  3.16499879,  3.11504331,  2.96196052,  2.80067895,\n",
      "        2.79026433,  2.51334098,  2.48103676,  2.40207785,  2.32831069,\n",
      "        2.30608015,  2.17188618,  2.06210747,  2.06900239,  1.92654006,\n",
      "        1.87952036,  1.82895545,  1.8124343 ,  1.69831793,  1.68072766,\n",
      "        1.62296136,  1.58932649,  1.5805217 ,  1.50721895,  1.4996393 ]), 'val_accs': array([0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44656716, 0.47104478, 0.47552239, 0.44597015, 0.39462687,\n",
      "       0.40686567, 0.44328358, 0.46358209, 0.44955224, 0.47671642,\n",
      "       0.48895522, 0.47671642, 0.44776119, 0.46089552, 0.47850746,\n",
      "       0.5358209 , 0.5319403 , 0.52      , 0.53820896, 0.54328358,\n",
      "       0.54328358, 0.50447761, 0.54238806, 0.55462687, 0.55970149,\n",
      "       0.52985075, 0.54447761, 0.5880597 , 0.59313433, 0.61134328,\n",
      "       0.57044776, 0.61432836, 0.55671642, 0.6       , 0.66477612,\n",
      "       0.68298507, 0.65492537, 0.68358209, 0.70447761, 0.67343284,\n",
      "       0.69104478, 0.69223881, 0.69253731, 0.71373134, 0.67492537,\n",
      "       0.70447761, 0.64985075, 0.66656716, 0.7041791 , 0.70776119,\n",
      "       0.71970149, 0.70567164, 0.69313433, 0.68268657, 0.68925373]), 'val_losses': array([57.93742271, 43.09821542, 28.50053071, 21.68941945, 16.18315338,\n",
      "       10.39504342,  5.23223981,  5.13725923,  3.96060253,  3.43523098,\n",
      "        3.93163316,  3.41911258,  3.35536801,  3.24911402,  2.92814687,\n",
      "        3.07913227,  3.02546522,  3.54407886,  3.50763363,  3.85357025,\n",
      "        2.76332303,  2.61112542,  2.74472187,  2.39554397,  2.64680943,\n",
      "        1.99648893,  1.85940678,  1.90658206,  2.0859246 ,  1.67613992,\n",
      "        1.89946654,  1.85288448,  1.27751402,  1.31915252,  1.11161064,\n",
      "        1.50732326,  1.28808288,  1.49008423,  1.18803194,  1.02102996,\n",
      "        0.98831661,  1.13668502,  0.95630845,  0.93258258,  1.00699684,\n",
      "        1.11322555,  0.92245319,  0.81973316,  0.87454029,  0.99203987,\n",
      "        0.78164037,  0.89121662,  0.91640034,  0.8113458 ,  0.92567777,\n",
      "        0.79389675,  0.8952424 ,  0.74298403,  0.90598548,  0.8322818 ])}, 'ATTACH::10::best_val_loss': 0.7429840338052208, 'ATTACH::11::history': {'train_accs': array([0.44995895, 0.51070975, 0.53018882, 0.5536234 , 0.56780357,\n",
      "       0.56108665, 0.57138592, 0.58332711, 0.58802896, 0.6014628 ,\n",
      "       0.59153668, 0.61870289, 0.61915068, 0.6201209 , 0.6205687 ,\n",
      "       0.6173595 , 0.6178073 , 0.62758415, 0.62086723, 0.62616613,\n",
      "       0.60713486, 0.58593925, 0.60832898, 0.60183596, 0.61258303,\n",
      "       0.62161355]), 'train_losses': array([2.87738631, 0.95391013, 0.91352678, 0.88491299, 0.85985531,\n",
      "       0.85775029, 0.85050321, 0.83028249, 0.82499918, 0.79764591,\n",
      "       0.83353732, 0.78163459, 0.78182303, 0.77916432, 0.78523818,\n",
      "       0.79411915, 0.78133661, 0.77189378, 0.78340216, 0.77604367,\n",
      "       0.80129899, 0.85072542, 0.80535919, 0.82896473, 0.8225493 ,\n",
      "       0.80983023]), 'val_accs': array([0.58686567, 0.55253731, 0.59134328, 0.60835821, 0.64119403,\n",
      "       0.56447761, 0.64208955, 0.69791045, 0.61731343, 0.69402985,\n",
      "       0.64835821, 0.7280597 , 0.70149254, 0.62776119, 0.64865672,\n",
      "       0.73820896, 0.73283582, 0.67940299, 0.71761194, 0.68328358,\n",
      "       0.57731343, 0.69641791, 0.71701493, 0.62208955, 0.66746269,\n",
      "       0.64208955]), 'val_losses': array([0.86805606, 0.83636311, 0.7973906 , 0.80822277, 0.75538279,\n",
      "       0.81432988, 0.74288993, 0.71934   , 0.72596292, 0.67234292,\n",
      "       0.70045178, 0.67762608, 0.64864304, 0.72161603, 0.68546326,\n",
      "       0.63995604, 0.65945857, 0.67144268, 0.66126196, 0.67747133,\n",
      "       0.76353459, 0.6917174 , 0.6880991 , 0.70971504, 0.70893633,\n",
      "       0.70126376])}, 'ATTACH::11::best_val_loss': 0.6399560420192889, 'ATTACH::12::history': {'train_accs': array([0.18673035, 0.32838272, 0.36316143, 0.37823718, 0.38883499,\n",
      "       0.39585044, 0.40032838, 0.40167177, 0.4153295 , 0.42241958,\n",
      "       0.43145011, 0.44816777, 0.45376521, 0.45331741, 0.47458766,\n",
      "       0.48533473, 0.48921561, 0.49675349, 0.50720203, 0.5204866 ,\n",
      "       0.53877155, 0.53869692, 0.54683185, 0.54884693, 0.56011643,\n",
      "       0.56728114, 0.57884917, 0.58138667, 0.59079036, 0.59855213,\n",
      "       0.6069856 , 0.61318009, 0.62676319, 0.63676394, 0.64228674,\n",
      "       0.65534741, 0.66557206, 0.66781103, 0.6810956 , 0.683857  ,\n",
      "       0.68512576, 0.70251511, 0.70266438, 0.70982909, 0.71953131,\n",
      "       0.73065154, 0.72027763, 0.73535338, 0.75042914, 0.74408538,\n",
      "       0.76319128, 0.76334055, 0.75848944, 0.76931114, 0.77811777,\n",
      "       0.78155086, 0.78520785, 0.79311889, 0.79573102, 0.79976118]), 'train_losses': array([79.77063765, 28.87776126, 21.27414994, 17.34792952, 13.7847781 ,\n",
      "       12.10808806, 10.22607885,  9.22364518,  8.16228723,  7.42930058,\n",
      "        6.72804737,  6.1657359 ,  5.49528296,  5.13865671,  4.61708389,\n",
      "        4.33820533,  4.19157124,  3.81886488,  3.54791126,  3.28049598,\n",
      "        3.12211605,  2.99605247,  2.80478048,  2.67768681,  2.51607761,\n",
      "        2.39043196,  2.21563404,  2.17368492,  2.07887033,  1.9518652 ,\n",
      "        1.88961972,  1.83677584,  1.75377704,  1.66672253,  1.63168176,\n",
      "        1.54269906,  1.42796021,  1.42858276,  1.33061885,  1.29803975,\n",
      "        1.28016421,  1.1764155 ,  1.20361336,  1.12452082,  1.0618987 ,\n",
      "        1.03164427,  1.03794211,  1.00091001,  0.92227209,  0.9368186 ,\n",
      "        0.87860967,  0.86925221,  0.87697488,  0.80869046,  0.7968885 ,\n",
      "        0.76197732,  0.7359233 ,  0.73452841,  0.702571  ,  0.69255794]), 'val_accs': array([0.3041791 , 0.46656716, 0.44925373, 0.46119403, 0.43492537,\n",
      "       0.43820896, 0.5161194 , 0.52      , 0.53044776, 0.5161194 ,\n",
      "       0.58029851, 0.58925373, 0.60238806, 0.62149254, 0.66597015,\n",
      "       0.66507463, 0.65104478, 0.6841791 , 0.6761194 , 0.67343284,\n",
      "       0.68597015, 0.66447761, 0.70328358, 0.70268657, 0.68656716,\n",
      "       0.66925373, 0.73044776, 0.71492537, 0.72268657, 0.69074627,\n",
      "       0.72268657, 0.72985075, 0.75164179, 0.74746269, 0.76865672,\n",
      "       0.79134328, 0.77462687, 0.77283582, 0.78238806, 0.78      ,\n",
      "       0.7919403 , 0.79402985, 0.80955224, 0.78477612, 0.81940299,\n",
      "       0.81492537, 0.7641791 , 0.80447761, 0.82477612, 0.82626866,\n",
      "       0.73880597, 0.84865672, 0.84597015, 0.83164179, 0.85104478,\n",
      "       0.86179104, 0.86179104, 0.86059701, 0.8558209 , 0.84358209]), 'val_losses': array([11.15806352,  7.77134872,  6.9720241 ,  5.43250364,  5.4826    ,\n",
      "        4.22546865,  3.46213699,  3.26406031,  3.26375842,  2.39897265,\n",
      "        2.06357177,  1.97211731,  1.83525181,  1.802268  ,  1.61434974,\n",
      "        1.4070853 ,  1.43432389,  1.46757331,  1.365243  ,  1.20985523,\n",
      "        1.24791534,  1.08127292,  1.15537861,  1.07898541,  0.95350826,\n",
      "        0.92737134,  0.96192293,  0.90283736,  0.94704391,  1.14046098,\n",
      "        0.9586101 ,  0.80245615,  0.83344046,  0.89541173,  0.7559795 ,\n",
      "        0.7232916 ,  0.72489631,  0.75303595,  0.75810837,  0.75559435,\n",
      "        0.65958979,  0.68450874,  0.56525663,  0.73504434,  0.58677416,\n",
      "        0.56433635,  0.82601518,  0.63685256,  0.54311535,  0.56118729,\n",
      "        0.86764257,  0.47880887,  0.49600752,  0.55781075,  0.47678605,\n",
      "        0.4654987 ,  0.44107087,  0.42593237,  0.45328092,  0.48062485])}, 'ATTACH::12::best_val_loss': 0.42593237083349655, 'ATTACH::13::history': {'train_accs': array([0.47219942, 0.5844466 , 0.64497351, 0.66706471, 0.69482797,\n",
      "       0.70184342, 0.70908277, 0.71393388, 0.71953131, 0.72132249,\n",
      "       0.72602433, 0.72609896, 0.73468169, 0.74378685, 0.7337861 ,\n",
      "       0.71162027, 0.72445705, 0.72960669, 0.74520487, 0.74005523,\n",
      "       0.73192029, 0.73848795, 0.71281439, 0.7372192 , 0.73139787,\n",
      "       0.73386074, 0.70542578, 0.73789089, 0.73945817]), 'train_losses': array([8.77546287, 1.17389722, 0.79705287, 0.73513293, 0.67985669,\n",
      "       0.66161284, 0.63814604, 0.65383742, 0.63233668, 0.64232298,\n",
      "       0.62424348, 0.62762302, 0.60773386, 0.58742249, 0.60378688,\n",
      "       0.65402342, 0.62897394, 0.6301895 , 0.59001261, 0.59340671,\n",
      "       0.60923986, 0.60419741, 0.64105373, 0.61103392, 0.61312764,\n",
      "       0.60914602, 0.65672266, 0.59496125, 0.59574375]), 'val_accs': array([0.55313433, 0.68      , 0.77641791, 0.75552239, 0.68358209,\n",
      "       0.72865672, 0.74089552, 0.64507463, 0.69164179, 0.73552239,\n",
      "       0.78537313, 0.72059701, 0.76149254, 0.77313433, 0.78507463,\n",
      "       0.80746269, 0.70029851, 0.77940299, 0.80029851, 0.71850746,\n",
      "       0.72059701, 0.70029851, 0.71850746, 0.66597015, 0.76477612,\n",
      "       0.61850746, 0.63402985, 0.67462687, 0.65880597]), 'val_losses': array([1.76406779, 0.7341253 , 0.60536849, 0.54913147, 0.64051666,\n",
      "       0.59059755, 0.60362939, 0.78095893, 0.68514996, 0.59831155,\n",
      "       0.56699287, 0.70093458, 0.53354581, 0.54350108, 0.54962932,\n",
      "       0.54973477, 0.70014902, 0.55963272, 0.48892144, 0.63253374,\n",
      "       0.60050474, 0.697773  , 0.70110726, 0.6697295 , 0.59055051,\n",
      "       0.7793011 , 0.78646618, 0.71738671, 0.75155531])}, 'ATTACH::13::best_val_loss': 0.4889214371522861, 'ATTACH::14::history': {'train_accs': array([0.41189641, 0.43279349, 0.43913725, 0.46115382, 0.47570714,\n",
      "       0.50041048, 0.54713038, 0.57646093, 0.59795507, 0.65198895,\n",
      "       0.68214046, 0.71296365, 0.74087618, 0.76147474, 0.79274573,\n",
      "       0.80909023, 0.83849541, 0.86319875, 0.87939398, 0.88969326,\n",
      "       0.90223151, 0.9058885 , 0.917158  , 0.92506904, 0.92902455,\n",
      "       0.9362639 , 0.93962236, 0.9449959 , 0.94566759, 0.9477573 ,\n",
      "       0.95342936, 0.95357863, 0.95842973, 0.95701172, 0.95999701,\n",
      "       0.96163893, 0.96656467, 0.9668632 , 0.97007239, 0.96746026,\n",
      "       0.9696246 , 0.972386  , 0.97402791, 0.97477424, 0.9781327 ,\n",
      "       0.9746996 , 0.97775953, 0.9749235 , 0.97887902, 0.97887902,\n",
      "       0.97940145, 0.97992387, 0.97984924, 0.98208822, 0.9806702 ,\n",
      "       0.98313307, 0.98492425, 0.98111799, 0.98484962, 0.98462572]), 'train_losses': array([22.89545614,  8.4616864 ,  5.6368703 ,  3.97808662,  2.98461895,\n",
      "        2.2521207 ,  1.70353936,  1.36940591,  1.1445097 ,  0.91108946,\n",
      "        0.81160121,  0.73178202,  0.64267594,  0.59891362,  0.52265231,\n",
      "        0.49210667,  0.4343357 ,  0.38054021,  0.34363335,  0.31131873,\n",
      "        0.28233385,  0.27040085,  0.24029958,  0.2248597 ,  0.20459951,\n",
      "        0.19112684,  0.17707118,  0.16808543,  0.17082478,  0.14818594,\n",
      "        0.13661593,  0.1373017 ,  0.1237869 ,  0.12880268,  0.12004379,\n",
      "        0.11700407,  0.09949809,  0.09746769,  0.09323011,  0.09823928,\n",
      "        0.09312703,  0.08740841,  0.08045097,  0.08075198,  0.069582  ,\n",
      "        0.07799366,  0.06475722,  0.07298438,  0.06269601,  0.0673655 ,\n",
      "        0.06199357,  0.06572208,  0.06046888,  0.05723722,  0.05950721,\n",
      "        0.05407547,  0.04495735,  0.05970211,  0.04962362,  0.04638927]), 'val_accs': array([0.50059701, 0.52358209, 0.45671642, 0.45522388, 0.46208955,\n",
      "       0.62776119, 0.66149254, 0.63492537, 0.65671642, 0.74358209,\n",
      "       0.72298507, 0.69522388, 0.73940299, 0.83820896, 0.84716418,\n",
      "       0.67940299, 0.88208955, 0.90776119, 0.86597015, 0.82179104,\n",
      "       0.82985075, 0.90029851, 0.93044776, 0.92686567, 0.93492537,\n",
      "       0.96358209, 0.95701493, 0.90746269, 0.93313433, 0.94537313,\n",
      "       0.94925373, 0.94746269, 0.96955224, 0.96238806, 0.96716418,\n",
      "       0.97522388, 0.96447761, 0.97104478, 0.94059701, 0.96089552,\n",
      "       0.92776119, 0.96149254, 0.9761194 , 0.96656716, 0.94626866,\n",
      "       0.98179104, 0.97074627, 0.98029851, 0.97641791, 0.98447761,\n",
      "       0.98179104, 0.97940299, 0.9758209 , 0.95462687, 0.98119403,\n",
      "       0.9841791 , 0.98089552, 0.96925373, 0.96029851, 0.98328358]), 'val_losses': array([4.15732972, 2.58462184, 3.01838805, 2.434254  , 7.28648551,\n",
      "       1.05140653, 0.818313  , 0.86880906, 1.10876988, 0.57377646,\n",
      "       0.70506195, 0.68803031, 0.57075394, 0.40919585, 0.37733734,\n",
      "       0.8120401 , 0.32309388, 0.27255952, 0.35841514, 0.46081525,\n",
      "       0.50405626, 0.30019677, 0.19818314, 0.20576556, 0.20800592,\n",
      "       0.11216023, 0.12239801, 0.2732302 , 0.18075016, 0.16000026,\n",
      "       0.14733792, 0.12695935, 0.08955792, 0.10861951, 0.09721543,\n",
      "       0.07907463, 0.12071801, 0.09028493, 0.21423916, 0.13342978,\n",
      "       0.23921083, 0.11780453, 0.06852335, 0.1028378 , 0.17827372,\n",
      "       0.05541085, 0.08201763, 0.06483149, 0.0802002 , 0.05536848,\n",
      "       0.05765484, 0.06722039, 0.08636747, 0.13823425, 0.05558885,\n",
      "       0.05616771, 0.05205229, 0.09356813, 0.13333165, 0.048533  ])}, 'ATTACH::14::best_val_loss': 0.04853299623105063, 'ATTACH::15::history': {'train_accs': array([0.39413389, 0.47555788, 0.49787298, 0.55071274, 0.57228151,\n",
      "       0.587208  , 0.60773192, 0.63042018, 0.6396746 , 0.64945145,\n",
      "       0.65982536, 0.67505038, 0.68475259, 0.70669453, 0.72251661,\n",
      "       0.7344578 , 0.75013061, 0.76446003, 0.77281887, 0.78341667,\n",
      "       0.78699903, 0.79177551, 0.78550638, 0.80416449, 0.79692514,\n",
      "       0.80953803, 0.80237331, 0.80692589, 0.81401597, 0.81259796,\n",
      "       0.8140906 , 0.82588253, 0.82894246, 0.82655422, 0.82953952,\n",
      "       0.82871856, 0.82506157, 0.83192776, 0.82386745, 0.83573401,\n",
      "       0.83207702, 0.82588253, 0.8396149 , 0.81946414]), 'train_losses': array([16.62532551,  2.84580122,  2.14025948,  1.28470288,  1.08626372,\n",
      "        0.97143977,  0.88365844,  0.80134112,  0.7589672 ,  0.73762631,\n",
      "        0.71206479,  0.67893137,  0.67065175,  0.63371308,  0.6126145 ,\n",
      "        0.57964126,  0.56293534,  0.5388883 ,  0.52235944,  0.50451283,\n",
      "        0.49903047,  0.48555825,  0.49921728,  0.46068937,  0.471495  ,\n",
      "        0.44603315,  0.45862839,  0.45120891,  0.44105546,  0.44096821,\n",
      "        0.43705526,  0.40650232,  0.40578353,  0.41105751,  0.41580075,\n",
      "        0.41102452,  0.41389567,  0.40498786,  0.4179393 ,  0.3962997 ,\n",
      "        0.40091696,  0.41149885,  0.38674839,  0.42319612]), 'val_accs': array([0.55641791, 0.54298507, 0.63791045, 0.54597015, 0.68895522,\n",
      "       0.62358209, 0.58985075, 0.66686567, 0.71552239, 0.66746269,\n",
      "       0.70626866, 0.71850746, 0.71074627, 0.69402985, 0.75104478,\n",
      "       0.78597015, 0.6719403 , 0.69253731, 0.68059701, 0.75164179,\n",
      "       0.73671642, 0.72597015, 0.79373134, 0.76029851, 0.77731343,\n",
      "       0.74029851, 0.75761194, 0.75761194, 0.71880597, 0.68119403,\n",
      "       0.74865672, 0.75223881, 0.76955224, 0.79164179, 0.79791045,\n",
      "       0.76358209, 0.72955224, 0.76149254, 0.75462687, 0.79731343,\n",
      "       0.77253731, 0.82716418, 0.74567164, 0.79552239]), 'val_losses': array([2.51957322, 1.97136817, 0.78108288, 1.12625889, 0.84460729,\n",
      "       0.741186  , 0.85328942, 0.61477595, 0.61637886, 0.67460878,\n",
      "       0.60004756, 0.57556447, 0.58236744, 0.58993866, 0.51480969,\n",
      "       0.49431117, 0.55890888, 0.58148394, 0.55255908, 0.49409917,\n",
      "       0.52497034, 0.52560007, 0.47358062, 0.4652    , 0.46320345,\n",
      "       0.50679793, 0.49588179, 0.44487819, 0.52251318, 0.73606152,\n",
      "       0.44370053, 0.46802832, 0.44862254, 0.42297871, 0.44502194,\n",
      "       0.49682654, 0.52015699, 0.53812454, 0.49035848, 0.4331261 ,\n",
      "       0.49227024, 0.43181128, 0.57405028, 0.4794688 ])}, 'ATTACH::15::best_val_loss': 0.4229787123381202, 'ATTACH::16::history': {'train_accs': array([0.39808941, 0.44577954, 0.44973505, 0.45630271, 0.35554892,\n",
      "       0.3849541 , 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718]), 'train_losses': array([2.83192238, 1.11463897, 1.0977357 , 1.07165724, 1.23639103,\n",
      "       1.27639125, 1.28188707, 1.26893043, 1.26069673, 1.25597833,\n",
      "       1.25287694, 1.25078582, 1.24848162]), 'val_accs': array([0.45044776, 0.42059701, 0.50925373, 0.41074627, 0.27432836,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955]), 'val_losses': array([1.09715558, 1.01871179, 1.00242871, 1.0906996 , 1.3179182 ,\n",
      "       1.29456112, 1.27716324, 1.26631824, 1.25938245, 1.25489012,\n",
      "       1.25172165, 1.24953775, 1.24789889])}, 'ATTACH::16::best_val_loss': 1.0024287080053074, 'ATTACH::17::history': {'train_accs': array([0.44100306, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718]), 'train_losses': array([4.38056025, 1.25571847, 1.24632364, 1.24347783, 1.24252184,\n",
      "       1.24226594, 1.24214356, 1.24215298, 1.24207136, 1.24221837,\n",
      "       1.24206974, 1.24204703, 1.24193642, 1.24206119, 1.24196313,\n",
      "       1.24188823, 1.24199565, 1.24218003, 1.24199658, 1.2420214 ,\n",
      "       1.24201236, 1.24206237, 1.24204281, 1.24215358, 1.24216528,\n",
      "       1.24206105, 1.2419731 , 1.24206389, 1.24212072, 1.24193975,\n",
      "       1.24203935, 1.24203944, 1.24188808, 1.242005  , 1.2419851 ,\n",
      "       1.24197139, 1.24203511, 1.24204465, 1.24188053, 1.24195017,\n",
      "       1.24194994, 1.24193479, 1.24189651, 1.24199897, 1.24192281,\n",
      "       1.24189022, 1.24191293, 1.24184125]), 'val_accs': array([0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955]), 'val_losses': array([1.26526041, 1.24874224, 1.24398686, 1.24252292, 1.24198087,\n",
      "       1.24191518, 1.2418165 , 1.2417584 , 1.24174121, 1.24174234,\n",
      "       1.24174953, 1.24183421, 1.24213401, 1.24174485, 1.24188658,\n",
      "       1.24170912, 1.24181924, 1.24173861, 1.24170445, 1.24168329,\n",
      "       1.24180886, 1.24169334, 1.24169752, 1.24174338, 1.24164779,\n",
      "       1.24164502, 1.24163228, 1.24162587, 1.24171054, 1.24161884,\n",
      "       1.2416297 , 1.24159691, 1.2416167 , 1.2416419 , 1.24159213,\n",
      "       1.2416097 , 1.24160053, 1.24154243, 1.2415637 , 1.24163064,\n",
      "       1.24166922, 1.24159532, 1.24157107, 1.24157194, 1.24157181,\n",
      "       1.24155716, 1.24154663, 1.24154766])}, 'ATTACH::17::best_val_loss': 1.2415424250488851, 'ATTACH::18::history': {'train_accs': array([0.36793791, 0.4072692 , 0.4130159 , 0.43667438, 0.46257183,\n",
      "       0.47496082, 0.50026121, 0.50757519, 0.51951638, 0.53100978,\n",
      "       0.54466751, 0.56026569, 0.58840212, 0.61818046, 0.62161355,\n",
      "       0.64340622, 0.6704978 , 0.68333458, 0.70020151, 0.71042615,\n",
      "       0.73244272, 0.73401   , 0.74684678, 0.77438615, 0.77886409,\n",
      "       0.79677588, 0.81535936, 0.83147996, 0.85775058, 0.87708038,\n",
      "       0.88573774, 0.89864915, 0.90999328, 0.92357639, 0.92215837,\n",
      "       0.92999478, 0.93544294, 0.94163744, 0.94238376, 0.949847  ,\n",
      "       0.94857825, 0.95626539, 0.9555937 , 0.9526084 , 0.96141503,\n",
      "       0.96447496, 0.96790805, 0.97074409, 0.96969923, 0.96760952,\n",
      "       0.9726099 , 0.97425181, 0.97395328, 0.97940145, 0.97693858,\n",
      "       0.97783417, 0.9776849 , 0.97723711, 0.97902829, 0.98178969]), 'train_losses': array([30.47355683,  8.33960829,  4.90802102,  3.34339905,  2.48434549,\n",
      "        2.07870927,  1.70669746,  1.58265944,  1.44227306,  1.32878752,\n",
      "        1.2388887 ,  1.14384251,  1.03774408,  0.94706868,  0.90812257,\n",
      "        0.89269408,  0.79622043,  0.79171051,  0.75047905,  0.72183649,\n",
      "        0.67413235,  0.68045163,  0.62846321,  0.57212918,  0.5560617 ,\n",
      "        0.51238994,  0.48129624,  0.45316821,  0.39369518,  0.34536377,\n",
      "        0.32448976,  0.29016025,  0.25531215,  0.22396938,  0.21893232,\n",
      "        0.20710444,  0.18817208,  0.16549649,  0.17150751,  0.14743087,\n",
      "        0.15316646,  0.1296118 ,  0.13460991,  0.13790298,  0.11584347,\n",
      "        0.10954823,  0.09776315,  0.0940095 ,  0.09458918,  0.09955703,\n",
      "        0.0808561 ,  0.07947479,  0.08356277,  0.06575844,  0.07068648,\n",
      "        0.07140128,  0.06777959,  0.0707784 ,  0.06259538,  0.05617101]), 'val_accs': array([0.3880597 , 0.41701493, 0.47134328, 0.53134328, 0.53462687,\n",
      "       0.59402985, 0.62      , 0.48358209, 0.67492537, 0.67850746,\n",
      "       0.55104478, 0.65283582, 0.63104478, 0.66447761, 0.70656716,\n",
      "       0.6561194 , 0.74597015, 0.6719403 , 0.74328358, 0.79910448,\n",
      "       0.7438806 , 0.7558209 , 0.8080597 , 0.76865672, 0.80865672,\n",
      "       0.84985075, 0.83074627, 0.87283582, 0.8638806 , 0.91164179,\n",
      "       0.87970149, 0.90686567, 0.92656716, 0.92      , 0.91552239,\n",
      "       0.90776119, 0.93820896, 0.94746269, 0.93940299, 0.95074627,\n",
      "       0.94358209, 0.96358209, 0.92179104, 0.95044776, 0.93313433,\n",
      "       0.96149254, 0.96776119, 0.96567164, 0.96925373, 0.96865672,\n",
      "       0.97462687, 0.97731343, 0.96029851, 0.97432836, 0.97880597,\n",
      "       0.9758209 , 0.97223881, 0.97522388, 0.97014925, 0.98328358]), 'val_losses': array([3.23837774, 2.20179584, 1.630859  , 1.20617074, 1.32177159,\n",
      "       0.97993702, 0.94897668, 1.46757498, 0.81142763, 0.87485826,\n",
      "       1.25384281, 0.69250823, 0.83877523, 0.69432739, 0.8514397 ,\n",
      "       0.83541548, 0.61319637, 0.6866419 , 0.56299312, 0.49054052,\n",
      "       0.54525518, 0.51906447, 0.45064454, 0.47003788, 0.43626817,\n",
      "       0.38457655, 0.43065575, 0.36046331, 0.34752103, 0.23911981,\n",
      "       0.31198893, 0.25712343, 0.22252599, 0.22734954, 0.23919416,\n",
      "       0.2687605 , 0.17548576, 0.13981608, 0.15769331, 0.1466317 ,\n",
      "       0.15860491, 0.11624382, 0.24292061, 0.13898241, 0.19693444,\n",
      "       0.10583832, 0.09155053, 0.08968626, 0.09502823, 0.09607796,\n",
      "       0.07909773, 0.07349309, 0.11496983, 0.07619623, 0.07489849,\n",
      "       0.08254836, 0.08419282, 0.08493217, 0.0885562 , 0.05748987])}, 'ATTACH::18::best_val_loss': 0.057489865784769625, 'ATTACH::19::history': {'train_accs': array([0.43622658, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718]), 'train_losses': array([65.39784914,  1.24250757,  1.24212349,  1.24278875,  1.24232228,\n",
      "        1.24242686,  1.24237765,  1.24269527,  1.24235999,  1.24274078,\n",
      "        1.24257819,  1.2425541 ,  1.24263901,  1.24227822,  1.24230173,\n",
      "        1.24262262,  1.24224782,  1.24227342,  1.24270975,  1.24233189,\n",
      "        1.24227493,  1.24243972,  1.24240217,  1.24249797,  1.24245663,\n",
      "        1.24248936,  1.24241823,  1.24244824,  1.24239503,  1.24248584,\n",
      "        1.24221019,  1.24242514,  1.24244816,  1.24233435,  1.24256812,\n",
      "        1.24252746,  1.24247317,  1.24251996]), 'val_accs': array([0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955]), 'val_losses': array([1.24224921, 1.24295274, 1.24441555, 1.24206449, 1.24196069,\n",
      "       1.24250115, 1.2420455 , 1.24191889, 1.24261965, 1.24201175,\n",
      "       1.24212859, 1.24196642, 1.24202054, 1.24266616, 1.24268731,\n",
      "       1.24185208, 1.24222364, 1.24292236, 1.24189821, 1.24217071,\n",
      "       1.24194571, 1.24199669, 1.24218423, 1.24182294, 1.24228531,\n",
      "       1.24201835, 1.24228243, 1.24180711, 1.24191514, 1.24223339,\n",
      "       1.24294642, 1.24183944, 1.24230136, 1.24266534, 1.24227228,\n",
      "       1.24222298, 1.24231919, 1.24182761])}, 'ATTACH::19::best_val_loss': 1.2418071057903233, 'ATTACH::20::history': {'train_accs': array([0.316143  , 0.3430853 , 0.36905739, 0.37353534, 0.39256661,\n",
      "       0.40137324, 0.4102545 , 0.41637436, 0.42510635, 0.4263751 ,\n",
      "       0.42898724, 0.43861482, 0.45063064, 0.45958654, 0.47040824,\n",
      "       0.47563251, 0.4873498 , 0.48637958, 0.509441  , 0.52384506,\n",
      "       0.53369654, 0.55601164, 0.55317561, 0.5901933 , 0.60205986,\n",
      "       0.61467274, 0.63124114, 0.64489887, 0.65303381, 0.67102023,\n",
      "       0.67825957, 0.69758937, 0.70408239, 0.71848645, 0.73468169,\n",
      "       0.74012986, 0.75595194, 0.76729607, 0.78513322, 0.78826778,\n",
      "       0.80528398, 0.80953803, 0.81715053, 0.82692738, 0.84274946,\n",
      "       0.84909322, 0.85125756, 0.86066124, 0.86103441, 0.86536309,\n",
      "       0.87513994, 0.87991641, 0.88767818, 0.88782745, 0.89238003,\n",
      "       0.90111202, 0.90208224, 0.90745578, 0.9109635 , 0.91081424]), 'train_losses': array([72.6284927 , 38.04120529, 27.16332704, 20.83778183, 16.59795549,\n",
      "       13.42255263, 11.00212477,  9.31218823,  7.7460843 ,  6.69540515,\n",
      "        5.91744897,  5.05769952,  4.53356352,  3.86546369,  3.53499584,\n",
      "        3.14463008,  2.8424884 ,  2.63520795,  2.38348647,  2.1697373 ,\n",
      "        2.0173039 ,  1.80070335,  1.78454332,  1.56766997,  1.46817491,\n",
      "        1.39261161,  1.29959314,  1.21459538,  1.15910601,  1.09227323,\n",
      "        1.04379704,  0.98299929,  0.95345818,  0.89132017,  0.82041286,\n",
      "        0.80092981,  0.76884596,  0.71696603,  0.68326876,  0.64574422,\n",
      "        0.59406476,  0.58932935,  0.55969702,  0.52887344,  0.49515059,\n",
      "        0.47327296,  0.45903406,  0.43627245,  0.42925277,  0.41724142,\n",
      "        0.38317171,  0.37522087,  0.35520498,  0.34307235,  0.33131411,\n",
      "        0.31500346,  0.30751911,  0.30054941,  0.27749818,  0.28058519]), 'val_accs': array([0.34089552, 0.36955224, 0.36208955, 0.37761194, 0.34597015,\n",
      "       0.4280597 , 0.42567164, 0.44656716, 0.47283582, 0.46865672,\n",
      "       0.49671642, 0.52029851, 0.54238806, 0.5161194 , 0.60328358,\n",
      "       0.5561194 , 0.61761194, 0.60776119, 0.62179104, 0.63671642,\n",
      "       0.57164179, 0.69850746, 0.66925373, 0.66776119, 0.68179104,\n",
      "       0.74268657, 0.73641791, 0.66059701, 0.75462687, 0.71492537,\n",
      "       0.78626866, 0.81761194, 0.65164179, 0.71373134, 0.79313433,\n",
      "       0.81731343, 0.84656716, 0.7841791 , 0.85910448, 0.87432836,\n",
      "       0.84925373, 0.79253731, 0.8641791 , 0.85014925, 0.86507463,\n",
      "       0.76358209, 0.88925373, 0.84895522, 0.82925373, 0.89492537,\n",
      "       0.86746269, 0.90895522, 0.88835821, 0.89791045, 0.84955224,\n",
      "       0.8958209 , 0.89940299, 0.91552239, 0.92656716, 0.92776119]), 'val_losses': array([28.06895957, 22.21543621, 17.15134725, 10.52113488,  8.64219525,\n",
      "        5.02328647,  3.26707   ,  2.82900685,  1.93189506,  1.75316352,\n",
      "        1.43008932,  1.43956448,  1.3742865 ,  1.50989053,  0.96778277,\n",
      "        1.23870824,  0.97437875,  1.03288691,  0.8411855 ,  0.81688345,\n",
      "        1.28385092,  0.75795891,  0.73018462,  0.63154327,  0.78870011,\n",
      "        0.62167866,  0.70962812,  0.85816004,  0.67926084,  0.6871014 ,\n",
      "        0.52066328,  0.5317311 ,  1.0046231 ,  0.89676281,  0.54736846,\n",
      "        0.52284433,  0.46207095,  0.61123285,  0.41650003,  0.40000243,\n",
      "        0.43267917,  0.63418434,  0.42045699,  0.44575383,  0.42337008,\n",
      "        0.8193409 ,  0.3190651 ,  0.44997834,  0.50156606,  0.29727053,\n",
      "        0.36661067,  0.27675261,  0.32529866,  0.29448732,  0.4294189 ,\n",
      "        0.32334579,  0.27051721,  0.24272586,  0.23859403,  0.21580711])}, 'ATTACH::20::best_val_loss': 0.215807109359485, 'ATTACH::21::history': {'train_accs': array([0.36450481, 0.39905963, 0.42712143, 0.43973431, 0.45846705,\n",
      "       0.49003657, 0.50376894, 0.51847153, 0.53571162, 0.5513098 ,\n",
      "       0.57078886, 0.59370102, 0.61220987, 0.62347936, 0.63676394,\n",
      "       0.66303455, 0.67176655, 0.68460333, 0.69594746, 0.71244123,\n",
      "       0.72162102, 0.72975595, 0.73804015, 0.7452795 , 0.75304127,\n",
      "       0.761624  , 0.77490858, 0.77475931, 0.78110307, 0.78826778,\n",
      "       0.79789537, 0.80147772, 0.80364206, 0.81371744, 0.81379207,\n",
      "       0.81886708, 0.82170311, 0.82842003, 0.83103217, 0.8334204 ,\n",
      "       0.83655497, 0.84177924, 0.84827226, 0.84976491, 0.85640719,\n",
      "       0.85961639, 0.86319875, 0.85924323, 0.87133368, 0.86849765,\n",
      "       0.87096052, 0.87946862, 0.8837973 , 0.88618554, 0.88864841,\n",
      "       0.88961863, 0.89618628, 0.89245466, 0.89932084, 0.90364953]), 'train_losses': array([24.72767391, 13.33423195,  8.94747203,  6.59083098,  4.75846987,\n",
      "        3.55767488,  2.95153674,  2.40241048,  2.04963194,  1.83042394,\n",
      "        1.59207945,  1.38838576,  1.24785212,  1.1683265 ,  1.10072676,\n",
      "        1.006278  ,  0.95869325,  0.90004336,  0.86241709,  0.78869538,\n",
      "        0.78615988,  0.74222163,  0.72699551,  0.69566215,  0.65581863,\n",
      "        0.64927463,  0.61005062,  0.61328384,  0.59615821,  0.57827068,\n",
      "        0.55495802,  0.5450751 ,  0.53391903,  0.51623085,  0.50745796,\n",
      "        0.49746283,  0.49065645,  0.47223172,  0.46963386,  0.4587587 ,\n",
      "        0.44529205,  0.43955058,  0.41812556,  0.41542073,  0.4058305 ,\n",
      "        0.39529537,  0.38505216,  0.38982876,  0.37034822,  0.38316424,\n",
      "        0.36010166,  0.34508187,  0.33963587,  0.33272506,  0.32177919,\n",
      "        0.31746898,  0.29858542,  0.30392397,  0.2975724 ,  0.27990535]), 'val_accs': array([0.51850746, 0.52985075, 0.55402985, 0.5038806 , 0.55373134,\n",
      "       0.51313433, 0.61283582, 0.59492537, 0.63791045, 0.65014925,\n",
      "       0.67104478, 0.64238806, 0.65134328, 0.70328358, 0.61134328,\n",
      "       0.72656716, 0.74537313, 0.69373134, 0.7480597 , 0.71014925,\n",
      "       0.72895522, 0.71253731, 0.74059701, 0.70746269, 0.66895522,\n",
      "       0.72626866, 0.81134328, 0.7319403 , 0.78895522, 0.76656716,\n",
      "       0.72059701, 0.81522388, 0.78059701, 0.75940299, 0.82776119,\n",
      "       0.79014925, 0.84208955, 0.79701493, 0.82835821, 0.84298507,\n",
      "       0.80358209, 0.84059701, 0.8441791 , 0.82507463, 0.86089552,\n",
      "       0.75552239, 0.86119403, 0.84716418, 0.88059701, 0.85910448,\n",
      "       0.88119403, 0.89014925, 0.84776119, 0.89552239, 0.85134328,\n",
      "       0.80149254, 0.85671642, 0.80298507, 0.86925373, 0.89014925]), 'val_losses': array([3.59693275, 2.79285213, 2.20868339, 2.06081166, 1.63180854,\n",
      "       1.42477788, 1.09137859, 1.07708701, 0.96935225, 0.8887331 ,\n",
      "       0.85756657, 0.90705129, 0.82455924, 0.71332385, 0.89287372,\n",
      "       0.72121842, 0.62175851, 0.66117784, 0.62276694, 0.72712303,\n",
      "       0.69699865, 0.76797417, 0.64585885, 0.73466604, 0.95757374,\n",
      "       0.81734463, 0.49537194, 0.78415551, 0.5426786 , 0.52874144,\n",
      "       0.85269279, 0.50554589, 0.65299785, 0.6332099 , 0.43912036,\n",
      "       0.50990097, 0.44905843, 0.56110216, 0.49187282, 0.45536415,\n",
      "       0.56375183, 0.45647944, 0.4211482 , 0.48722438, 0.37346244,\n",
      "       0.83116275, 0.37348697, 0.43636982, 0.35034245, 0.38998575,\n",
      "       0.35375953, 0.31763368, 0.40477094, 0.31529486, 0.40492583,\n",
      "       0.6397225 , 0.39905053, 0.54955928, 0.34881345, 0.32620498])}, 'ATTACH::21::best_val_loss': 0.31529486261196993, 'ATTACH::22::history': {'train_accs': array([0.40891111, 0.45682514, 0.5264572 , 0.6092992 , 0.64049556,\n",
      "       0.6672886 , 0.68908127, 0.71982984, 0.73483096, 0.74595119,\n",
      "       0.76102694, 0.76878872, 0.78565565, 0.78737219, 0.80013434,\n",
      "       0.80311964, 0.803045  , 0.80938876, 0.81088141, 0.81655347,\n",
      "       0.82177774, 0.82132995, 0.83140533, 0.82767371, 0.83356967,\n",
      "       0.83879394, 0.83633107, 0.84282409, 0.84207777, 0.84498843,\n",
      "       0.84364505, 0.84618255, 0.84901858]), 'train_losses': array([16.49841427,  3.14093325,  1.3296011 ,  0.87210891,  0.79315413,\n",
      "        0.72474845,  0.68345029,  0.62665141,  0.60384887,  0.56972087,\n",
      "        0.54887501,  0.53563844,  0.50887253,  0.50487046,  0.48112601,\n",
      "        0.4669166 ,  0.46151875,  0.45160923,  0.44029096,  0.43559669,\n",
      "        0.42354866,  0.42222227,  0.41455703,  0.40960907,  0.40019202,\n",
      "        0.38744145,  0.38888271,  0.38687074,  0.38333581,  0.37936801,\n",
      "        0.37957603,  0.37165829,  0.3648433 ]), 'val_accs': array([0.54358209, 0.60985075, 0.56119403, 0.71313433, 0.74238806,\n",
      "       0.72208955, 0.79970149, 0.78029851, 0.80029851, 0.82      ,\n",
      "       0.78567164, 0.8080597 , 0.82358209, 0.82059701, 0.78298507,\n",
      "       0.83462687, 0.85940299, 0.76776119, 0.86089552, 0.82895522,\n",
      "       0.79701493, 0.83522388, 0.88447761, 0.86955224, 0.78567164,\n",
      "       0.85820896, 0.83223881, 0.82059701, 0.8080597 , 0.86835821,\n",
      "       0.79820896, 0.78567164, 0.7880597 ]), 'val_losses': array([5.2178656 , 0.9891869 , 0.76704522, 0.76183611, 0.56716142,\n",
      "       0.71256639, 0.55173695, 0.53548507, 0.44019728, 0.46826001,\n",
      "       0.53171338, 0.4580616 , 0.43263671, 0.4068624 , 0.47722975,\n",
      "       0.4358528 , 0.34803591, 0.59914978, 0.32733519, 0.38835769,\n",
      "       0.64058759, 0.41742181, 0.32031451, 0.34125014, 0.55423301,\n",
      "       0.34550617, 0.38840819, 0.52589196, 0.47727515, 0.39925909,\n",
      "       0.44801199, 0.57131242, 0.48527404])}, 'ATTACH::22::best_val_loss': 0.3203145140142583, 'ATTACH::23::history': {'train_accs': array([0.36487798, 0.40249272, 0.41876259, 0.43495783, 0.44585417,\n",
      "       0.45204866, 0.45966117, 0.47190089, 0.46981118, 0.47309501,\n",
      "       0.48488693, 0.49384282, 0.49242481, 0.50533622, 0.50369431,\n",
      "       0.5179491 , 0.51003806, 0.51697888, 0.52906933, 0.54675722,\n",
      "       0.55728039, 0.56519143, 0.57153519, 0.57825211, 0.59220837,\n",
      "       0.60019404, 0.6065378 , 0.62847974, 0.637361  , 0.65534741,\n",
      "       0.66370625, 0.68505112, 0.69982835, 0.70706769, 0.72191955,\n",
      "       0.72774088, 0.74065229, 0.75147399, 0.7590865 , 0.78490932,\n",
      "       0.77461005, 0.79087992, 0.79140234, 0.81192626, 0.8120009 ,\n",
      "       0.81968804, 0.83267408, 0.84110755, 0.84610792, 0.85551161,\n",
      "       0.86304948, 0.86797522, 0.87767744, 0.87902082, 0.88857377,\n",
      "       0.89663408, 0.89775356, 0.9058885 , 0.91454586, 0.91208299]), 'train_losses': array([55.2376823 , 24.80365366, 17.63501696, 13.62836874, 11.10288155,\n",
      "        9.17289447,  7.77794588,  6.54567496,  5.78815975,  5.18480705,\n",
      "        4.41827328,  3.93119866,  3.6075    ,  3.20350723,  2.89455876,\n",
      "        2.7069845 ,  2.49066345,  2.35921731,  2.12868034,  1.9778791 ,\n",
      "        1.79196911,  1.64587872,  1.58020808,  1.46729604,  1.33983987,\n",
      "        1.28246016,  1.19852633,  1.1211789 ,  1.10364775,  1.0089532 ,\n",
      "        0.958199  ,  0.88276728,  0.82053135,  0.79180261,  0.74690753,\n",
      "        0.71015191,  0.68417938,  0.64415966,  0.61583847,  0.57554388,\n",
      "        0.57532805,  0.54103938,  0.54122646,  0.49784721,  0.49688529,\n",
      "        0.46146872,  0.44560833,  0.42470731,  0.41322235,  0.39797913,\n",
      "        0.37987726,  0.35627466,  0.33912436,  0.33387168,  0.31313329,\n",
      "        0.29327795,  0.29730199,  0.27694252,  0.25501818,  0.2559696 ]), 'val_accs': array([0.4641791 , 0.3438806 , 0.48537313, 0.53223881, 0.54776119,\n",
      "       0.58597015, 0.57940299, 0.57223881, 0.55402985, 0.6080597 ,\n",
      "       0.5758209 , 0.56985075, 0.50626866, 0.58686567, 0.62358209,\n",
      "       0.53223881, 0.62119403, 0.67343284, 0.66119403, 0.53701493,\n",
      "       0.6438806 , 0.65014925, 0.67283582, 0.64358209, 0.66089552,\n",
      "       0.6480597 , 0.58358209, 0.71880597, 0.69880597, 0.58955224,\n",
      "       0.63074627, 0.7880597 , 0.70179104, 0.6638806 , 0.76895522,\n",
      "       0.78626866, 0.82358209, 0.83462687, 0.82328358, 0.82835821,\n",
      "       0.83134328, 0.66597015, 0.79701493, 0.81283582, 0.75104478,\n",
      "       0.82477612, 0.87820896, 0.78865672, 0.87970149, 0.90328358,\n",
      "       0.88059701, 0.7961194 , 0.87910448, 0.85970149, 0.84626866,\n",
      "       0.85104478, 0.91402985, 0.69880597, 0.91373134, 0.90865672]), 'val_losses': array([11.80690055, 15.40046039,  7.12884956,  4.77195824,  5.01523795,\n",
      "        3.76295713,  2.59023461,  2.0339921 ,  2.05823352,  1.57793734,\n",
      "        1.57533263,  1.80506333,  2.498011  ,  1.70866875,  1.51706865,\n",
      "        1.60292769,  1.4883013 ,  1.2080276 ,  0.94104988,  2.39653944,\n",
      "        0.96790341,  0.93370767,  0.79576234,  0.85334797,  0.78777989,\n",
      "        0.8995633 ,  1.42635718,  0.70172051,  0.70064547,  1.45458851,\n",
      "        0.90071407,  0.527508  ,  0.65733766,  0.87103802,  0.62875222,\n",
      "        0.53900545,  0.42039306,  0.45809781,  0.41607297,  0.41672416,\n",
      "        0.40265806,  0.6677587 ,  0.47047498,  0.51529753,  0.55589942,\n",
      "        0.44694534,  0.3871686 ,  0.53361962,  0.32239116,  0.28468507,\n",
      "        0.31998732,  0.52302388,  0.32233807,  0.3385896 ,  0.38425907,\n",
      "        0.38888635,  0.25241818,  0.875755  ,  0.23719035,  0.24867255])}, 'ATTACH::23::best_val_loss': 0.2371903450809308, 'ATTACH::24::history': {'train_accs': array([0.39010374, 0.44667513, 0.48966341, 0.53645794, 0.59325323,\n",
      "       0.6428838 , 0.67661766, 0.69482797, 0.69669378, 0.7119188 ,\n",
      "       0.72759161, 0.73012911, 0.74027913, 0.75363833, 0.76363908,\n",
      "       0.75804164, 0.76699754, 0.77080379, 0.78229719, 0.78834241,\n",
      "       0.78655124, 0.79393985, 0.80080603, 0.79729831, 0.78490932,\n",
      "       0.81065751, 0.79797   , 0.80520934, 0.81013508, 0.79767147,\n",
      "       0.80655273, 0.81185163, 0.8140906 , 0.8092395 , 0.83036047,\n",
      "       0.81938951, 0.81744906, 0.8338682 , 0.82640496]), 'train_losses': array([24.82557901,  4.66606201,  2.30713685,  1.39875272,  0.93352722,\n",
      "        0.78211428,  0.70334924,  0.67091022,  0.65404478,  0.62079424,\n",
      "        0.58691621,  0.58429296,  0.56560947,  0.54233163,  0.5156211 ,\n",
      "        0.53863656,  0.5108818 ,  0.50189706,  0.48625508,  0.4707163 ,\n",
      "        0.47867849,  0.46940452,  0.45637346,  0.46643538,  0.47903819,\n",
      "        0.42870917,  0.45479941,  0.4572746 ,  0.43368628,  0.45955051,\n",
      "        0.42980529,  0.43097312,  0.43581776,  0.4342248 ,  0.39028714,\n",
      "        0.41189648,  0.41267076,  0.39221405,  0.40740455]), 'val_accs': array([0.4719403 , 0.54328358, 0.52716418, 0.65940299, 0.68626866,\n",
      "       0.68656716, 0.71074627, 0.64835821, 0.73671642, 0.59761194,\n",
      "       0.78955224, 0.78358209, 0.74865672, 0.79761194, 0.7841791 ,\n",
      "       0.76656716, 0.77671642, 0.74      , 0.77104478, 0.67223881,\n",
      "       0.79373134, 0.74059701, 0.76149254, 0.68686567, 0.83761194,\n",
      "       0.75940299, 0.7119403 , 0.73343284, 0.82895522, 0.77462687,\n",
      "       0.77104478, 0.7761194 , 0.72238806, 0.81791045, 0.76537313,\n",
      "       0.82358209, 0.70656716, 0.83074627, 0.74268657]), 'val_losses': array([6.54151456, 1.6847847 , 2.2253552 , 1.00463117, 0.64969306,\n",
      "       0.66115758, 0.58930402, 0.66990781, 0.55495867, 0.87020068,\n",
      "       0.44787468, 0.53204024, 0.50403953, 0.47561226, 0.4154066 ,\n",
      "       0.51817243, 0.49496597, 0.60642682, 0.54912727, 0.6655876 ,\n",
      "       0.40543972, 0.49884355, 0.50097725, 0.58991322, 0.44188517,\n",
      "       0.51549595, 0.71341835, 0.60830528, 0.39469747, 0.52850432,\n",
      "       0.53963328, 0.49651719, 0.63424644, 0.40361801, 0.53823016,\n",
      "       0.42814187, 0.8072738 , 0.42402964, 0.64943231])}, 'ATTACH::24::best_val_loss': 0.3946974727644849, 'ATTACH::25::history': {'train_accs': array([0.30800806, 0.37062467, 0.3854019 , 0.38920815, 0.39913426,\n",
      "       0.44503321, 0.45824315, 0.47973729, 0.50384357, 0.52354653,\n",
      "       0.55444436, 0.58787969, 0.63198746, 0.65340697, 0.66512426,\n",
      "       0.69952982, 0.72117322, 0.73550265, 0.75774312, 0.77520711,\n",
      "       0.78431226, 0.78237182, 0.81065751, 0.81812076, 0.82662885,\n",
      "       0.83655497, 0.84580939, 0.84924248, 0.86297485, 0.87357265,\n",
      "       0.8695425 , 0.87887156, 0.88558848, 0.88961863, 0.89342488,\n",
      "       0.8950668 , 0.90394806, 0.89402194, 0.91014255, 0.91447123,\n",
      "       0.92424808, 0.92611389, 0.92887529, 0.92887529, 0.93044257,\n",
      "       0.93506978, 0.93551758, 0.93618927, 0.94163744, 0.94424957,\n",
      "       0.94977237, 0.95253377, 0.94768266, 0.95477274, 0.96208672,\n",
      "       0.95932532, 0.95850437, 0.9611165 , 0.96051944, 0.96298231]), 'train_losses': array([68.51391801, 19.86317253, 13.35946223, 10.00417896,  7.72895958,\n",
      "        6.00798173,  4.88549151,  4.02790486,  3.43224375,  2.9991409 ,\n",
      "        2.46061301,  2.20682353,  1.83975556,  1.63039365,  1.53647114,\n",
      "        1.29725685,  1.179811  ,  1.08838809,  0.9759684 ,  0.92645631,\n",
      "        0.82735086,  0.79790427,  0.69726694,  0.66435768,  0.61052511,\n",
      "        0.56111238,  0.5421459 ,  0.51756762,  0.48249956,  0.42536955,\n",
      "        0.42619908,  0.40221423,  0.37594042,  0.36032908,  0.33747529,\n",
      "        0.34278653,  0.32177506,  0.33741993,  0.30334649,  0.28791366,\n",
      "        0.26284793,  0.25296918,  0.25174798,  0.24017309,  0.23928178,\n",
      "        0.21929039,  0.2232151 ,  0.21892351,  0.20973908,  0.19818974,\n",
      "        0.18392178,  0.17296056,  0.1820792 ,  0.15709715,  0.13500037,\n",
      "        0.13853025,  0.14465844,  0.1274105 ,  0.13394127,  0.12873703]), 'val_accs': array([0.3719403 , 0.43641791, 0.42656716, 0.46358209, 0.57462687,\n",
      "       0.51432836, 0.58268657, 0.58059701, 0.64477612, 0.66089552,\n",
      "       0.71850746, 0.7238806 , 0.72985075, 0.74686567, 0.74208955,\n",
      "       0.76716418, 0.81910448, 0.80895522, 0.81373134, 0.83104478,\n",
      "       0.84507463, 0.84089552, 0.8558209 , 0.84925373, 0.84835821,\n",
      "       0.89671642, 0.8438806 , 0.89522388, 0.84328358, 0.81820896,\n",
      "       0.90985075, 0.9080597 , 0.89014925, 0.87044776, 0.89701493,\n",
      "       0.81940299, 0.87373134, 0.85074627, 0.92925373, 0.88447761,\n",
      "       0.88537313, 0.92507463, 0.91014925, 0.92626866, 0.91343284,\n",
      "       0.92716418, 0.87791045, 0.89791045, 0.93701493, 0.91880597,\n",
      "       0.93283582, 0.94328358, 0.92477612, 0.89492537, 0.92208955,\n",
      "       0.91223881, 0.94686567, 0.92447761, 0.95820896, 0.93462687]), 'val_losses': array([8.57228586, 3.45870029, 3.0866306 , 2.35459622, 1.78373422,\n",
      "       2.38730948, 1.50534636, 1.24798711, 1.17551716, 1.01436897,\n",
      "       0.95794563, 0.94147825, 0.88307022, 0.80614467, 0.88955403,\n",
      "       0.8278527 , 0.51897113, 0.5924997 , 0.56390078, 0.52762983,\n",
      "       0.52467026, 0.4822599 , 0.48087063, 0.48192618, 0.49975672,\n",
      "       0.36280773, 0.54200307, 0.40113844, 0.51306993, 0.60071974,\n",
      "       0.32086427, 0.34693336, 0.37794601, 0.41990563, 0.3631858 ,\n",
      "       0.57913432, 0.42058886, 0.49681267, 0.23699592, 0.38575489,\n",
      "       0.3375606 , 0.2633551 , 0.31319779, 0.2791429 , 0.30069121,\n",
      "       0.30041729, 0.37984162, 0.3134582 , 0.21570813, 0.26049109,\n",
      "       0.23160357, 0.21977754, 0.2376032 , 0.28405087, 0.23660092,\n",
      "       0.37758795, 0.19101278, 0.29194918, 0.14169751, 0.21132542])}, 'ATTACH::25::best_val_loss': 0.14169751293623625, 'ATTACH::26::history': {'train_accs': array([0.29606687, 0.35905665, 0.37749086, 0.39249198, 0.41100082,\n",
      "       0.42674826, 0.44600343, 0.46436301, 0.48466303, 0.50048511,\n",
      "       0.51473991, 0.53205463, 0.550862  , 0.573401  , 0.57265468,\n",
      "       0.59258154, 0.6012389 , 0.60997089, 0.63250989, 0.62900216,\n",
      "       0.65027241, 0.65885514, 0.67064706, 0.68303605, 0.69064856,\n",
      "       0.70751549, 0.71968057, 0.73221882, 0.75550414, 0.76856482,\n",
      "       0.7867005 , 0.79513397, 0.81312038, 0.82140458, 0.83595791,\n",
      "       0.84461527, 0.86028808, 0.86334801, 0.86588551, 0.88111053,\n",
      "       0.88566311, 0.89215613, 0.89603702, 0.90409732, 0.90715725,\n",
      "       0.9137249 , 0.91618778, 0.92014329, 0.92380028, 0.92671095,\n",
      "       0.93574147, 0.93506978, 0.93693559, 0.94081648, 0.94507053,\n",
      "       0.9473095 , 0.95066796, 0.94798119, 0.9560415 , 0.95850437]), 'train_losses': array([39.475992  , 20.69473934, 13.87164048, 10.11768108,  7.70712491,\n",
      "        6.2350504 ,  5.47344012,  4.6297865 ,  3.9745196 ,  3.56811416,\n",
      "        3.30696038,  2.94250844,  2.64805568,  2.44080967,  2.29621425,\n",
      "        2.0419152 ,  1.87330627,  1.76792315,  1.57875911,  1.57808833,\n",
      "        1.41095937,  1.32344888,  1.24315329,  1.20775025,  1.15378108,\n",
      "        1.0649407 ,  0.99714718,  0.91950475,  0.84167079,  0.78901639,\n",
      "        0.75416938,  0.73295184,  0.66317927,  0.61934204,  0.58179191,\n",
      "        0.55626559,  0.49733311,  0.49481914,  0.480717  ,  0.41673983,\n",
      "        0.40076721,  0.38415862,  0.37511892,  0.33578147,  0.32255062,\n",
      "        0.30310609,  0.28461774,  0.27861025,  0.27817009,  0.25130355,\n",
      "        0.22812837,  0.22620464,  0.22685745,  0.20655275,  0.18382216,\n",
      "        0.17521375,  0.17004349,  0.17295672,  0.14913772,  0.14469478]), 'val_accs': array([0.43223881, 0.44597015, 0.49641791, 0.55701493, 0.56567164,\n",
      "       0.56268657, 0.55940299, 0.57402985, 0.63820896, 0.67343284,\n",
      "       0.65761194, 0.58567164, 0.59104478, 0.62567164, 0.63970149,\n",
      "       0.65850746, 0.68179104, 0.6361194 , 0.66955224, 0.67522388,\n",
      "       0.71552239, 0.71283582, 0.74746269, 0.75910448, 0.77343284,\n",
      "       0.74268657, 0.72208955, 0.84746269, 0.85164179, 0.80447761,\n",
      "       0.86626866, 0.86029851, 0.86835821, 0.7758209 , 0.85791045,\n",
      "       0.87074627, 0.86567164, 0.90985075, 0.90029851, 0.89731343,\n",
      "       0.8961194 , 0.90089552, 0.83731343, 0.88746269, 0.85462687,\n",
      "       0.90537313, 0.90985075, 0.9280597 , 0.88      , 0.89014925,\n",
      "       0.92537313, 0.92955224, 0.9241791 , 0.90119403, 0.93761194,\n",
      "       0.91820896, 0.83880597, 0.94567164, 0.9361194 , 0.94119403]), 'val_losses': array([13.51650388,  9.39700117,  4.92679093,  4.43355769,  3.3300972 ,\n",
      "        2.79876737,  2.1118473 ,  2.95947401,  1.51684866,  1.23966768,\n",
      "        1.53077618,  1.0731081 ,  1.36321866,  0.99967878,  1.07866786,\n",
      "        1.23322565,  0.8124702 ,  1.3956537 ,  0.80419549,  1.02458588,\n",
      "        0.89006754,  0.81413806,  0.62249802,  0.6320657 ,  0.50089926,\n",
      "        0.62585222,  0.66799764,  0.41221217,  0.37718232,  0.57721474,\n",
      "        0.37487735,  0.39867324,  0.37652683,  0.74617863,  0.41301791,\n",
      "        0.40269596,  0.39090855,  0.27759253,  0.29523435,  0.30842276,\n",
      "        0.3143926 ,  0.35830713,  0.55527819,  0.3599511 ,  0.47874742,\n",
      "        0.29952627,  0.28209844,  0.24611256,  0.42617183,  0.3525038 ,\n",
      "        0.25154753,  0.27219689,  0.2468821 ,  0.41074284,  0.21642145,\n",
      "        0.28536127,  0.63270769,  0.18732867,  0.22008341,  0.21449268])}, 'ATTACH::26::best_val_loss': 0.18732867135930417, 'ATTACH::27::history': {'train_accs': array([0.3854019 , 0.412792  , 0.42734532, 0.44279424, 0.45227256,\n",
      "       0.46734831, 0.48929025, 0.50764982, 0.52966639, 0.5460109 ,\n",
      "       0.57907307, 0.60116427, 0.62064333, 0.64780954, 0.67952832,\n",
      "       0.70072393, 0.72505411, 0.75222031, 0.76946041, 0.78483469,\n",
      "       0.80132846, 0.82207627, 0.83759982, 0.8529741 , 0.86737816,\n",
      "       0.87693111, 0.88812598, 0.90133592, 0.90417195, 0.90872453,\n",
      "       0.91932234, 0.92029256, 0.9249944 , 0.93059184, 0.93857751,\n",
      "       0.94074185, 0.94208523, 0.94820509, 0.94619001, 0.95141428,\n",
      "       0.95634003, 0.9532801 , 0.95723561, 0.96126577, 0.96372864,\n",
      "       0.96559445, 0.96738563, 0.96671393, 0.96977386, 0.9694007 ,\n",
      "       0.97089335, 0.97335622, 0.97089335, 0.97305769, 0.97738637,\n",
      "       0.97596836, 0.977461  , 0.98014777, 0.98104336, 0.97992387]), 'train_losses': array([22.19126567,  8.81542176,  5.99077587,  4.57566044,  3.78311225,\n",
      "        3.19158195,  2.59391707,  2.34369723,  2.01851767,  1.74671949,\n",
      "        1.50922331,  1.32763781,  1.2043997 ,  1.0769783 ,  0.95936163,\n",
      "        0.85045186,  0.77875597,  0.70461901,  0.64704973,  0.58155485,\n",
      "        0.55424493,  0.48401648,  0.44260067,  0.40301946,  0.3577623 ,\n",
      "        0.33424872,  0.31845016,  0.27753731,  0.2646797 ,  0.26528745,\n",
      "        0.23453813,  0.22834484,  0.21615924,  0.19942262,  0.17740892,\n",
      "        0.17330132,  0.17086985,  0.15347197,  0.15493904,  0.1431786 ,\n",
      "        0.12649318,  0.12984314,  0.12504448,  0.11983257,  0.10734826,\n",
      "        0.10637599,  0.10049401,  0.10005015,  0.09312071,  0.09313564,\n",
      "        0.08655049,  0.08590001,  0.08522711,  0.0829098 ,  0.07413706,\n",
      "        0.07509171,  0.06772662,  0.06433915,  0.06213851,  0.06532298]), 'val_accs': array([0.40089552, 0.30686567, 0.4319403 , 0.52746269, 0.5919403 ,\n",
      "       0.53701493, 0.63850746, 0.55014925, 0.59791045, 0.60537313,\n",
      "       0.62089552, 0.60746269, 0.64776119, 0.65671642, 0.69791045,\n",
      "       0.76328358, 0.69791045, 0.68835821, 0.72537313, 0.76179104,\n",
      "       0.84477612, 0.88477612, 0.88089552, 0.88656716, 0.85970149,\n",
      "       0.89283582, 0.8558209 , 0.90298507, 0.88477612, 0.92328358,\n",
      "       0.9119403 , 0.91283582, 0.8761194 , 0.92656716, 0.92238806,\n",
      "       0.92925373, 0.93731343, 0.94835821, 0.94656716, 0.9319403 ,\n",
      "       0.94895522, 0.94835821, 0.93552239, 0.96179104, 0.9438806 ,\n",
      "       0.96328358, 0.93910448, 0.92029851, 0.96149254, 0.96298507,\n",
      "       0.97074627, 0.95761194, 0.96626866, 0.97164179, 0.94507463,\n",
      "       0.97432836, 0.96119403, 0.97283582, 0.94746269, 0.97671642]), 'val_losses': array([9.17911894, 5.24346108, 3.87829101, 2.32424407, 1.77472015,\n",
      "       2.58755212, 1.01702998, 2.12041539, 0.9438738 , 1.23036975,\n",
      "       1.48388916, 2.08173038, 0.84148852, 0.99859323, 0.69875102,\n",
      "       0.62205562, 0.98111779, 1.03736672, 0.87000145, 0.70673459,\n",
      "       0.38183455, 0.3281637 , 0.30438294, 0.30561792, 0.42410768,\n",
      "       0.28236317, 0.39312354, 0.28977965, 0.31095034, 0.18942796,\n",
      "       0.24464603, 0.24569064, 0.3312284 , 0.24529893, 0.22614222,\n",
      "       0.18644193, 0.17175754, 0.14238925, 0.13974543, 0.1861442 ,\n",
      "       0.13429493, 0.15735075, 0.1905634 , 0.11419993, 0.16378627,\n",
      "       0.11309315, 0.18373574, 0.3302448 , 0.13354786, 0.10702674,\n",
      "       0.08872616, 0.12327235, 0.11463836, 0.08121929, 0.15261725,\n",
      "       0.07673485, 0.11438701, 0.08992238, 0.17808859, 0.0817152 ])}, 'ATTACH::27::best_val_loss': 0.07673484686372885, 'ATTACH::28::history': {'train_accs': array([0.31412792, 0.35928054, 0.37950593, 0.39480558, 0.40510486,\n",
      "       0.42868871, 0.43122621, 0.45339204, 0.47749832, 0.49384282,\n",
      "       0.50600791, 0.52630793, 0.54593626, 0.56974401, 0.58840212,\n",
      "       0.60228375, 0.61892679, 0.64385402, 0.65430256, 0.66154191,\n",
      "       0.68475259, 0.69930592, 0.70773938, 0.72900963, 0.74751847,\n",
      "       0.75751922, 0.76587805, 0.7756549 , 0.78207329, 0.79535786,\n",
      "       0.8032689 , 0.80274647, 0.81834465, 0.82722591, 0.839391  ,\n",
      "       0.84685424, 0.84998881, 0.86431823, 0.86469139, 0.87685648,\n",
      "       0.88140906, 0.889544  , 0.89820136, 0.90238077, 0.908426  ,\n",
      "       0.90066423, 0.9139488 , 0.92141205, 0.92678558, 0.92283006,\n",
      "       0.92872602, 0.93701022, 0.93477125, 0.93842824, 0.93492052,\n",
      "       0.94275692, 0.94275692, 0.94745877, 0.94581685, 0.95566833]), 'train_losses': array([65.38745364, 27.29457579, 17.57213025, 12.6136506 , 10.02580778,\n",
      "        7.71206057,  6.61077338,  5.55080912,  4.6223271 ,  4.04508682,\n",
      "        3.55011307,  3.15365775,  2.84720378,  2.51554027,  2.23662532,\n",
      "        2.05122481,  1.82223214,  1.67091782,  1.52622988,  1.39164676,\n",
      "        1.25068984,  1.17665827,  1.11395702,  1.02421578,  0.96576431,\n",
      "        0.88695491,  0.82985554,  0.79265684,  0.75860199,  0.70385348,\n",
      "        0.67211673,  0.64488781,  0.59307265,  0.55994248,  0.52711166,\n",
      "        0.49689704,  0.47194358,  0.43440947,  0.44129482,  0.39451079,\n",
      "        0.36324458,  0.34534999,  0.33601851,  0.31424746,  0.28896778,\n",
      "        0.30476402,  0.28274206,  0.25863519,  0.23770371,  0.25037427,\n",
      "        0.23076202,  0.20762692,  0.21537947,  0.1960732 ,  0.20562926,\n",
      "        0.19187208,  0.179572  ,  0.17180218,  0.17879979,  0.15296831]), 'val_accs': array([0.48119403, 0.46567164, 0.4680597 , 0.45880597, 0.52328358,\n",
      "       0.51402985, 0.51970149, 0.52925373, 0.62477612, 0.57940299,\n",
      "       0.64865672, 0.61313433, 0.60567164, 0.67343284, 0.7158209 ,\n",
      "       0.7241791 , 0.76208955, 0.77104478, 0.78985075, 0.78089552,\n",
      "       0.77731343, 0.80328358, 0.79373134, 0.81373134, 0.80955224,\n",
      "       0.82686567, 0.80746269, 0.83641791, 0.83283582, 0.84985075,\n",
      "       0.87134328, 0.85880597, 0.87074627, 0.84925373, 0.85343284,\n",
      "       0.89343284, 0.87074627, 0.8838806 , 0.88507463, 0.91671642,\n",
      "       0.89283582, 0.90716418, 0.91402985, 0.88059701, 0.90179104,\n",
      "       0.91343284, 0.88477612, 0.8958209 , 0.89761194, 0.9358209 ,\n",
      "       0.90716418, 0.92119403, 0.89432836, 0.89522388, 0.91791045,\n",
      "       0.9161194 , 0.92179104, 0.90865672, 0.9319403 , 0.93373134]), 'val_losses': array([11.72538342,  9.48167131,  4.35532393,  5.02346766,  2.96595666,\n",
      "        2.44550343,  2.33749138,  2.03474164,  1.44505118,  1.59082062,\n",
      "        1.74163164,  1.43170687,  1.60377177,  1.25600081,  1.08650442,\n",
      "        1.15150318,  0.86853329,  0.87782099,  0.8035868 ,  0.85600999,\n",
      "        0.84046067,  0.6907665 ,  0.7648854 ,  0.65606699,  0.76251222,\n",
      "        0.6388549 ,  0.68519805,  0.59191097,  0.62076025,  0.50761483,\n",
      "        0.46915235,  0.45128158,  0.42664253,  0.4704521 ,  0.43183161,\n",
      "        0.34882421,  0.36476907,  0.35634986,  0.33100627,  0.28386204,\n",
      "        0.31208209,  0.28479529,  0.22773577,  0.31739916,  0.27642956,\n",
      "        0.23964597,  0.32433838,  0.26675944,  0.28822393,  0.18512817,\n",
      "        0.22644314,  0.21158052,  0.26045672,  0.26828292,  0.20052882,\n",
      "        0.21865494,  0.21189641,  0.24816952,  0.177765  ,  0.17104736])}, 'ATTACH::28::best_val_loss': 0.17104735902885893, 'ATTACH::29::history': {'train_accs': array([0.43428614, 0.48749907, 0.54018957, 0.57967012, 0.61258303,\n",
      "       0.64736174, 0.66594522, 0.68236436, 0.70102246, 0.72781551,\n",
      "       0.75416076, 0.7781924 , 0.80267184, 0.82192701, 0.83782372,\n",
      "       0.84618255, 0.85939249, 0.87096052, 0.86498992, 0.87581163,\n",
      "       0.88409583, 0.88133443, 0.89827599, 0.9001418 , 0.90223151,\n",
      "       0.91185909, 0.91573998, 0.92006866, 0.92051646, 0.92439734,\n",
      "       0.92238227, 0.92924845, 0.9335025 , 0.93290544, 0.93163669,\n",
      "       0.94036868, 0.94118964, 0.94574222, 0.94245839, 0.9470856 ,\n",
      "       0.9473095 , 0.94999627, 0.95208598, 0.95708635]), 'train_losses': array([17.081403  ,  4.71647505,  2.35347987,  1.45589534,  1.05681285,\n",
      "        0.83683256,  0.76004043,  0.73761892,  0.69573343,  0.64144928,\n",
      "        0.60204221,  0.56905481,  0.52663296,  0.48461895,  0.44957464,\n",
      "        0.43635462,  0.40131684,  0.37322123,  0.38748815,  0.35197234,\n",
      "        0.34248567,  0.34139494,  0.30386326,  0.29144362,  0.28176858,\n",
      "        0.26872935,  0.25372141,  0.24098955,  0.23889731,  0.22826676,\n",
      "        0.2275133 ,  0.21882014,  0.20323201,  0.20157445,  0.21051376,\n",
      "        0.17775581,  0.18058484,  0.17043721,  0.17296307,  0.16112825,\n",
      "        0.1591737 ,  0.16187727,  0.1570447 ,  0.1358409 ]), 'val_accs': array([0.46865672, 0.52716418, 0.60358209, 0.57492537, 0.5438806 ,\n",
      "       0.69880597, 0.69850746, 0.64567164, 0.69432836, 0.79462687,\n",
      "       0.82835821, 0.87014925, 0.85313433, 0.79164179, 0.8438806 ,\n",
      "       0.77253731, 0.85791045, 0.88119403, 0.78029851, 0.82835821,\n",
      "       0.87910448, 0.90149254, 0.90656716, 0.83671642, 0.85552239,\n",
      "       0.9161194 , 0.88447761, 0.91820896, 0.92507463, 0.92925373,\n",
      "       0.86298507, 0.81432836, 0.72865672, 0.94835821, 0.84895522,\n",
      "       0.87014925, 0.8838806 , 0.76955224, 0.87462687, 0.91373134,\n",
      "       0.93104478, 0.90656716, 0.87134328, 0.93820896]), 'val_losses': array([6.99715221, 1.65114361, 2.02338974, 1.2852799 , 1.46538679,\n",
      "       0.69998436, 0.66089116, 0.8466964 , 0.67109323, 0.50832161,\n",
      "       0.4552486 , 0.42064318, 0.49882034, 0.44921714, 0.34190117,\n",
      "       0.5095068 , 0.40878985, 0.3096794 , 0.50082226, 0.36135589,\n",
      "       0.28565428, 0.27027126, 0.23908292, 0.39606889, 0.38131639,\n",
      "       0.23194916, 0.27875712, 0.20261607, 0.2385313 , 0.23887641,\n",
      "       0.37992486, 0.4958888 , 0.92220393, 0.14135821, 0.4207913 ,\n",
      "       0.35721099, 0.27496112, 0.71106933, 0.36535047, 0.27781847,\n",
      "       0.19699771, 0.29687772, 0.33427191, 0.16559011])}, 'ATTACH::29::best_val_loss': 0.1413582127930513}\n",
      "{'ATTACH::0::history': {'train_accs': array([0.42727069, 0.48839466, 0.54287633, 0.59646242, 0.65325771,\n",
      "       0.69945518, 0.73087544, 0.74811553, 0.77214718, 0.79013359,\n",
      "       0.79856706, 0.80946339, 0.81543399, 0.81953877, 0.82431525,\n",
      "       0.82677812, 0.82827077, 0.83185312, 0.84155534, 0.84394358,\n",
      "       0.85125756, 0.85230241, 0.85379506, 0.86155683]), 'train_losses': array([13.64595142,  2.6412895 ,  1.35655506,  0.9405972 ,  0.75138228,\n",
      "        0.66724081,  0.60847054,  0.57068169,  0.52156873,  0.49373956,\n",
      "        0.47677927,  0.4556888 ,  0.4350108 ,  0.41644737,  0.4147776 ,\n",
      "        0.40487542,  0.42463622,  0.41915914,  0.38677576,  0.38188443,\n",
      "        0.3680787 ,  0.36760645,  0.36471095,  0.35171357]), 'val_accs': array([0.46268657, 0.51820896, 0.62746269, 0.71791045, 0.66567164,\n",
      "       0.6638806 , 0.72029851, 0.76597015, 0.77343284, 0.78686567,\n",
      "       0.61074627, 0.82149254, 0.71074627, 0.80328358, 0.77074627,\n",
      "       0.82358209, 0.74208955, 0.72119403, 0.70955224, 0.7838806 ,\n",
      "       0.82119403, 0.75492537, 0.7438806 , 0.72477612]), 'val_losses': array([2.56614262, 1.83371192, 0.86297181, 0.70014938, 0.78370151,\n",
      "       0.69215718, 0.67175659, 0.50872135, 0.48579525, 0.44991003,\n",
      "       0.93068688, 0.46251741, 0.51247703, 0.39679981, 0.59413744,\n",
      "       0.43499426, 0.55096031, 0.65039427, 0.67374697, 0.60401662,\n",
      "       0.4412217 , 0.5119683 , 0.66568747, 0.63970277])}, 'ATTACH::0::best_val_loss': 0.3967998089897099, 'ATTACH::1::history': {'train_accs': array([0.40950817, 0.44174938, 0.48264796, 0.52608404, 0.57272931,\n",
      "       0.62534518, 0.67758788, 0.72147175, 0.76102694, 0.78752146,\n",
      "       0.81946414, 0.85028733, 0.86991567, 0.88708113, 0.90364953,\n",
      "       0.91185909, 0.92439734, 0.93342787, 0.94492126, 0.94768266,\n",
      "       0.95641466, 0.9643257 , 0.96268378, 0.9694007 , 0.97089335,\n",
      "       0.9751474 , 0.97455034, 0.97723711, 0.98052093, 0.98253601,\n",
      "       0.98223748, 0.98552131, 0.98484962, 0.98761102, 0.98611837,\n",
      "       0.98634226, 0.98776028, 0.9887305 , 0.99059631, 0.98940219,\n",
      "       0.98932756, 0.99007389, 0.99096948, 0.99037242, 0.99104411,\n",
      "       0.99141727, 0.99104411, 0.99186506, 0.9919397 ]), 'train_losses': array([14.03066079,  4.93907392,  3.38337857,  2.40569172,  1.81773315,\n",
      "        1.39596017,  1.1004392 ,  0.91449392,  0.75564628,  0.66929569,\n",
      "        0.54418484,  0.45464886,  0.40091318,  0.33437864,  0.29463358,\n",
      "        0.26638451,  0.22873211,  0.19511333,  0.16855004,  0.15810893,\n",
      "        0.14488343,  0.11523461,  0.11505106,  0.09127734,  0.08816154,\n",
      "        0.07979277,  0.07953155,  0.06905594,  0.0621237 ,  0.0554142 ,\n",
      "        0.05539705,  0.04607911,  0.04492709,  0.04010654,  0.04531765,\n",
      "        0.04438896,  0.03933612,  0.03385544,  0.03255907,  0.03688434,\n",
      "        0.03393461,  0.03350802,  0.0315912 ,  0.03348345,  0.02625844,\n",
      "        0.02860989,  0.02670443,  0.0252306 ,  0.02659802]), 'val_accs': array([0.43373134, 0.52447761, 0.59850746, 0.49910448, 0.59253731,\n",
      "       0.68328358, 0.71761194, 0.77552239, 0.82358209, 0.86149254,\n",
      "       0.77462687, 0.87791045, 0.89253731, 0.9038806 , 0.85253731,\n",
      "       0.91671642, 0.91253731, 0.94477612, 0.95104478, 0.95522388,\n",
      "       0.93552239, 0.95223881, 0.94955224, 0.93134328, 0.96776119,\n",
      "       0.93850746, 0.97462687, 0.96865672, 0.96029851, 0.96865672,\n",
      "       0.96925373, 0.89970149, 0.92179104, 0.94447761, 0.97731343,\n",
      "       0.95940299, 0.9761194 , 0.97432836, 0.9841791 , 0.95164179,\n",
      "       0.98059701, 0.9761194 , 0.97223881, 0.98149254, 0.97791045,\n",
      "       0.9758209 , 0.9761194 , 0.97880597, 0.98208955]), 'val_losses': array([2.9242572 , 2.25836395, 1.35982534, 1.88890744, 2.3025924 ,\n",
      "       1.03563659, 1.08991431, 0.70112799, 0.53019678, 0.43157321,\n",
      "       0.86623459, 0.34261658, 0.33030031, 0.29290865, 0.44746353,\n",
      "       0.24065521, 0.24720013, 0.14804492, 0.1398145 , 0.12167456,\n",
      "       0.20650436, 0.1450734 , 0.15265423, 0.24690128, 0.10317951,\n",
      "       0.20102633, 0.08795426, 0.10645401, 0.11608537, 0.10444328,\n",
      "       0.10123476, 0.36500721, 0.2999205 , 0.18492971, 0.07163436,\n",
      "       0.12599573, 0.0801093 , 0.08232168, 0.05107833, 0.16255751,\n",
      "       0.06236475, 0.0777106 , 0.09598944, 0.06921745, 0.09198939,\n",
      "       0.08567945, 0.08892124, 0.08599064, 0.06351876])}, 'ATTACH::1::best_val_loss': 0.05107833040099758, 'ATTACH::2::history': {'train_accs': array([0.34323457, 0.36905739, 0.40376147, 0.41488171, 0.41525487,\n",
      "       0.4236137 , 0.43637585, 0.43943578, 0.44309277, 0.44906336,\n",
      "       0.45563102, 0.46772147, 0.46533323, 0.48175237, 0.48317039,\n",
      "       0.48906635, 0.50458989, 0.51690425, 0.52772595, 0.52571087,\n",
      "       0.54093589, 0.53459213, 0.55713113, 0.56616165, 0.56623629,\n",
      "       0.57160982, 0.57996865, 0.5954922 , 0.59049183, 0.59735801,\n",
      "       0.59982088, 0.6063139 , 0.60683633, 0.61377715, 0.62213598,\n",
      "       0.63094261, 0.6426599 , 0.64027166, 0.65086947, 0.65512352,\n",
      "       0.65825808, 0.66706471, 0.66684081, 0.67213971, 0.67751325,\n",
      "       0.68549892, 0.6923651 , 0.69408165, 0.70012688, 0.70057467,\n",
      "       0.70445556, 0.70691843, 0.72109859, 0.73460706, 0.72363609,\n",
      "       0.73751773, 0.74565266, 0.74535413, 0.7558773 , 0.76640048]), 'train_losses': array([75.00758779, 25.89876403, 15.62455885, 11.223733  ,  9.07374611,\n",
      "        7.5735901 ,  6.15607091,  5.37222011,  4.87436309,  4.27154606,\n",
      "        3.90880312,  3.34438639,  3.16734069,  2.83986414,  2.67072718,\n",
      "        2.57355389,  2.25319135,  2.10554189,  1.97404273,  1.90478103,\n",
      "        1.75948184,  1.76022069,  1.60391215,  1.55870655,  1.53046077,\n",
      "        1.45217295,  1.40701459,  1.32148026,  1.29553386,  1.24288461,\n",
      "        1.22569645,  1.18320662,  1.14959179,  1.09451556,  1.04494645,\n",
      "        1.03117958,  0.9768687 ,  0.9770081 ,  0.90513544,  0.92722081,\n",
      "        0.88146278,  0.87005625,  0.84864189,  0.84301548,  0.83355621,\n",
      "        0.79721999,  0.77985889,  0.7825523 ,  0.74758013,  0.75261763,\n",
      "        0.74032994,  0.74328351,  0.71324051,  0.67094435,  0.67974301,\n",
      "        0.66501714,  0.64557671,  0.63435301,  0.62607381,  0.59363974]), 'val_accs': array([0.31850746, 0.31761194, 0.40895522, 0.44328358, 0.54865672,\n",
      "       0.5441791 , 0.53253731, 0.53074627, 0.54      , 0.53880597,\n",
      "       0.59791045, 0.59044776, 0.62089552, 0.60567164, 0.6080597 ,\n",
      "       0.54507463, 0.64208955, 0.64865672, 0.67850746, 0.63910448,\n",
      "       0.65522388, 0.65492537, 0.67820896, 0.6641791 , 0.66567164,\n",
      "       0.67164179, 0.67492537, 0.66746269, 0.70865672, 0.65701493,\n",
      "       0.6919403 , 0.67492537, 0.67164179, 0.66119403, 0.60656716,\n",
      "       0.70179104, 0.68179104, 0.66686567, 0.61820896, 0.69492537,\n",
      "       0.70507463, 0.69104478, 0.66507463, 0.73044776, 0.65850746,\n",
      "       0.70597015, 0.73761194, 0.71731343, 0.74597015, 0.71104478,\n",
      "       0.68895522, 0.73761194, 0.7038806 , 0.7561194 , 0.73641791,\n",
      "       0.71014925, 0.68029851, 0.73761194, 0.72119403, 0.78      ]), 'val_losses': array([25.11631708, 21.74068657, 10.82660192,  5.82321718,  2.19874184,\n",
      "        2.36318587,  1.98856252,  1.47087056,  1.2881445 ,  1.15254328,\n",
      "        1.26325551,  1.18314704,  1.02060002,  1.01779772,  1.17653124,\n",
      "        1.40428556,  0.98974834,  1.06659375,  0.88038897,  0.93113626,\n",
      "        0.97102859,  0.94698139,  0.78851524,  0.94969137,  0.87429064,\n",
      "        0.79103315,  0.87172342,  0.88408656,  0.7596917 ,  0.91868642,\n",
      "        0.70805457,  0.76270493,  0.88982052,  0.88695059,  0.83699594,\n",
      "        0.74228915,  0.78300549,  0.80283078,  0.77814083,  0.655732  ,\n",
      "        0.73416035,  0.67535538,  0.75562109,  0.65687144,  0.77744332,\n",
      "        0.6229983 ,  0.5809017 ,  0.62956485,  0.59554271,  0.63386102,\n",
      "        0.75472995,  0.57912194,  0.67381741,  0.56966202,  0.58466609,\n",
      "        0.66164073,  0.79512195,  0.61336311,  0.60099431,  0.52205014])}, 'ATTACH::2::best_val_loss': 0.5220501378045154, 'ATTACH::3::history': {'train_accs': array([0.37211732, 0.4210762 , 0.45413837, 0.50041048, 0.52272558,\n",
      "       0.56907232, 0.58907381, 0.60564221, 0.64773491, 0.67781178,\n",
      "       0.71281439, 0.75169789, 0.76834092, 0.79513397, 0.81274722,\n",
      "       0.83095753, 0.830659  , 0.84357042, 0.85931786, 0.86513919,\n",
      "       0.87611016, 0.89006642, 0.89976864, 0.90133592, 0.90909769,\n",
      "       0.90999328, 0.91835212, 0.92633779, 0.92260616, 0.92626315,\n",
      "       0.93290544, 0.93686096, 0.94529442, 0.94193597, 0.9445481 ,\n",
      "       0.95350399, 0.95290693, 0.95223524, 0.95678782, 0.96059407,\n",
      "       0.95977312, 0.96141503, 0.96492276, 0.96462423, 0.96798269,\n",
      "       0.96723636, 0.96977386, 0.96828122, 0.97149041, 0.97134114,\n",
      "       0.97410254]), 'train_losses': array([41.55028891,  8.36657344,  4.49514153,  2.78419472,  2.03429655,\n",
      "        1.50787467,  1.29703277,  1.18891213,  1.03138936,  0.92974389,\n",
      "        0.84554626,  0.73558802,  0.67544515,  0.60329553,  0.54311983,\n",
      "        0.4970249 ,  0.50484431,  0.46121501,  0.42885942,  0.39565029,\n",
      "        0.37562383,  0.3344353 ,  0.30409107,  0.30281325,  0.27926531,\n",
      "        0.27554712,  0.25916899,  0.22528044,  0.23577472,  0.22436227,\n",
      "        0.201304  ,  0.19742814,  0.16450752,  0.17157729,  0.17058145,\n",
      "        0.14208238,  0.15006431,  0.14416736,  0.13141729,  0.1275015 ,\n",
      "        0.12420873,  0.12358054,  0.10354714,  0.11139448,  0.09802793,\n",
      "        0.10211752,  0.09686842,  0.0961851 ,  0.09215337,  0.09021309,\n",
      "        0.08194264]), 'val_accs': array([0.43761194, 0.46776119, 0.58149254, 0.6238806 , 0.59940299,\n",
      "       0.62179104, 0.69820896, 0.64686567, 0.77253731, 0.78955224,\n",
      "       0.79552239, 0.81074627, 0.84119403, 0.86      , 0.85164179,\n",
      "       0.87522388, 0.83402985, 0.89373134, 0.90567164, 0.88716418,\n",
      "       0.91014925, 0.91074627, 0.94298507, 0.93253731, 0.89850746,\n",
      "       0.93402985, 0.90597015, 0.93164179, 0.9361194 , 0.95134328,\n",
      "       0.94895522, 0.92686567, 0.96029851, 0.93910448, 0.96089552,\n",
      "       0.96238806, 0.95970149, 0.9638806 , 0.94447761, 0.96238806,\n",
      "       0.97671642, 0.96686567, 0.93940299, 0.97014925, 0.96895522,\n",
      "       0.9641791 , 0.97104478, 0.9519403 , 0.96537313, 0.9641791 ,\n",
      "       0.97432836]), 'val_losses': array([5.09153889, 2.47671284, 1.63168118, 0.99656442, 1.25405476,\n",
      "       1.01383499, 0.72663215, 0.93943766, 0.59724767, 0.52699657,\n",
      "       0.56632499, 0.51240932, 0.50813338, 0.40619582, 0.4562622 ,\n",
      "       0.3534145 , 0.50873657, 0.2996704 , 0.27482292, 0.33320986,\n",
      "       0.24884447, 0.25862313, 0.20121832, 0.20140674, 0.32274757,\n",
      "       0.19497408, 0.27537728, 0.20790476, 0.17741842, 0.14883466,\n",
      "       0.15554571, 0.20971616, 0.12931834, 0.16617448, 0.11874461,\n",
      "       0.10973766, 0.11947276, 0.11696758, 0.18048644, 0.12044147,\n",
      "       0.07532   , 0.1017146 , 0.18697731, 0.09056296, 0.10554201,\n",
      "       0.11788892, 0.08688175, 0.16079029, 0.11044947, 0.1168767 ,\n",
      "       0.09154253])}, 'ATTACH::3::best_val_loss': 0.07532000278081022, 'ATTACH::4::history': {'train_accs': array([0.39032764, 0.43734607, 0.48914098, 0.54033883, 0.5984775 ,\n",
      "       0.65930293, 0.68878274, 0.72497948, 0.77080379, 0.8007314 ,\n",
      "       0.83573401, 0.87148295, 0.89514143, 0.91200836, 0.92708411,\n",
      "       0.94589148, 0.9526084 , 0.96081797, 0.96208672, 0.96999776,\n",
      "       0.97313232, 0.97283379, 0.9749235 , 0.97992387, 0.98029704,\n",
      "       0.98261064, 0.97619225, 0.97783417, 0.98671543, 0.98492425,\n",
      "       0.98835734, 0.98522278, 0.98440182, 0.98865587, 0.9919397 ,\n",
      "       0.98634226]), 'train_losses': array([18.70305693,  5.13317333,  3.15142571,  2.31151291,  1.66202232,\n",
      "        1.25778542,  1.06075428,  0.86615661,  0.70441754,  0.59222832,\n",
      "        0.4788696 ,  0.37027512,  0.30334356,  0.25719418,  0.21512591,\n",
      "        0.15972743,  0.14343791,  0.12268128,  0.1184108 ,  0.09120351,\n",
      "        0.0831428 ,  0.08306321,  0.07846098,  0.06300349,  0.05896648,\n",
      "        0.05318524,  0.07713877,  0.06716542,  0.04410299,  0.0478187 ,\n",
      "        0.03657784,  0.04446863,  0.05013249,  0.0341155 ,  0.0243676 ,\n",
      "        0.04027583]), 'val_accs': array([0.52656716, 0.52      , 0.65343284, 0.6238806 , 0.60149254,\n",
      "       0.75223881, 0.75432836, 0.7841791 , 0.78746269, 0.82059701,\n",
      "       0.89462687, 0.9038806 , 0.92686567, 0.93164179, 0.93641791,\n",
      "       0.95223881, 0.94716418, 0.93970149, 0.9558209 , 0.9441791 ,\n",
      "       0.9761194 , 0.95970149, 0.95402985, 0.97402985, 0.97343284,\n",
      "       0.97641791, 0.96059701, 0.97373134, 0.97671642, 0.96716418,\n",
      "       0.97313433, 0.96865672, 0.96985075, 0.96716418, 0.9758209 ,\n",
      "       0.97104478]), 'val_losses': array([1.90628804, 2.05065502, 1.31877742, 1.30228555, 1.50031332,\n",
      "       0.83324069, 0.68400362, 0.65189955, 0.718777  , 0.51702   ,\n",
      "       0.3291952 , 0.29997716, 0.2396199 , 0.21441229, 0.19043919,\n",
      "       0.15031521, 0.17860572, 0.19378957, 0.1284893 , 0.16456995,\n",
      "       0.08250596, 0.13020937, 0.16262566, 0.08622408, 0.08797847,\n",
      "       0.08127253, 0.12350026, 0.08858124, 0.08658489, 0.10830899,\n",
      "       0.0934168 , 0.1215534 , 0.10319563, 0.12467293, 0.1143619 ,\n",
      "       0.10227329])}, 'ATTACH::4::best_val_loss': 0.08127252928891789, 'ATTACH::5::history': {'train_accs': array([0.36226584, 0.38084932, 0.39189492, 0.41032913, 0.41794164,\n",
      "       0.44309277, 0.47876707, 0.50048511, 0.52018807, 0.54899619,\n",
      "       0.58265542, 0.60295544, 0.64027166, 0.664975  , 0.68639451,\n",
      "       0.71550116, 0.73356221, 0.75319054, 0.78102843, 0.79864169,\n",
      "       0.82670349, 0.83140533, 0.84782446, 0.86372117, 0.86730353,\n",
      "       0.88596164, 0.9005896 , 0.90656019, 0.91857601, 0.93044257,\n",
      "       0.92641242, 0.93865214, 0.93872677, 0.93812971, 0.944772  ,\n",
      "       0.95126502, 0.95178745, 0.95402642, 0.95425032, 0.96014628,\n",
      "       0.96589298, 0.96298231, 0.96619151, 0.96574371, 0.9668632 ,\n",
      "       0.96604224, 0.97544593, 0.97305769, 0.97163967, 0.97037092,\n",
      "       0.97865512, 0.97671468, 0.97947608, 0.98059557, 0.97880439,\n",
      "       0.98081946, 0.97932682, 0.98208822, 0.98380476, 0.97969998]), 'train_losses': array([5.01485384e+01, 1.26541462e+01, 8.12636282e+00, 6.08252017e+00,\n",
      "       4.79250379e+00, 3.82520348e+00, 3.06579687e+00, 2.56584624e+00,\n",
      "       2.21439359e+00, 1.86066683e+00, 1.62122515e+00, 1.45827107e+00,\n",
      "       1.27236698e+00, 1.14252025e+00, 1.03743862e+00, 9.03533424e-01,\n",
      "       8.48989793e-01, 7.63000766e-01, 6.58034594e-01, 6.09353387e-01,\n",
      "       5.29231525e-01, 4.94272733e-01, 4.39376667e-01, 3.93097562e-01,\n",
      "       4.05275311e-01, 3.25412100e-01, 2.94723335e-01, 2.69656619e-01,\n",
      "       2.37990479e-01, 2.17722467e-01, 2.21032578e-01, 1.84917280e-01,\n",
      "       1.83843181e-01, 1.78185790e-01, 1.65502872e-01, 1.49590382e-01,\n",
      "       1.41504326e-01, 1.38313359e-01, 1.31362417e-01, 1.17789014e-01,\n",
      "       1.05720421e-01, 1.12945567e-01, 1.03264725e-01, 1.01684950e-01,\n",
      "       9.95969506e-02, 9.89373880e-02, 7.67425156e-02, 7.80672367e-02,\n",
      "       8.13653000e-02, 8.89842261e-02, 6.32805870e-02, 6.46488265e-02,\n",
      "       6.12494524e-02, 6.06355486e-02, 6.47645032e-02, 5.90160859e-02,\n",
      "       6.01656383e-02, 5.38466217e-02, 4.87356677e-02, 5.78892814e-02]), 'val_accs': array([0.37970149, 0.40686567, 0.46746269, 0.48447761, 0.53223881,\n",
      "       0.54149254, 0.50119403, 0.59761194, 0.61641791, 0.63731343,\n",
      "       0.56477612, 0.60656716, 0.7       , 0.74298507, 0.76656716,\n",
      "       0.76686567, 0.8038806 , 0.82268657, 0.81731343, 0.83731343,\n",
      "       0.82567164, 0.84835821, 0.83044776, 0.87970149, 0.89552239,\n",
      "       0.92059701, 0.92835821, 0.94029851, 0.92358209, 0.93880597,\n",
      "       0.94716418, 0.94208955, 0.92895522, 0.93432836, 0.95462687,\n",
      "       0.94537313, 0.93044776, 0.95462687, 0.96925373, 0.96179104,\n",
      "       0.96656716, 0.95880597, 0.96567164, 0.96925373, 0.9680597 ,\n",
      "       0.96029851, 0.96895522, 0.96626866, 0.9680597 , 0.97552239,\n",
      "       0.97522388, 0.96477612, 0.97701493, 0.96537313, 0.97970149,\n",
      "       0.97373134, 0.97432836, 0.96776119, 0.97462687, 0.96447761]), 'val_losses': array([7.64358018, 2.43079346, 1.76532331, 2.93138676, 2.61066552,\n",
      "       1.56014504, 1.7910272 , 1.33962916, 1.0372664 , 0.96810372,\n",
      "       1.53932164, 1.15721538, 0.90084436, 0.83622477, 0.79505744,\n",
      "       0.71836258, 0.603927  , 0.54306272, 0.59170814, 0.43863447,\n",
      "       0.50541547, 0.41427998, 0.4595873 , 0.31920452, 0.31700177,\n",
      "       0.23461046, 0.23050071, 0.18913125, 0.23802574, 0.1852458 ,\n",
      "       0.16319474, 0.18768267, 0.21147341, 0.21034082, 0.14107879,\n",
      "       0.16503275, 0.19748262, 0.13947395, 0.10607237, 0.12624019,\n",
      "       0.10287955, 0.13070296, 0.10634409, 0.10645712, 0.10302145,\n",
      "       0.13483494, 0.09827231, 0.10066916, 0.1088776 , 0.0826629 ,\n",
      "       0.0818614 , 0.12398659, 0.07901506, 0.10547518, 0.07224861,\n",
      "       0.09329568, 0.07781693, 0.11794639, 0.09579388, 0.11430275])}, 'ATTACH::5::best_val_loss': 0.07224860882025157, 'ATTACH::6::history': {'train_accs': array([0.43645048, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44160012,\n",
      "       0.4571983 , 0.47518472, 0.48540936, 0.4901112 , 0.49779834,\n",
      "       0.50250019, 0.51115755, 0.51556086, 0.52951713, 0.53384581,\n",
      "       0.54630943, 0.56392268, 0.57974476, 0.59444735, 0.60370177,\n",
      "       0.60937383, 0.62527054, 0.63848048, 0.64251064, 0.66049705,\n",
      "       0.66191507, 0.67714009, 0.68393164, 0.69370849, 0.71520263,\n",
      "       0.71654601, 0.7287111 , 0.75625047, 0.77132622, 0.78438689,\n",
      "       0.79789537, 0.80640346, 0.81416524, 0.82200164, 0.82334503,\n",
      "       0.83461452, 0.84133144, 0.84916785, 0.853198  , 0.85991492]), 'train_losses': array([2.94339609, 1.24420971, 1.23877174, 1.23813148, 1.23796647,\n",
      "       1.23792437, 1.23832047, 1.23744761, 1.23740694, 1.23824121,\n",
      "       1.23773665, 1.23716183, 1.23710076, 1.23638232, 1.23529957,\n",
      "       1.23499553, 1.23368448, 1.23044543, 1.22518197, 1.21240442,\n",
      "       1.18905104, 1.16812121, 1.15299453, 1.1322555 , 1.12317897,\n",
      "       1.10959146, 1.09278883, 1.08387325, 1.06232833, 1.04337678,\n",
      "       1.01472362, 0.98020567, 0.94662596, 0.92380009, 0.90212845,\n",
      "       0.88466332, 0.86034976, 0.83859251, 0.83039241, 0.802554  ,\n",
      "       0.80449061, 0.77359785, 0.75095813, 0.73430423, 0.70618536,\n",
      "       0.6951567 , 0.67955802, 0.63444378, 0.59535445, 0.5597656 ,\n",
      "       0.53316763, 0.50806693, 0.48513271, 0.46990297, 0.45945049,\n",
      "       0.43017911, 0.41014059, 0.39529817, 0.37646114, 0.3676939 ]), 'val_accs': array([0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.46477612,\n",
      "       0.46865672, 0.45850746, 0.46955224, 0.47970149, 0.46597015,\n",
      "       0.51223881, 0.45970149, 0.53044776, 0.52776119, 0.55820896,\n",
      "       0.5119403 , 0.57761194, 0.59552239, 0.56567164, 0.61940299,\n",
      "       0.61283582, 0.63462687, 0.67164179, 0.58925373, 0.68149254,\n",
      "       0.70238806, 0.6641791 , 0.68567164, 0.71283582, 0.70149254,\n",
      "       0.7080597 , 0.75462687, 0.76358209, 0.7919403 , 0.77104478,\n",
      "       0.8161194 , 0.80507463, 0.8280597 , 0.81970149, 0.84298507,\n",
      "       0.84328358, 0.82089552, 0.85970149, 0.86089552, 0.87044776]), 'val_losses': array([1.24982143, 1.23900979, 1.23716031, 1.23704738, 1.2371555 ,\n",
      "       1.23720455, 1.23689241, 1.23702239, 1.23609886, 1.235855  ,\n",
      "       1.23583671, 1.23519846, 1.23489815, 1.23419378, 1.23330631,\n",
      "       1.23244987, 1.23110219, 1.22709114, 1.21826175, 1.21455152,\n",
      "       1.18724597, 1.16942674, 1.15281115, 1.14500776, 1.13002972,\n",
      "       1.08457167, 1.12940888, 1.04958636, 1.03759399, 0.99427333,\n",
      "       1.03193552, 0.96230956, 0.90094227, 0.94888475, 0.89435026,\n",
      "       0.91130891, 0.83333635, 0.78806874, 0.95344535, 0.76927221,\n",
      "       0.73305828, 0.81167599, 0.7580359 , 0.72396648, 0.7250567 ,\n",
      "       0.71698235, 0.66456261, 0.63300912, 0.52752847, 0.62555058,\n",
      "       0.48607293, 0.52273106, 0.4526541 , 0.51412896, 0.41759371,\n",
      "       0.41329773, 0.50951393, 0.38367439, 0.36627327, 0.35038728])}, 'ATTACH::6::best_val_loss': 0.3503872836436798, 'ATTACH::7::history': {'train_accs': array([0.36957982, 0.40137324, 0.40883648, 0.43749534, 0.45943727,\n",
      "       0.48473767, 0.50302261, 0.52503918, 0.53944324, 0.56347489,\n",
      "       0.57608777, 0.59608926, 0.60989626, 0.62549444, 0.64086872,\n",
      "       0.66236286, 0.66848272, 0.69221584, 0.69594746, 0.71370998,\n",
      "       0.7342339 , 0.7425181 , 0.75438466, 0.76826629, 0.77625196,\n",
      "       0.79408911, 0.80334353, 0.81207553, 0.82043436, 0.82752444,\n",
      "       0.83648033, 0.85812374, 0.85842227, 0.86192999, 0.87872229,\n",
      "       0.88305097, 0.89006642, 0.89178297, 0.90447048, 0.90223151,\n",
      "       0.90984402, 0.91409807, 0.92559146, 0.92902455, 0.92760654,\n",
      "       0.93178595, 0.94089111, 0.94290619, 0.94462273, 0.95081723,\n",
      "       0.94507053]), 'train_losses': array([25.84331208, 11.54148207,  8.14701441,  6.0109264 ,  4.59748076,\n",
      "        3.82110981,  3.17736299,  2.7857648 ,  2.51793585,  2.22612901,\n",
      "        2.05960044,  1.83839793,  1.70654131,  1.57039184,  1.45367707,\n",
      "        1.31358509,  1.27106713,  1.12121604,  1.09672553,  0.99674227,\n",
      "        0.94956892,  0.90114492,  0.84811805,  0.81239575,  0.79615391,\n",
      "        0.69820713,  0.6638607 ,  0.6255709 ,  0.59229994,  0.56240261,\n",
      "        0.53827682,  0.46228582,  0.47024199,  0.45831501,  0.39972173,\n",
      "        0.38266042,  0.35085479,  0.36630276,  0.30251746,  0.31077583,\n",
      "        0.28641546,  0.28040334,  0.24726021,  0.23168292,  0.23401697,\n",
      "        0.21584624,  0.19507065,  0.19280151,  0.18722579,  0.15784267,\n",
      "        0.18990514]), 'val_accs': array([0.48179104, 0.47791045, 0.48626866, 0.57074627, 0.55492537,\n",
      "       0.56656716, 0.64597015, 0.61522388, 0.67223881, 0.60716418,\n",
      "       0.66447761, 0.62149254, 0.64985075, 0.71164179, 0.68149254,\n",
      "       0.68208955, 0.71671642, 0.74298507, 0.66089552, 0.78179104,\n",
      "       0.54955224, 0.69940299, 0.69731343, 0.72059701, 0.81492537,\n",
      "       0.69761194, 0.88537313, 0.89552239, 0.85432836, 0.87641791,\n",
      "       0.87343284, 0.8438806 , 0.87432836, 0.89850746, 0.9       ,\n",
      "       0.8680597 , 0.91164179, 0.93791045, 0.89940299, 0.86776119,\n",
      "       0.92776119, 0.92626866, 0.92776119, 0.92835821, 0.89522388,\n",
      "       0.93432836, 0.91104478, 0.93283582, 0.93402985, 0.85343284,\n",
      "       0.91522388]), 'val_losses': array([5.45733528, 3.58920219, 2.77797074, 1.67541708, 1.92527203,\n",
      "       1.33537149, 1.36626501, 1.89430173, 0.85057327, 1.1795598 ,\n",
      "       1.12636744, 1.05477718, 1.06270118, 0.7206151 , 0.99827984,\n",
      "       0.87541747, 0.82470797, 0.69455074, 1.09184971, 0.67067511,\n",
      "       2.12903244, 1.08725666, 1.18116956, 1.03626988, 0.52685476,\n",
      "       1.35075008, 0.31999108, 0.31375744, 0.44627744, 0.35096953,\n",
      "       0.3687658 , 0.47568393, 0.36879987, 0.32142844, 0.29574785,\n",
      "       0.49498406, 0.26617949, 0.20982395, 0.29737967, 0.42911426,\n",
      "       0.20158312, 0.23388099, 0.2429851 , 0.24618194, 0.36379993,\n",
      "       0.23084594, 0.32596331, 0.21965761, 0.2249733 , 0.6012029 ,\n",
      "       0.31719235])}, 'ATTACH::7::best_val_loss': 0.20158311704646295, 'ATTACH::8::history': {'train_accs': array([0.28405105, 0.33853273, 0.36457945, 0.39017837, 0.41368759,\n",
      "       0.43286812, 0.43637585, 0.4463766 , 0.45682514, 0.46122845,\n",
      "       0.4735428 , 0.48272259, 0.48906635, 0.49667886, 0.51593402,\n",
      "       0.52593477, 0.53698037, 0.54175685, 0.56071349, 0.56683335,\n",
      "       0.58086424, 0.58123741, 0.60362714, 0.6063139 , 0.61624002,\n",
      "       0.6233301 , 0.62385253, 0.63706247, 0.64504814, 0.65825808,\n",
      "       0.66721397, 0.66646765, 0.67452795, 0.68669304, 0.69288753,\n",
      "       0.70296291, 0.7089335 , 0.71124711, 0.7261736 , 0.73415927,\n",
      "       0.7397567 , 0.75072767, 0.75139936, 0.76565415, 0.77020673,\n",
      "       0.77393835, 0.78028211, 0.7869244 , 0.79020822]), 'train_losses': array([61.25470808, 27.1047451 , 18.26275698, 13.75057647, 10.75806645,\n",
      "        8.71213319,  7.30034588,  6.11801241,  5.27962993,  4.6022638 ,\n",
      "        4.03817698,  3.61442551,  3.16756883,  2.78680958,  2.54408726,\n",
      "        2.26418365,  2.06117436,  1.89689913,  1.72601786,  1.59096673,\n",
      "        1.47725325,  1.38079467,  1.28477888,  1.21065605,  1.15035585,\n",
      "        1.11007929,  1.06691765,  1.00873525,  0.97656438,  0.94960514,\n",
      "        0.90306598,  0.89552527,  0.8599604 ,  0.82823422,  0.80242498,\n",
      "        0.77935148,  0.75882879,  0.75562512,  0.71440029,  0.7051091 ,\n",
      "        0.68240691,  0.66130284,  0.66065302,  0.6298855 ,  0.63041661,\n",
      "        0.5991146 ,  0.59305875,  0.58099208,  0.57616534]), 'val_accs': array([0.41940299, 0.36029851, 0.41820896, 0.48208955, 0.46985075,\n",
      "       0.52895522, 0.55104478, 0.55880597, 0.59402985, 0.51701493,\n",
      "       0.55761194, 0.5119403 , 0.62835821, 0.62716418, 0.6161194 ,\n",
      "       0.57104478, 0.64985075, 0.56955224, 0.54925373, 0.69761194,\n",
      "       0.63134328, 0.56835821, 0.70149254, 0.6041791 , 0.64776119,\n",
      "       0.6438806 , 0.65044776, 0.71791045, 0.67044776, 0.66298507,\n",
      "       0.72      , 0.68537313, 0.65970149, 0.65761194, 0.6158209 ,\n",
      "       0.6958209 , 0.68208955, 0.65761194, 0.7761194 , 0.65223881,\n",
      "       0.63164179, 0.69134328, 0.70149254, 0.77134328, 0.69164179,\n",
      "       0.66      , 0.67104478, 0.69970149, 0.68597015]), 'val_losses': array([19.99513312,  6.50607014,  5.04338458,  3.09466541,  4.75523096,\n",
      "        3.57516   ,  2.90655924,  2.02369703,  1.93407371,  1.54166621,\n",
      "        1.35927647,  2.53406222,  0.97472091,  0.96295262,  1.30748046,\n",
      "        1.64474044,  0.98813029,  1.1403762 ,  1.58682503,  0.811237  ,\n",
      "        1.04745504,  1.57118737,  0.76659344,  1.00590985,  1.09804627,\n",
      "        0.9167172 ,  0.85927293,  0.80177176,  0.73156238,  0.83332061,\n",
      "        0.75090743,  0.90911109,  0.78591167,  0.89159695,  0.93586719,\n",
      "        0.66459696,  0.92540774,  0.70217162,  0.56627904,  0.73546534,\n",
      "        0.82125772,  0.93312197,  0.80024448,  0.63035607,  0.92182081,\n",
      "        1.01481576,  0.79572706,  0.82604921,  0.71181666])}, 'ATTACH::8::best_val_loss': 0.5662790411265929, 'ATTACH::9::history': {'train_accs': array([0.33927905, 0.38876036, 0.39905963, 0.42092693, 0.43286812,\n",
      "       0.4516755 , 0.46130308, 0.48331965, 0.48309575, 0.50033585,\n",
      "       0.49809687, 0.5041421 , 0.51362042, 0.52541234, 0.5345175 ,\n",
      "       0.53936861, 0.54705575, 0.55183223, 0.55586238, 0.57071423,\n",
      "       0.57974476, 0.57952086, 0.58481976, 0.58959624, 0.60497052,\n",
      "       0.60900067, 0.61444884, 0.62385253, 0.62668856, 0.62773341,\n",
      "       0.62959922, 0.6426599 , 0.65564594, 0.65758639, 0.66990074,\n",
      "       0.67661766, 0.69117098, 0.70378386, 0.71430704, 0.72236734,\n",
      "       0.73542802, 0.7420703 , 0.75848944, 0.76722143, 0.77602806,\n",
      "       0.79438764, 0.80364206, 0.81565788, 0.82991268, 0.8368535 ,\n",
      "       0.85469065, 0.86140757, 0.87208001, 0.88275244, 0.89215613,\n",
      "       0.89932084, 0.90753041, 0.91275468, 0.91857601, 0.92551683]), 'train_losses': array([38.35857212, 14.40062735,  9.51933644,  7.07432955,  5.3098744 ,\n",
      "        4.29449145,  3.36806931,  2.88473938,  2.53709817,  2.27786982,\n",
      "        2.06220359,  1.9094102 ,  1.76726966,  1.62265115,  1.52446148,\n",
      "        1.44340627,  1.38901377,  1.28921117,  1.27314321,  1.20581896,\n",
      "        1.11832944,  1.12639236,  1.08290258,  1.07129003,  1.01775742,\n",
      "        0.99736879,  0.97640012,  0.95516785,  0.93760899,  0.91738392,\n",
      "        0.90369011,  0.87881967,  0.85110853,  0.83666779,  0.81350835,\n",
      "        0.79250675,  0.77819235,  0.74302722,  0.72199536,  0.69737261,\n",
      "        0.6844049 ,  0.65000667,  0.6197786 ,  0.60072689,  0.56773777,\n",
      "        0.53582471,  0.50677426,  0.47315735,  0.44385262,  0.42252104,\n",
      "        0.38875323,  0.37630156,  0.35082573,  0.31886591,  0.30781606,\n",
      "        0.28147569,  0.26239028,  0.24214476,  0.22486106,  0.20393077]), 'val_accs': array([0.39402985, 0.46925373, 0.46      , 0.50925373, 0.41462687,\n",
      "       0.49701493, 0.5280597 , 0.55641791, 0.50597015, 0.46089552,\n",
      "       0.52477612, 0.45223881, 0.54985075, 0.54507463, 0.57791045,\n",
      "       0.68      , 0.55641791, 0.4958209 , 0.54895522, 0.61432836,\n",
      "       0.64268657, 0.61761194, 0.58686567, 0.52358209, 0.61253731,\n",
      "       0.61104478, 0.64328358, 0.65134328, 0.50716418, 0.56626866,\n",
      "       0.76447761, 0.55940299, 0.62298507, 0.78029851, 0.77044776,\n",
      "       0.63074627, 0.71104478, 0.6119403 , 0.62716418, 0.77104478,\n",
      "       0.74746269, 0.61492537, 0.74895522, 0.77731343, 0.80298507,\n",
      "       0.72507463, 0.84029851, 0.86686567, 0.68925373, 0.8280597 ,\n",
      "       0.86865672, 0.85044776, 0.89910448, 0.7241791 , 0.85731343,\n",
      "       0.81343284, 0.93462687, 0.93014925, 0.93910448, 0.93134328]), 'val_losses': array([7.14110909, 4.78411883, 4.23703026, 2.08020534, 3.17450414,\n",
      "       1.92601714, 1.20259033, 1.08043177, 1.71449948, 1.51240119,\n",
      "       1.05829744, 1.51546913, 1.13855951, 1.0110428 , 0.87125042,\n",
      "       0.71336622, 0.92009884, 1.32557168, 0.89592711, 0.83406463,\n",
      "       0.69082855, 0.9095847 , 0.74832343, 0.79536883, 0.75410582,\n",
      "       0.75175372, 0.86868681, 0.73058166, 0.95675251, 0.94927172,\n",
      "       0.63947569, 1.59223592, 0.85799677, 0.58887104, 0.58990962,\n",
      "       0.74491697, 0.62571588, 0.97158905, 0.82322488, 0.54558835,\n",
      "       0.59679312, 0.92888931, 0.56208337, 0.51281833, 0.47577169,\n",
      "       0.59197387, 0.40217152, 0.34398617, 0.92427213, 0.46599897,\n",
      "       0.32480706, 0.37072305, 0.27927552, 0.83747761, 0.34337722,\n",
      "       0.49254681, 0.20041148, 0.2021677 , 0.17636333, 0.19196489])}, 'ATTACH::9::best_val_loss': 0.1763633322004062, 'ATTACH::10::history': {'train_accs': array([0.38353608, 0.44413762, 0.48496156, 0.51354579, 0.53735353,\n",
      "       0.56175834, 0.62683782, 0.69035003, 0.74610046, 0.78976043,\n",
      "       0.82991268, 0.86685574, 0.89096201, 0.9056646 , 0.9167102 ,\n",
      "       0.93163669, 0.9394731 , 0.94701097, 0.94969774, 0.95939996,\n",
      "       0.95686245, 0.96425106, 0.96850511, 0.97178894, 0.96880364,\n",
      "       0.97484887, 0.97246063, 0.97917755, 0.9806702 , 0.97604299,\n",
      "       0.98380476, 0.9804463 , 0.98514815, 0.98477498, 0.98544668,\n",
      "       0.98522278, 0.98611837, 0.98589447, 0.98940219, 0.98440182,\n",
      "       0.9885066 , 0.98940219, 0.98835734, 0.99059631, 0.9885066 ,\n",
      "       0.9889544 , 0.98910366, 0.99410404, 0.98865587, 0.99029778,\n",
      "       0.99223823, 0.98977536, 0.99164117, 0.99268602, 0.98962609,\n",
      "       0.98932756, 0.9889544 , 0.99231286, 0.99156653, 0.99320845]), 'train_losses': array([14.9519341 ,  3.79783375,  2.14995375,  1.56066992,  1.25199107,\n",
      "        1.08906749,  0.88485343,  0.74901756,  0.60768208,  0.51463669,\n",
      "        0.43043434,  0.34433076,  0.29433117,  0.25609551,  0.22068672,\n",
      "        0.18333802,  0.16680745,  0.14704011,  0.14824127,  0.11810057,\n",
      "        0.12727746,  0.09908   ,  0.09124441,  0.08897114,  0.08819664,\n",
      "        0.07449365,  0.08207407,  0.06301042,  0.05440254,  0.06830914,\n",
      "        0.05321979,  0.05641201,  0.04671069,  0.0454402 ,  0.04355486,\n",
      "        0.0450285 ,  0.039976  ,  0.04078265,  0.03391863,  0.04854423,\n",
      "        0.034649  ,  0.03397343,  0.03296972,  0.03042568,  0.03300944,\n",
      "        0.03332884,  0.03318422,  0.01778189,  0.03299168,  0.03060056,\n",
      "        0.02221503,  0.03204642,  0.02534973,  0.02422503,  0.03162204,\n",
      "        0.02914966,  0.03474719,  0.02443777,  0.02387782,  0.02080019]), 'val_accs': array([0.51343284, 0.53522388, 0.52507463, 0.60507463, 0.56925373,\n",
      "       0.62567164, 0.70537313, 0.80835821, 0.78      , 0.8719403 ,\n",
      "       0.87313433, 0.89940299, 0.91940299, 0.93880597, 0.93761194,\n",
      "       0.94835821, 0.94268657, 0.96      , 0.96208955, 0.94776119,\n",
      "       0.95492537, 0.97164179, 0.96537313, 0.97074627, 0.97134328,\n",
      "       0.96268657, 0.97074627, 0.96507463, 0.98      , 0.96686567,\n",
      "       0.97522388, 0.97641791, 0.95313433, 0.97432836, 0.98477612,\n",
      "       0.97910448, 0.98179104, 0.9880597 , 0.9841791 , 0.98298507,\n",
      "       0.97970149, 0.98      , 0.98597015, 0.98477612, 0.98776119,\n",
      "       0.97701493, 0.9841791 , 0.98507463, 0.98059701, 0.9880597 ,\n",
      "       0.98238806, 0.98238806, 0.97313433, 0.98477612, 0.9680597 ,\n",
      "       0.9841791 , 0.98089552, 0.97820896, 0.98268657, 0.9680597 ]), 'val_losses': array([2.73682232, 1.50057368, 1.44335252, 0.94596258, 0.93554757,\n",
      "       0.69232985, 0.59549387, 0.46889245, 0.53895724, 0.34776552,\n",
      "       0.31694309, 0.26487641, 0.20253575, 0.17137436, 0.1819185 ,\n",
      "       0.14259379, 0.1597392 , 0.11279212, 0.11058288, 0.14242045,\n",
      "       0.13147275, 0.08552588, 0.10212452, 0.09057611, 0.07826989,\n",
      "       0.12349213, 0.08614778, 0.10189676, 0.06434078, 0.09400674,\n",
      "       0.07696796, 0.06462825, 0.12967943, 0.08768788, 0.05293358,\n",
      "       0.0664055 , 0.05485759, 0.04537796, 0.06216119, 0.05057775,\n",
      "       0.05974646, 0.06845939, 0.04417634, 0.06246867, 0.04380853,\n",
      "       0.06787631, 0.04586343, 0.05298175, 0.06024358, 0.04324142,\n",
      "       0.05635903, 0.05493381, 0.08378885, 0.05365035, 0.10827512,\n",
      "       0.05992097, 0.05838416, 0.07109134, 0.05842232, 0.10644619])}, 'ATTACH::10::best_val_loss': 0.04324142360990283, 'ATTACH::11::history': {'train_accs': array([0.42040451, 0.43861482, 0.44189865, 0.44160012, 0.44100306,\n",
      "       0.44189865, 0.44145085, 0.44197328, 0.44167475, 0.44212255,\n",
      "       0.44219718, 0.4401821 ]), 'train_losses': array([26.57262274,  1.20749523,  1.27278705,  1.24736178,  1.26347211,\n",
      "        1.24321909,  1.25782418,  1.2428291 ,  1.25411893,  1.24331001,\n",
      "        1.24239963,  1.74183415]), 'val_accs': array([0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955]), 'val_losses': array([1.16586148, 1.16255597, 1.24212261, 1.24180492, 1.20705134,\n",
      "       1.24197905, 1.24190226, 1.24182352, 1.2421189 , 1.24245019,\n",
      "       1.24184804, 1.24230227])}, 'ATTACH::11::best_val_loss': 1.162555969295217, 'ATTACH::12::history': {'train_accs': array([0.4595119 , 0.53571162, 0.54399582, 0.55377267, 0.57758042,\n",
      "       0.56295246, 0.56526606, 0.56481827, 0.56444511, 0.5621315 ,\n",
      "       0.56989328, 0.55205612, 0.56825136, 0.57034107]), 'train_losses': array([8.13423054, 0.97704027, 0.90409037, 0.89859204, 0.83669328,\n",
      "       0.86009678, 0.85371911, 0.87287583, 0.87796819, 0.88386338,\n",
      "       0.87675821, 0.87787627, 0.8695777 , 0.89146614]), 'val_accs': array([0.5041791 , 0.48597015, 0.61164179, 0.58865672, 0.52686567,\n",
      "       0.50298507, 0.52507463, 0.62865672, 0.55880597, 0.38746269,\n",
      "       0.41432836, 0.57940299, 0.59731343, 0.52208955]), 'val_losses': array([1.09401475, 1.42358541, 0.8275141 , 0.784629  , 0.92727171,\n",
      "       1.27661872, 0.93926064, 0.9115835 , 0.86530026, 2.79805982,\n",
      "       1.03763371, 0.8188548 , 0.81528772, 0.87817105])}, 'ATTACH::12::best_val_loss': 0.7846290015462619, 'ATTACH::13::history': {'train_accs': array([0.39420852, 0.44070453, 0.46428838, 0.50824688, 0.57534144,\n",
      "       0.62534518, 0.64064482, 0.66116874, 0.67273677, 0.68087171,\n",
      "       0.70960519, 0.71841182, 0.72960669, 0.74042839, 0.75050377,\n",
      "       0.76789313, 0.77461005, 0.76774386, 0.78110307, 0.78826778,\n",
      "       0.79319352, 0.79550713, 0.80461228, 0.80423912, 0.80744832,\n",
      "       0.81909098, 0.81737443, 0.83050974, 0.8253601 , 0.82132995,\n",
      "       0.82707665, 0.82894246, 0.83274871, 0.83633107, 0.84274946,\n",
      "       0.83707739, 0.84364505, 0.84036122, 0.85200388, 0.84752593,\n",
      "       0.84648108]), 'train_losses': array([23.91478169,  6.02120804,  3.11693078,  1.64848421,  1.02370526,\n",
      "        0.86527076,  0.81505556,  0.77225361,  0.72370478,  0.70859993,\n",
      "        0.63674883,  0.62273143,  0.59935905,  0.5863847 ,  0.56747857,\n",
      "        0.53738593,  0.52150793,  0.52057597,  0.50226575,  0.49070856,\n",
      "        0.47124878,  0.48235902,  0.45741723,  0.45540837,  0.44274891,\n",
      "        0.43053327,  0.42895561,  0.40687338,  0.41385318,  0.42706305,\n",
      "        0.41524826,  0.4152721 ,  0.40222108,  0.3915782 ,  0.38951036,\n",
      "        0.40101951,  0.38138802,  0.38831855,  0.3630417 ,  0.38278545,\n",
      "        0.37442669]), 'val_accs': array([0.47134328, 0.52716418, 0.44686567, 0.64238806, 0.60597015,\n",
      "       0.68149254, 0.69731343, 0.7119403 , 0.70955224, 0.75552239,\n",
      "       0.66      , 0.79701493, 0.80626866, 0.74895522, 0.78716418,\n",
      "       0.76895522, 0.75552239, 0.79970149, 0.77731343, 0.81552239,\n",
      "       0.7361194 , 0.80925373, 0.83134328, 0.85701493, 0.73104478,\n",
      "       0.83731343, 0.79492537, 0.71820896, 0.80746269, 0.80179104,\n",
      "       0.83522388, 0.85343284, 0.74328358, 0.83373134, 0.83432836,\n",
      "       0.82089552, 0.85343284, 0.76865672, 0.83671642, 0.77970149,\n",
      "       0.84925373]), 'val_losses': array([3.54839406, 2.84942876, 1.97381265, 0.78730003, 0.8066555 ,\n",
      "       0.76656232, 0.68093417, 0.65399404, 0.58995216, 0.55424898,\n",
      "       0.81176534, 0.48592785, 0.46706444, 0.57391083, 0.49521467,\n",
      "       0.54276805, 0.5922405 , 0.48298297, 0.45803   , 0.38273527,\n",
      "       0.56556552, 0.44021952, 0.4213129 , 0.37664926, 0.54128486,\n",
      "       0.40706103, 0.40281345, 0.49944429, 0.44587992, 0.41775366,\n",
      "       0.35353366, 0.40079943, 0.54397916, 0.39806735, 0.36575075,\n",
      "       0.40881019, 0.3834092 , 0.53174392, 0.39844475, 0.44565473,\n",
      "       0.45840682])}, 'ATTACH::13::best_val_loss': 0.3535336597997751, 'ATTACH::14::history': {'train_accs': array([0.23218151, 0.33614449, 0.36137025, 0.3930144 , 0.41167251,\n",
      "       0.41868796, 0.43256959, 0.44152549, 0.44786924, 0.45518322,\n",
      "       0.47018434, 0.48242406, 0.4875737 , 0.49294723, 0.49391746,\n",
      "       0.50003732, 0.5122024 , 0.50862005, 0.51720278, 0.52615867,\n",
      "       0.53168147, 0.53257706, 0.53832376, 0.54765281, 0.54780207,\n",
      "       0.55601164, 0.55795209, 0.5538473 , 0.55780282, 0.56332562,\n",
      "       0.5676543 , 0.5786999 , 0.59235764, 0.5954922 , 0.60609001,\n",
      "       0.61601612, 0.63751026, 0.63542055, 0.6426599 , 0.65131726,\n",
      "       0.65795955, 0.67766251, 0.6843048 , 0.69057392, 0.70729159,\n",
      "       0.72184491, 0.72863647, 0.74654825, 0.76446003, 0.77341593,\n",
      "       0.79349205, 0.80177625, 0.81558325, 0.8308829 , 0.8336443 ,\n",
      "       0.8506605 , 0.85528771, 0.86409434, 0.86566162, 0.87842376]), 'train_losses': array([61.50261878, 22.84002654, 14.6261075 , 10.5590729 ,  8.09796719,\n",
      "        6.48575071,  5.30317889,  4.51907351,  3.93594481,  3.58805707,\n",
      "        3.25714167,  2.86626612,  2.66730275,  2.4353855 ,  2.33555486,\n",
      "        2.1531332 ,  2.02248725,  1.89303061,  1.78675265,  1.70988468,\n",
      "        1.61615008,  1.58869265,  1.53072977,  1.44121462,  1.39468291,\n",
      "        1.34147194,  1.30422086,  1.29034308,  1.26296432,  1.20637148,\n",
      "        1.18544279,  1.133095  ,  1.11543978,  1.0863859 ,  1.04285788,\n",
      "        1.01866996,  0.96120251,  0.93640658,  0.94177448,  0.90857846,\n",
      "        0.88357773,  0.85601933,  0.83348558,  0.80020784,  0.77414212,\n",
      "        0.73852434,  0.71445724,  0.68164285,  0.62640341,  0.61502598,\n",
      "        0.55798877,  0.53415247,  0.51638329,  0.47055953,  0.45853754,\n",
      "        0.42427901,  0.40894424,  0.38713698,  0.37218797,  0.35781612]), 'val_accs': array([0.44208955, 0.40746269, 0.47044776, 0.50447761, 0.54208955,\n",
      "       0.5119403 , 0.58507463, 0.57462687, 0.5838806 , 0.61283582,\n",
      "       0.53253731, 0.56776119, 0.56895522, 0.56477612, 0.5641791 ,\n",
      "       0.56656716, 0.54895522, 0.63492537, 0.65880597, 0.55940299,\n",
      "       0.57074627, 0.60447761, 0.56268657, 0.65343284, 0.5719403 ,\n",
      "       0.63880597, 0.63044776, 0.65104478, 0.66029851, 0.67164179,\n",
      "       0.65343284, 0.64029851, 0.63373134, 0.66537313, 0.61970149,\n",
      "       0.68507463, 0.68447761, 0.68835821, 0.73492537, 0.6880597 ,\n",
      "       0.67791045, 0.69104478, 0.73940299, 0.72119403, 0.78089552,\n",
      "       0.75313433, 0.77313433, 0.79014925, 0.7719403 , 0.84626866,\n",
      "       0.79552239, 0.84179104, 0.83223881, 0.85970149, 0.87014925,\n",
      "       0.87343284, 0.86507463, 0.85164179, 0.86328358, 0.89313433]), 'val_losses': array([15.93778053,  5.94056435,  4.54055434,  3.16579165,  2.36819872,\n",
      "        1.74411379,  1.41122305,  1.30401528,  1.39906924,  1.27259579,\n",
      "        1.64920318,  1.16920198,  1.27649809,  1.19166965,  1.16279665,\n",
      "        1.09871821,  1.10321417,  0.99393371,  0.87907549,  1.05740195,\n",
      "        0.92516438,  0.90863948,  1.06031993,  0.82053988,  1.03470149,\n",
      "        0.79963648,  0.86882554,  0.78161339,  0.75784447,  0.7162126 ,\n",
      "        0.78271341,  0.80236763,  0.75112543,  0.7033241 ,  0.883396  ,\n",
      "        0.70518186,  0.71999926,  0.71415918,  0.63798719,  0.69607702,\n",
      "        0.66315423,  0.67539108,  0.63572694,  0.6290298 ,  0.5589824 ,\n",
      "        0.61291777,  0.59631282,  0.55443829,  0.57427793,  0.42909733,\n",
      "        0.49074178,  0.43083191,  0.44100258,  0.37814923,  0.35704767,\n",
      "        0.33453407,  0.36344103,  0.37587509,  0.35504057,  0.30886702])}, 'ATTACH::14::best_val_loss': 0.3088670180804694, 'ATTACH::15::history': {'train_accs': array([0.37793865, 0.42801702, 0.44958579, 0.48339428, 0.49302187,\n",
      "       0.51720278, 0.53989104, 0.57205762, 0.60347787, 0.62362863,\n",
      "       0.65863124, 0.69355922, 0.71527726, 0.73766699, 0.7618479 ,\n",
      "       0.7867005 , 0.80386596, 0.8200612 , 0.83782372, 0.85312337,\n",
      "       0.86797522, 0.87603553, 0.88790208, 0.89842526, 0.90200761,\n",
      "       0.9141727 , 0.9139488 , 0.92193447, 0.92700948, 0.93357713,\n",
      "       0.94171207, 0.94536906, 0.944772  , 0.94962311, 0.95148892,\n",
      "       0.95492201, 0.95910143, 0.96148966, 0.96320621, 0.96559445,\n",
      "       0.96678857, 0.97149041, 0.97462497, 0.97372938, 0.97320696,\n",
      "       0.97335622, 0.97671468, 0.977461  , 0.97955071, 0.97940145,\n",
      "       0.9804463 , 0.98111799, 0.98171505, 0.98268528, 0.98253601,\n",
      "       0.98462572, 0.98141652, 0.98470035, 0.98626763, 0.98746175]), 'train_losses': array([20.02065372,  6.66952801,  4.53868219,  3.48068023,  2.78118615,\n",
      "        2.41383268,  1.99690169,  1.73995773,  1.48783447,  1.30443607,\n",
      "        1.14885968,  0.99534456,  0.87879535,  0.80414774,  0.70129442,\n",
      "        0.62921784,  0.5587869 ,  0.51691141,  0.4661231 ,  0.4154047 ,\n",
      "        0.36680753,  0.34809266,  0.32427758,  0.29121578,  0.28192698,\n",
      "        0.25310149,  0.24506192,  0.22442838,  0.20333488,  0.1947456 ,\n",
      "        0.1710673 ,  0.16276732,  0.16114353,  0.13974689,  0.14205737,\n",
      "        0.13416311,  0.12222467,  0.11291228,  0.11247816,  0.10616938,\n",
      "        0.09930316,  0.08310443,  0.07893336,  0.07658725,  0.07673841,\n",
      "        0.08021106,  0.07076527,  0.07085784,  0.06420261,  0.061638  ,\n",
      "        0.05772102,  0.05658846,  0.0533242 ,  0.0546109 ,  0.04842057,\n",
      "        0.04872921,  0.05578087,  0.04366837,  0.04219354,  0.04105934]), 'val_accs': array([0.47373134, 0.53791045, 0.54507463, 0.64298507, 0.56955224,\n",
      "       0.65940299, 0.6838806 , 0.59164179, 0.62925373, 0.62686567,\n",
      "       0.68298507, 0.5041791 , 0.79820896, 0.62835821, 0.72567164,\n",
      "       0.84746269, 0.86358209, 0.80626866, 0.89014925, 0.72567164,\n",
      "       0.59402985, 0.83731343, 0.86238806, 0.85492537, 0.93223881,\n",
      "       0.89313433, 0.90656716, 0.89134328, 0.93940299, 0.95641791,\n",
      "       0.94985075, 0.90835821, 0.88238806, 0.94746269, 0.95880597,\n",
      "       0.95880597, 0.91104478, 0.8158209 , 0.96597015, 0.95970149,\n",
      "       0.90507463, 0.9638806 , 0.92656716, 0.96447761, 0.70656716,\n",
      "       0.97074627, 0.96985075, 0.97402985, 0.94955224, 0.85970149,\n",
      "       0.97313433, 0.98089552, 0.97671642, 0.9758209 , 0.97731343,\n",
      "       0.98089552, 0.96746269, 0.95701493, 0.97880597, 0.97313433]), 'val_losses': array([3.24818744, 2.32472696, 2.40705426, 1.50186905, 1.668065  ,\n",
      "       1.12153794, 1.09085695, 1.63606183, 1.70480384, 1.16536567,\n",
      "       1.04629245, 1.95265027, 0.49954973, 1.30337891, 0.83617379,\n",
      "       0.41469198, 0.37791897, 0.55762597, 0.31028701, 0.98460676,\n",
      "       1.57842546, 0.44855072, 0.38130701, 0.39059327, 0.19777038,\n",
      "       0.33806841, 0.25867639, 0.32857166, 0.16954981, 0.12464768,\n",
      "       0.15228592, 0.28724054, 0.4330383 , 0.14817543, 0.11485465,\n",
      "       0.11581089, 0.25952817, 0.73940882, 0.10189552, 0.12297605,\n",
      "       0.32338417, 0.1229801 , 0.24214779, 0.10964201, 1.2763725 ,\n",
      "       0.08520384, 0.09039089, 0.07775308, 0.18971656, 0.60937535,\n",
      "       0.08589004, 0.05762646, 0.07732886, 0.0832803 , 0.0781121 ,\n",
      "       0.06247152, 0.11230883, 0.1449414 , 0.07419317, 0.08483253])}, 'ATTACH::15::best_val_loss': 0.05762645891742475, 'ATTACH::16::history': {'train_accs': array([0.2685275 , 0.3405478 , 0.38375998, 0.39256661, 0.40697067,\n",
      "       0.41368759, 0.42025524, 0.42518098, 0.4321218 , 0.43719681,\n",
      "       0.44242108, 0.4518994 , 0.46107919, 0.45943727, 0.46055676,\n",
      "       0.46719904, 0.46951265, 0.47384133, 0.48526009, 0.51473991,\n",
      "       0.53795059, 0.57384879, 0.60094037, 0.63153967, 0.65982536,\n",
      "       0.69870886, 0.73714456, 0.76699754, 0.80222405, 0.83468916,\n",
      "       0.86058661, 0.87319949, 0.88066274, 0.89693261, 0.90506754,\n",
      "       0.91350101, 0.92633779, 0.92036719, 0.93260691, 0.94365251,\n",
      "       0.94193597, 0.94559295, 0.95066796, 0.95566833, 0.96126577,\n",
      "       0.96186283, 0.96514665, 0.96201209, 0.96790805, 0.96865438,\n",
      "       0.97052019, 0.97007239, 0.97305769, 0.97619225, 0.97820733,\n",
      "       0.97574446, 0.97798343, 0.97589372, 0.98149116, 0.98134189]), 'train_losses': array([6.42483098e+01, 2.43650962e+01, 1.35044233e+01, 9.71607767e+00,\n",
      "       7.38313701e+00, 6.21003852e+00, 5.20616321e+00, 4.50760373e+00,\n",
      "       3.96904308e+00, 3.53243354e+00, 3.20145109e+00, 2.90269820e+00,\n",
      "       2.65776801e+00, 2.47057079e+00, 2.29117569e+00, 2.13945377e+00,\n",
      "       2.05160911e+00, 1.88994783e+00, 1.74047995e+00, 1.65815027e+00,\n",
      "       1.47035103e+00, 1.36276938e+00, 1.23980513e+00, 1.15532708e+00,\n",
      "       1.03322776e+00, 9.05137914e-01, 8.10075410e-01, 7.42504379e-01,\n",
      "       6.20870950e-01, 5.36357494e-01, 4.50656341e-01, 4.06698413e-01,\n",
      "       3.95862010e-01, 3.29169117e-01, 3.07037387e-01, 2.84188865e-01,\n",
      "       2.43398204e-01, 2.49930727e-01, 2.16609195e-01, 1.89912655e-01,\n",
      "       1.81201111e-01, 1.70348816e-01, 1.55919698e-01, 1.49451960e-01,\n",
      "       1.29151410e-01, 1.31600381e-01, 1.13359708e-01, 1.19623858e-01,\n",
      "       1.06384263e-01, 1.04576734e-01, 9.09857166e-02, 9.30928703e-02,\n",
      "       8.31338191e-02, 7.44820503e-02, 6.85132546e-02, 7.64594266e-02,\n",
      "       6.72979464e-02, 7.54888601e-02, 5.56857607e-02, 6.33280221e-02]), 'val_accs': array([0.41402985, 0.46328358, 0.42716418, 0.48059701, 0.51373134,\n",
      "       0.53970149, 0.58268657, 0.56597015, 0.53641791, 0.57164179,\n",
      "       0.49671642, 0.57731343, 0.52268657, 0.52507463, 0.51671642,\n",
      "       0.51253731, 0.53880597, 0.55731343, 0.56985075, 0.60537313,\n",
      "       0.63432836, 0.67731343, 0.69820896, 0.71462687, 0.64656716,\n",
      "       0.73701493, 0.82328358, 0.75253731, 0.8438806 , 0.89701493,\n",
      "       0.89283582, 0.90328358, 0.88059701, 0.91313433, 0.91104478,\n",
      "       0.89880597, 0.90089552, 0.9361194 , 0.92447761, 0.92328358,\n",
      "       0.93940299, 0.93940299, 0.9358209 , 0.94716418, 0.95164179,\n",
      "       0.93283582, 0.94626866, 0.9438806 , 0.93731343, 0.95044776,\n",
      "       0.94537313, 0.95641791, 0.96179104, 0.94477612, 0.9561194 ,\n",
      "       0.96208955, 0.95462687, 0.95910448, 0.94567164, 0.95791045]), 'val_losses': array([20.92641475,  8.31603322,  5.21852975,  3.03397573,  2.19177746,\n",
      "        1.94700922,  1.39073522,  1.46518586,  1.28395169,  1.29239575,\n",
      "        1.50402001,  1.23614058,  1.27033658,  1.30291738,  1.25511498,\n",
      "        1.23436723,  1.18149964,  0.92841418,  0.95594429,  0.93699212,\n",
      "        0.90425793,  0.72454624,  0.71275997,  0.66120454,  0.91666429,\n",
      "        0.6962047 ,  0.49618533,  0.60078158,  0.42153791,  0.28722163,\n",
      "        0.32060807,  0.28026804,  0.34040211,  0.23904883,  0.25258932,\n",
      "        0.27651206,  0.27942905,  0.19274044,  0.21623728,  0.22738255,\n",
      "        0.20296068,  0.1741637 ,  0.19744789,  0.15787141,  0.16076119,\n",
      "        0.22086643,  0.1670237 ,  0.18824863,  0.19274343,  0.15994701,\n",
      "        0.1873272 ,  0.14550709,  0.11700074,  0.18022158,  0.15032187,\n",
      "        0.12439052,  0.1484774 ,  0.13500365,  0.17185609,  0.13559627])}, 'ATTACH::16::best_val_loss': 0.11700073759057629, 'ATTACH::17::history': {'train_accs': array([0.3743563 , 0.43451004, 0.45861631, 0.49652959, 0.54750355,\n",
      "       0.58243152, 0.62795731, 0.66460184, 0.70169416, 0.72080006,\n",
      "       0.75259348, 0.77453541, 0.78528248, 0.81006045, 0.8251362 ,\n",
      "       0.84730204, 0.87073662, 0.87708038, 0.89984327, 0.9029032 ,\n",
      "       0.91506829, 0.92297933, 0.92566609, 0.93268154, 0.93521905,\n",
      "       0.93036794, 0.94551832, 0.9530562 , 0.95238451, 0.95947459,\n",
      "       0.963654  , 0.96283305, 0.97014703, 0.97014703, 0.9670871 ,\n",
      "       0.96902754, 0.97320696, 0.97052019, 0.97126651, 0.97902829,\n",
      "       0.97365475]), 'train_losses': array([23.6509133 ,  6.8144757 ,  4.03658551,  2.78962009,  2.01737062,\n",
      "        1.60435433,  1.34637166,  1.1514568 ,  0.93625398,  0.85759156,\n",
      "        0.74021881,  0.6668127 ,  0.65959753,  0.59334445,  0.51964334,\n",
      "        0.45230241,  0.38896296,  0.36740859,  0.30901469,  0.30282412,\n",
      "        0.25029521,  0.23690198,  0.22635722,  0.20658246,  0.20906775,\n",
      "        0.22410001,  0.17433393,  0.14648331,  0.14940744,  0.13003584,\n",
      "        0.11088236,  0.12012952,  0.09572694,  0.09894774,  0.1043906 ,\n",
      "        0.09814196,  0.08659536,  0.0899095 ,  0.0902479 ,  0.07384418,\n",
      "        0.08220217]), 'val_accs': array([0.49492537, 0.51462687, 0.55701493, 0.59402985, 0.6358209 ,\n",
      "       0.65104478, 0.64776119, 0.69910448, 0.69791045, 0.81522388,\n",
      "       0.79940299, 0.83850746, 0.79283582, 0.86567164, 0.83462687,\n",
      "       0.90298507, 0.87044776, 0.92149254, 0.88597015, 0.91910448,\n",
      "       0.91373134, 0.93134328, 0.89492537, 0.9361194 , 0.93014925,\n",
      "       0.9119403 , 0.94238806, 0.9319403 , 0.94985075, 0.91820896,\n",
      "       0.96925373, 0.97223881, 0.96597015, 0.96208955, 0.97522388,\n",
      "       0.96597015, 0.96656716, 0.97104478, 0.94477612, 0.97223881,\n",
      "       0.95880597]), 'val_losses': array([5.09667657, 2.70852295, 2.17217958, 0.99617905, 1.27898909,\n",
      "       0.78324766, 1.03579385, 1.01440119, 0.71177135, 0.49797642,\n",
      "       0.49708203, 0.43746945, 0.72458247, 0.39266911, 0.48926063,\n",
      "       0.29439834, 0.39659223, 0.28420919, 0.31965095, 0.25714435,\n",
      "       0.25564759, 0.23067029, 0.32677166, 0.23633586, 0.23112514,\n",
      "       0.25846292, 0.16681783, 0.18523015, 0.14242333, 0.27565759,\n",
      "       0.09440389, 0.10240157, 0.11062479, 0.11434977, 0.10105697,\n",
      "       0.11715515, 0.11767925, 0.09440752, 0.19915905, 0.09842812,\n",
      "       0.13891158])}, 'ATTACH::17::best_val_loss': 0.09440389450285028, 'ATTACH::18::history': {'train_accs': array([0.43309202, 0.4429435 , 0.45130234, 0.48660348, 0.49346966,\n",
      "       0.50175386, 0.5211583 , 0.5264572 , 0.53100978, 0.53974177,\n",
      "       0.54369729, 0.56048959, 0.56519143, 0.56795283, 0.58638704,\n",
      "       0.60616464, 0.61579222, 0.65139189, 0.67445332, 0.69990298,\n",
      "       0.70423166, 0.72990522, 0.74318979, 0.75639973, 0.77946115,\n",
      "       0.79058139, 0.79894022, 0.81117994, 0.82095679, 0.83095753,\n",
      "       0.84498843, 0.85118292, 0.85386969, 0.85834764, 0.86439287,\n",
      "       0.87081125, 0.87857303, 0.88267781, 0.88775282, 0.89140981,\n",
      "       0.89663408, 0.90073886, 0.90641093, 0.90969475, 0.91775506,\n",
      "       0.91641167, 0.91268005, 0.91820285, 0.91857601, 0.91932234,\n",
      "       0.92969625, 0.92208374, 0.93171132, 0.93499515, 0.93245765,\n",
      "       0.93641317, 0.93305471, 0.93223375, 0.93857751, 0.94380178]), 'train_losses': array([13.13955641,  1.24093408,  1.20166097,  1.16454899,  1.14742691,\n",
      "        1.12936339,  1.10603731,  1.09171672,  1.0762983 ,  1.05998159,\n",
      "        1.05162863,  1.02259419,  1.01471333,  1.00982095,  0.96915187,\n",
      "        0.92183481,  0.89581089,  0.85117296,  0.81403574,  0.76925591,\n",
      "        0.76246396,  0.7034613 ,  0.67346111,  0.65005438,  0.59351203,\n",
      "        0.56873777,  0.53601474,  0.49367118,  0.47877454,  0.45440362,\n",
      "        0.420517  ,  0.40673689,  0.3957782 ,  0.38123912,  0.37086773,\n",
      "        0.35096992,  0.3399544 ,  0.3250554 ,  0.30979863,  0.3107695 ,\n",
      "        0.2959769 ,  0.29762213,  0.27296983,  0.27076162,  0.25556163,\n",
      "        0.25450267,  0.25271088,  0.24459192,  0.24921364,  0.24414586,\n",
      "        0.21502856,  0.23705113,  0.20893464,  0.1956757 ,  0.20234608,\n",
      "        0.18696216,  0.20674059,  0.20294075,  0.19013845,  0.17161515]), 'val_accs': array([0.44208955, 0.44208955, 0.47940299, 0.5       , 0.48776119,\n",
      "       0.48686567, 0.51731343, 0.50328358, 0.53373134, 0.52507463,\n",
      "       0.52835821, 0.56328358, 0.58328358, 0.53522388, 0.60358209,\n",
      "       0.61761194, 0.64238806, 0.68925373, 0.66656716, 0.71850746,\n",
      "       0.74597015, 0.73492537, 0.76537313, 0.78865672, 0.78925373,\n",
      "       0.80865672, 0.81701493, 0.82656716, 0.84179104, 0.8361194 ,\n",
      "       0.85671642, 0.85164179, 0.86358209, 0.82358209, 0.88149254,\n",
      "       0.8641791 , 0.87671642, 0.86179104, 0.89791045, 0.89343284,\n",
      "       0.90746269, 0.8880597 , 0.91104478, 0.89134328, 0.89731343,\n",
      "       0.90029851, 0.89880597, 0.91791045, 0.9119403 , 0.91850746,\n",
      "       0.92626866, 0.91164179, 0.90985075, 0.93641791, 0.92716418,\n",
      "       0.92686567, 0.89462687, 0.92358209, 0.9280597 , 0.92925373]), 'val_losses': array([1.26098913, 1.2163124 , 1.1808981 , 1.14038297, 1.13864973,\n",
      "       1.11950787, 1.09904859, 1.08637449, 1.06126287, 1.0531241 ,\n",
      "       1.03097082, 1.00037077, 0.97481608, 1.04114307, 0.91985659,\n",
      "       0.90864209, 0.86849703, 0.79270637, 0.83990873, 0.7540568 ,\n",
      "       0.68831677, 0.79944095, 0.64626027, 0.5915902 , 0.63112213,\n",
      "       0.54533045, 0.51736018, 0.49271986, 0.4232463 , 0.46998718,\n",
      "       0.3929022 , 0.40061755, 0.4075942 , 0.56954071, 0.3338776 ,\n",
      "       0.38466736, 0.36530044, 0.40874648, 0.27872137, 0.33619443,\n",
      "       0.29347876, 0.34814154, 0.29791135, 0.36376815, 0.36552587,\n",
      "       0.36635059, 0.35578952, 0.27815996, 0.28530402, 0.26129416,\n",
      "       0.23022345, 0.28293087, 0.31272715, 0.19608672, 0.26173506,\n",
      "       0.25707783, 0.3962638 , 0.25069587, 0.25959791, 0.24429612])}, 'ATTACH::18::best_val_loss': 0.19608671533527658, 'ATTACH::19::history': {'train_accs': array([0.42749459, 0.43995821, 0.44212255, 0.43913725, 0.45794462,\n",
      "       0.49085753, 0.50772446, 0.53586089, 0.57996865, 0.62124039,\n",
      "       0.64833197, 0.65960146, 0.67019927, 0.68325994, 0.65654153,\n",
      "       0.65848198, 0.68400627, 0.68669304, 0.67519964, 0.68669304,\n",
      "       0.68714083, 0.67937906, 0.69333532, 0.697664  , 0.70736622,\n",
      "       0.68475259, 0.61310546, 0.55675797, 0.62400179, 0.65975073,\n",
      "       0.60205986, 0.62788268, 0.46578103]), 'train_losses': array([1.47009885e+06, 1.24752475e+00, 1.24064358e+00, 1.23314830e+00,\n",
      "       1.18611500e+00, 1.12530397e+00, 1.07619404e+00, 1.02030729e+00,\n",
      "       9.59165724e-01, 9.02951302e-01, 8.58178412e-01, 8.33818745e-01,\n",
      "       8.15023417e-01, 7.83242046e-01, 8.44865715e-01, 8.26610589e-01,\n",
      "       7.75296833e-01, 7.70433613e-01, 8.01363239e-01, 7.78615202e-01,\n",
      "       7.84668945e-01, 7.83774610e-01, 7.65547127e-01, 7.54074536e-01,\n",
      "       7.30503473e-01, 7.86372365e-01, 9.24556631e-01, 1.04512253e+00,\n",
      "       9.17851117e-01, 8.55214655e-01, 9.47606249e-01, 9.04863582e-01,\n",
      "       1.19191197e+00]), 'val_accs': array([0.44208955, 0.44208955, 0.44208955, 0.45283582, 0.46567164,\n",
      "       0.51134328, 0.52686567, 0.45343284, 0.56537313, 0.65074627,\n",
      "       0.69522388, 0.70298507, 0.65522388, 0.70686567, 0.69134328,\n",
      "       0.6880597 , 0.71641791, 0.70776119, 0.69701493, 0.71343284,\n",
      "       0.6758209 , 0.71791045, 0.7441791 , 0.73402985, 0.72895522,\n",
      "       0.73104478, 0.64865672, 0.60029851, 0.62746269, 0.70029851,\n",
      "       0.62298507, 0.52149254, 0.49223881]), 'val_losses': array([1.24595492, 1.24025931, 1.23194167, 1.21272309, 1.15973944,\n",
      "       1.03743025, 1.00906837, 1.15455521, 0.93790323, 0.87306976,\n",
      "       0.75947836, 0.75168899, 0.8516015 , 0.73512988, 0.7658626 ,\n",
      "       0.78614109, 0.726191  , 0.72921771, 0.7436296 , 0.71928607,\n",
      "       0.78695695, 0.70261429, 0.65094069, 0.68142292, 0.66294788,\n",
      "       0.69832612, 0.89546434, 0.95937801, 0.89127088, 0.7607067 ,\n",
      "       0.90473028, 1.13027544, 1.15069001])}, 'ATTACH::19::best_val_loss': 0.6509406877631572, 'ATTACH::20::history': {'train_accs': array([0.43018136, 0.46167624, 0.50130607, 0.51108292, 0.52212852,\n",
      "       0.53795059, 0.53683111, 0.5481006 , 0.56451974, 0.56325099,\n",
      "       0.57347563, 0.57153519, 0.59168595, 0.58862602, 0.60168669,\n",
      "       0.60064184, 0.60803045, 0.61616539, 0.6205687 , 0.62676319,\n",
      "       0.62594223, 0.6290768 , 0.63288305, 0.64743638, 0.64974998,\n",
      "       0.65012314, 0.65997462, 0.66064632, 0.6624375 , 0.66848272,\n",
      "       0.66415404, 0.66736324, 0.67273677, 0.66952758, 0.67340846,\n",
      "       0.67408016, 0.67930443, 0.67482648, 0.6790059 , 0.68348384,\n",
      "       0.68490186, 0.683857  , 0.68057318, 0.69012613, 0.68549892,\n",
      "       0.68855885, 0.68572282, 0.69273826, 0.69602209, 0.69057392,\n",
      "       0.69221584, 0.69206657, 0.69288753, 0.69452944, 0.69841033,\n",
      "       0.69923129, 0.69676842, 0.70878424, 0.70251511, 0.70251511]), 'train_losses': array([6.5430067 , 1.25855749, 1.06621081, 1.03169029, 0.98164451,\n",
      "       0.93272786, 0.93371073, 0.90527671, 0.88667643, 0.87128083,\n",
      "       0.85952959, 0.85010201, 0.82932587, 0.83395831, 0.81663021,\n",
      "       0.81387262, 0.80340492, 0.79769896, 0.79450825, 0.78045507,\n",
      "       0.77236402, 0.770943  , 0.76182025, 0.74490981, 0.73631319,\n",
      "       0.73257928, 0.72797538, 0.7194245 , 0.71963186, 0.71273496,\n",
      "       0.7044205 , 0.70566157, 0.69738453, 0.69756882, 0.69662766,\n",
      "       0.68483761, 0.68836704, 0.68486872, 0.68292302, 0.6715373 ,\n",
      "       0.67416641, 0.67347948, 0.66988866, 0.66290459, 0.6679454 ,\n",
      "       0.66040816, 0.66729241, 0.66182981, 0.6531066 , 0.65246742,\n",
      "       0.65452   , 0.64919044, 0.65068489, 0.64764645, 0.64014234,\n",
      "       0.64298852, 0.64434627, 0.63169835, 0.63946124, 0.6362569 ]), 'val_accs': array([0.56029851, 0.59940299, 0.51134328, 0.62835821, 0.59641791,\n",
      "       0.61313433, 0.65343284, 0.61343284, 0.66656716, 0.61970149,\n",
      "       0.67134328, 0.64      , 0.65940299, 0.7119403 , 0.67850746,\n",
      "       0.65701493, 0.68955224, 0.74656716, 0.66985075, 0.70029851,\n",
      "       0.73164179, 0.72447761, 0.7361194 , 0.69910448, 0.76089552,\n",
      "       0.74298507, 0.76477612, 0.77432836, 0.77910448, 0.74776119,\n",
      "       0.75641791, 0.74537313, 0.76746269, 0.76656716, 0.77432836,\n",
      "       0.77850746, 0.77074627, 0.7638806 , 0.78537313, 0.79522388,\n",
      "       0.77104478, 0.76686567, 0.75970149, 0.79850746, 0.76686567,\n",
      "       0.80029851, 0.78955224, 0.77044776, 0.80776119, 0.78149254,\n",
      "       0.78059701, 0.77701493, 0.80029851, 0.78149254, 0.78238806,\n",
      "       0.77552239, 0.79701493, 0.79880597, 0.75074627, 0.7638806 ]), 'val_losses': array([0.92580684, 0.8841403 , 0.91259302, 0.88463307, 0.8306128 ,\n",
      "       0.82027292, 0.82537515, 0.80224696, 0.76855766, 0.73222336,\n",
      "       0.73824679, 0.72736656, 0.73495211, 0.7369306 , 0.71654553,\n",
      "       0.71659431, 0.6905788 , 0.66576905, 0.69531819, 0.66130382,\n",
      "       0.65278752, 0.64076636, 0.64326595, 0.63755727, 0.61044963,\n",
      "       0.61078574, 0.60578945, 0.59558937, 0.60112125, 0.5939437 ,\n",
      "       0.57415264, 0.58198331, 0.57554365, 0.55890131, 0.57184953,\n",
      "       0.57440456, 0.57043158, 0.56133125, 0.54772956, 0.53373879,\n",
      "       0.54385349, 0.54278988, 0.54048285, 0.52120187, 0.55942472,\n",
      "       0.52671   , 0.53660199, 0.55012398, 0.51145031, 0.52997647,\n",
      "       0.53320589, 0.51698932, 0.51243294, 0.52419575, 0.51149052,\n",
      "       0.52814992, 0.50825719, 0.51175407, 0.52768576, 0.50387958])}, 'ATTACH::20::best_val_loss': 0.5038795836647945, 'ATTACH::21::history': {'train_accs': array([0.39450705, 0.44966042, 0.48257333, 0.50854541, 0.53190537,\n",
      "       0.54735428, 0.5931786 , 0.67505038, 0.72288977, 0.79267109,\n",
      "       0.84349578, 0.87327412, 0.8948429 , 0.91297858, 0.93044257,\n",
      "       0.9422345 , 0.95036943, 0.9558176 , 0.96193746, 0.96626614,\n",
      "       0.96245989, 0.97402791, 0.97402791, 0.97089335, 0.97372938,\n",
      "       0.9781327 , 0.97820733, 0.9808941 , 0.98074483, 0.97999851,\n",
      "       0.9802224 , 0.98328233, 0.9829838 , 0.98261064, 0.98425256,\n",
      "       0.98820808, 0.98746175]), 'train_losses': array([17.39285829,  4.30437599,  2.51183355,  1.73387899,  1.33362532,\n",
      "        1.1757736 ,  0.96269425,  0.75861584,  0.64231842,  0.51573801,\n",
      "        0.41490936,  0.34070147,  0.29404004,  0.23531911,  0.19940173,\n",
      "        0.16932308,  0.14646421,  0.13036801,  0.10918844,  0.10488037,\n",
      "        0.10907597,  0.08169469,  0.0818678 ,  0.08257655,  0.07408292,\n",
      "        0.06676476,  0.06233953,  0.06114575,  0.05909503,  0.06327384,\n",
      "        0.06030794,  0.04672937,  0.05016581,  0.05558009,  0.04734635,\n",
      "        0.03734262,  0.03936991]), 'val_accs': array([0.46985075, 0.54985075, 0.5838806 , 0.66059701, 0.60537313,\n",
      "       0.64925373, 0.63343284, 0.66656716, 0.81313433, 0.83283582,\n",
      "       0.83641791, 0.90865672, 0.8958209 , 0.94179104, 0.89820896,\n",
      "       0.93402985, 0.95432836, 0.95492537, 0.95373134, 0.96149254,\n",
      "       0.96597015, 0.97134328, 0.96477612, 0.96298507, 0.96626866,\n",
      "       0.95552239, 0.97820896, 0.97671642, 0.97164179, 0.97552239,\n",
      "       0.95701493, 0.9719403 , 0.96716418, 0.97701493, 0.97432836,\n",
      "       0.97910448, 0.97104478]), 'val_losses': array([2.96402284, 1.56936799, 0.97502272, 0.79857328, 1.10733402,\n",
      "       0.80393845, 0.78837484, 0.7152604 , 0.46381577, 0.39936342,\n",
      "       0.43259672, 0.25771776, 0.28662266, 0.16981118, 0.31531138,\n",
      "       0.18850441, 0.13109362, 0.13069841, 0.14195396, 0.11688576,\n",
      "       0.10694524, 0.08546483, 0.10075881, 0.13822629, 0.09189855,\n",
      "       0.14068073, 0.06235457, 0.07283267, 0.08494231, 0.07536097,\n",
      "       0.14512335, 0.08768145, 0.11214242, 0.07717946, 0.08211725,\n",
      "       0.06354151, 0.08057664])}, 'ATTACH::21::best_val_loss': 0.06235456522053747, 'ATTACH::22::history': {'train_accs': array([0.41659825, 0.50541085, 0.60713486, 0.70676916, 0.78349131,\n",
      "       0.84080902, 0.88111053, 0.90745578, 0.92559146, 0.94059258,\n",
      "       0.95014553, 0.95454885, 0.96104187, 0.96507202, 0.97208747,\n",
      "       0.9726099 , 0.9753713 , 0.97604299, 0.97910292, 0.98052093,\n",
      "       0.97947608, 0.98484962, 0.98238675, 0.98581984, 0.98671543,\n",
      "       0.98552131, 0.986193  , 0.98716322, 0.98813344, 0.98798418,\n",
      "       0.98843197, 0.98962609, 0.98970072, 0.98910366, 0.98962609,\n",
      "       0.9917158 , 0.99216359, 0.99208896, 0.99119337, 0.98947683,\n",
      "       0.99082021, 0.99365624, 0.99164117, 0.99305918, 0.9914919 ,\n",
      "       0.9914919 , 0.99283529, 0.99231286, 0.9917158 , 0.99290992,\n",
      "       0.99320845, 0.99455183, 0.99283529, 0.99268602, 0.99410404,\n",
      "       0.99514889, 0.99320845]), 'train_losses': array([8.40697773, 2.26939059, 1.23403671, 0.79773173, 0.57719117,\n",
      "       0.42405252, 0.32700489, 0.25026988, 0.20605665, 0.16758156,\n",
      "       0.14456836, 0.12945773, 0.11072734, 0.10171502, 0.08801868,\n",
      "       0.08476383, 0.07476064, 0.07274364, 0.06454192, 0.06281638,\n",
      "       0.06155064, 0.04620556, 0.05536517, 0.04469768, 0.04512425,\n",
      "       0.04717195, 0.04219265, 0.04105487, 0.03989639, 0.0325234 ,\n",
      "       0.03250517, 0.03252292, 0.0331076 , 0.03201995, 0.03380247,\n",
      "       0.02431147, 0.02328568, 0.02410794, 0.03561557, 0.03026647,\n",
      "       0.02799767, 0.02077177, 0.02558294, 0.02521673, 0.02520948,\n",
      "       0.03076766, 0.02273415, 0.02505666, 0.02585778, 0.02256735,\n",
      "       0.02093501, 0.01805391, 0.02279777, 0.02235644, 0.01776266,\n",
      "       0.01464855, 0.02374493]), 'val_accs': array([0.53970149, 0.52328358, 0.69671642, 0.80447761, 0.81253731,\n",
      "       0.85014925, 0.90268657, 0.93641791, 0.91791045, 0.93641791,\n",
      "       0.94358209, 0.96358209, 0.96835821, 0.97910448, 0.96149254,\n",
      "       0.97104478, 0.97791045, 0.96626866, 0.97343284, 0.91462687,\n",
      "       0.97134328, 0.87343284, 0.98179104, 0.97850746, 0.96716418,\n",
      "       0.98238806, 0.97373134, 0.98179104, 0.97850746, 0.96865672,\n",
      "       0.98358209, 0.98656716, 0.98358209, 0.97701493, 0.9880597 ,\n",
      "       0.98238806, 0.96179104, 0.97044776, 0.97402985, 0.9919403 ,\n",
      "       0.97791045, 0.98298507, 0.97253731, 0.98567164, 0.94895522,\n",
      "       0.98537313, 0.99134328, 0.98567164, 0.96716418, 0.98537313,\n",
      "       0.98268657, 0.9880597 , 0.98746269, 0.98626866, 0.98746269,\n",
      "       0.98597015, 0.99253731]), 'val_losses': array([1.32590933, 1.56049239, 0.70494217, 0.51760476, 0.47459639,\n",
      "       0.3723566 , 0.24891996, 0.16630808, 0.20686646, 0.18640722,\n",
      "       0.1462265 , 0.11043863, 0.09843394, 0.06983697, 0.112906  ,\n",
      "       0.08694515, 0.07039508, 0.10332104, 0.08087334, 0.31703946,\n",
      "       0.093838  , 0.4785009 , 0.06988579, 0.05986488, 0.11552047,\n",
      "       0.05655119, 0.09795838, 0.05674507, 0.06684485, 0.12665386,\n",
      "       0.06046471, 0.04818584, 0.05875476, 0.07779827, 0.05007368,\n",
      "       0.0577645 , 0.14538031, 0.09677672, 0.09426464, 0.03233738,\n",
      "       0.07830691, 0.05997576, 0.09071778, 0.0472659 , 0.195561  ,\n",
      "       0.06092472, 0.0290983 , 0.04989196, 0.10881681, 0.05604759,\n",
      "       0.06230684, 0.04409307, 0.0478327 , 0.05418486, 0.04635153,\n",
      "       0.04900559, 0.03035652])}, 'ATTACH::22::best_val_loss': 0.029098299862677927, 'ATTACH::23::history': {'train_accs': array([0.4045078 , 0.44622733, 0.47749832, 0.50197776, 0.51705351,\n",
      "       0.5315322 , 0.53302485, 0.55436973, 0.57355026, 0.60407493,\n",
      "       0.63288305, 0.71370998, 0.77110232, 0.82939025, 0.86312411,\n",
      "       0.90178372, 0.91633704, 0.93021867, 0.93633853, 0.94551832,\n",
      "       0.9502948 , 0.95969848, 0.96126577, 0.96343011, 0.96447496,\n",
      "       0.97111725, 0.97044556, 0.96925144, 0.97253526, 0.97820733,\n",
      "       0.97619225, 0.97604299, 0.97850586, 0.97761027, 0.98156579,\n",
      "       0.98126726, 0.98104336, 0.98283454, 0.98402866, 0.98552131,\n",
      "       0.98470035, 0.98522278, 0.98238675, 0.98664079, 0.98887977,\n",
      "       0.98402866, 0.98932756, 0.98708859, 0.9864169 , 0.98649153,\n",
      "       0.99022315, 0.98723785, 0.98798418, 0.99096948, 0.98432719,\n",
      "       0.99059631, 0.99044705, 0.9859691 , 0.99096948, 0.99096948]), 'train_losses': array([17.22685707,  4.09669267,  2.53762444,  1.87997117,  1.51299934,\n",
      "        1.29461094,  1.22162409,  0.98646998,  0.88464823,  0.82663135,\n",
      "        0.76424032,  0.64455744,  0.53362049,  0.42427981,  0.35405501,\n",
      "        0.26806864,  0.2268421 ,  0.19845314,  0.17992276,  0.15577533,\n",
      "        0.14103466,  0.1159343 ,  0.11175114,  0.10807667,  0.09984387,\n",
      "        0.08842273,  0.08453112,  0.08795545,  0.08215414,  0.06569309,\n",
      "        0.06866654,  0.0685304 ,  0.06431706,  0.06057271,  0.05476652,\n",
      "        0.05317105,  0.05346616,  0.04796276,  0.04837076,  0.04120451,\n",
      "        0.04417716,  0.04263533,  0.04679103,  0.03793145,  0.03477111,\n",
      "        0.04382751,  0.03012736,  0.03848984,  0.03856805,  0.03941501,\n",
      "        0.02793097,  0.0360832 ,  0.0343059 ,  0.03167359,  0.04353815,\n",
      "        0.02839625,  0.02652903,  0.04372099,  0.03121841,  0.02623247]), 'val_accs': array([0.48      , 0.55791045, 0.52238806, 0.53731343, 0.55731343,\n",
      "       0.53850746, 0.62119403, 0.56716418, 0.67343284, 0.71910448,\n",
      "       0.75880597, 0.78358209, 0.83044776, 0.88895522, 0.9041791 ,\n",
      "       0.93343284, 0.95074627, 0.95283582, 0.95373134, 0.96238806,\n",
      "       0.96686567, 0.97283582, 0.97044776, 0.96149254, 0.95552239,\n",
      "       0.97343284, 0.96716418, 0.96656716, 0.97701493, 0.96776119,\n",
      "       0.97850746, 0.97432836, 0.97641791, 0.96567164, 0.97761194,\n",
      "       0.97791045, 0.98208955, 0.9841791 , 0.98477612, 0.98328358,\n",
      "       0.98447761, 0.97731343, 0.98358209, 0.98298507, 0.97910448,\n",
      "       0.9841791 , 0.98447761, 0.98537313, 0.98746269, 0.98537313,\n",
      "       0.97044776, 0.98597015, 0.98567164, 0.97880597, 0.98507463,\n",
      "       0.98358209, 0.98208955, 0.98507463, 0.98507463, 0.98149254]), 'val_losses': array([1.89468378, 1.83285724, 2.47166394, 1.5288476 , 1.15850987,\n",
      "       0.97151154, 0.74553009, 0.80887712, 0.7111815 , 0.63298076,\n",
      "       0.57231003, 0.48629013, 0.39816984, 0.29645466, 0.25300008,\n",
      "       0.19024986, 0.14775781, 0.13986857, 0.1359831 , 0.11535032,\n",
      "       0.10539102, 0.08809159, 0.09476307, 0.11811706, 0.12868431,\n",
      "       0.08321961, 0.0954825 , 0.09736756, 0.0696598 , 0.09306923,\n",
      "       0.06415916, 0.07663595, 0.08111044, 0.10590528, 0.0754133 ,\n",
      "       0.06341573, 0.05587456, 0.04856688, 0.05844789, 0.0514216 ,\n",
      "       0.04399828, 0.07388702, 0.0591692 , 0.05370819, 0.06644894,\n",
      "       0.05233376, 0.0573838 , 0.04695784, 0.04211922, 0.04662786,\n",
      "       0.10007006, 0.04576053, 0.04492733, 0.06925677, 0.05408479,\n",
      "       0.04617075, 0.0496422 , 0.05270933, 0.04093211, 0.05797279])}, 'ATTACH::23::best_val_loss': 0.04093211361443374, 'ATTACH::24::history': {'train_accs': array([0.40353758, 0.4735428 , 0.5204866 , 0.55168296, 0.60332861,\n",
      "       0.61870289, 0.64198821, 0.65086947, 0.66251213, 0.69579819,\n",
      "       0.70311217, 0.71885962, 0.73550265, 0.73781625, 0.74677215,\n",
      "       0.75528024, 0.77110232, 0.77550563, 0.76513173, 0.78356594,\n",
      "       0.78184939, 0.79573102, 0.79483544, 0.78595418, 0.79602955,\n",
      "       0.7977461 , 0.79946265, 0.80401523, 0.80550787, 0.8064781 ,\n",
      "       0.80685126, 0.81789686, 0.79968654, 0.80909023, 0.80685126,\n",
      "       0.81506083, 0.81110531, 0.80894097]), 'train_losses': array([16.03889569,  2.19515275,  1.29391518,  1.07387643,  0.89801148,\n",
      "        0.84312471,  0.77860417,  0.75458682,  0.73289168,  0.67487867,\n",
      "        0.6484318 ,  0.63633878,  0.59789105,  0.59238361,  0.57093914,\n",
      "        0.56399133,  0.52857738,  0.52409635,  0.52487556,  0.50213798,\n",
      "        0.4938638 ,  0.48085678,  0.47358904,  0.49138355,  0.47954751,\n",
      "        0.4621431 ,  0.46359075,  0.45310379,  0.44221918,  0.44387292,\n",
      "        0.44600809,  0.42979477,  0.4550002 ,  0.44750437,  0.45031606,\n",
      "        0.42118643,  0.43884649,  0.45391815]), 'val_accs': array([0.48208955, 0.58328358, 0.62925373, 0.58507463, 0.67761194,\n",
      "       0.64298507, 0.59850746, 0.68268657, 0.62925373, 0.71701493,\n",
      "       0.73283582, 0.74985075, 0.76507463, 0.78895522, 0.75641791,\n",
      "       0.79850746, 0.80507463, 0.77432836, 0.76686567, 0.7358209 ,\n",
      "       0.78328358, 0.74656716, 0.75970149, 0.78776119, 0.80119403,\n",
      "       0.77820896, 0.73820896, 0.82238806, 0.78238806, 0.70597015,\n",
      "       0.78656716, 0.8119403 , 0.78328358, 0.79074627, 0.79970149,\n",
      "       0.75970149, 0.81134328, 0.79552239]), 'val_losses': array([1.94539531, 0.98202207, 0.77052355, 1.04942817, 0.69061751,\n",
      "       0.74862377, 0.83808466, 0.68907436, 0.74147903, 0.62701146,\n",
      "       0.59503985, 0.55676698, 0.59414193, 0.54242786, 0.61295685,\n",
      "       0.49818941, 0.51577963, 0.52470319, 0.5330061 , 0.5822795 ,\n",
      "       0.49340638, 0.57595967, 0.46971134, 0.50898219, 0.45504077,\n",
      "       0.52705151, 0.58458572, 0.44203546, 0.50229328, 0.66360174,\n",
      "       0.47378778, 0.44806293, 0.54509429, 0.49913626, 0.48561997,\n",
      "       0.59311115, 0.49234714, 0.53275607])}, 'ATTACH::24::best_val_loss': 0.44203545559698076, 'ATTACH::25::history': {'train_accs': array([0.45764609, 0.53026345, 0.5874319 , 0.637361  , 0.68012538,\n",
      "       0.71654601, 0.74647362, 0.77662512, 0.80244794, 0.8196134 ,\n",
      "       0.83080827, 0.84036122, 0.84677961, 0.85745205, 0.85685499,\n",
      "       0.86573625, 0.8729756 , 0.87402045, 0.87013956, 0.87655795,\n",
      "       0.87842376, 0.88305097, 0.8784984 , 0.88588701, 0.8893201 ,\n",
      "       0.88543921, 0.88864841, 0.89327562, 0.89372341, 0.89036495]), 'train_losses': array([8.09351036, 2.16606102, 1.22531389, 0.86278196, 0.72423623,\n",
      "       0.6423391 , 0.57757849, 0.51599002, 0.46013818, 0.42817594,\n",
      "       0.41609693, 0.39378706, 0.38330084, 0.36612828, 0.36497166,\n",
      "       0.34974151, 0.33649151, 0.32712195, 0.33551686, 0.32405194,\n",
      "       0.30683384, 0.31043474, 0.30717537, 0.3078118 , 0.3000665 ,\n",
      "       0.30529163, 0.29538265, 0.28167799, 0.28337693, 0.29543431]), 'val_accs': array([0.42208955, 0.52835821, 0.66835821, 0.71432836, 0.71402985,\n",
      "       0.78746269, 0.81373134, 0.80835821, 0.77940299, 0.8241791 ,\n",
      "       0.8719403 , 0.80567164, 0.81820896, 0.85373134, 0.85522388,\n",
      "       0.77641791, 0.78776119, 0.83313433, 0.82358209, 0.87850746,\n",
      "       0.72537313, 0.78059701, 0.69940299, 0.77761194, 0.83850746,\n",
      "       0.79820896, 0.75671642, 0.79014925, 0.77820896, 0.80029851]), 'val_losses': array([2.38123107, 1.2052481 , 0.75592685, 0.66403278, 0.69226156,\n",
      "       0.52772028, 0.45178483, 0.46466914, 0.51468044, 0.40282082,\n",
      "       0.38225012, 0.45797381, 0.45406704, 0.39727107, 0.43459941,\n",
      "       0.55519993, 0.44009608, 0.40708426, 0.38419456, 0.3773231 ,\n",
      "       0.52987696, 0.46898956, 0.51088044, 0.44011961, 0.47649899,\n",
      "       0.4422144 , 0.48726392, 0.45500917, 0.43846869, 0.50168484])}, 'ATTACH::25::best_val_loss': 0.37732309960607274, 'ATTACH::26::history': {'train_accs': array([0.43480857, 0.44219718, 0.44212255, 0.47622957, 0.54974252,\n",
      "       0.60504515, 0.66945294, 0.70885887, 0.74154788, 0.78259572,\n",
      "       0.81312038, 0.8389432 , 0.82021046, 0.85252631, 0.88461826,\n",
      "       0.88842451, 0.89849989, 0.90447048, 0.91626241, 0.92178521,\n",
      "       0.91820285, 0.92603926, 0.93006941, 0.9335025 , 0.94447347,\n",
      "       0.93992089, 0.94372714, 0.94581685, 0.94283155, 0.9332786 ,\n",
      "       0.94350325, 0.94357788, 0.94790656, 0.95529517, 0.94260766,\n",
      "       0.94551832, 0.949847  , 0.95611613, 0.95507127, 0.94126427,\n",
      "       0.94081648, 0.95872826, 0.96656467, 0.9449959 , 0.95425032,\n",
      "       0.94372714, 0.94954847, 0.95842973, 0.94902605, 0.96328084,\n",
      "       0.96245989, 0.96096724, 0.96589298, 0.9719382 , 0.96828122,\n",
      "       0.95820584, 0.94104038, 0.95529517, 0.944772  , 0.9581312 ]), 'train_losses': array([10.40719357,  1.24142963,  1.2227116 ,  1.14649999,  1.01625911,\n",
      "        0.92073636,  0.80791781,  0.72200831,  0.64417172,  0.55606737,\n",
      "        0.48382363,  0.42170914,  0.47668573,  0.40213735,  0.32810034,\n",
      "        0.3084311 ,  0.28651164,  0.26831635,  0.23860222,  0.2189136 ,\n",
      "        0.23095037,  0.1984614 ,  0.19838911,  0.1855129 ,  0.15753403,\n",
      "        0.1692312 ,  0.1542697 ,  0.15178411,  0.15745235,  0.18939665,\n",
      "        0.15652782,  0.15511173,  0.14138595,  0.13018373,  0.15314923,\n",
      "        0.15673114,  0.13979032,  0.12161269,  0.12425276,  0.15970326,\n",
      "        0.15747454,  0.11877841,  0.09983047,  0.15612427,  0.12812615,\n",
      "        0.16003676,  0.14097125,  0.11558895,  0.14307152,  0.10616966,\n",
      "        0.10989623,  0.10851674,  0.09646979,  0.07903092,  0.0872852 ,\n",
      "        0.11574243,  0.17155031,  0.12372492,  0.15353015,  0.11525158]), 'val_accs': array([0.44208955, 0.44208955, 0.44268657, 0.53044776, 0.53462687,\n",
      "       0.63253731, 0.68208955, 0.73731343, 0.73223881, 0.76776119,\n",
      "       0.84119403, 0.86059701, 0.7758209 , 0.85223881, 0.87880597,\n",
      "       0.88089552, 0.87820896, 0.89044776, 0.87910448, 0.9080597 ,\n",
      "       0.90626866, 0.90119403, 0.89850746, 0.9241791 , 0.91552239,\n",
      "       0.92238806, 0.90835821, 0.91552239, 0.92298507, 0.88149254,\n",
      "       0.91641791, 0.92716418, 0.93164179, 0.90477612, 0.91552239,\n",
      "       0.92597015, 0.9080597 , 0.92149254, 0.92      , 0.89074627,\n",
      "       0.91074627, 0.9280597 , 0.92716418, 0.91731343, 0.92      ,\n",
      "       0.88865672, 0.9241791 , 0.93552239, 0.93044776, 0.92298507,\n",
      "       0.9358209 , 0.93641791, 0.92268657, 0.94776119, 0.93552239,\n",
      "       0.91791045, 0.91253731, 0.92358209, 0.92447761, 0.91731343]), 'val_losses': array([1.24675587, 1.23683012, 1.20322009, 1.06098539, 1.0002403 ,\n",
      "       0.87111729, 0.76690709, 0.6634981 , 0.65537402, 0.61339196,\n",
      "       0.42167474, 0.38980858, 0.59354372, 0.40872478, 0.34637771,\n",
      "       0.34126801, 0.32517579, 0.3089124 , 0.35394549, 0.26662097,\n",
      "       0.26192437, 0.27804294, 0.28497702, 0.2206792 , 0.24631218,\n",
      "       0.22854615, 0.26889113, 0.25791284, 0.22179725, 0.35969022,\n",
      "       0.2419169 , 0.23098511, 0.21130777, 0.29900979, 0.26506907,\n",
      "       0.22481924, 0.26391153, 0.22647269, 0.23052138, 0.31371652,\n",
      "       0.25415883, 0.20860753, 0.21649455, 0.24753132, 0.24046995,\n",
      "       0.30415341, 0.22514827, 0.20554695, 0.20559876, 0.21986131,\n",
      "       0.20154397, 0.2090346 , 0.22465831, 0.16606568, 0.18696202,\n",
      "       0.25203209, 0.25236561, 0.22555826, 0.22158352, 0.24932662])}, 'ATTACH::26::best_val_loss': 0.16606568244855796, 'ATTACH::27::history': {'train_accs': array([0.43988357, 0.5124263 , 0.54160758, 0.60243302, 0.70572431,\n",
      "       0.7563251 , 0.79797   , 0.84289872, 0.87200537, 0.90238077,\n",
      "       0.92036719, 0.93432346, 0.94133891, 0.9422345 , 0.95686245,\n",
      "       0.9583551 , 0.95895216, 0.9670871 , 0.96589298, 0.97119188,\n",
      "       0.96738563, 0.96216136, 0.97156504, 0.97298306, 0.97410254,\n",
      "       0.97574446, 0.97753564, 0.97895365, 0.97940145, 0.97731174,\n",
      "       0.97626689, 0.98126726, 0.98462572, 0.97902829, 0.97484887,\n",
      "       0.98052093, 0.98447645, 0.97835659, 0.9829838 , 0.98634226,\n",
      "       0.9802224 , 0.98395403, 0.98268528, 0.99029778, 0.98246138,\n",
      "       0.9864169 , 0.98492425, 0.98820808, 0.98052093, 0.9836555 ,\n",
      "       0.98649153, 0.98350623, 0.98828271, 0.98693932, 0.98701396,\n",
      "       0.98701396]), 'train_losses': array([7.30584129, 1.87484693, 1.28475061, 0.95849282, 0.70851222,\n",
      "       0.59301814, 0.50463667, 0.40956877, 0.34349308, 0.2693675 ,\n",
      "       0.22281882, 0.18296293, 0.16527367, 0.16889255, 0.12462413,\n",
      "       0.12787465, 0.12270883, 0.09920323, 0.10027256, 0.08508844,\n",
      "       0.10222598, 0.11326021, 0.08561879, 0.07813445, 0.08287672,\n",
      "       0.07788571, 0.0698789 , 0.06445769, 0.06387052, 0.06691069,\n",
      "       0.0707031 , 0.05714498, 0.04731252, 0.06496295, 0.07497111,\n",
      "       0.06135738, 0.04882049, 0.06180963, 0.05200017, 0.04109461,\n",
      "       0.06283642, 0.04808564, 0.05525454, 0.02935107, 0.04996839,\n",
      "       0.04118556, 0.04735972, 0.04009404, 0.06023066, 0.0528647 ,\n",
      "       0.04050026, 0.0499312 , 0.03598999, 0.04077191, 0.04308695,\n",
      "       0.04261081]), 'val_accs': array([0.59164179, 0.62776119, 0.64059701, 0.62567164, 0.78776119,\n",
      "       0.76746269, 0.82835821, 0.86059701, 0.88597015, 0.88567164,\n",
      "       0.93223881, 0.93432836, 0.94477612, 0.9561194 , 0.95134328,\n",
      "       0.93820896, 0.92029851, 0.97014925, 0.95253731, 0.96029851,\n",
      "       0.96477612, 0.94716418, 0.97462687, 0.94656716, 0.97343284,\n",
      "       0.97940299, 0.96029851, 0.9719403 , 0.97462687, 0.95253731,\n",
      "       0.96985075, 0.9838806 , 0.97701493, 0.97820896, 0.96238806,\n",
      "       0.98089552, 0.97014925, 0.97432836, 0.98268657, 0.9758209 ,\n",
      "       0.98149254, 0.97343284, 0.98358209, 0.97522388, 0.98208955,\n",
      "       0.98835821, 0.98      , 0.96746269, 0.98      , 0.98059701,\n",
      "       0.98089552, 0.9838806 , 0.98686567, 0.98119403, 0.97940299,\n",
      "       0.97104478]), 'val_losses': array([1.72426334, 0.83076837, 0.81498344, 0.81543456, 0.51889956,\n",
      "       0.55358584, 0.41708384, 0.3597962 , 0.31017168, 0.27220779,\n",
      "       0.1864698 , 0.17082164, 0.16338958, 0.12613172, 0.14862933,\n",
      "       0.18600555, 0.21447258, 0.10079579, 0.13929028, 0.12682099,\n",
      "       0.11673599, 0.19323726, 0.08741391, 0.16998783, 0.0846776 ,\n",
      "       0.06925266, 0.13838574, 0.08795305, 0.08631157, 0.17329446,\n",
      "       0.09616535, 0.0616587 , 0.08179188, 0.07104892, 0.12333618,\n",
      "       0.06268737, 0.12125011, 0.0965661 , 0.05546632, 0.08884646,\n",
      "       0.07049462, 0.10246005, 0.06455545, 0.0863145 , 0.07418975,\n",
      "       0.04751946, 0.07103359, 0.12849973, 0.06490489, 0.0707438 ,\n",
      "       0.07762884, 0.05396614, 0.05434946, 0.08342878, 0.07440488,\n",
      "       0.11314411])}, 'ATTACH::27::best_val_loss': 0.04751946150867352, 'ATTACH::28::history': {'train_accs': array([0.48555862, 0.59399955, 0.62594223, 0.65952683, 0.65654153,\n",
      "       0.66370625, 0.66713934, 0.67206508, 0.67937906, 0.67937906,\n",
      "       0.67437868, 0.68288678, 0.69691768, 0.68654377, 0.69826106,\n",
      "       0.66713934, 0.65154116]), 'train_losses': array([5.74037556, 0.87746437, 0.78179131, 0.73902625, 0.72435002,\n",
      "       0.73455861, 0.71198848, 0.69974813, 0.68817669, 0.69270654,\n",
      "       0.68949247, 0.68109068, 0.6610672 , 0.6729947 , 0.65314812,\n",
      "       0.72969698, 0.72175948]), 'val_accs': array([0.54149254, 0.69701493, 0.66238806, 0.71820896, 0.72      ,\n",
      "       0.70865672, 0.70507463, 0.55134328, 0.62      , 0.67164179,\n",
      "       0.54149254, 0.58776119, 0.5841791 , 0.6       , 0.58597015,\n",
      "       0.57343284, 0.56597015]), 'val_losses': array([0.82963136, 0.69675626, 0.65289206, 0.65123229, 0.60903058,\n",
      "       0.6484151 , 0.5851315 , 0.91786835, 0.78255862, 0.77394145,\n",
      "       0.77718637, 0.88287115, 0.80486791, 0.74261976, 0.82921637,\n",
      "       0.8792511 , 0.87729345])}, 'ATTACH::28::best_val_loss': 0.5851315018668104, 'ATTACH::29::history': {'train_accs': array([0.3932383 , 0.36278827, 0.35308605, 0.35174267, 0.35763863,\n",
      "       0.37226659, 0.38734234, 0.40219419, 0.3957758 , 0.4047317 ,\n",
      "       0.42010598, 0.42316591, 0.42338981, 0.43988357, 0.43995821,\n",
      "       0.45063064, 0.454213  , 0.46854243, 0.47257258, 0.48085678,\n",
      "       0.490559  , 0.50130607, 0.51048586, 0.52138219, 0.52429286,\n",
      "       0.537055  , 0.54011493, 0.55317561, 0.56399731, 0.56952011,\n",
      "       0.58041645, 0.58862602, 0.59691022, 0.60295544, 0.6037764 ,\n",
      "       0.62340473, 0.62459885, 0.62974849, 0.63758489, 0.64721248,\n",
      "       0.65706396, 0.66430331, 0.66273603, 0.66848272, 0.67587133,\n",
      "       0.68415553, 0.6921412 , 0.69646989, 0.70229122, 0.7091574 ,\n",
      "       0.72012837, 0.71915815, 0.73042764, 0.74005523, 0.73729383,\n",
      "       0.74221957, 0.74542876, 0.7480409 , 0.75834017, 0.76132547]), 'train_losses': array([86.4590831 , 51.33708805, 39.74492696, 31.69434151, 25.04044682,\n",
      "       20.75981799, 15.93318964, 13.85516788, 11.84820858, 10.41421529,\n",
      "        8.9083513 ,  8.33924527,  7.64219859,  6.94705259,  6.37898688,\n",
      "        5.98118323,  5.60681476,  5.27655919,  4.93871693,  4.64060681,\n",
      "        4.43091621,  4.08060262,  3.96688191,  3.74091078,  3.66230515,\n",
      "        3.42984272,  3.24493723,  3.09408575,  3.06872283,  2.84790566,\n",
      "        2.71511931,  2.61939352,  2.51938432,  2.43751082,  2.38462137,\n",
      "        2.29145532,  2.24639091,  2.11481624,  2.03201991,  1.93847234,\n",
      "        1.86185473,  1.80801496,  1.72971657,  1.67619815,  1.64921433,\n",
      "        1.59204188,  1.52920131,  1.47465811,  1.41588338,  1.36974994,\n",
      "        1.31345591,  1.26954592,  1.22859625,  1.13598838,  1.11699958,\n",
      "        1.09442466,  1.08910174,  1.05855953,  1.01532533,  0.99623839]), 'val_accs': array([0.43731343, 0.38537313, 0.33552239, 0.33044776, 0.41492537,\n",
      "       0.44477612, 0.4561194 , 0.45820896, 0.45671642, 0.5438806 ,\n",
      "       0.58059701, 0.60626866, 0.62029851, 0.54925373, 0.62955224,\n",
      "       0.63820896, 0.64686567, 0.62835821, 0.69044776, 0.63492537,\n",
      "       0.64507463, 0.71970149, 0.67134328, 0.70029851, 0.72567164,\n",
      "       0.72059701, 0.74298507, 0.73492537, 0.75791045, 0.69134328,\n",
      "       0.74686567, 0.70895522, 0.78119403, 0.77402985, 0.75044776,\n",
      "       0.77641791, 0.77223881, 0.77402985, 0.7358209 , 0.79970149,\n",
      "       0.75134328, 0.82149254, 0.80358209, 0.80925373, 0.83313433,\n",
      "       0.81731343, 0.78477612, 0.79223881, 0.81074627, 0.76656716,\n",
      "       0.80029851, 0.84626866, 0.8241791 , 0.80477612, 0.81044776,\n",
      "       0.79671642, 0.8441791 , 0.81880597, 0.80507463, 0.82238806]), 'val_losses': array([31.46111281, 14.55330477,  8.87294567,  6.4783736 ,  5.96017931,\n",
      "        4.86939291,  4.05553773,  3.09773368,  2.77292827,  2.50772404,\n",
      "        2.19246283,  1.97936159,  1.82948894,  1.81834266,  1.95886312,\n",
      "        1.74190361,  1.59063714,  1.54475237,  1.36926758,  1.54333892,\n",
      "        1.36905504,  1.16988239,  1.23619304,  1.1617224 ,  1.19306074,\n",
      "        1.1174951 ,  1.08621192,  1.02155953,  1.00625526,  1.06820392,\n",
      "        0.87918494,  0.97626219,  0.79131045,  0.77887654,  0.86293287,\n",
      "        0.83662721,  0.73294015,  0.7496068 ,  0.76475044,  0.63818479,\n",
      "        0.79194157,  0.59065113,  0.63534777,  0.62185093,  0.54415895,\n",
      "        0.61928618,  0.64848383,  0.56736237,  0.65374054,  0.67722061,\n",
      "        0.52414841,  0.48277478,  0.49320753,  0.51458973,  0.52020691,\n",
      "        0.52792333,  0.47943409,  0.49457428,  0.62180927,  0.51280657])}, 'ATTACH::29::best_val_loss': 0.479434092898867}\n",
      "{'ATTACH::0::history': {'train_accs': array([0.3242033 , 0.34233898, 0.36398239, 0.38779013, 0.40062691,\n",
      "       0.42480782, 0.44480931, 0.47160236, 0.4788417 , 0.49824614,\n",
      "       0.5011568 , 0.51459064, 0.52526308, 0.54272707, 0.55496679,\n",
      "       0.56362415, 0.57272931, 0.57996865, 0.59601463, 0.59519367,\n",
      "       0.60683633, 0.61198597, 0.6284051 , 0.63534592, 0.637361  ,\n",
      "       0.66161654, 0.66624375, 0.68102097, 0.69311143, 0.69818643,\n",
      "       0.71632211, 0.71527726, 0.73177103, 0.74445854, 0.74110008,\n",
      "       0.76587805, 0.76647511, 0.77229644, 0.77938652, 0.78252108,\n",
      "       0.79177551, 0.79759684, 0.80573177, 0.81028435, 0.82200164,\n",
      "       0.82035973, 0.830659  , 0.83752519, 0.83834615, 0.84842152,\n",
      "       0.85207851, 0.85730278, 0.86491529, 0.86827375, 0.8755131 ,\n",
      "       0.87961788, 0.88267781, 0.88902157, 0.88521531, 0.89252929]), 'train_losses': array([46.01152327, 24.21323748, 15.00247465, 10.75581327,  8.63966076,\n",
      "        6.62751773,  5.55674382,  4.62611336,  3.97926219,  3.51268352,\n",
      "        3.14235011,  2.82103144,  2.51418971,  2.23693824,  2.11027073,\n",
      "        1.89346284,  1.78453129,  1.62907047,  1.5329607 ,  1.45682055,\n",
      "        1.38509315,  1.30500689,  1.19243138,  1.1503268 ,  1.08422319,\n",
      "        1.00058141,  0.93951071,  0.90980104,  0.87631355,  0.82038606,\n",
      "        0.78433493,  0.78102441,  0.72704561,  0.68932065,  0.69489467,\n",
      "        0.63006455,  0.62735411,  0.60130873,  0.59211431,  0.5858373 ,\n",
      "        0.54366935,  0.54830938,  0.515011  ,  0.49691325,  0.48923474,\n",
      "        0.48052277,  0.46100527,  0.44630913,  0.43609142,  0.40999984,\n",
      "        0.40145126,  0.39168013,  0.37852088,  0.36485021,  0.35436131,\n",
      "        0.34407055,  0.33350059,  0.31521627,  0.32491543,  0.29752129]), 'val_accs': array([0.42686567, 0.39223881, 0.39671642, 0.47313433, 0.47970149,\n",
      "       0.58716418, 0.53373134, 0.59343284, 0.58447761, 0.55164179,\n",
      "       0.59820896, 0.6158209 , 0.63641791, 0.62029851, 0.63910448,\n",
      "       0.60298507, 0.65761194, 0.67880597, 0.63731343, 0.64208955,\n",
      "       0.6638806 , 0.7280597 , 0.73044776, 0.72835821, 0.69134328,\n",
      "       0.63402985, 0.72925373, 0.74985075, 0.77014925, 0.78686567,\n",
      "       0.71940299, 0.79074627, 0.77880597, 0.77701493, 0.78895522,\n",
      "       0.79641791, 0.82537313, 0.82179104, 0.69104478, 0.83791045,\n",
      "       0.82776119, 0.74507463, 0.85343284, 0.81223881, 0.82238806,\n",
      "       0.85492537, 0.80179104, 0.84268657, 0.80656716, 0.86776119,\n",
      "       0.80059701, 0.85402985, 0.8319403 , 0.88626866, 0.75761194,\n",
      "       0.86626866, 0.85641791, 0.88358209, 0.90746269, 0.89641791]), 'val_losses': array([12.1409431 ,  6.94096761,  5.02329774,  2.81989922,  2.90480906,\n",
      "        1.92208327,  1.9579122 ,  1.54564279,  1.41339953,  1.76816214,\n",
      "        1.20389651,  0.91441829,  1.08216301,  1.09043059,  0.92608865,\n",
      "        1.0982525 ,  0.88969236,  0.77494452,  0.94629086,  1.18526057,\n",
      "        0.83379005,  0.64459803,  0.67489869,  0.63516106,  0.68376076,\n",
      "        0.86939421,  0.694156  ,  0.63576979,  0.57520925,  0.54962707,\n",
      "        0.65930982,  0.52402569,  0.54032884,  0.55054912,  0.49670084,\n",
      "        0.50154229,  0.45951282,  0.5015962 ,  0.74135561,  0.42196819,\n",
      "        0.43207809,  0.64095792,  0.4058082 ,  0.4642508 ,  0.45859328,\n",
      "        0.40311687,  0.53520342,  0.39261977,  0.46132119,  0.37267102,\n",
      "        0.50089208,  0.36712009,  0.41422151,  0.32141571,  0.61054014,\n",
      "        0.36906092,  0.37128431,  0.31959061,  0.25813928,  0.28804502])}, 'ATTACH::0::best_val_loss': 0.258139280934832, 'ATTACH::1::history': {'train_accs': array([0.29285768, 0.34472722, 0.36054929, 0.37704306, 0.37704306,\n",
      "       0.39010374, 0.40107471, 0.41010523, 0.4151056 , 0.43033062,\n",
      "       0.44324203, 0.45391447, 0.46130308, 0.47212479, 0.47391596,\n",
      "       0.48787223, 0.49675349, 0.5039182 , 0.51406821, 0.52563624,\n",
      "       0.51951638, 0.54235391, 0.5402642 , 0.54630943, 0.55854915,\n",
      "       0.573401  , 0.57795358, 0.58116277, 0.58929771, 0.5982536 ,\n",
      "       0.60541831, 0.61571759, 0.62168819, 0.62616613, 0.63512202,\n",
      "       0.64273453, 0.65273528, 0.6569147 , 0.66266139, 0.67019927,\n",
      "       0.6762445 , 0.68542429, 0.69363385, 0.70176879, 0.70960519,\n",
      "       0.71423241, 0.72483021, 0.73669677, 0.73550265, 0.74401075,\n",
      "       0.75072767, 0.76393761, 0.75998209, 0.77072916, 0.77684902,\n",
      "       0.78259572, 0.789238  , 0.79192477, 0.79976118, 0.80677663]), 'train_losses': array([86.29069748, 47.76921528, 35.66852664, 27.24226409, 22.26698756,\n",
      "       18.07173404, 14.97689788, 12.74470448, 10.92924302,  9.59930046,\n",
      "        8.47212435,  7.20652022,  6.63667538,  5.84893293,  5.47862042,\n",
      "        4.90690263,  4.58234982,  4.19900327,  3.95463073,  3.56134052,\n",
      "        3.43548084,  3.06134948,  2.94209546,  2.83309506,  2.58836911,\n",
      "        2.40281099,  2.29260851,  2.18145749,  2.08221736,  1.91553163,\n",
      "        1.9110014 ,  1.71531025,  1.66911657,  1.58895597,  1.51596149,\n",
      "        1.46652592,  1.37735088,  1.33887802,  1.26756621,  1.20841055,\n",
      "        1.19042633,  1.10621029,  1.06736558,  1.02735947,  0.97717131,\n",
      "        0.95158062,  0.90291976,  0.85640915,  0.83320345,  0.79010383,\n",
      "        0.76693083,  0.73429477,  0.72623613,  0.68793132,  0.66468577,\n",
      "        0.66160683,  0.64094589,  0.60536362,  0.5896877 ,  0.56078673]), 'val_accs': array([0.4519403 , 0.47761194, 0.43373134, 0.47253731, 0.49104478,\n",
      "       0.51761194, 0.56537313, 0.57164179, 0.57462687, 0.60656716,\n",
      "       0.58626866, 0.63432836, 0.60865672, 0.60895522, 0.63014925,\n",
      "       0.62089552, 0.63253731, 0.6119403 , 0.62656716, 0.63313433,\n",
      "       0.65432836, 0.65880597, 0.67283582, 0.64238806, 0.65313433,\n",
      "       0.65432836, 0.6758209 , 0.68895522, 0.70149254, 0.6880597 ,\n",
      "       0.73641791, 0.72179104, 0.73701493, 0.74537313, 0.74298507,\n",
      "       0.75313433, 0.73820896, 0.76895522, 0.75223881, 0.76059701,\n",
      "       0.74746269, 0.76656716, 0.7841791 , 0.75522388, 0.77343284,\n",
      "       0.78089552, 0.80179104, 0.81253731, 0.79701493, 0.81044776,\n",
      "       0.82895522, 0.76985075, 0.81761194, 0.81701493, 0.82865672,\n",
      "       0.83164179, 0.85402985, 0.83731343, 0.84477612, 0.83820896]), 'val_losses': array([21.26593773, 12.20130013,  9.48189358,  8.34342136,  6.11452315,\n",
      "        6.59495633,  4.3511628 ,  3.70035689,  3.85305991,  2.84974246,\n",
      "        3.00536622,  2.11299159,  2.14754946,  2.14386079,  1.81828867,\n",
      "        1.71035222,  1.92698235,  1.61876989,  1.55732706,  1.77611366,\n",
      "        1.47568535,  1.37215734,  1.20370882,  1.39100441,  1.28446435,\n",
      "        1.20290969,  1.27992441,  1.22783341,  1.07024033,  1.01118149,\n",
      "        0.88420019,  0.98777499,  1.01334307,  0.91667668,  0.90504876,\n",
      "        0.83342511,  0.89279716,  0.80067798,  0.83323156,  0.73327793,\n",
      "        0.85014098,  0.75326263,  0.67498878,  0.67991182,  0.68685449,\n",
      "        0.70348277,  0.60639328,  0.5753092 ,  0.63935658,  0.59232242,\n",
      "        0.54528578,  0.74777718,  0.58752554,  0.55619223,  0.52579826,\n",
      "        0.51345177,  0.45139771,  0.50953041,  0.510955  ,  0.53713886])}, 'ATTACH::1::best_val_loss': 0.451397705327219, 'ATTACH::2::history': {'train_accs': array([0.21665796, 0.21755355, 0.25554146, 0.2968132 , 0.31927756,\n",
      "       0.33024853, 0.32532279, 0.34562281, 0.34890663, 0.35286215,\n",
      "       0.35957907, 0.36077319, 0.36054929, 0.37749086, 0.37353534,\n",
      "       0.39129786, 0.39002911, 0.39741772, 0.39823867, 0.40219419,\n",
      "       0.40920964, 0.40816479, 0.41652362, 0.41764311, 0.42212105,\n",
      "       0.42898724, 0.42846481, 0.43630122, 0.4401821 , 0.43868945,\n",
      "       0.44510784, 0.44704829, 0.44786924, 0.45622808, 0.45406374,\n",
      "       0.45563102, 0.45786999, 0.46003433, 0.46040749, 0.46122845,\n",
      "       0.47272184, 0.48093141, 0.48921561, 0.4958579 , 0.49137995,\n",
      "       0.49757445, 0.50085827, 0.51466527]), 'train_losses': array([189.62762403, 124.71971594,  72.27003484,  57.01019351,\n",
      "        51.35476255,  46.57167913,  42.40533686,  38.23562205,\n",
      "        34.59993467,  31.38630801,  27.80340989,  26.09279087,\n",
      "        24.05652268,  20.89269216,  19.47226617,  18.1383531 ,\n",
      "        16.74161723,  15.4921487 ,  14.30461207,  13.69931619,\n",
      "        12.55414099,  11.74948516,  11.39112393,  10.71798224,\n",
      "         9.95379236,   9.63564546,   9.20548833,   8.62687076,\n",
      "         8.18954304,   7.80635961,   7.56959152,   7.36414263,\n",
      "         7.07756317,   6.68296152,   6.64947001,   6.40648758,\n",
      "         6.26628016,   6.06998353,   5.72313836,   5.84894677,\n",
      "         5.38592751,   5.33799705,   5.10530662,   4.88937697,\n",
      "         4.78973423,   4.66820109,   4.55383192,   4.214426  ]), 'val_accs': array([0.25910448, 0.26537313, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44895522,\n",
      "       0.4441791 , 0.4441791 , 0.4441791 , 0.4441791 , 0.43791045,\n",
      "       0.44328358, 0.44447761, 0.40179104, 0.43492537, 0.46925373,\n",
      "       0.41820896, 0.45791045, 0.43940299, 0.4438806 , 0.50238806,\n",
      "       0.51552239, 0.56686567, 0.51731343, 0.55402985, 0.54179104,\n",
      "       0.56089552, 0.56537313, 0.55402985, 0.55731343, 0.56268657,\n",
      "       0.5638806 , 0.5719403 , 0.58955224, 0.58119403, 0.56985075,\n",
      "       0.56865672, 0.57880597, 0.58597015, 0.59522388, 0.59492537,\n",
      "       0.58447761, 0.61104478, 0.56358209]), 'val_losses': array([100.86658774,  37.66628293,  31.54940425,  34.43157342,\n",
      "        30.89799833,  29.24875775,  24.5054439 ,  22.60263676,\n",
      "        19.77618608,  17.48590195,  15.69406583,  13.53387743,\n",
      "        11.94218292,  10.24890399,   8.5994835 ,   7.89321591,\n",
      "         6.75207216,   5.83364527,   5.13697149,   4.58981503,\n",
      "         4.46618127,   4.21877109,   3.61433312,   3.28203312,\n",
      "         2.94986927,   2.68362362,   2.73159989,   2.45724889,\n",
      "         2.59330135,   2.50546124,   2.49668228,   2.34901548,\n",
      "         2.37679219,   2.11165282,   2.20171461,   2.19143919,\n",
      "         2.14940226,   1.96741874,   2.12582216,   2.13267384,\n",
      "         2.16053449,   2.20824373,   2.26667785,   2.07172188,\n",
      "         2.23146468,   2.15831182,   2.11023867,   2.23721005])}, 'ATTACH::2::best_val_loss': 1.9674187436744348, 'ATTACH::3::history': {'train_accs': array([0.42182252, 0.49414135, 0.53556236, 0.56474364, 0.58952161,\n",
      "       0.64945145, 0.66542279, 0.67691619, 0.69117098, 0.71012762,\n",
      "       0.72706918, 0.73729383, 0.74602582, 0.75431002, 0.75751922,\n",
      "       0.74647362, 0.75259348, 0.76662437, 0.76908725, 0.76214643,\n",
      "       0.76699754, 0.78125233, 0.78072991, 0.78304351, 0.7896858 ,\n",
      "       0.79908948, 0.79349205, 0.76662437, 0.76237033, 0.78184939,\n",
      "       0.79640272, 0.78811852, 0.79401448, 0.78729756, 0.78565565,\n",
      "       0.78498395, 0.78565565, 0.79304426, 0.79528323, 0.79132771,\n",
      "       0.78804388, 0.80408986, 0.78744682]), 'train_losses': array([9.14280427, 1.1151326 , 0.96689698, 0.9055572 , 0.83508606,\n",
      "       0.74105201, 0.71584018, 0.69998704, 0.66277166, 0.63337026,\n",
      "       0.60550439, 0.59836616, 0.5799803 , 0.56867478, 0.55882173,\n",
      "       0.57952727, 0.5692468 , 0.5463779 , 0.53482679, 0.55087012,\n",
      "       0.54784276, 0.53020991, 0.52760364, 0.52144506, 0.51158866,\n",
      "       0.5027443 , 0.49456893, 0.61070945, 0.5626313 , 0.52718233,\n",
      "       0.4997495 , 0.50773526, 0.49541233, 0.51099351, 0.51837628,\n",
      "       0.52254344, 0.52769622, 0.52245365, 0.49775894, 0.52484311,\n",
      "       0.54661147, 0.48150595, 0.52821801]), 'val_accs': array([0.49791045, 0.51044776, 0.57761194, 0.60059701, 0.65910448,\n",
      "       0.51701493, 0.70328358, 0.72358209, 0.76656716, 0.77253731,\n",
      "       0.62208955, 0.73253731, 0.74059701, 0.81373134, 0.67223881,\n",
      "       0.71373134, 0.68447761, 0.75134328, 0.7038806 , 0.73820896,\n",
      "       0.72626866, 0.70567164, 0.70925373, 0.80955224, 0.76597015,\n",
      "       0.64149254, 0.81880597, 0.65373134, 0.64955224, 0.80537313,\n",
      "       0.71402985, 0.62865672, 0.85492537, 0.80298507, 0.77731343,\n",
      "       0.73761194, 0.66716418, 0.73701493, 0.71313433, 0.68925373,\n",
      "       0.82089552, 0.71492537, 0.82      ]), 'val_losses': array([0.96634172, 0.94259302, 0.89046339, 0.8698339 , 0.69846732,\n",
      "       0.86840167, 0.63002873, 0.6382555 , 0.58361699, 0.59950878,\n",
      "       0.81601717, 0.67740395, 0.56329918, 0.52716004, 0.56840431,\n",
      "       0.72950202, 0.61589863, 0.58396267, 0.65158699, 0.62349583,\n",
      "       0.54725767, 0.64751689, 0.5714936 , 0.45667745, 0.51664836,\n",
      "       0.82553713, 0.42437711, 0.6701433 , 0.73196708, 0.42078893,\n",
      "       0.7281877 , 0.75655377, 0.40915085, 0.49866063, 0.63196544,\n",
      "       0.59539477, 0.72241254, 0.49700373, 0.97376252, 0.77739103,\n",
      "       0.43349012, 0.67012106, 0.46074091])}, 'ATTACH::3::best_val_loss': 0.4091508536018542, 'ATTACH::4::history': {'train_accs': array([0.38375998, 0.4567505 , 0.49354429, 0.52809911, 0.54929472,\n",
      "       0.54086126, 0.55272782, 0.54623479, 0.55839988, 0.58064035,\n",
      "       0.57258004, 0.58399881, 0.58884991, 0.57474438, 0.58541682,\n",
      "       0.59989551, 0.5731771 , 0.47921487, 0.51093365, 0.53556236,\n",
      "       0.54959325, 0.52690499, 0.53302485, 0.54086126, 0.53548772]), 'train_losses': array([13.45422745,  1.18318491,  0.98001852,  0.92961982,  0.90480452,\n",
      "        0.89383607,  0.87445142,  0.88440474,  0.87732009,  0.86107622,\n",
      "        0.87494616,  0.83090623,  0.8435712 ,  0.91019722,  0.87825376,\n",
      "        0.83460279,  0.88746476,  0.98297471,  0.93820595,  0.92005689,\n",
      "        0.90571895,  0.93225679,  0.92956584,  0.92474967,  0.9570297 ]), 'val_accs': array([0.44208955, 0.52507463, 0.54059701, 0.6       , 0.46656716,\n",
      "       0.57701493, 0.62268657, 0.52656716, 0.46      , 0.65164179,\n",
      "       0.56507463, 0.62597015, 0.59970149, 0.50208955, 0.69253731,\n",
      "       0.66656716, 0.45343284, 0.57522388, 0.48925373, 0.55044776,\n",
      "       0.58059701, 0.57522388, 0.59313433, 0.57343284, 0.59522388]), 'val_losses': array([1.17016578, 0.95567157, 0.87985195, 0.84864202, 0.90469893,\n",
      "       0.82865465, 0.78107345, 0.87786351, 0.83309593, 0.78095112,\n",
      "       0.84434578, 0.82828251, 0.77898002, 0.84794306, 0.72721747,\n",
      "       0.744269  , 1.50683586, 0.85994109, 0.90918141, 0.93087541,\n",
      "       0.85170096, 0.8486522 , 0.80905243, 0.86489783, 0.87771131])}, 'ATTACH::4::best_val_loss': 0.7272174744819527, 'ATTACH::5::history': {'train_accs': array([0.46010896, 0.55698186, 0.58123741, 0.57026644, 0.57138592,\n",
      "       0.56914695, 0.5814613 , 0.57974476, 0.56862452, 0.5209344 ,\n",
      "       0.55399657, 0.55645944, 0.55332487, 0.54153295, 0.49817151,\n",
      "       0.52175535, 0.51697888, 0.50317188, 0.52003881, 0.52436749]), 'train_losses': array([8.0592303 , 0.90667618, 0.85219812, 0.86396607, 0.89742378,\n",
      "       0.86332068, 0.86385883, 0.85540618, 0.86680999, 0.95666249,\n",
      "       0.92336247, 0.90148359, 0.92110545, 0.94977991, 1.0021159 ,\n",
      "       0.93142358, 0.92803477, 0.95470227, 0.96786616, 0.92754734]), 'val_accs': array([0.50477612, 0.5238806 , 0.4561194 , 0.61552239, 0.52835821,\n",
      "       0.65164179, 0.49343284, 0.53283582, 0.45343284, 0.60059701,\n",
      "       0.48985075, 0.58447761, 0.42835821, 0.52686567, 0.46865672,\n",
      "       0.43761194, 0.50865672, 0.53402985, 0.4958209 , 0.57432836]), 'val_losses': array([0.8808137 , 1.10503442, 1.11258813, 0.83764078, 0.94190274,\n",
      "       0.77702451, 0.94697171, 0.8863318 , 0.95124841, 0.76323291,\n",
      "       0.99947361, 0.80238883, 0.92176525, 0.99101363, 1.04281516,\n",
      "       0.9179755 , 0.93931594, 0.84240663, 0.92677615, 0.85360815])}, 'ATTACH::5::best_val_loss': 0.7632329067543372, 'ATTACH::6::history': {'train_accs': array([0.45749683, 0.53847302, 0.60183596, 0.63900291, 0.66549743,\n",
      "       0.69669378, 0.71139637, 0.72878573, 0.75281737, 0.76393761,\n",
      "       0.77252034, 0.77572953, 0.78610344, 0.79446227, 0.80573177,\n",
      "       0.80640346, 0.81073214, 0.80953803, 0.79505933, 0.80185088,\n",
      "       0.81319501, 0.81737443, 0.82700202, 0.84028659, 0.84946638,\n",
      "       0.85939249, 0.86506456, 0.87611016, 0.88200612, 0.8867826 ,\n",
      "       0.8920815 , 0.89820136, 0.90976939, 0.9086499 , 0.91775506,\n",
      "       0.92663632, 0.92156131, 0.92671095, 0.930965  , 0.92984551,\n",
      "       0.93663706, 0.93909993, 0.93738339, 0.94230913, 0.94350325,\n",
      "       0.94865288, 0.95096649, 0.95126502, 0.95059333, 0.95380252,\n",
      "       0.95641466, 0.96260915, 0.95850437, 0.95663856, 0.95992238,\n",
      "       0.96231062, 0.96320621]), 'train_losses': array([10.28461409,  1.88016537,  1.08083187,  0.85526235,  0.77543537,\n",
      "        0.6945434 ,  0.66318119,  0.61195389,  0.56848967,  0.55633675,\n",
      "        0.53290865,  0.52323318,  0.50246141,  0.49778869,  0.46729611,\n",
      "        0.47640693,  0.46499139,  0.45950074,  0.51157606,  0.48911455,\n",
      "        0.46729664,  0.45469846,  0.44182287,  0.41777808,  0.38947365,\n",
      "        0.36845298,  0.35192567,  0.32984073,  0.30964102,  0.29354044,\n",
      "        0.29001315,  0.26412506,  0.24680605,  0.24563446,  0.22278771,\n",
      "        0.21139419,  0.21555856,  0.20542503,  0.19086591,  0.20100904,\n",
      "        0.1781772 ,  0.17758913,  0.17723164,  0.15788206,  0.15808639,\n",
      "        0.1489121 ,  0.14938654,  0.14394381,  0.14287299,  0.13699515,\n",
      "        0.13268853,  0.11498275,  0.12256703,  0.12781895,  0.11606214,\n",
      "        0.1129247 ,  0.10988298]), 'val_accs': array([0.64507463, 0.58835821, 0.72238806, 0.57940299, 0.70477612,\n",
      "       0.77343284, 0.71313433, 0.75313433, 0.7561194 , 0.75343284,\n",
      "       0.81343284, 0.70865672, 0.85044776, 0.87343284, 0.83402985,\n",
      "       0.89343284, 0.83522388, 0.74029851, 0.8361194 , 0.74507463,\n",
      "       0.78656716, 0.81223881, 0.83850746, 0.88119403, 0.78835821,\n",
      "       0.85970149, 0.87432836, 0.88925373, 0.90089552, 0.93253731,\n",
      "       0.9158209 , 0.9041791 , 0.93044776, 0.90656716, 0.91761194,\n",
      "       0.92955224, 0.92208955, 0.92746269, 0.91850746, 0.93731343,\n",
      "       0.92149254, 0.95313433, 0.94      , 0.9438806 , 0.96686567,\n",
      "       0.92179104, 0.97641791, 0.97074627, 0.90477612, 0.94716418,\n",
      "       0.96686567, 0.94865672, 0.95910448, 0.96268657, 0.96716418,\n",
      "       0.95940299, 0.94      ]), 'val_losses': array([0.86761109, 1.29953945, 0.67891543, 1.52950186, 0.57562212,\n",
      "       0.5617325 , 0.81873995, 0.49132487, 0.61423297, 0.52171629,\n",
      "       0.46137767, 0.62177374, 0.36874032, 0.37468075, 0.38265444,\n",
      "       0.31994197, 0.37066958, 0.77536735, 0.38704052, 0.56536023,\n",
      "       0.58655384, 0.49625434, 0.44824004, 0.31426827, 0.66505303,\n",
      "       0.32611544, 0.29686374, 0.25228866, 0.24558254, 0.17533619,\n",
      "       0.20760995, 0.22641647, 0.18381537, 0.23062563, 0.19833628,\n",
      "       0.1786866 , 0.19900829, 0.17621191, 0.21075154, 0.17361   ,\n",
      "       0.19234703, 0.12282548, 0.14133767, 0.17565835, 0.10169987,\n",
      "       0.24026636, 0.07889489, 0.08415431, 0.27200768, 0.14545914,\n",
      "       0.08767539, 0.15982639, 0.1247576 , 0.1016171 , 0.09734193,\n",
      "       0.12524505, 0.17442094])}, 'ATTACH::6::best_val_loss': 0.07889489037777062, 'ATTACH::7::history': {'train_accs': array([0.43458467, 0.51608329, 0.60474662, 0.64310769, 0.67661766,\n",
      "       0.68796179, 0.69714158, 0.7174416 , 0.73804015, 0.73132323,\n",
      "       0.72535264, 0.74214494, 0.74542876, 0.75281737, 0.7510262 ,\n",
      "       0.74304053, 0.75132473, 0.74960818, 0.74520487, 0.7696843 ,\n",
      "       0.76072841, 0.75483245, 0.77110232, 0.76931114, 0.77259497,\n",
      "       0.75736995, 0.77632659, 0.76446003, 0.77759534, 0.7894619 ,\n",
      "       0.77632659, 0.77222181, 0.77080379, 0.78058064, 0.77207254,\n",
      "       0.76677364, 0.77916262, 0.78356594, 0.78453616, 0.77401299,\n",
      "       0.77744608, 0.78580491]), 'train_losses': array([11.66103055,  1.79381337,  1.0252379 ,  0.86311877,  0.7565277 ,\n",
      "        0.70990659,  0.6993192 ,  0.66004842,  0.60214631,  0.60733638,\n",
      "        0.61520329,  0.57939393,  0.58546947,  0.55518697,  0.57859959,\n",
      "        0.58442967,  0.5676731 ,  0.56971787,  0.57496137,  0.53358573,\n",
      "        0.55368288,  0.56789993,  0.53455148,  0.52202695,  0.51235475,\n",
      "        0.5567002 ,  0.50394602,  0.52838164,  0.51015138,  0.48084866,\n",
      "        0.51784205,  0.52755139,  0.51771811,  0.50041568,  0.52840759,\n",
      "        0.52420718,  0.49212287,  0.48712262,  0.50231785,  0.50419395,\n",
      "        0.50891158,  0.48136816]), 'val_accs': array([0.48656716, 0.58626866, 0.63283582, 0.60477612, 0.7319403 ,\n",
      "       0.67044776, 0.72179104, 0.7480597 , 0.73850746, 0.68358209,\n",
      "       0.75850746, 0.78746269, 0.83164179, 0.78268657, 0.76089552,\n",
      "       0.82686567, 0.83104478, 0.80119403, 0.82358209, 0.83373134,\n",
      "       0.73223881, 0.80179104, 0.8038806 , 0.79701493, 0.81641791,\n",
      "       0.79731343, 0.81850746, 0.76      , 0.8238806 , 0.79462687,\n",
      "       0.82925373, 0.84895522, 0.75283582, 0.78507463, 0.82298507,\n",
      "       0.83641791, 0.79552239, 0.82746269, 0.83880597, 0.78537313,\n",
      "       0.7958209 , 0.78895522]), 'val_losses': array([1.33677603, 0.87293748, 0.9366741 , 0.85028129, 0.57973977,\n",
      "       0.73142707, 0.64790824, 0.56290683, 0.57231959, 0.65638124,\n",
      "       0.51187532, 0.49755899, 0.46886919, 0.50052202, 0.56004566,\n",
      "       0.42335448, 0.43385762, 0.47283322, 0.40710615, 0.42573968,\n",
      "       0.55093799, 0.43494799, 0.44689094, 0.43458145, 0.42794379,\n",
      "       0.4678133 , 0.4246833 , 0.45427396, 0.40374894, 0.44076055,\n",
      "       0.42033537, 0.34060767, 0.49183335, 0.39202505, 0.37272338,\n",
      "       0.38983079, 0.40580966, 0.41744266, 0.39901802, 0.54052353,\n",
      "       0.48327277, 0.40004244])}, 'ATTACH::7::best_val_loss': 0.3406076749758934, 'ATTACH::8::history': {'train_accs': array([0.40017912, 0.43719681, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718]), 'train_losses': array([20.48074749,  1.18028435,  1.32494981,  1.29780011,  1.27978839,\n",
      "        1.26648379,  1.25594214,  1.24707561,  1.23910318,  1.23113773,\n",
      "        1.22343698]), 'val_accs': array([0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955]), 'val_losses': array([1.20954127, 1.35045074, 1.30751582, 1.28675669, 1.27150642,\n",
      "       1.25977662, 1.25042431, 1.24102252, 1.23281743, 1.2253318 ,\n",
      "       1.21687022])}, 'ATTACH::8::best_val_loss': 1.209541274753969, 'ATTACH::9::history': {'train_accs': array([0.31748638, 0.3462945 , 0.36084782, 0.37122173, 0.37898351,\n",
      "       0.38890962, 0.39808941, 0.39443242, 0.40637361, 0.41017986,\n",
      "       0.41786701, 0.42801702, 0.42876334, 0.44130159, 0.44734682,\n",
      "       0.4539891 , 0.45555638, 0.4652586 , 0.47279648, 0.47697589,\n",
      "       0.49578327, 0.48234943, 0.49787298, 0.4988432 , 0.50824688,\n",
      "       0.52145683, 0.52339727, 0.5347414 , 0.53869692, 0.54630943,\n",
      "       0.55198149, 0.56399731, 0.57168445, 0.58832749, 0.60116427,\n",
      "       0.60720949, 0.6178073 , 0.62646466, 0.63153967, 0.6481827 ,\n",
      "       0.65751175, 0.66810956, 0.67549817, 0.68057318, 0.69564893,\n",
      "       0.70594821, 0.71781476, 0.71945668, 0.73483096, 0.73736846,\n",
      "       0.74610046, 0.75072767, 0.75729532, 0.76975894, 0.77669975,\n",
      "       0.77961042, 0.7894619 , 0.80282111, 0.80946339, 0.81222479]), 'train_losses': array([80.31123125, 51.76823835, 38.71082495, 29.53171489, 23.48774044,\n",
      "       19.3204622 , 16.98648486, 15.03010401, 13.07612414, 11.98900943,\n",
      "       10.80451017,  9.80682005,  9.08160379,  8.19487592,  7.74969875,\n",
      "        7.11766926,  6.59452538,  6.10656661,  5.87262114,  5.35166837,\n",
      "        4.99728542,  4.7097939 ,  4.63390238,  4.31194986,  4.01310866,\n",
      "        3.68948818,  3.47808718,  3.2743165 ,  3.23488413,  3.03000735,\n",
      "        2.87484597,  2.67115854,  2.52161313,  2.34864366,  2.25746368,\n",
      "        2.10664993,  1.99890255,  1.88055723,  1.83261463,  1.73554596,\n",
      "        1.63162989,  1.55270448,  1.47752434,  1.4049219 ,  1.32824744,\n",
      "        1.2641479 ,  1.22232763,  1.18001611,  1.09686003,  1.07137437,\n",
      "        1.00030873,  0.97840928,  0.96234777,  0.90850139,  0.86354521,\n",
      "        0.81498895,  0.78197094,  0.73954747,  0.69640774,  0.67396278]), 'val_accs': array([0.45104478, 0.44567164, 0.44567164, 0.41283582, 0.37462687,\n",
      "       0.37641791, 0.35104478, 0.34358209, 0.3841791 , 0.43134328,\n",
      "       0.45701493, 0.52776119, 0.44149254, 0.61940299, 0.54835821,\n",
      "       0.6558209 , 0.62716418, 0.64358209, 0.60656716, 0.68179104,\n",
      "       0.66865672, 0.66477612, 0.68567164, 0.67343284, 0.6558209 ,\n",
      "       0.67850746, 0.6761194 , 0.68776119, 0.70328358, 0.66716418,\n",
      "       0.6838806 , 0.69373134, 0.69343284, 0.73492537, 0.73462687,\n",
      "       0.71134328, 0.77402985, 0.69820896, 0.76716418, 0.7280597 ,\n",
      "       0.79492537, 0.77850746, 0.77850746, 0.76776119, 0.81432836,\n",
      "       0.74865672, 0.81014925, 0.80925373, 0.79701493, 0.78716418,\n",
      "       0.8041791 , 0.85074627, 0.82208955, 0.82149254, 0.82597015,\n",
      "       0.85432836, 0.8158209 , 0.83910448, 0.87074627, 0.81074627]), 'val_losses': array([44.46155364, 27.41051877, 15.80131667, 10.24133969,  8.62151332,\n",
      "        7.06332313,  6.57010168,  5.46450679,  4.08857412,  3.22432383,\n",
      "        2.81043779,  2.30379097,  3.32998231,  1.87571417,  1.90339797,\n",
      "        1.70849831,  1.90373188,  1.60919623,  1.7746781 ,  1.45187967,\n",
      "        1.60266434,  1.54899448,  1.44240045,  1.484823  ,  1.43755881,\n",
      "        1.3874842 ,  1.41085155,  1.36585986,  1.29810561,  1.64193928,\n",
      "        1.41958963,  1.19816256,  1.34164935,  1.10630359,  1.05911677,\n",
      "        1.19129178,  0.84565466,  1.02954002,  0.8791091 ,  1.00228321,\n",
      "        0.75120237,  0.83253309,  0.82094479,  0.8433468 ,  0.65132909,\n",
      "        0.74780791,  0.69371475,  0.68007373,  0.65222147,  0.67812627,\n",
      "        0.6098954 ,  0.52270848,  0.60644971,  0.58225911,  0.60105879,\n",
      "        0.50572343,  0.67865972,  0.5877797 ,  0.44644506,  0.66175909])}, 'ATTACH::9::best_val_loss': 0.4464450604880034, 'ATTACH::10::history': {'train_accs': array([0.46107919, 0.56108665, 0.62982312, 0.66721397, 0.68893201,\n",
      "       0.70684379, 0.7174416 , 0.73781625, 0.74654825, 0.74856333,\n",
      "       0.74557803, 0.7259497 , 0.73326368, 0.74587656, 0.76087768,\n",
      "       0.74192104, 0.73363684, 0.74169714, 0.75580267]), 'train_losses': array([7.50155654, 1.20681984, 0.85795282, 0.75262085, 0.70274945,\n",
      "       0.65249787, 0.64193506, 0.59569122, 0.57934662, 0.56706375,\n",
      "       0.58575134, 0.61198369, 0.60764297, 0.57504738, 0.55213621,\n",
      "       0.59066359, 0.60321785, 0.58550557, 0.55256516]), 'val_accs': array([0.64208955, 0.59223881, 0.67044776, 0.73462687, 0.73373134,\n",
      "       0.75641791, 0.68686567, 0.74746269, 0.75104478, 0.71134328,\n",
      "       0.74298507, 0.73223881, 0.72925373, 0.75402985, 0.73164179,\n",
      "       0.67373134, 0.70746269, 0.71223881, 0.69313433]), 'val_losses': array([0.89016212, 1.0971507 , 0.69513212, 0.61494055, 0.63689506,\n",
      "       0.57114488, 0.65326716, 0.60051612, 0.54305026, 0.60798709,\n",
      "       0.58721244, 0.6250279 , 0.5889805 , 0.67351047, 0.65508156,\n",
      "       0.8922128 , 0.69074005, 0.81284062, 0.75019781])}, 'ATTACH::10::best_val_loss': 0.5430502586400331, 'ATTACH::11::history': {'train_accs': array([0.43921188, 0.49018583, 0.52742742, 0.52772595, 0.50347041,\n",
      "       0.48033435, 0.47861781, 0.45555638, 0.45249645, 0.4657064 ,\n",
      "       0.45533249, 0.44242108, 0.44100306, 0.43615195, 0.4597358 ,\n",
      "       0.43525636]), 'train_losses': array([19.40853503,  1.02712712,  0.96014293,  0.9413378 ,  0.95636006,\n",
      "        1.00247267,  1.02333419,  1.11480694,  1.14875658,  1.05176973,\n",
      "        1.17622583,  1.15191162,  1.0862252 ,  1.71733197,  1.00514077,\n",
      "        1.27372823]), 'val_accs': array([0.50776119, 0.58298507, 0.5041791 , 0.53014925, 0.46746269,\n",
      "       0.52865672, 0.51970149, 0.51253731, 0.49850746, 0.54507463,\n",
      "       0.48716418, 0.38925373, 0.33880597, 0.44208955, 0.47283582,\n",
      "       0.44208955]), 'val_losses': array([1.05119034, 0.87554442, 0.87317797, 0.92307824, 1.01867613,\n",
      "       0.87175023, 0.94022879, 1.0406575 , 1.07097044, 0.92407901,\n",
      "       0.97448098, 0.96175813, 1.16948889, 0.97804717, 0.99727453,\n",
      "       1.24101387])}, 'ATTACH::11::best_val_loss': 0.8717502259496432, 'ATTACH::12::history': {'train_accs': array([0.44779461, 0.5289947 , 0.59974625, 0.61982237, 0.62877827,\n",
      "       0.63236062, 0.62706172, 0.64661542, 0.6318382 , 0.61101575,\n",
      "       0.60586611, 0.60847824, 0.60914994, 0.62736025, 0.63377864,\n",
      "       0.64833197, 0.64086872, 0.66012389, 0.65534741, 0.66281066,\n",
      "       0.6284051 , 0.59728338, 0.63751026, 0.63124114, 0.63392791,\n",
      "       0.64221211, 0.63952534]), 'train_losses': array([16.3524163 ,  1.28337158,  0.90265882,  0.82967839,  0.8211052 ,\n",
      "        0.80044632,  0.80165144,  0.77041449,  0.79425035,  0.80629177,\n",
      "        0.79994426,  0.80730989,  0.83989372,  0.77593784,  0.75662964,\n",
      "        0.73453782,  0.73523634,  0.70422021,  0.71974847,  0.70059907,\n",
      "        0.81157146,  0.82446034,  0.74806082,  0.77123832,  0.74776278,\n",
      "        0.72531762,  0.77522805]), 'val_accs': array([0.50865672, 0.54298507, 0.58776119, 0.59134328, 0.6241791 ,\n",
      "       0.55104478, 0.69910448, 0.61910448, 0.6558209 , 0.53552239,\n",
      "       0.66776119, 0.5719403 , 0.64656716, 0.67343284, 0.61164179,\n",
      "       0.72985075, 0.73791045, 0.67731343, 0.60865672, 0.70895522,\n",
      "       0.62089552, 0.65910448, 0.52149254, 0.61432836, 0.53462687,\n",
      "       0.6919403 , 0.68686567]), 'val_losses': array([0.94418007, 0.97902587, 0.84588273, 0.89935116, 0.72896645,\n",
      "       1.03610275, 0.65906237, 0.77544356, 0.76745467, 0.82855014,\n",
      "       0.74461157, 0.79603173, 0.73674773, 0.70760197, 0.73846935,\n",
      "       0.64925021, 0.63095977, 0.68784628, 0.76139292, 0.7065895 ,\n",
      "       0.80242823, 0.67804767, 0.84041255, 0.77091929, 0.98271878,\n",
      "       0.64185559, 0.70953246])}, 'ATTACH::12::best_val_loss': 0.6309597676903455, 'ATTACH::13::history': {'train_accs': array([0.36472871, 0.40614971, 0.42540488, 0.44055527, 0.46376595,\n",
      "       0.48309575, 0.50526159, 0.52832301, 0.5398164 , 0.55795209,\n",
      "       0.57504291, 0.59407418, 0.62474812, 0.65079484, 0.67146802,\n",
      "       0.69744011, 0.72632286, 0.75423539, 0.77640122, 0.79759684,\n",
      "       0.82491231, 0.85469065, 0.87215464, 0.88723039, 0.90275394,\n",
      "       0.91581461, 0.92230763, 0.92723338, 0.93738339, 0.94141354,\n",
      "       0.94671244, 0.95163818, 0.95119039, 0.95828047, 0.96126577,\n",
      "       0.96231062, 0.96760952, 0.96887827, 0.97149041, 0.97253526,\n",
      "       0.97410254, 0.97701321, 0.97865512, 0.97969998, 0.97984924,\n",
      "       0.98059557, 0.98261064, 0.98417792, 0.98492425, 0.98611837,\n",
      "       0.98671543, 0.98514815, 0.98798418, 0.98798418, 0.98701396,\n",
      "       0.98887977, 0.98955146, 0.98992462, 0.9885066 , 0.98910366]), 'train_losses': array([23.91680951,  7.95981334,  4.64521671,  3.15128734,  2.33200629,\n",
      "        1.81102422,  1.45618184,  1.24431383,  1.11584561,  1.02539716,\n",
      "        0.95024373,  0.88436412,  0.82217461,  0.74853497,  0.71487769,\n",
      "        0.67232494,  0.62377787,  0.58132827,  0.53556544,  0.48880516,\n",
      "        0.44894754,  0.38947019,  0.34565531,  0.31570013,  0.28077729,\n",
      "        0.24865028,  0.22546797,  0.20886966,  0.18928658,  0.17266649,\n",
      "        0.15621599,  0.14424722,  0.14287116,  0.12273403,  0.11401428,\n",
      "        0.10546822,  0.09716615,  0.09022471,  0.08082879,  0.08214203,\n",
      "        0.07704157,  0.07124049,  0.061398  ,  0.05902628,  0.05575977,\n",
      "        0.05552892,  0.05120189,  0.05217319,  0.04649757,  0.04388345,\n",
      "        0.04196503,  0.04371836,  0.03466155,  0.03679276,  0.040774  ,\n",
      "        0.03553178,  0.03171242,  0.03158613,  0.03442304,  0.03323056]), 'val_accs': array([0.45223881, 0.48477612, 0.47402985, 0.50059701, 0.55791045,\n",
      "       0.55104478, 0.60328358, 0.6158209 , 0.64268657, 0.67134328,\n",
      "       0.62268657, 0.6280597 , 0.61044776, 0.69731343, 0.67074627,\n",
      "       0.71432836, 0.76059701, 0.66537313, 0.79791045, 0.83671642,\n",
      "       0.85970149, 0.88895522, 0.87343284, 0.82567164, 0.9158209 ,\n",
      "       0.92179104, 0.92716418, 0.9480597 , 0.93641791, 0.93671642,\n",
      "       0.95044776, 0.9438806 , 0.94029851, 0.94626866, 0.95880597,\n",
      "       0.95791045, 0.95134328, 0.94      , 0.93850746, 0.96835821,\n",
      "       0.95820896, 0.96507463, 0.97402985, 0.95402985, 0.97522388,\n",
      "       0.97522388, 0.98059701, 0.95970149, 0.93044776, 0.97074627,\n",
      "       0.97492537, 0.9758209 , 0.9758209 , 0.97253731, 0.97910448,\n",
      "       0.97940299, 0.95940299, 0.9758209 , 0.97522388, 0.97671642]), 'val_losses': array([5.22339803, 2.29682194, 1.7171047 , 1.88725172, 1.12969216,\n",
      "       1.07382245, 1.10897815, 0.99456845, 0.81621406, 0.7633593 ,\n",
      "       0.806256  , 0.74404331, 0.77907471, 0.63599481, 0.62622205,\n",
      "       0.58510382, 0.57374906, 0.61247193, 0.489005  , 0.39051233,\n",
      "       0.37869701, 0.29604397, 0.31117944, 0.43139962, 0.21602064,\n",
      "       0.21761761, 0.20219965, 0.1517933 , 0.17692875, 0.17355462,\n",
      "       0.13409978, 0.15118181, 0.17571557, 0.15898973, 0.11554664,\n",
      "       0.12527027, 0.12595321, 0.17451871, 0.18742821, 0.09763937,\n",
      "       0.11311098, 0.09548645, 0.07640024, 0.13245261, 0.07750671,\n",
      "       0.07396808, 0.06499197, 0.12626875, 0.21814571, 0.07790474,\n",
      "       0.07963786, 0.07401234, 0.07604616, 0.08108119, 0.06336223,\n",
      "       0.06264183, 0.12750602, 0.08328764, 0.08347167, 0.08112158])}, 'ATTACH::13::best_val_loss': 0.06264183430751757, 'ATTACH::14::history': {'train_accs': array([0.43622658, 0.5066796 , 0.52474065, 0.50585865, 0.5126502 ,\n",
      "       0.50570938, 0.47966266, 0.47996119, 0.48697664, 0.46906486,\n",
      "       0.48533473, 0.49197701]), 'train_losses': array([16.59162427,  1.04444196,  0.98642141,  1.02280238,  0.99652617,\n",
      "        1.01757064,  1.06715254,  1.07885806,  1.1126712 ,  1.16817655,\n",
      "        1.24929709,  1.06935932]), 'val_accs': array([0.44149254, 0.5919403 , 0.33522388, 0.48955224, 0.51044776,\n",
      "       0.50358209, 0.4158209 , 0.51253731, 0.45432836, 0.47910448,\n",
      "       0.50507463, 0.51761194]), 'val_losses': array([1.07920251, 0.89478026, 1.14383276, 0.93601488, 0.94283722,\n",
      "       1.11586106, 1.05096936, 0.98984072, 0.95518059, 0.97272369,\n",
      "       0.94738953, 0.94055405])}, 'ATTACH::14::best_val_loss': 0.8947802621926835, 'ATTACH::15::history': {'train_accs': array([0.43279349, 0.52996492, 0.59079036, 0.59780581, 0.63855512,\n",
      "       0.65198895, 0.65952683, 0.68057318, 0.67743861, 0.70035077,\n",
      "       0.70139563, 0.70856034, 0.69990298, 0.70550041, 0.71482946,\n",
      "       0.71348608, 0.71475483, 0.72191955, 0.73177103, 0.73669677,\n",
      "       0.72886036, 0.7310247 , 0.74624972, 0.76498246, 0.76804239,\n",
      "       0.76513173, 0.76386297, 0.775431  , 0.77356519, 0.78334204,\n",
      "       0.7535637 , 0.75960893, 0.77229644, 0.77759534, 0.77849093,\n",
      "       0.79155161, 0.79707441, 0.79453691, 0.79811926]), 'train_losses': array([10.34711453,  1.62186574,  1.0416416 ,  0.97125597,  0.81089459,\n",
      "        0.75538208,  0.74646628,  0.68970556,  0.68980297,  0.64993783,\n",
      "        0.63153793,  0.62538217,  0.64564028,  0.62264069,  0.61042835,\n",
      "        0.61280858,  0.60951928,  0.59875364,  0.58116232,  0.57964893,\n",
      "        0.58352152,  0.58543458,  0.55378369,  0.53029935,  0.52069772,\n",
      "        0.52216344,  0.53428286,  0.5196427 ,  0.50500561,  0.49330979,\n",
      "        0.57751867,  0.56045862,  0.53563221,  0.53450916,  0.52857272,\n",
      "        0.48560757,  0.48418558,  0.48720682,  0.49438057]), 'val_accs': array([0.56776119, 0.6038806 , 0.72925373, 0.7241791 , 0.71104478,\n",
      "       0.64656716, 0.73761194, 0.78626866, 0.73402985, 0.77970149,\n",
      "       0.8119403 , 0.76746269, 0.71701493, 0.75134328, 0.82298507,\n",
      "       0.80597015, 0.74597015, 0.75910448, 0.7719403 , 0.80776119,\n",
      "       0.70686567, 0.69074627, 0.77134328, 0.79164179, 0.79074627,\n",
      "       0.76477612, 0.80119403, 0.73164179, 0.80716418, 0.76686567,\n",
      "       0.74029851, 0.75313433, 0.78835821, 0.77641791, 0.7880597 ,\n",
      "       0.78895522, 0.7638806 , 0.79731343, 0.73432836]), 'val_losses': array([2.01829684, 0.87601939, 0.64516289, 0.66609447, 0.63131948,\n",
      "       0.74823201, 0.61022831, 0.55381426, 0.63261293, 0.51061709,\n",
      "       0.47177381, 0.57657808, 0.57033111, 0.59481432, 0.47326762,\n",
      "       0.48929793, 0.5407095 , 0.50603493, 0.50981244, 0.46252647,\n",
      "       0.61356584, 0.57111804, 0.45440275, 0.46036293, 0.50716668,\n",
      "       0.46017349, 0.47916389, 0.55037775, 0.45040244, 0.58018363,\n",
      "       0.59081548, 0.52913642, 0.49650913, 0.45327468, 0.5214967 ,\n",
      "       0.50331291, 0.48567839, 0.50170976, 0.60412568])}, 'ATTACH::15::best_val_loss': 0.4504024358827676, 'ATTACH::16::history': {'train_accs': array([0.33300993, 0.34950369, 0.36368386, 0.38555116, 0.39204418,\n",
      "       0.40465706, 0.4153295 , 0.42503172, 0.4374207 , 0.45503396,\n",
      "       0.46510934, 0.48309575, 0.49137995, 0.50242555, 0.52168072,\n",
      "       0.5260094 , 0.5428017 , 0.54787671, 0.55250392, 0.57235615,\n",
      "       0.58571535, 0.59280543, 0.59735801, 0.61094112, 0.62713635,\n",
      "       0.63654004, 0.64236137, 0.65042167, 0.66281066, 0.66788566,\n",
      "       0.6868423 , 0.69654452, 0.71482946, 0.70467945, 0.72333756,\n",
      "       0.72759161, 0.74438391, 0.74490634, 0.75901187, 0.76013135,\n",
      "       0.76699754, 0.77916262, 0.77729681, 0.78841705, 0.78983506,\n",
      "       0.79685051, 0.80543324, 0.80752295, 0.81468766, 0.81117994,\n",
      "       0.82550937, 0.82409135, 0.83453989, 0.84342115, 0.84394358,\n",
      "       0.84573476, 0.85125756, 0.85177998, 0.85476528, 0.85543697]), 'train_losses': array([50.07897694, 28.52209633, 18.76410628, 14.38823833, 11.91145766,\n",
      "        9.9572557 ,  8.46575056,  7.57346278,  6.49390954,  5.65230002,\n",
      "        5.20743387,  4.74762723,  4.25143375,  3.70241246,  3.46310227,\n",
      "        3.19314976,  2.899557  ,  2.72786348,  2.55447883,  2.32149285,\n",
      "        2.15894128,  2.04516217,  1.92818903,  1.79748936,  1.64386804,\n",
      "        1.54994397,  1.5284743 ,  1.39805691,  1.32935184,  1.29158034,\n",
      "        1.22752925,  1.15438377,  1.07733559,  1.03729195,  1.01744029,\n",
      "        0.96664906,  0.87954555,  0.88606532,  0.81573404,  0.80173974,\n",
      "        0.78201241,  0.75092172,  0.72409183,  0.68918622,  0.66310532,\n",
      "        0.63361178,  0.61952051,  0.60084068,  0.566174  ,  0.57251458,\n",
      "        0.5426939 ,  0.53375114,  0.51776423,  0.48702735,  0.47760064,\n",
      "        0.47248637,  0.46277122,  0.45679006,  0.43074165,  0.43843002]), 'val_accs': array([0.3680597 , 0.35104478, 0.31044776, 0.43880597, 0.39910448,\n",
      "       0.43044776, 0.46776119, 0.56268657, 0.52985075, 0.62895522,\n",
      "       0.55671642, 0.58985075, 0.55522388, 0.63940299, 0.59373134,\n",
      "       0.59164179, 0.53820896, 0.58149254, 0.55492537, 0.5641791 ,\n",
      "       0.59223881, 0.55761194, 0.65641791, 0.62447761, 0.52      ,\n",
      "       0.54447761, 0.62      , 0.55343284, 0.69492537, 0.72686567,\n",
      "       0.65641791, 0.62656716, 0.64328358, 0.76985075, 0.59492537,\n",
      "       0.7238806 , 0.77671642, 0.67791045, 0.66      , 0.7561194 ,\n",
      "       0.77641791, 0.73164179, 0.80537313, 0.86149254, 0.77970149,\n",
      "       0.73522388, 0.79671642, 0.79253731, 0.79731343, 0.85940299,\n",
      "       0.8519403 , 0.79014925, 0.83164179, 0.81373134, 0.87432836,\n",
      "       0.83940299, 0.85313433, 0.86597015, 0.83880597, 0.81910448]), 'val_losses': array([14.71014445,  7.70309496,  6.44411877,  5.4946072 ,  4.67794465,\n",
      "        3.5874703 ,  3.07809341,  2.98391331,  2.56503043,  2.01850294,\n",
      "        1.99079081,  2.09493948,  2.23257808,  1.49087884,  1.57111153,\n",
      "        1.49872089,  1.70413249,  1.56118945,  1.56180376,  1.49250293,\n",
      "        1.43241026,  1.38259551,  1.0035345 ,  1.44642898,  1.92040476,\n",
      "        1.71856272,  1.08123704,  1.51206721,  0.90302456,  0.77369549,\n",
      "        0.91285759,  0.92831576,  1.04303253,  0.64470313,  1.29515995,\n",
      "        0.75106504,  0.70670671,  0.80603761,  0.8880273 ,  0.60175011,\n",
      "        0.71161735,  0.6523547 ,  0.54863869,  0.40963394,  0.6250324 ,\n",
      "        0.79495276,  0.53174069,  0.5052061 ,  0.49211483,  0.35847768,\n",
      "        0.38858237,  0.58217705,  0.45107492,  0.54278614,  0.34768953,\n",
      "        0.48553845,  0.40340837,  0.37891975,  0.41075032,  0.4632644 ])}, 'ATTACH::16::best_val_loss': 0.34768953440794303, 'ATTACH::17::history': {'train_accs': array([0.4346593 , 0.43824166, 0.43816703, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718]), 'train_losses': array([811.43491604,   1.24128165,   1.5021779 ,   1.24454332,\n",
      "         1.24448571,   1.2437777 ,   1.24532495,   1.24419545,\n",
      "         1.24378542,   1.24385262,   1.24419851,   1.24347501]), 'val_accs': array([0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955]), 'val_losses': array([1.24662765, 1.2317137 , 1.24793872, 1.24208698, 1.24231151,\n",
      "       1.24185401, 1.24332356, 1.24217599, 1.24378492, 1.24443169,\n",
      "       1.24152083, 1.24721479])}, 'ATTACH::17::best_val_loss': 1.2317137049205267, 'ATTACH::18::history': {'train_accs': array([0.43622658, 0.51877006, 0.54586163, 0.5980297 , 0.62206135,\n",
      "       0.64124188, 0.6622136 , 0.67788641, 0.69609672, 0.71475483,\n",
      "       0.72012837, 0.74102545, 0.75289201, 0.76796776, 0.76528099,\n",
      "       0.77766998, 0.78811852, 0.7949847 , 0.80005971, 0.80252258,\n",
      "       0.80938876, 0.81386671, 0.82483767, 0.82528547, 0.82409135,\n",
      "       0.83595791, 0.82685275, 0.84006269, 0.84222703, 0.83849541,\n",
      "       0.84603329, 0.84274946, 0.85513844, 0.8529741 , 0.85103366,\n",
      "       0.85819837, 0.8529741 , 0.85110829]), 'train_losses': array([8.37420455, 1.75536258, 1.17707108, 0.91294033, 0.83750114,\n",
      "       0.78554508, 0.73423242, 0.69922602, 0.68031194, 0.63591089,\n",
      "       0.62815674, 0.59004815, 0.57165134, 0.54073278, 0.54117854,\n",
      "       0.52240425, 0.49526477, 0.49544196, 0.46666307, 0.47203795,\n",
      "       0.45593697, 0.45093846, 0.42275864, 0.43244032, 0.42179327,\n",
      "       0.38959603, 0.40871141, 0.38910844, 0.38849689, 0.38166446,\n",
      "       0.37622853, 0.38444993, 0.35271553, 0.35185053, 0.35408142,\n",
      "       0.34556527, 0.35746678, 0.36168532]), 'val_accs': array([0.58149254, 0.61791045, 0.60298507, 0.60447761, 0.60149254,\n",
      "       0.73462687, 0.71074627, 0.72716418, 0.73731343, 0.64238806,\n",
      "       0.77492537, 0.78716418, 0.76776119, 0.7361194 , 0.82656716,\n",
      "       0.81253731, 0.86208955, 0.75970149, 0.80567164, 0.84328358,\n",
      "       0.76298507, 0.83641791, 0.84895522, 0.84537313, 0.82029851,\n",
      "       0.80358209, 0.80238806, 0.86328358, 0.8080597 , 0.86865672,\n",
      "       0.86567164, 0.86537313, 0.84865672, 0.84865672, 0.86985075,\n",
      "       0.82119403, 0.79910448, 0.82626866]), 'val_losses': array([1.705598  , 1.10920358, 0.82232713, 0.82821996, 0.83986326,\n",
      "       0.65287736, 0.63859825, 0.58781936, 0.5785351 , 0.6715031 ,\n",
      "       0.52994775, 0.48677977, 0.51516964, 0.50694355, 0.44963303,\n",
      "       0.44993031, 0.3907516 , 0.59943566, 0.457579  , 0.41927457,\n",
      "       0.58174624, 0.37625854, 0.36555816, 0.36830051, 0.43911234,\n",
      "       0.44274555, 0.39418927, 0.33463577, 0.41911962, 0.34584078,\n",
      "       0.341295  , 0.38145122, 0.38635707, 0.35310808, 0.34433672,\n",
      "       0.41868162, 0.41766585, 0.3491148 ])}, 'ATTACH::18::best_val_loss': 0.33463576766092384, 'ATTACH::19::history': {'train_accs': array([0.29703709, 0.33442794, 0.34845884, 0.34248825, 0.35584745,\n",
      "       0.36898276, 0.37555041, 0.37010225, 0.37234122, 0.37816255,\n",
      "       0.38652138, 0.38831256, 0.39271587, 0.39808941, 0.4047317 ,\n",
      "       0.40204493, 0.40734383, 0.41271737, 0.41980745, 0.42839018,\n",
      "       0.43548026, 0.44271961, 0.43749534, 0.4431674 , 0.44771998,\n",
      "       0.45592955, 0.46682588, 0.47018434, 0.47040824, 0.48667811,\n",
      "       0.48951414, 0.49443988, 0.5098888 , 0.51272483, 0.51526233,\n",
      "       0.51294873, 0.51854616, 0.52951713, 0.53503993, 0.53668184,\n",
      "       0.54407045, 0.54265244, 0.55511605, 0.55354877, 0.55847451,\n",
      "       0.56078812, 0.56131055, 0.5708635 , 0.57355026, 0.57847601,\n",
      "       0.57832674, 0.58370028, 0.5842227 , 0.5929547 , 0.59661169,\n",
      "       0.61109038, 0.60579148, 0.61377715, 0.61347862, 0.62116576]), 'train_losses': array([74.27978389, 47.75994801, 38.98231898, 32.41579125, 26.43098978,\n",
      "       23.08150897, 20.69915202, 19.01889625, 17.11893274, 15.96575071,\n",
      "       14.68140591, 13.87134502, 12.86379876, 12.26267192, 11.28955183,\n",
      "       10.78599439,  9.88692042,  9.46707879,  8.84784049,  8.62992815,\n",
      "        7.90207149,  7.50307771,  7.29758687,  6.95180483,  6.57272828,\n",
      "        6.22868447,  5.87659128,  5.71774115,  5.55774698,  5.17251606,\n",
      "        5.07150344,  4.77808624,  4.50422286,  4.33214355,  4.17114209,\n",
      "        4.16092161,  4.10296522,  3.81895381,  3.66446133,  3.49969774,\n",
      "        3.37837143,  3.30361068,  3.15243919,  3.04973539,  2.91817032,\n",
      "        2.87548387,  2.77774156,  2.70475294,  2.61567567,  2.50973095,\n",
      "        2.42028164,  2.38237443,  2.29069234,  2.23580522,  2.11878689,\n",
      "        2.07132762,  2.02827217,  1.94158232,  1.94863837,  1.85258636]), 'val_accs': array([0.37940299, 0.40656716, 0.3761194 , 0.39522388, 0.36776119,\n",
      "       0.37223881, 0.40985075, 0.47134328, 0.46716418, 0.54      ,\n",
      "       0.56567164, 0.50298507, 0.57731343, 0.59432836, 0.53223881,\n",
      "       0.57970149, 0.60179104, 0.60089552, 0.58537313, 0.59223881,\n",
      "       0.56089552, 0.58179104, 0.56447761, 0.53880597, 0.56328358,\n",
      "       0.55343284, 0.59731343, 0.55402985, 0.63373134, 0.61552239,\n",
      "       0.62507463, 0.65402985, 0.60776119, 0.66776119, 0.6638806 ,\n",
      "       0.67761194, 0.6638806 , 0.71402985, 0.70358209, 0.68626866,\n",
      "       0.62179104, 0.66686567, 0.71164179, 0.66298507, 0.69432836,\n",
      "       0.68089552, 0.71014925, 0.69731343, 0.73014925, 0.68895522,\n",
      "       0.70149254, 0.71104478, 0.69880597, 0.74298507, 0.74238806,\n",
      "       0.67343284, 0.74686567, 0.72298507, 0.72656716, 0.76328358]), 'val_losses': array([19.63870896, 17.09911948, 11.39967645,  7.52655934,  6.57125069,\n",
      "        5.48291875,  5.13993793,  4.39583749,  3.43354862,  2.71506294,\n",
      "        2.87995817,  3.47397301,  2.43478961,  2.19208649,  2.71442133,\n",
      "        2.20236094,  2.11562832,  2.01027514,  1.90132917,  1.87662445,\n",
      "        2.25564132,  2.00387386,  2.29113242,  2.02791816,  2.01137908,\n",
      "        2.0784683 ,  1.61829253,  2.31710224,  1.67887868,  1.55702122,\n",
      "        1.62893682,  1.53457587,  1.54339548,  1.55888463,  1.58161975,\n",
      "        1.36365507,  1.53950792,  1.37393718,  1.3180128 ,  1.43091898,\n",
      "        1.91131845,  1.39301641,  1.16326285,  1.54018972,  1.34110051,\n",
      "        1.23465388,  1.26590612,  1.06482281,  1.06434818,  1.14976157,\n",
      "        1.18944507,  1.11797625,  1.18526911,  0.94092725,  0.97013362,\n",
      "        1.11861314,  0.85535852,  0.9412009 ,  1.03195689,  0.78890887])}, 'ATTACH::19::best_val_loss': 0.7889088672666408, 'ATTACH::20::history': {'train_accs': array([0.44473468, 0.45242182, 0.45018285, 0.4518994 , 0.454213  ,\n",
      "       0.45428763, 0.45510859, 0.45436227, 0.45451153, 0.45413837,\n",
      "       0.45354131, 0.45510859, 0.45413837, 0.45473543, 0.45503396,\n",
      "       0.45413837, 0.45481006, 0.45578028, 0.45518322, 0.45540712,\n",
      "       0.45600418, 0.45555638, 0.45458616, 0.45555638, 0.45495933,\n",
      "       0.45533249, 0.45406374, 0.45540712, 0.45533249, 0.45563102,\n",
      "       0.4544369 , 0.45458616, 0.45525786, 0.45563102, 0.45495933,\n",
      "       0.4546608 , 0.45540712, 0.45630271, 0.45451153]), 'train_losses': array([3.0295304 , 1.2930777 , 1.27551374, 1.25942737, 1.24532264,\n",
      "       1.23759283, 1.22876784, 1.22423977, 1.21929943, 1.21674377,\n",
      "       1.2157152 , 1.20997182, 1.20920743, 1.20705076, 1.20392576,\n",
      "       1.20524618, 1.2030503 , 1.20006039, 1.20078765, 1.19963956,\n",
      "       1.19681608, 1.19865961, 1.20014033, 1.19772389, 1.19924702,\n",
      "       1.19749909, 1.20062248, 1.19647407, 1.19698103, 1.1955027 ,\n",
      "       1.19945232, 1.20061261, 1.19837428, 1.1972989 , 1.19824734,\n",
      "       1.19927215, 1.19729267, 1.19550158, 1.20026723]), 'val_accs': array([0.45373134, 0.46      , 0.46      , 0.46      , 0.46      ,\n",
      "       0.46      , 0.46      , 0.46      , 0.46      , 0.46      ,\n",
      "       0.46      , 0.46      , 0.46      , 0.46      , 0.46      ,\n",
      "       0.46      , 0.46      , 0.46      , 0.46      , 0.46      ,\n",
      "       0.46      , 0.46      , 0.46      , 0.46      , 0.46      ,\n",
      "       0.46      , 0.46      , 0.46      , 0.46      , 0.46      ,\n",
      "       0.46      , 0.46      , 0.46      , 0.46      , 0.46      ,\n",
      "       0.46      , 0.46      , 0.46      , 0.46      ]), 'val_losses': array([1.29947799, 1.27624103, 1.25862803, 1.24431309, 1.23232062,\n",
      "       1.22399408, 1.21650802, 1.21102346, 1.20719048, 1.2033053 ,\n",
      "       1.20051764, 1.19859792, 1.19629697, 1.19450478, 1.19280057,\n",
      "       1.19147616, 1.19060725, 1.18981969, 1.18844756, 1.18790927,\n",
      "       1.18773228, 1.18667913, 1.18629075, 1.18625357, 1.18575989,\n",
      "       1.18580545, 1.1854005 , 1.18527052, 1.18461462, 1.18506776,\n",
      "       1.18650813, 1.18607217, 1.18591058, 1.18568191, 1.18585797,\n",
      "       1.18572999, 1.18572612, 1.1854264 , 1.18561491])}, 'ATTACH::20::best_val_loss': 1.1846146152980292, 'ATTACH::21::history': {'train_accs': array([0.41585193, 0.45861631, 0.48003582, 0.51205314, 0.53824912,\n",
      "       0.57608777, 0.61220987, 0.6401224 , 0.66400478, 0.69393238,\n",
      "       0.72415852, 0.747817  , 0.77125159, 0.79573102, 0.82506157,\n",
      "       0.84834689, 0.86804985, 0.8893201 , 0.89872379, 0.91066497,\n",
      "       0.93029331, 0.9332786 , 0.93954773, 0.94626465, 0.95133965,\n",
      "       0.95656392, 0.96216136, 0.96462423, 0.96604224, 0.97186357,\n",
      "       0.97089335, 0.972386  , 0.97611762, 0.97544593, 0.97895365,\n",
      "       0.97999851, 0.98081946, 0.98216285, 0.986193  , 0.98544668,\n",
      "       0.9864169 , 0.9857452 , 0.98671543, 0.98798418, 0.98776028,\n",
      "       0.9885066 , 0.98813344, 0.99029778, 0.99059631, 0.99044705,\n",
      "       0.99037242, 0.99134264, 0.99179043, 0.99179043, 0.99305918,\n",
      "       0.99216359, 0.99328308, 0.99216359, 0.99253676, 0.99380551]), 'train_losses': array([10.64380795,  4.16537923,  2.70883219,  1.87602764,  1.39810507,\n",
      "        1.15156735,  0.98539669,  0.87435643,  0.8051563 ,  0.72764183,\n",
      "        0.65699487,  0.59940662,  0.55385647,  0.50464076,  0.4385351 ,\n",
      "        0.39258824,  0.34498143,  0.29708432,  0.27121594,  0.24040896,\n",
      "        0.19946197,  0.18966598,  0.16711448,  0.15736773,  0.13491458,\n",
      "        0.12835186,  0.11185748,  0.10413291,  0.09825386,  0.0877275 ,\n",
      "        0.08761817,  0.08545382,  0.07120629,  0.07080408,  0.06262905,\n",
      "        0.0619858 ,  0.05720676,  0.05514802,  0.04406006,  0.04582382,\n",
      "        0.04042743,  0.04213565,  0.04140678,  0.03695001,  0.03362182,\n",
      "        0.03783587,  0.03693339,  0.03498524,  0.02771206,  0.02912476,\n",
      "        0.03328593,  0.02830411,  0.02671226,  0.02367122,  0.02455039,\n",
      "        0.02420563,  0.02110521,  0.02336434,  0.02342148,  0.02046092]), 'val_accs': array([0.45373134, 0.52865672, 0.60119403, 0.67492537, 0.68895522,\n",
      "       0.68985075, 0.73820896, 0.7241791 , 0.73432836, 0.74626866,\n",
      "       0.75820896, 0.80298507, 0.7958209 , 0.82686567, 0.86686567,\n",
      "       0.90298507, 0.92626866, 0.89791045, 0.92865672, 0.93283582,\n",
      "       0.88985075, 0.87522388, 0.96208955, 0.95462687, 0.94477612,\n",
      "       0.96477612, 0.95701493, 0.93402985, 0.97402985, 0.9761194 ,\n",
      "       0.97313433, 0.9719403 , 0.9719403 , 0.97701493, 0.97044776,\n",
      "       0.98686567, 0.98      , 0.98149254, 0.98      , 0.9719403 ,\n",
      "       0.98567164, 0.98238806, 0.98208955, 0.97791045, 0.98089552,\n",
      "       0.98567164, 0.98895522, 0.98089552, 0.98567164, 0.98656716,\n",
      "       0.98626866, 0.98089552, 0.98835821, 0.99134328, 0.9880597 ,\n",
      "       0.99014925, 0.97432836, 0.98686567, 0.9880597 , 0.99134328]), 'val_losses': array([2.37203157, 2.06998119, 1.02930555, 0.88325295, 0.76719025,\n",
      "       0.77476509, 0.61174646, 0.58892157, 0.64655594, 0.62960936,\n",
      "       0.56106137, 0.44475392, 0.45866973, 0.44778126, 0.3357316 ,\n",
      "       0.25211587, 0.20506696, 0.2522178 , 0.18526202, 0.17494165,\n",
      "       0.28554596, 0.33441468, 0.10053851, 0.12326052, 0.15405413,\n",
      "       0.09657921, 0.12313278, 0.17589937, 0.07290103, 0.07110301,\n",
      "       0.08239043, 0.07876659, 0.08023189, 0.06844245, 0.07392069,\n",
      "       0.0424462 , 0.05835807, 0.05510794, 0.05424988, 0.07666333,\n",
      "       0.03884223, 0.04049744, 0.05514667, 0.05865077, 0.04900993,\n",
      "       0.0411775 , 0.03230016, 0.05038168, 0.04273139, 0.04145854,\n",
      "       0.03997617, 0.05078389, 0.03456843, 0.03059375, 0.03726638,\n",
      "       0.03261531, 0.08299692, 0.04271462, 0.04007715, 0.0298604 ])}, 'ATTACH::21::best_val_loss': 0.029860404194193654, 'ATTACH::22::history': {'train_accs': array([0.35957907, 0.42689753, 0.4622733 , 0.49548474, 0.5262333 ,\n",
      "       0.53406971, 0.56011643, 0.57549071, 0.60161206, 0.64460034,\n",
      "       0.67266214, 0.69191731, 0.70632137, 0.71856109, 0.73886111,\n",
      "       0.76654974, 0.79095455, 0.80789611, 0.83200239, 0.84394358,\n",
      "       0.86804985, 0.87678185, 0.89417121, 0.90402269, 0.90976939,\n",
      "       0.92156131, 0.92865139, 0.93275618, 0.9394731 , 0.94335398,\n",
      "       0.94701097, 0.94954847, 0.95529517, 0.9583551 , 0.96059407,\n",
      "       0.96313158, 0.96910217, 0.9670871 , 0.96835585, 0.96947533,\n",
      "       0.97268453, 0.97335622, 0.97678931, 0.97693858, 0.97649078,\n",
      "       0.98052093, 0.98081946, 0.97992387, 0.98373013, 0.98380476,\n",
      "       0.98507351, 0.98507351, 0.98380476, 0.9836555 , 0.98686469,\n",
      "       0.9864169 , 0.9859691 , 0.99007389, 0.98865587, 0.98955146]), 'train_losses': array([23.55282391,  6.84025828,  3.88245838,  2.62924973,  1.93552699,\n",
      "        1.63100154,  1.32040346,  1.1874873 ,  1.05093506,  0.89976752,\n",
      "        0.81540863,  0.75311514,  0.71442065,  0.6846457 ,  0.63115047,\n",
      "        0.57957757,  0.52380414,  0.48355788,  0.43359056,  0.40180019,\n",
      "        0.34636273,  0.33034301,  0.28928184,  0.26548443,  0.24855454,\n",
      "        0.22550963,  0.1972456 ,  0.18689471,  0.17375838,  0.16126921,\n",
      "        0.15047618,  0.14297632,  0.13569261,  0.12176168,  0.11499017,\n",
      "        0.10976027,  0.09352424,  0.09802459,  0.08948693,  0.08563176,\n",
      "        0.08288364,  0.07921523,  0.07006948,  0.06677861,  0.0664828 ,\n",
      "        0.06126749,  0.05732249,  0.05893632,  0.05496928,  0.05095062,\n",
      "        0.04697254,  0.04638787,  0.04717466,  0.04790625,  0.03798648,\n",
      "        0.03995132,  0.03839191,  0.03302473,  0.03472668,  0.03285932]), 'val_accs': array([0.49880597, 0.57104478, 0.5758209 , 0.57373134, 0.60865672,\n",
      "       0.62835821, 0.62507463, 0.67313433, 0.73313433, 0.80208955,\n",
      "       0.64119403, 0.74      , 0.77223881, 0.74686567, 0.79701493,\n",
      "       0.69910448, 0.84626866, 0.82477612, 0.85492537, 0.85074627,\n",
      "       0.88686567, 0.92746269, 0.93641791, 0.92149254, 0.88985075,\n",
      "       0.94537313, 0.93820896, 0.96089552, 0.91462687, 0.95074627,\n",
      "       0.94985075, 0.9519403 , 0.96149254, 0.95104478, 0.93014925,\n",
      "       0.97313433, 0.96776119, 0.96238806, 0.96059701, 0.96925373,\n",
      "       0.96925373, 0.98149254, 0.97402985, 0.97134328, 0.97850746,\n",
      "       0.9838806 , 0.97104478, 0.96985075, 0.9838806 , 0.98328358,\n",
      "       0.98477612, 0.98      , 0.94      , 0.96895522, 0.98238806,\n",
      "       0.98328358, 0.9838806 , 0.98626866, 0.98149254, 0.98179104]), 'val_losses': array([4.34165103, 1.4962514 , 2.09609508, 1.67520401, 1.64326859,\n",
      "       0.92057785, 0.89422852, 0.75144177, 0.77272319, 0.58324334,\n",
      "       0.75907266, 0.66302329, 0.5499249 , 0.57520221, 0.47368234,\n",
      "       0.61001057, 0.37249385, 0.4047469 , 0.35342801, 0.37907273,\n",
      "       0.28990446, 0.20846323, 0.18118528, 0.2000305 , 0.26525345,\n",
      "       0.14779766, 0.16695153, 0.10952892, 0.22889134, 0.13943078,\n",
      "       0.13401974, 0.14044139, 0.10613496, 0.13245788, 0.18177698,\n",
      "       0.07167702, 0.0915153 , 0.11343788, 0.10714326, 0.08034622,\n",
      "       0.08789153, 0.05768733, 0.06813457, 0.07873056, 0.06296415,\n",
      "       0.04354111, 0.08403252, 0.0761301 , 0.04244984, 0.04594639,\n",
      "       0.03994861, 0.05421399, 0.18744779, 0.08907471, 0.05737845,\n",
      "       0.05074424, 0.04167525, 0.0346759 , 0.0519322 , 0.05664328])}, 'ATTACH::22::best_val_loss': 0.03467590105288954, 'ATTACH::23::history': {'train_accs': array([0.3046496 , 0.41383685, 0.43988357, 0.44048063, 0.43794313,\n",
      "       0.43518173, 0.43451004, 0.44010747, 0.46025823, 0.4705575 ,\n",
      "       0.48951414, 0.48533473, 0.48615568, 0.48929025, 0.49369356,\n",
      "       0.49317113, 0.50093291, 0.49697739, 0.49018583, 0.49772371,\n",
      "       0.49421599, 0.49921636, 0.49958952, 0.49623106, 0.49436525,\n",
      "       0.50063438, 0.49421599, 0.49137995, 0.49735055, 0.50518695,\n",
      "       0.50100754, 0.48234943, 0.49511158, 0.50279872, 0.50041048,\n",
      "       0.50145533, 0.50973953, 0.4986193 , 0.51078439, 0.50294798,\n",
      "       0.51399358, 0.4763042 , 0.46063139, 0.48212553, 0.47719979,\n",
      "       0.48466303, 0.50093291, 0.48682738, 0.47719979, 0.48272259]), 'train_losses': array([3.3543941 , 1.30627709, 1.26229911, 1.2387178 , 1.1830355 ,\n",
      "       1.1211523 , 1.08370808, 1.06689955, 1.0484056 , 1.03571079,\n",
      "       1.01824885, 1.0154342 , 1.01203803, 1.0005938 , 0.9990437 ,\n",
      "       0.99013717, 0.98838541, 0.99043609, 0.99011709, 0.99392181,\n",
      "       0.989206  , 0.98431616, 0.98013524, 0.98420986, 0.98289275,\n",
      "       0.97697218, 0.9768929 , 0.98552523, 0.9808908 , 0.96650295,\n",
      "       0.97505197, 1.01911299, 0.98343518, 0.97179516, 0.96921552,\n",
      "       0.98419809, 0.97558239, 0.98384622, 0.97208506, 0.99204611,\n",
      "       0.97449006, 1.05812321, 1.12663409, 1.04530802, 1.02467841,\n",
      "       1.0181274 , 1.01898909, 1.06763579, 1.02089471, 1.05077604]), 'val_accs': array([0.27432836, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.45940299, 0.51671642, 0.53671642,\n",
      "       0.50895522, 0.52537313, 0.4961194 , 0.55104478, 0.51223881,\n",
      "       0.52507463, 0.48208955, 0.53014925, 0.56447761, 0.47731343,\n",
      "       0.53820896, 0.53164179, 0.49462687, 0.50298507, 0.54835821,\n",
      "       0.48686567, 0.48865672, 0.55104478, 0.53014925, 0.58537313,\n",
      "       0.53910448, 0.50238806, 0.52626866, 0.55104478, 0.59402985,\n",
      "       0.51074627, 0.56208955, 0.5158209 , 0.51462687, 0.57253731,\n",
      "       0.5680597 , 0.52238806, 0.50895522, 0.4880597 , 0.47940299,\n",
      "       0.48179104, 0.54746269, 0.50358209, 0.49074627, 0.49164179]), 'val_losses': array([1.33798408, 1.3130947 , 1.29784768, 1.28661436, 1.15191318,\n",
      "       1.05193501, 1.04019181, 0.99912464, 0.98504688, 0.98862178,\n",
      "       0.97182498, 0.95430795, 0.99238836, 0.91705835, 0.92047419,\n",
      "       0.93338707, 0.97464517, 0.90902213, 0.90221033, 1.00691562,\n",
      "       0.93331309, 0.91794487, 0.94222439, 0.92486383, 0.91338775,\n",
      "       1.00695021, 0.95907186, 0.88628079, 0.90280999, 0.86851511,\n",
      "       0.91114821, 0.95287613, 0.89792369, 0.87800116, 0.87979823,\n",
      "       0.91421673, 0.8962351 , 0.98269322, 0.91927655, 0.85412741,\n",
      "       0.92480621, 0.91690656, 0.98074203, 1.00649712, 0.98380339,\n",
      "       0.96109651, 0.89484741, 0.971479  , 0.97564859, 0.969373  ])}, 'ATTACH::23::best_val_loss': 0.8541274130522315, 'ATTACH::24::history': {'train_accs': array([0.36898276, 0.42264348, 0.45316815, 0.47048287, 0.49115606,\n",
      "       0.5066796 , 0.53399507, 0.57146056, 0.5904172 , 0.61818046,\n",
      "       0.64571983, 0.67064706, 0.69460407, 0.71139637, 0.73468169,\n",
      "       0.76125084, 0.78237182, 0.80192552, 0.82364356, 0.83700276,\n",
      "       0.85073513, 0.86894544, 0.87976715, 0.89252929, 0.90200761,\n",
      "       0.91305321, 0.91805359, 0.93290544, 0.93798045, 0.94268229,\n",
      "       0.95066796, 0.95171281, 0.95783267, 0.96231062, 0.96484812,\n",
      "       0.9664154 , 0.97275916, 0.97059482, 0.97484887, 0.97455034,\n",
      "       0.97798343, 0.97895365, 0.98104336, 0.98126726, 0.98380476,\n",
      "       0.98462572, 0.98589447, 0.98544668, 0.98581984, 0.98753638,\n",
      "       0.98716322, 0.98537204, 0.98940219, 0.98626763, 0.9891783 ,\n",
      "       0.98940219, 0.9891783 , 0.99156653, 0.99179043, 0.9914919 ]), 'train_losses': array([26.42978629,  9.59125419,  6.2410837 ,  4.6437762 ,  3.65709492,\n",
      "        2.9600537 ,  2.35395216,  1.95028914,  1.71332673,  1.43915122,\n",
      "        1.27793104,  1.12181264,  1.02870156,  0.94684734,  0.84140769,\n",
      "        0.75715876,  0.65426627,  0.59191865,  0.53424544,  0.48478269,\n",
      "        0.43510018,  0.38973652,  0.36479473,  0.32472253,  0.301838  ,\n",
      "        0.2614225 ,  0.24295902,  0.20836511,  0.19355333,  0.18208101,\n",
      "        0.1485365 ,  0.14802227,  0.13012634,  0.12314464,  0.11290856,\n",
      "        0.10541319,  0.09097902,  0.08728771,  0.08191692,  0.07991945,\n",
      "        0.07010832,  0.0637469 ,  0.06418002,  0.06029216,  0.0537194 ,\n",
      "        0.05029807,  0.04522402,  0.04808198,  0.04818664,  0.04250496,\n",
      "        0.04488398,  0.04758029,  0.03536236,  0.04144335,  0.03342952,\n",
      "        0.03508782,  0.03708252,  0.03150847,  0.02884227,  0.03055088]), 'val_accs': array([0.35104478, 0.53134328, 0.52716418, 0.60477612, 0.61820896,\n",
      "       0.59462687, 0.59343284, 0.6841791 , 0.64447761, 0.70626866,\n",
      "       0.7041791 , 0.75044776, 0.75641791, 0.74537313, 0.81970149,\n",
      "       0.84537313, 0.78746269, 0.85820896, 0.86716418, 0.86179104,\n",
      "       0.89402985, 0.81283582, 0.90626866, 0.91283582, 0.90955224,\n",
      "       0.93731343, 0.94447761, 0.92268657, 0.9480597 , 0.95074627,\n",
      "       0.95462687, 0.95522388, 0.95910448, 0.95552239, 0.96119403,\n",
      "       0.96447761, 0.95641791, 0.9558209 , 0.96746269, 0.96149254,\n",
      "       0.96238806, 0.96835821, 0.96537313, 0.96776119, 0.97014925,\n",
      "       0.97343284, 0.97104478, 0.97074627, 0.96656716, 0.94716418,\n",
      "       0.9641791 , 0.97104478, 0.96716418, 0.96447761, 0.96865672,\n",
      "       0.98059701, 0.96835821, 0.9719403 , 0.96238806, 0.95910448]), 'val_losses': array([5.85930355, 1.71435873, 2.20650004, 1.28390356, 1.75117317,\n",
      "       1.20313828, 1.04371812, 0.85126902, 0.98269812, 0.94417733,\n",
      "       1.00651402, 0.81714695, 0.61614003, 0.68394261, 0.52895453,\n",
      "       0.47202133, 0.6547233 , 0.39256713, 0.39445626, 0.46310105,\n",
      "       0.30448808, 0.61657659, 0.26050198, 0.25917913, 0.25544861,\n",
      "       0.20047447, 0.18157067, 0.23235315, 0.15206203, 0.14928143,\n",
      "       0.13689709, 0.1316448 , 0.1249654 , 0.12720289, 0.10850339,\n",
      "       0.0988015 , 0.12730724, 0.12878499, 0.08986561, 0.10685096,\n",
      "       0.12633231, 0.08703336, 0.09522878, 0.09077217, 0.09520653,\n",
      "       0.08930913, 0.12520576, 0.08727771, 0.11077258, 0.20519506,\n",
      "       0.10731089, 0.08005211, 0.1099276 , 0.11332369, 0.10424973,\n",
      "       0.07799274, 0.10178842, 0.08152645, 0.1329387 , 0.12383887])}, 'ATTACH::24::best_val_loss': 0.0779927376518721, 'ATTACH::25::history': {'train_accs': array([0.39808941, 0.45548175, 0.47839391, 0.49832077, 0.52339727,\n",
      "       0.5510859 , 0.57131129, 0.57728189, 0.60340324, 0.61474737,\n",
      "       0.63862975, 0.6651989 , 0.67728935, 0.6978879 , 0.70624673,\n",
      "       0.7229644 , 0.72945742, 0.74281663, 0.74945891, 0.75804164,\n",
      "       0.76580342, 0.77296813, 0.78319278, 0.78587954, 0.79214867,\n",
      "       0.79707441, 0.80438839, 0.81207553, 0.81618031, 0.81647884,\n",
      "       0.82983805, 0.83147996, 0.8366296 , 0.84327189, 0.85439212,\n",
      "       0.85857154, 0.86581088, 0.87081125, 0.88618554, 0.89066348,\n",
      "       0.9029032 , 0.9061124 , 0.9196955 , 0.9226808 , 0.92872602,\n",
      "       0.93730875, 0.94283155, 0.94633928, 0.95022017, 0.95156355,\n",
      "       0.95716098, 0.96171356, 0.96007165, 0.96477349, 0.96768416,\n",
      "       0.96977386, 0.96843048, 0.96992313, 0.97268453, 0.97619225]), 'train_losses': array([11.83984882,  3.83988586,  2.53193891,  1.84620656,  1.48474692,\n",
      "        1.26513949,  1.14213972,  1.0562142 ,  0.95010606,  0.88478401,\n",
      "        0.81319206,  0.76226533,  0.72754943,  0.67391365,  0.66217581,\n",
      "        0.63268799,  0.6055156 ,  0.58396119,  0.57194052,  0.55195621,\n",
      "        0.54059582,  0.52868356,  0.51416741,  0.50513933,  0.48704042,\n",
      "        0.4811539 ,  0.46850983,  0.45678921,  0.45610629,  0.44390055,\n",
      "        0.42398084,  0.4277708 ,  0.41537264,  0.3904713 ,  0.38127285,\n",
      "        0.36307452,  0.35263181,  0.34086948,  0.3071212 ,  0.29841753,\n",
      "        0.26534996,  0.26346065,  0.23110243,  0.23206967,  0.20464962,\n",
      "        0.18948912,  0.16887176,  0.15794944,  0.15028752,  0.14251022,\n",
      "        0.12396807,  0.11710176,  0.12138402,  0.10595956,  0.10395399,\n",
      "        0.0944251 ,  0.09453283,  0.08884597,  0.0833489 ,  0.0733464 ]), 'val_accs': array([0.44865672, 0.49492537, 0.46955224, 0.45731343, 0.45552239,\n",
      "       0.49761194, 0.65492537, 0.59761194, 0.63761194, 0.73074627,\n",
      "       0.54029851, 0.70597015, 0.74149254, 0.74507463, 0.78447761,\n",
      "       0.57761194, 0.66238806, 0.64059701, 0.69223881, 0.86985075,\n",
      "       0.72656716, 0.7680597 , 0.7358209 , 0.79373134, 0.78298507,\n",
      "       0.8719403 , 0.85402985, 0.89134328, 0.78626866, 0.80447761,\n",
      "       0.84029851, 0.85970149, 0.88746269, 0.83134328, 0.88955224,\n",
      "       0.85880597, 0.92626866, 0.89074627, 0.88059701, 0.86686567,\n",
      "       0.81462687, 0.93223881, 0.81074627, 0.91253731, 0.89671642,\n",
      "       0.94985075, 0.96567164, 0.97044776, 0.96059701, 0.97432836,\n",
      "       0.97104478, 0.88776119, 0.94865672, 0.98238806, 0.9761194 ,\n",
      "       0.97164179, 0.97731343, 0.96835821, 0.98985075, 0.98985075]), 'val_losses': array([7.46188881, 2.03113834, 1.8593641 , 2.16501975, 2.87675058,\n",
      "       1.37650659, 0.7069492 , 1.00530327, 0.68521671, 0.63658152,\n",
      "       1.92660154, 0.61316368, 0.54451122, 0.61220668, 0.6201206 ,\n",
      "       1.13268067, 0.75430467, 1.14965145, 0.67093066, 0.37760612,\n",
      "       0.56300596, 0.49626521, 0.62166009, 0.48815484, 0.57003096,\n",
      "       0.31345943, 0.40285191, 0.30422256, 0.51172579, 0.39618449,\n",
      "       0.32429537, 0.36967968, 0.29433325, 0.38132153, 0.29932632,\n",
      "       0.32682301, 0.2347544 , 0.25282259, 0.28807571, 0.35335532,\n",
      "       0.61680662, 0.22150192, 0.53011723, 0.22893617, 0.35911839,\n",
      "       0.1441674 , 0.12642687, 0.11119682, 0.12212667, 0.0901388 ,\n",
      "       0.09094224, 0.36586912, 0.13622565, 0.06494461, 0.07899898,\n",
      "       0.09414317, 0.07254818, 0.09804719, 0.04400304, 0.03376296])}, 'ATTACH::25::best_val_loss': 0.0337629642393162, 'ATTACH::26::history': {'train_accs': array([0.41152325, 0.45757146, 0.48966341, 0.49988805, 0.518173  ,\n",
      "       0.53817449, 0.55048884, 0.56444511, 0.58064035, 0.59668632,\n",
      "       0.60989626, 0.63549519, 0.64885439, 0.66639301, 0.6868423 ,\n",
      "       0.69967908, 0.72221808, 0.7395328 , 0.74177177, 0.76513173,\n",
      "       0.78849168, 0.8064781 , 0.82170311, 0.84685424, 0.86394507,\n",
      "       0.8755131 , 0.89073811, 0.90320173, 0.91850138, 0.93312934,\n",
      "       0.9417867 , 0.94417494, 0.95425032, 0.95387716, 0.95850437,\n",
      "       0.96469886, 0.9643257 , 0.96954997, 0.97178894, 0.97544593,\n",
      "       0.97484887, 0.97984924, 0.97843123, 0.98059557, 0.98178969,\n",
      "       0.98290917, 0.98275991, 0.98634226, 0.98537204, 0.98455109,\n",
      "       0.9887305 , 0.98686469, 0.98723785, 0.98671543, 0.9885066 ,\n",
      "       0.98813344, 0.98947683, 0.98783491, 0.98940219, 0.99007389]), 'train_losses': array([8.84315883, 3.05548156, 2.19734952, 1.77965072, 1.55695443,\n",
      "       1.39448472, 1.29827819, 1.20483898, 1.11127435, 1.03566101,\n",
      "       0.94748731, 0.87703213, 0.81838817, 0.7636871 , 0.71537582,\n",
      "       0.67351411, 0.63274549, 0.60617173, 0.58500446, 0.55169194,\n",
      "       0.51150692, 0.47443562, 0.44200655, 0.40278054, 0.36141867,\n",
      "       0.33169468, 0.28504783, 0.26813765, 0.22159919, 0.19229305,\n",
      "       0.16785021, 0.15724479, 0.1402459 , 0.12773321, 0.11708922,\n",
      "       0.10165162, 0.10281299, 0.08803243, 0.081673  , 0.07640562,\n",
      "       0.07801461, 0.06355335, 0.0637103 , 0.05442992, 0.0531865 ,\n",
      "       0.05270695, 0.04957377, 0.04291431, 0.04487205, 0.04788015,\n",
      "       0.03533295, 0.04401406, 0.03622702, 0.04079708, 0.03327094,\n",
      "       0.03630903, 0.03287994, 0.03576724, 0.03232911, 0.02900728]), 'val_accs': array([0.54298507, 0.46597015, 0.42089552, 0.49671642, 0.60029851,\n",
      "       0.5958209 , 0.68149254, 0.58029851, 0.46626866, 0.73044776,\n",
      "       0.57462687, 0.64597015, 0.62119403, 0.7438806 , 0.53522388,\n",
      "       0.71880597, 0.72119403, 0.76835821, 0.83373134, 0.80567164,\n",
      "       0.73283582, 0.79910448, 0.78      , 0.85044776, 0.77731343,\n",
      "       0.84059701, 0.86059701, 0.93014925, 0.96567164, 0.95731343,\n",
      "       0.94507463, 0.88865672, 0.9641791 , 0.96      , 0.97492537,\n",
      "       0.97880597, 0.94059701, 0.95283582, 0.97850746, 0.93462687,\n",
      "       0.97820896, 0.97731343, 0.98119403, 0.97820896, 0.98179104,\n",
      "       0.98507463, 0.97761194, 0.98059701, 0.98447761, 0.98507463,\n",
      "       0.98567164, 0.98238806, 0.97313433, 0.9838806 , 0.98686567,\n",
      "       0.9838806 , 0.98089552, 0.94746269, 0.98656716, 0.98955224]), 'val_losses': array([2.08852261, 3.19537451, 4.10447298, 1.60330474, 1.0149632 ,\n",
      "       1.21824   , 0.68701926, 0.74716671, 1.97141415, 0.67817512,\n",
      "       1.09194752, 0.82426354, 1.2795094 , 0.50169918, 1.49631905,\n",
      "       0.6506032 , 0.5965813 , 0.52091948, 0.42694779, 0.45153617,\n",
      "       0.62213889, 0.50332579, 0.50284198, 0.39725086, 0.78373225,\n",
      "       0.4003277 , 0.37199891, 0.185179  , 0.11814031, 0.13525375,\n",
      "       0.14441859, 0.31788866, 0.10059166, 0.11501689, 0.07628149,\n",
      "       0.06207695, 0.17650104, 0.12419614, 0.06427865, 0.19293859,\n",
      "       0.06692824, 0.06324832, 0.06345887, 0.06326953, 0.05697655,\n",
      "       0.04344927, 0.05798423, 0.05618768, 0.04874402, 0.04669001,\n",
      "       0.04866961, 0.05694402, 0.08360267, 0.05033998, 0.03704852,\n",
      "       0.04118685, 0.05174787, 0.16684428, 0.04330594, 0.03655709])}, 'ATTACH::26::best_val_loss': 0.036557093162868, 'ATTACH::27::history': {'train_accs': array([0.41876259, 0.44219718, 0.44219718, 0.44219718, 0.44197328,\n",
      "       0.44257034, 0.44324203, 0.44346593, 0.44548101, 0.45473543,\n",
      "       0.4622733 , 0.46958728, 0.48637958, 0.48958878, 0.50235092,\n",
      "       0.504366  , 0.51205314, 0.51332189, 0.53026345, 0.56175834,\n",
      "       0.58564072, 0.60459736, 0.61534443, 0.62847974, 0.64168968,\n",
      "       0.65833271, 0.66676618, 0.68311068, 0.69124562, 0.70990372,\n",
      "       0.71079931, 0.72393462, 0.73438316, 0.73460706, 0.75393686,\n",
      "       0.76401224, 0.77363982, 0.77737145, 0.79080528, 0.803045  ,\n",
      "       0.80759758, 0.81894171, 0.8283454 , 0.82767371, 0.84700351,\n",
      "       0.85603403, 0.85566087, 0.87096052, 0.87133368, 0.87521457,\n",
      "       0.88752892, 0.8888723 , 0.89148444, 0.90185835, 0.90514217,\n",
      "       0.9086499 , 0.9058885 , 0.92253153, 0.92081499, 0.92603926]), 'train_losses': array([3.53314673, 1.26696613, 1.25115225, 1.24264067, 1.23787249,\n",
      "       1.23444222, 1.22945377, 1.22190949, 1.21131118, 1.19959387,\n",
      "       1.18434732, 1.17398038, 1.15972662, 1.15119019, 1.13867433,\n",
      "       1.12239664, 1.10693083, 1.08232855, 1.05402804, 1.01474706,\n",
      "       0.98525046, 0.95897986, 0.92839374, 0.90391191, 0.88146996,\n",
      "       0.85807345, 0.83008215, 0.79937829, 0.77351222, 0.74298026,\n",
      "       0.73155597, 0.7019114 , 0.68095353, 0.67797484, 0.62629603,\n",
      "       0.60242134, 0.57648709, 0.56309677, 0.52474618, 0.49899526,\n",
      "       0.49114845, 0.4682667 , 0.45060106, 0.44426253, 0.40796565,\n",
      "       0.39953949, 0.391099  , 0.36740692, 0.36009211, 0.34791448,\n",
      "       0.32029385, 0.31545724, 0.30969289, 0.27860949, 0.27186404,\n",
      "       0.26037828, 0.26707598, 0.22605405, 0.22405039, 0.21695555]), 'val_accs': array([0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44268657,\n",
      "       0.44238806, 0.44298507, 0.44746269, 0.45910448, 0.45492537,\n",
      "       0.44835821, 0.48567164, 0.47343284, 0.48507463, 0.48776119,\n",
      "       0.48477612, 0.50477612, 0.50567164, 0.51283582, 0.57014925,\n",
      "       0.58955224, 0.59910448, 0.6161194 , 0.58597015, 0.65432836,\n",
      "       0.64477612, 0.68089552, 0.71164179, 0.68447761, 0.68119403,\n",
      "       0.75432836, 0.7361194 , 0.71044776, 0.77850746, 0.77074627,\n",
      "       0.78089552, 0.75014925, 0.79970149, 0.79462687, 0.7880597 ,\n",
      "       0.81313433, 0.80179104, 0.80985075, 0.82268657, 0.85880597,\n",
      "       0.83761194, 0.8438806 , 0.84716418, 0.87492537, 0.84      ,\n",
      "       0.87313433, 0.87014925, 0.88835821, 0.87462687, 0.8561194 ,\n",
      "       0.8719403 , 0.86776119, 0.9041791 , 0.89701493, 0.90776119]), 'val_losses': array([1.27687764, 1.25534073, 1.24421023, 1.23760213, 1.23302447,\n",
      "       1.22862793, 1.22290692, 1.21989113, 1.20756545, 1.20283812,\n",
      "       1.1915828 , 1.16046197, 1.15364265, 1.14096104, 1.1275334 ,\n",
      "       1.11554386, 1.10052452, 1.13339405, 1.12121269, 0.973814  ,\n",
      "       0.96184943, 0.93235632, 0.92535065, 0.96789522, 0.84901772,\n",
      "       0.89001513, 0.8054827 , 0.73806096, 0.82045718, 0.76965389,\n",
      "       0.65191548, 0.67554484, 0.78647406, 0.57854143, 0.5685666 ,\n",
      "       0.56995743, 0.65573742, 0.50742635, 0.52467178, 0.56492589,\n",
      "       0.47500386, 0.5536561 , 0.51708835, 0.46313294, 0.37388562,\n",
      "       0.44265877, 0.43046647, 0.42986998, 0.3601249 , 0.46064913,\n",
      "       0.36132448, 0.37892191, 0.32073884, 0.3753087 , 0.4334663 ,\n",
      "       0.40984451, 0.40065595, 0.28976701, 0.30206288, 0.26599425])}, 'ATTACH::27::best_val_loss': 0.26599425463534115, 'ATTACH::28::history': {'train_accs': array([0.40017912, 0.44533174, 0.45779536, 0.47272184, 0.49764908,\n",
      "       0.51832226, 0.5345175 , 0.54907083, 0.56578849, 0.59758191,\n",
      "       0.62325547, 0.6456452 , 0.67363236, 0.71154564, 0.72736771,\n",
      "       0.74595119, 0.76110157, 0.78632734, 0.81565788, 0.83991343,\n",
      "       0.86103441, 0.87237854, 0.8918576 , 0.89902232, 0.91820285,\n",
      "       0.9224569 , 0.92969625, 0.93730875, 0.94596612, 0.94969774,\n",
      "       0.95820584, 0.9581312 , 0.96395253, 0.96760952, 0.96910217,\n",
      "       0.9698485 , 0.97089335, 0.97634152, 0.9776849 , 0.97850586,\n",
      "       0.98164042, 0.97843123, 0.98149116, 0.98193895, 0.98380476,\n",
      "       0.98417792, 0.98544668, 0.98529741, 0.98649153, 0.98761102,\n",
      "       0.98887977, 0.98947683, 0.9885066 , 0.99029778, 0.98753638,\n",
      "       0.98970072, 0.99126801, 0.99082021, 0.99089484, 0.99067095]), 'train_losses': array([16.81572533,  6.65575413,  4.18154515,  2.90979626,  2.20033217,\n",
      "        1.74892644,  1.47198474,  1.21394039,  1.07217982,  0.93189995,\n",
      "        0.83782761,  0.78326637,  0.71969176,  0.65254772,  0.6214175 ,\n",
      "        0.58201687,  0.54819022,  0.51133441,  0.45115969,  0.39935793,\n",
      "        0.35695657,  0.32835535,  0.29062886,  0.26788386,  0.23160974,\n",
      "        0.21751128,  0.19392103,  0.17903137,  0.15490807,  0.14679704,\n",
      "        0.12398111,  0.1191679 ,  0.10695426,  0.09528407,  0.0908651 ,\n",
      "        0.08714484,  0.07887876,  0.07256128,  0.06903411,  0.06354497,\n",
      "        0.05496716,  0.06134674,  0.05246154,  0.05259416,  0.05187573,\n",
      "        0.04555922,  0.04231765,  0.04173561,  0.03996851,  0.0382878 ,\n",
      "        0.03553956,  0.03362226,  0.03157671,  0.03040033,  0.03538953,\n",
      "        0.03443427,  0.0292874 ,  0.02886017,  0.02785079,  0.02896448]), 'val_accs': array([0.49552239, 0.46089552, 0.55701493, 0.52865672, 0.51701493,\n",
      "       0.6358209 , 0.54358209, 0.52597015, 0.5961194 , 0.68358209,\n",
      "       0.64686567, 0.5238806 , 0.68865672, 0.6680597 , 0.78119403,\n",
      "       0.6441791 , 0.76985075, 0.81432836, 0.76119403, 0.83313433,\n",
      "       0.89044776, 0.90179104, 0.8961194 , 0.72835821, 0.8961194 ,\n",
      "       0.92656716, 0.94      , 0.96955224, 0.88835821, 0.95223881,\n",
      "       0.96925373, 0.97940299, 0.97253731, 0.9680597 , 0.96268657,\n",
      "       0.97343284, 0.97432836, 0.97253731, 0.9638806 , 0.98477612,\n",
      "       0.96358209, 0.98716418, 0.98298507, 0.98298507, 0.97044776,\n",
      "       0.98865672, 0.9880597 , 0.97402985, 0.98985075, 0.96925373,\n",
      "       0.98716418, 0.89701493, 0.99402985, 0.98477612, 0.99044776,\n",
      "       0.98925373, 0.9880597 , 0.98746269, 0.99283582, 0.99014925]), 'val_losses': array([3.82469333, 2.74492257, 1.78855588, 1.26251275, 2.39980796,\n",
      "       0.78197199, 1.68756724, 1.9330732 , 0.73749431, 0.68530571,\n",
      "       0.90591304, 0.93889796, 0.60572675, 0.76267314, 0.51568479,\n",
      "       0.76038937, 0.54082272, 0.42528719, 0.52154543, 0.38756765,\n",
      "       0.2726356 , 0.24493807, 0.25294064, 0.89931167, 0.2601402 ,\n",
      "       0.18104716, 0.16511612, 0.0968704 , 0.30906084, 0.12965212,\n",
      "       0.09183208, 0.0689884 , 0.08493011, 0.09809244, 0.1156621 ,\n",
      "       0.07606983, 0.06910124, 0.08461621, 0.09575159, 0.04625065,\n",
      "       0.10191981, 0.03669696, 0.05105249, 0.04212017, 0.08979077,\n",
      "       0.03436847, 0.03341083, 0.07991099, 0.03049004, 0.0870248 ,\n",
      "       0.04217052, 0.32560541, 0.02091916, 0.04852515, 0.03177169,\n",
      "       0.02839752, 0.03829727, 0.04138492, 0.0233226 , 0.03605484])}, 'ATTACH::28::best_val_loss': 0.02091915539479745, 'ATTACH::29::history': {'train_accs': array([0.34577207, 0.398985  , 0.42092693, 0.4378685 , 0.45592955,\n",
      "       0.48526009, 0.51735204, 0.54056273, 0.55757892, 0.58967087,\n",
      "       0.62183745, 0.64624226, 0.67714009, 0.7034107 , 0.72154638,\n",
      "       0.73804015, 0.76378834, 0.78640197, 0.8034928 , 0.82244944,\n",
      "       0.82827077, 0.84610792, 0.84886932, 0.8642436 , 0.87387118,\n",
      "       0.88558848, 0.88924547, 0.90342563, 0.90663482, 0.91476976,\n",
      "       0.9169341 , 0.92312859, 0.93036794, 0.93059184, 0.93462199,\n",
      "       0.93812971, 0.94633928, 0.94596612, 0.95178745, 0.95454885,\n",
      "       0.95783267, 0.96126577, 0.9668632 , 0.97163967, 0.97096798,\n",
      "       0.97566983, 0.97731174, 0.97761027, 0.97880439, 0.98119263,\n",
      "       0.98037167, 0.98261064, 0.98432719, 0.98246138, 0.98611837,\n",
      "       0.98723785, 0.98417792, 0.98813344, 0.986193  , 0.98798418]), 'train_losses': array([28.62570346,  9.70688692,  6.39459354,  4.67504058,  3.67187502,\n",
      "        2.85098295,  2.33294786,  2.0006557 ,  1.72003958,  1.52626782,\n",
      "        1.32099179,  1.20383998,  1.05012105,  0.95072417,  0.87872827,\n",
      "        0.78851234,  0.70742273,  0.65617725,  0.58888765,  0.53819533,\n",
      "        0.50172079,  0.45783565,  0.44847022,  0.39783654,  0.38077162,\n",
      "        0.34372749,  0.32333528,  0.29524793,  0.28445099,  0.25441352,\n",
      "        0.24452674,  0.2240419 ,  0.20935298,  0.20501331,  0.19038547,\n",
      "        0.18387182,  0.16520097,  0.16043325,  0.14073242,  0.13995708,\n",
      "        0.12847887,  0.11915199,  0.10255181,  0.0920388 ,  0.0915744 ,\n",
      "        0.07938023,  0.07771583,  0.07162386,  0.0654147 ,  0.06453295,\n",
      "        0.0646601 ,  0.05611494,  0.05211096,  0.05825846,  0.04584022,\n",
      "        0.0450065 ,  0.04965625,  0.04228309,  0.04257172,  0.04003885]), 'val_accs': array([0.41641791, 0.44507463, 0.47164179, 0.49820896, 0.58      ,\n",
      "       0.58029851, 0.61940299, 0.56447761, 0.66626866, 0.73014925,\n",
      "       0.64477612, 0.67880597, 0.72179104, 0.81014925, 0.78656716,\n",
      "       0.77701493, 0.77253731, 0.71044776, 0.84746269, 0.77223881,\n",
      "       0.88985075, 0.66686567, 0.84328358, 0.84059701, 0.88656716,\n",
      "       0.82925373, 0.86895522, 0.92238806, 0.87880597, 0.76955224,\n",
      "       0.90746269, 0.91402985, 0.90865672, 0.93791045, 0.90298507,\n",
      "       0.93104478, 0.87164179, 0.94537313, 0.90835821, 0.93641791,\n",
      "       0.9519403 , 0.95283582, 0.95074627, 0.94447761, 0.94179104,\n",
      "       0.96238806, 0.95373134, 0.95791045, 0.96985075, 0.97014925,\n",
      "       0.95701493, 0.95701493, 0.96716418, 0.97014925, 0.9719403 ,\n",
      "       0.97253731, 0.93373134, 0.96238806, 0.96925373, 0.97641791]), 'val_losses': array([2.80678549, 2.69362029, 2.19599197, 2.1044533 , 1.46427394,\n",
      "       1.40598428, 1.61632055, 1.28163759, 1.10701042, 0.71576505,\n",
      "       1.18336188, 0.68685776, 0.86510414, 0.5209107 , 0.57292602,\n",
      "       0.60814899, 0.90050924, 0.74971497, 0.40361744, 0.718424  ,\n",
      "       0.34287944, 1.3173329 , 0.38660722, 0.4914445 , 0.37576024,\n",
      "       0.55202174, 0.4030812 , 0.24724043, 0.42372679, 0.64040226,\n",
      "       0.26279189, 0.23292642, 0.2528446 , 0.17161996, 0.29071015,\n",
      "       0.19227923, 0.38812731, 0.15162004, 0.25768256, 0.17650801,\n",
      "       0.13391192, 0.12997054, 0.15087262, 0.16183259, 0.17235955,\n",
      "       0.12230397, 0.14471162, 0.15399087, 0.10670533, 0.1135772 ,\n",
      "       0.1476295 , 0.13975505, 0.11090419, 0.10200429, 0.08711615,\n",
      "       0.09858369, 0.27170573, 0.14222715, 0.10439   , 0.08302819])}, 'ATTACH::29::best_val_loss': 0.08302818911066696}\n",
      "{'ATTACH::0::history': {'train_accs': array([0.39182029, 0.45585491, 0.47727442, 0.52421823, 0.53832376,\n",
      "       0.56451974, 0.57004254, 0.58534219, 0.60735876, 0.61728487,\n",
      "       0.63131577, 0.66228823, 0.67124412, 0.67825957, 0.68378237,\n",
      "       0.6921412 , 0.69930592, 0.70602284, 0.70714232, 0.71662064,\n",
      "       0.72677065, 0.73035301, 0.74341369, 0.74468244, 0.75401149,\n",
      "       0.77214718, 0.77722218, 0.78744682, 0.78431226, 0.79461154,\n",
      "       0.80483618, 0.80386596, 0.80938876, 0.82401672, 0.81468766,\n",
      "       0.82774834, 0.83177849, 0.83648033, 0.82110605, 0.82931562,\n",
      "       0.84528696, 0.82521084, 0.83215165]), 'train_losses': array([34.30502073,  5.00321175,  2.76046453,  1.78366147,  1.43949222,\n",
      "        1.20442431,  1.08683017,  0.95980313,  0.8870531 ,  0.84151949,\n",
      "        0.81877693,  0.74368056,  0.7229202 ,  0.70219864,  0.69034076,\n",
      "        0.68104314,  0.6610165 ,  0.65721849,  0.65680333,  0.63172681,\n",
      "        0.61484226,  0.60311677,  0.57698134,  0.59442882,  0.58435546,\n",
      "        0.54486131,  0.54141689,  0.50062657,  0.51421684,  0.4827659 ,\n",
      "        0.464421  ,  0.46668545,  0.45219865,  0.42164227,  0.44226797,\n",
      "        0.41831787,  0.40625021,  0.39936123,  0.43631817,  0.41857246,\n",
      "        0.38490893,  0.4170854 ,  0.41165011]), 'val_accs': array([0.46895522, 0.48716418, 0.52835821, 0.67552239, 0.72179104,\n",
      "       0.63283582, 0.69791045, 0.69880597, 0.71074627, 0.59462687,\n",
      "       0.67313433, 0.61134328, 0.66567164, 0.74477612, 0.74686567,\n",
      "       0.70298507, 0.68537313, 0.69701493, 0.71343284, 0.75134328,\n",
      "       0.75880597, 0.68358209, 0.76985075, 0.73701493, 0.76179104,\n",
      "       0.75134328, 0.72925373, 0.73283582, 0.83970149, 0.84955224,\n",
      "       0.81820896, 0.78537313, 0.82865672, 0.81014925, 0.8519403 ,\n",
      "       0.82358209, 0.86268657, 0.80776119, 0.82865672, 0.81462687,\n",
      "       0.80776119, 0.82059701, 0.80597015]), 'val_losses': array([2.99090335, 1.86875268, 1.13128706, 0.70673934, 0.72995779,\n",
      "       0.74448237, 0.71587884, 0.82015519, 0.69314753, 0.83003947,\n",
      "       0.73316216, 0.78484276, 0.70350557, 0.66225973, 0.66111859,\n",
      "       0.6138202 , 0.6363077 , 0.63214651, 0.6114882 , 0.60165218,\n",
      "       0.5590872 , 0.61261956, 0.56251365, 0.6166329 , 0.53488139,\n",
      "       0.53808684, 0.51989058, 0.57320163, 0.46004285, 0.44139666,\n",
      "       0.4685613 , 0.47255747, 0.36756918, 0.43993167, 0.3690304 ,\n",
      "       0.40934281, 0.37769047, 0.42492794, 0.40033234, 0.42779674,\n",
      "       0.5230131 , 0.47241935, 0.4767806 ])}, 'ATTACH::0::best_val_loss': 0.36756918172338116, 'ATTACH::1::history': {'train_accs': array([0.30263452, 0.34204045, 0.35345921, 0.36196731, 0.3796552 ,\n",
      "       0.39286514, 0.39592507, 0.39913426, 0.41197104, 0.4180909 ,\n",
      "       0.4210762 , 0.43428614, 0.43615195, 0.45040675, 0.46137771,\n",
      "       0.47137846, 0.47749832, 0.49361893, 0.50996343, 0.52406896,\n",
      "       0.5342936 , 0.53332338, 0.54392119, 0.5538473 , 0.5706396 ,\n",
      "       0.58131204, 0.59243227, 0.59474588, 0.6178073 , 0.62594223,\n",
      "       0.63512202, 0.65631764, 0.66587059, 0.67863273, 0.69870886,\n",
      "       0.69826106, 0.71535189, 0.73162176, 0.73087544, 0.74848869,\n",
      "       0.75602657, 0.76490783, 0.77043063, 0.78065527, 0.78162549,\n",
      "       0.78841705, 0.79662661, 0.80110456, 0.8064781 , 0.81573252,\n",
      "       0.82132995, 0.82267333, 0.83453989, 0.84222703, 0.84939175,\n",
      "       0.85245168, 0.85349653, 0.86178073, 0.86327338, 0.86633331]), 'train_losses': array([75.09796752, 33.41025147, 21.50952437, 15.48750488, 12.4088058 ,\n",
      "       10.25220268,  8.86196618,  7.78999262,  6.69349395,  6.10725996,\n",
      "        5.41271973,  4.86675585,  4.49829058,  4.06961687,  3.6620988 ,\n",
      "        3.31692781,  3.04637085,  2.83559404,  2.55637501,  2.37881305,\n",
      "        2.2701055 ,  2.09350396,  1.92298311,  1.90409677,  1.77805711,\n",
      "        1.64503203,  1.60963626,  1.54039497,  1.45295776,  1.36518598,\n",
      "        1.32676329,  1.25101984,  1.17864007,  1.17694454,  1.08462304,\n",
      "        1.03185296,  1.02767772,  0.92731914,  0.9250975 ,  0.86321286,\n",
      "        0.88153048,  0.81831703,  0.77932687,  0.75155622,  0.74195165,\n",
      "        0.7082606 ,  0.69372973,  0.67378495,  0.6663245 ,  0.63303053,\n",
      "        0.610591  ,  0.60241484,  0.55305397,  0.54950578,  0.51388805,\n",
      "        0.50567949,  0.49254773,  0.45637321,  0.46057292,  0.44948667]), 'val_accs': array([0.20925373, 0.29462687, 0.40597015, 0.4880597 , 0.42029851,\n",
      "       0.45343284, 0.3841791 , 0.41343284, 0.33014925, 0.4480597 ,\n",
      "       0.45253731, 0.49432836, 0.50656716, 0.43552239, 0.54746269,\n",
      "       0.55104478, 0.50298507, 0.53164179, 0.58358209, 0.59432836,\n",
      "       0.59432836, 0.64567164, 0.63552239, 0.62865672, 0.54179104,\n",
      "       0.61910448, 0.68895522, 0.69253731, 0.70537313, 0.50298507,\n",
      "       0.63074627, 0.77791045, 0.68656716, 0.75014925, 0.70656716,\n",
      "       0.80208955, 0.77731343, 0.64507463, 0.75432836, 0.80328358,\n",
      "       0.82835821, 0.67910448, 0.70537313, 0.80746269, 0.72507463,\n",
      "       0.82567164, 0.8558209 , 0.70029851, 0.84895522, 0.87492537,\n",
      "       0.87134328, 0.77074627, 0.86119403, 0.8558209 , 0.8519403 ,\n",
      "       0.85761194, 0.88208955, 0.86119403, 0.88656716, 0.89522388]), 'val_losses': array([42.1304238 , 15.92842745,  8.68387624,  3.60806126,  2.94422408,\n",
      "        2.70952301,  3.3299683 ,  2.25054795,  2.88116758,  1.9218025 ,\n",
      "        2.41507978,  3.30317015,  1.70777335,  2.21075888,  1.69992985,\n",
      "        1.35639858,  1.7893451 ,  1.61158054,  1.34208979,  1.13770431,\n",
      "        1.45484972,  0.93262101,  1.24069493,  0.90476473,  1.47290036,\n",
      "        1.21370396,  0.8377039 ,  0.94391437,  0.8519544 ,  2.31129293,\n",
      "        1.27275126,  0.63117772,  1.0039637 ,  0.79235097,  0.70587408,\n",
      "        0.5519389 ,  0.61901018,  1.04214392,  0.7663624 ,  0.56641156,\n",
      "        0.49589885,  0.91231816,  0.92601572,  0.65529004,  0.81294406,\n",
      "        0.46639459,  0.41173588,  0.92148594,  0.45381976,  0.38665916,\n",
      "        0.42525048,  0.81892387,  0.44181346,  0.46876229,  0.49899106,\n",
      "        0.46548992,  0.35731575,  0.4786146 ,  0.39952097,  0.33321057])}, 'ATTACH::1::best_val_loss': 0.33321057024286754, 'ATTACH::2::history': {'train_accs': array([0.4151056 , 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44219718, 0.44219718, 0.44219718, 0.44219718, 0.4484663 ,\n",
      "       0.49615643, 0.51526233, 0.51309799, 0.53623405, 0.55101127,\n",
      "       0.57563997, 0.58982014, 0.60556758, 0.61825509, 0.63362938,\n",
      "       0.63594298, 0.65183969, 0.6707217 , 0.69467871, 0.7227405 ,\n",
      "       0.74169714, 0.76408687, 0.78162549, 0.79371595, 0.8092395 ,\n",
      "       0.80968729, 0.81737443, 0.81341891, 0.81662811, 0.83476379,\n",
      "       0.83849541, 0.84013732, 0.84327189, 0.84580939, 0.84088365,\n",
      "       0.85887006, 0.85819837, 0.81924024, 0.85745205, 0.86178073,\n",
      "       0.86902008, 0.87946862, 0.86304948, 0.8699903 , 0.69982835,\n",
      "       0.84655571, 0.87237854, 0.87357265, 0.88200612, 0.77617733]), 'train_losses': array([4.72430113, 1.24551738, 1.24230363, 1.24172953, 1.24176068,\n",
      "       1.2415304 , 1.24152883, 1.24121967, 1.24066276, 1.24040363,\n",
      "       1.23986478, 1.23864686, 1.23431765, 1.22577103, 1.1968185 ,\n",
      "       1.15497248, 1.12931401, 1.12462389, 1.08060305, 1.04323528,\n",
      "       0.98992676, 0.95816697, 0.92007389, 0.89032473, 0.85471354,\n",
      "       0.84600895, 0.81850822, 0.76932528, 0.7392797 , 0.69966775,\n",
      "       0.684172  , 0.64472732, 0.60057715, 0.57210798, 0.53469657,\n",
      "       0.52285586, 0.50527929, 0.51327988, 0.50525883, 0.45292751,\n",
      "       0.43910629, 0.4440443 , 0.432536  , 0.4389359 , 0.45496977,\n",
      "       0.41184521, 0.41497183, 0.50480632, 0.41679675, 0.41303968,\n",
      "       0.38205055, 0.37287782, 0.41303567, 0.39775747, 0.73437929,\n",
      "       0.44692467, 0.38312526, 0.37464071, 0.35015397, 0.62522414]), 'val_accs': array([0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.47492537,\n",
      "       0.49462687, 0.47402985, 0.51253731, 0.54746269, 0.53731343,\n",
      "       0.5641791 , 0.58119403, 0.60358209, 0.62537313, 0.64059701,\n",
      "       0.65283582, 0.66358209, 0.64746269, 0.6958209 , 0.73880597,\n",
      "       0.68656716, 0.75850746, 0.78716418, 0.76208955, 0.79731343,\n",
      "       0.81343284, 0.79671642, 0.81761194, 0.81940299, 0.82119403,\n",
      "       0.85104478, 0.83910448, 0.84835821, 0.79522388, 0.83552239,\n",
      "       0.83402985, 0.74925373, 0.83014925, 0.85820896, 0.85044776,\n",
      "       0.85671642, 0.86268657, 0.83731343, 0.83253731, 0.80059701,\n",
      "       0.81970149, 0.86597015, 0.86358209, 0.86059701, 0.8519403 ]), 'val_losses': array([1.2502468 , 1.24275918, 1.24156557, 1.24344135, 1.24124599,\n",
      "       1.24109258, 1.24091091, 1.24077342, 1.24015874, 1.23957052,\n",
      "       1.23869616, 1.23778362, 1.2309545 , 1.22136011, 1.17753743,\n",
      "       1.15100298, 1.1456214 , 1.11106801, 1.04465595, 1.0531755 ,\n",
      "       0.99096656, 0.96272883, 0.90636747, 0.87871628, 0.8484426 ,\n",
      "       0.8156362 , 0.76573237, 0.80382303, 0.73638251, 0.67789102,\n",
      "       0.86770816, 0.64812204, 0.61110141, 0.69794005, 0.5530575 ,\n",
      "       0.52342852, 0.55954089, 0.50930574, 0.48348722, 0.51451366,\n",
      "       0.43817511, 0.45825704, 0.45211653, 0.66584757, 0.46021622,\n",
      "       0.46303531, 0.6756843 , 0.48936951, 0.44672264, 0.43264741,\n",
      "       0.4565475 , 0.43664561, 0.48110243, 0.52436803, 0.52036062,\n",
      "       0.48767936, 0.40039189, 0.41719493, 0.41951976, 0.42563141])}, 'ATTACH::2::best_val_loss': 0.40039188924120434, 'ATTACH::3::history': {'train_accs': array([0.2968132 , 0.33621912, 0.34808568, 0.3688335 , 0.37935667,\n",
      "       0.38316292, 0.40010449, 0.41167251, 0.41921039, 0.4291365 ,\n",
      "       0.44070453, 0.45406374, 0.46369132, 0.47503545, 0.48361818,\n",
      "       0.50212702, 0.50444063, 0.50914247, 0.52496455, 0.53347265,\n",
      "       0.53839839, 0.55138443, 0.56175834, 0.57131129, 0.58840212,\n",
      "       0.59885066, 0.61661318, 0.62071796, 0.63094261, 0.656243  ,\n",
      "       0.66109411, 0.67811031, 0.68691693, 0.70326144, 0.71199343,\n",
      "       0.71975521, 0.74102545, 0.74692141, 0.75042914, 0.7646093 ,\n",
      "       0.77475931, 0.78162549, 0.79714904, 0.79759684, 0.80953803,\n",
      "       0.81841929, 0.82655422, 0.83543548, 0.8449138 , 0.84752593,\n",
      "       0.85386969, 0.85961639, 0.86319875, 0.87499067, 0.87081125,\n",
      "       0.88051347, 0.88446899, 0.89148444, 0.89312635, 0.90178372]), 'train_losses': array([56.25552499, 35.06462789, 25.91020255, 20.4937503 , 16.5691571 ,\n",
      "       14.14712855, 12.36225226, 10.68902783,  9.82929372,  8.88671794,\n",
      "        8.19139531,  7.26352567,  6.85602627,  6.32139084,  5.77717105,\n",
      "        5.29200626,  4.99447541,  4.69587357,  4.33524663,  4.04367584,\n",
      "        3.86785858,  3.60857585,  3.45931134,  3.0671169 ,  2.95100197,\n",
      "        2.74530898,  2.44280431,  2.38626896,  2.23942375,  2.00162175,\n",
      "        1.9619691 ,  1.8023547 ,  1.62818767,  1.54227946,  1.46230923,\n",
      "        1.4235052 ,  1.27931671,  1.22819214,  1.19440188,  1.11574116,\n",
      "        1.05501775,  0.98507705,  0.93456116,  0.89957592,  0.81903166,\n",
      "        0.77265078,  0.71819859,  0.66782501,  0.65358452,  0.6273134 ,\n",
      "        0.58280441,  0.56786142,  0.53738666,  0.4961006 ,  0.50701936,\n",
      "        0.4629947 ,  0.43441364,  0.40665959,  0.38144615,  0.36328091]), 'val_accs': array([0.42835821, 0.4238806 , 0.44328358, 0.43910448, 0.44328358,\n",
      "       0.44358209, 0.46119403, 0.44179104, 0.51253731, 0.57343284,\n",
      "       0.59313433, 0.64149254, 0.65671642, 0.47880597, 0.67253731,\n",
      "       0.68835821, 0.66716418, 0.60477612, 0.63940299, 0.65701493,\n",
      "       0.66626866, 0.66358209, 0.69104478, 0.67134328, 0.64358209,\n",
      "       0.69850746, 0.7361194 , 0.79731343, 0.79373134, 0.75970149,\n",
      "       0.79761194, 0.79522388, 0.84179104, 0.80955224, 0.8438806 ,\n",
      "       0.84776119, 0.84298507, 0.76955224, 0.84985075, 0.73074627,\n",
      "       0.80238806, 0.79432836, 0.83671642, 0.82567164, 0.7919403 ,\n",
      "       0.87552239, 0.89552239, 0.84089552, 0.88985075, 0.84268657,\n",
      "       0.89164179, 0.88626866, 0.88597015, 0.86865672, 0.87432836,\n",
      "       0.85850746, 0.89671642, 0.82507463, 0.90776119, 0.8958209 ]), 'val_losses': array([16.9602281 , 10.21087522,  8.34570741,  6.86285527,  7.96736475,\n",
      "        5.91177363,  6.23086815,  4.9089989 ,  3.78969615,  3.00396434,\n",
      "        2.7995737 ,  2.48864923,  2.28573019,  2.4082502 ,  2.1218761 ,\n",
      "        1.93339853,  1.8935719 ,  1.82141426,  1.62803387,  1.85315682,\n",
      "        1.32559061,  1.31634016,  1.1956215 ,  1.35364221,  1.29344168,\n",
      "        1.14681169,  0.93044176,  0.74822775,  0.78681507,  0.94875002,\n",
      "        0.71455157,  0.74068552,  0.63743342,  0.74567061,  0.60715185,\n",
      "        0.57197319,  0.6111205 ,  0.91045377,  0.60099873,  1.04800453,\n",
      "        0.79063971,  0.77165978,  0.5742605 ,  0.72992841,  0.78643314,\n",
      "        0.42095871,  0.38038943,  0.54407471,  0.3826398 ,  0.4893158 ,\n",
      "        0.37050004,  0.40677632,  0.37217263,  0.43313418,  0.42499666,\n",
      "        0.41077667,  0.32744736,  0.61101818,  0.29416585,  0.30237838])}, 'ATTACH::3::best_val_loss': 0.2941658490807263, 'ATTACH::4::history': {'train_accs': array([0.38711844, 0.44854094, 0.47772222, 0.50220166, 0.53645794,\n",
      "       0.56952011, 0.63288305, 0.66564669, 0.70751549, 0.73221882,\n",
      "       0.75013061, 0.77483394, 0.803045  , 0.82737518, 0.84909322,\n",
      "       0.85394432, 0.87819987, 0.889544  , 0.90253004, 0.91469513,\n",
      "       0.92059109, 0.93148742, 0.93365177, 0.93827898, 0.94850362,\n",
      "       0.94357788, 0.95611613, 0.95783267, 0.95895216, 0.95738488,\n",
      "       0.9611165 , 0.96283305, 0.9641018 , 0.96350474, 0.96440033,\n",
      "       0.97223673, 0.97507277, 0.97298306, 0.97410254, 0.97141578,\n",
      "       0.96999776, 0.97835659, 0.97358012, 0.98052093, 0.9751474 ,\n",
      "       0.98268528, 0.97895365, 0.97910292, 0.98514815, 0.98171505,\n",
      "       0.97835659, 0.97969998, 0.98275991, 0.9808941 , 0.97701321,\n",
      "       0.98261064, 0.98425256, 0.98589447, 0.98328233, 0.9829838 ]), 'train_losses': array([16.41773261,  3.85407317,  2.34039558,  1.66828211,  1.37391759,\n",
      "        1.21906847,  0.97956919,  0.94202175,  0.76827036,  0.70594539,\n",
      "        0.6604519 ,  0.6160145 ,  0.54065218,  0.45525536,  0.40271341,\n",
      "        0.38330283,  0.33325186,  0.30527205,  0.26427908,  0.2381656 ,\n",
      "        0.22124031,  0.19901263,  0.17904672,  0.17582608,  0.15578412,\n",
      "        0.15787416,  0.13273944,  0.12428529,  0.12858223,  0.12755174,\n",
      "        0.11834991,  0.10778088,  0.10658522,  0.10547246,  0.11024392,\n",
      "        0.0851965 ,  0.08196531,  0.08455536,  0.07823508,  0.0896371 ,\n",
      "        0.08776501,  0.06863338,  0.08128287,  0.06021148,  0.07440919,\n",
      "        0.05510317,  0.06353209,  0.06459727,  0.04565033,  0.06132066,\n",
      "        0.06234881,  0.06267541,  0.05254379,  0.05895563,  0.07376658,\n",
      "        0.05698644,  0.04640922,  0.04318676,  0.05115749,  0.05098626]), 'val_accs': array([0.42358209, 0.52149254, 0.51104478, 0.57641791, 0.57761194,\n",
      "       0.59373134, 0.69134328, 0.72119403, 0.76328358, 0.78179104,\n",
      "       0.74895522, 0.75134328, 0.83641791, 0.85791045, 0.85641791,\n",
      "       0.88985075, 0.8958209 , 0.88089552, 0.91671642, 0.91343284,\n",
      "       0.90865672, 0.91104478, 0.93044776, 0.93283582, 0.93820896,\n",
      "       0.94059701, 0.94029851, 0.91014925, 0.92686567, 0.93253731,\n",
      "       0.93910448, 0.9441791 , 0.94537313, 0.96059701, 0.94597015,\n",
      "       0.90895522, 0.95074627, 0.95223881, 0.96328358, 0.96626866,\n",
      "       0.94208955, 0.94776119, 0.9561194 , 0.88985075, 0.97014925,\n",
      "       0.95462687, 0.95970149, 0.97432836, 0.95402985, 0.97134328,\n",
      "       0.94507463, 0.95432836, 0.94119403, 0.97044776, 0.95104478,\n",
      "       0.95791045, 0.96865672, 0.95223881, 0.9641791 , 0.89432836]), 'val_losses': array([3.30035357, 1.55672261, 1.56445022, 0.96113309, 1.10872528,\n",
      "       1.05059859, 0.68012277, 0.67578646, 0.52471309, 0.56370132,\n",
      "       0.62336674, 0.65969282, 0.44466105, 0.39420954, 0.35132601,\n",
      "       0.27854042, 0.26125805, 0.30660723, 0.20969209, 0.21658853,\n",
      "       0.24206725, 0.23537761, 0.18018113, 0.17665246, 0.16774974,\n",
      "       0.17545041, 0.18107246, 0.25726808, 0.18981577, 0.18638283,\n",
      "       0.18445029, 0.13317292, 0.16384123, 0.1212324 , 0.1449568 ,\n",
      "       0.26602994, 0.13380104, 0.14448289, 0.11079298, 0.10370068,\n",
      "       0.15503885, 0.1825833 , 0.12933945, 0.35838515, 0.10761518,\n",
      "       0.1442598 , 0.12037635, 0.09815713, 0.17440431, 0.10837411,\n",
      "       0.17056791, 0.13267078, 0.18152169, 0.09604044, 0.15793092,\n",
      "       0.16966865, 0.1065783 , 0.16721291, 0.12308799, 0.40076435])}, 'ATTACH::4::best_val_loss': 0.0960404396724345, 'ATTACH::5::history': {'train_accs': array([0.41271737, 0.44555564, 0.44943653, 0.44473468, 0.44368983,\n",
      "       0.44540637, 0.44122696, 0.43891335, 0.4376446 , 0.43973431,\n",
      "       0.44025674, 0.44048063, 0.43756997, 0.43906262, 0.43271886,\n",
      "       0.43809239, 0.43055452, 0.43988357, 0.42891261, 0.41659825,\n",
      "       0.43406224, 0.43398761, 0.44443615, 0.43928651, 0.43659975,\n",
      "       0.43861482]), 'train_losses': array([27.89485207,  1.23067595,  1.20431644,  1.19620615,  1.18190573,\n",
      "        1.22738072,  1.2359962 ,  1.21337839,  1.24857499,  1.27534551,\n",
      "        1.24372404,  1.26188838,  1.2236955 ,  1.23718275,  1.12846263,\n",
      "        1.1204669 ,  1.12515698,  1.11484668,  1.16841726,  1.16869558,\n",
      "        1.37228421,  1.45719187,  1.27726212,  1.13827386,  1.19161484,\n",
      "        1.11483924]), 'val_accs': array([0.44895522, 0.46      , 0.45820896, 0.44208955, 0.41731343,\n",
      "       0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.43164179,\n",
      "       0.44208955, 0.44895522, 0.44208955, 0.44208955, 0.44208955,\n",
      "       0.50179104, 0.44119403, 0.42597015, 0.25134328, 0.44208955,\n",
      "       0.44208955, 0.46865672, 0.44208955, 0.46716418, 0.44208955,\n",
      "       0.41492537]), 'val_losses': array([1.23205839, 1.20226497, 1.21308968, 1.22994855, 1.14115639,\n",
      "       1.23810525, 1.2421878 , 1.24188341, 1.2420355 , 1.23862325,\n",
      "       1.24198507, 1.16951667, 1.25779217, 1.23228972, 1.06431786,\n",
      "       0.99466736, 1.07357384, 1.01040165, 1.19992844, 1.1197701 ,\n",
      "       1.04807003, 1.14140743, 1.12789826, 1.0785941 , 1.03901506,\n",
      "       1.0759042 ])}, 'ATTACH::5::best_val_loss': 0.994667361387566, 'ATTACH::6::history': {'train_accs': array([0.35069781, 0.37532652, 0.3932383 , 0.40361221, 0.40517949,\n",
      "       0.40823942, 0.42018061, 0.42764385, 0.43152474, 0.43294276,\n",
      "       0.42980819, 0.44234644, 0.45689977, 0.47010971, 0.47272184,\n",
      "       0.47555788, 0.48854392, 0.49593253, 0.49809687, 0.51026196,\n",
      "       0.51526233, 0.52518845, 0.53123367, 0.54265244, 0.54720502,\n",
      "       0.550862  , 0.55944473, 0.57056497, 0.57653556, 0.57967012,\n",
      "       0.59601463, 0.5952683 , 0.60347787, 0.60892604, 0.60967236,\n",
      "       0.61400104, 0.62183745, 0.63579372, 0.65034704, 0.64392865,\n",
      "       0.65527278, 0.66669154, 0.66990074, 0.67587133, 0.68400627,\n",
      "       0.68646914, 0.69721621, 0.70370923, 0.70818718, 0.71557579,\n",
      "       0.72266587, 0.72393462, 0.72557654, 0.7287111 , 0.7448317 ,\n",
      "       0.74729457, 0.74595119, 0.76177327, 0.75236958, 0.76513173]), 'train_losses': array([51.34318055, 33.77428877, 25.36215157, 21.26584249, 18.09732623,\n",
      "       15.85450366, 13.83402565, 12.25064108, 10.84199144, 10.0370685 ,\n",
      "        9.02134425,  8.03664241,  7.24881706,  6.51017903,  5.93396042,\n",
      "        5.52331121,  4.96148742,  4.56331853,  4.3020418 ,  3.89146685,\n",
      "        3.63804547,  3.42667904,  3.19290997,  2.97809981,  2.89674873,\n",
      "        2.74391216,  2.60474124,  2.42363003,  2.32477077,  2.3266401 ,\n",
      "        2.1227063 ,  2.09960484,  1.93490011,  1.90635782,  1.79320759,\n",
      "        1.7269024 ,  1.6912969 ,  1.62425785,  1.5010922 ,  1.46868747,\n",
      "        1.46457366,  1.37304301,  1.29251394,  1.28591977,  1.23380273,\n",
      "        1.21380386,  1.14942078,  1.11815031,  1.08507084,  1.03707739,\n",
      "        1.01949572,  0.9712687 ,  0.93842368,  0.9339978 ,  0.88242233,\n",
      "        0.85576604,  0.84898713,  0.80658594,  0.83028723,  0.77203974]), 'val_accs': array([0.44238806, 0.48835821, 0.50925373, 0.51492537, 0.53940299,\n",
      "       0.53492537, 0.49850746, 0.5480597 , 0.51104478, 0.48537313,\n",
      "       0.48358209, 0.53850746, 0.54686567, 0.57014925, 0.5680597 ,\n",
      "       0.56358209, 0.56119403, 0.6158209 , 0.64179104, 0.67462687,\n",
      "       0.62298507, 0.68268657, 0.68776119, 0.69552239, 0.70537313,\n",
      "       0.69910448, 0.71761194, 0.71223881, 0.71641791, 0.74      ,\n",
      "       0.75223881, 0.73313433, 0.73373134, 0.74776119, 0.72119403,\n",
      "       0.73492537, 0.73223881, 0.79044776, 0.78268657, 0.76238806,\n",
      "       0.76238806, 0.78537313, 0.77761194, 0.78119403, 0.74835821,\n",
      "       0.76507463, 0.82477612, 0.79880597, 0.76179104, 0.7719403 ,\n",
      "       0.80746269, 0.80298507, 0.82149254, 0.8038806 , 0.83552239,\n",
      "       0.82925373, 0.83134328, 0.81074627, 0.8119403 , 0.83492537]), 'val_losses': array([12.620877  ,  7.03395445,  6.93267766,  5.72116572,  5.03403967,\n",
      "        5.0375105 ,  4.35699829,  4.42802233,  3.6983474 ,  3.53323101,\n",
      "        3.29559526,  2.96117574,  2.51801648,  2.44736007,  2.13492957,\n",
      "        2.14819545,  2.14705569,  1.74046049,  1.53467489,  1.25863748,\n",
      "        1.3398125 ,  1.23643599,  1.18887315,  1.11535518,  1.05751127,\n",
      "        1.1386684 ,  0.87010595,  1.0959304 ,  1.0289452 ,  0.91883387,\n",
      "        0.92881765,  0.89815456,  0.9144625 ,  0.86990972,  0.99523981,\n",
      "        0.94677423,  1.01316629,  0.70953431,  0.74945667,  0.88098989,\n",
      "        0.83862492,  0.74898977,  0.79184254,  0.74626944,  0.91959261,\n",
      "        0.81648546,  0.58622095,  0.70738274,  0.92933631,  0.85853689,\n",
      "        0.68336533,  0.62427194,  0.61919423,  0.67704583,  0.53892556,\n",
      "        0.57189996,  0.60292996,  0.6414607 ,  0.69272598,  0.57492697])}, 'ATTACH::6::best_val_loss': 0.5389255580083647, 'ATTACH::7::history': {'train_accs': array([0.34383163, 0.4130159 , 0.42853944, 0.45010822, 0.46742294,\n",
      "       0.5236958 , 0.56929622, 0.61452347, 0.6541533 , 0.67930443,\n",
      "       0.71266512, 0.76528099, 0.78267035, 0.80617957, 0.82692738,\n",
      "       0.85551161, 0.87208001, 0.88797671, 0.89536533, 0.90514217,\n",
      "       0.9056646 , 0.92096425, 0.92066572, 0.92753191, 0.93141279,\n",
      "       0.93551758, 0.94118964, 0.94589148, 0.9530562 , 0.94805583,\n",
      "       0.94939921, 0.95917606, 0.95775804, 0.96454959, 0.96544518,\n",
      "       0.96581834, 0.96701246, 0.97126651, 0.96611687, 0.97178894,\n",
      "       0.97372938, 0.98141652, 0.97917755, 0.97783417, 0.97552056,\n",
      "       0.97820733, 0.97835659, 0.98275991, 0.98514815, 0.98417792,\n",
      "       0.98671543, 0.98656616, 0.98790955, 0.98649153, 0.9834316 ,\n",
      "       0.98290917, 0.98984999, 0.98820808, 0.98626763, 0.98999925]), 'train_losses': array([4.69410368e+01, 1.00460856e+01, 6.27606469e+00, 4.49032235e+00,\n",
      "       3.61726618e+00, 2.74936018e+00, 2.24175655e+00, 1.83517970e+00,\n",
      "       1.56736988e+00, 1.42472655e+00, 1.20550756e+00, 9.39704267e-01,\n",
      "       8.55770346e-01, 6.93534166e-01, 6.11400467e-01, 5.05395762e-01,\n",
      "       4.46636514e-01, 3.95409926e-01, 3.60516819e-01, 3.23833066e-01,\n",
      "       3.21892222e-01, 2.59992554e-01, 2.58885600e-01, 2.47354530e-01,\n",
      "       2.20926869e-01, 2.21841678e-01, 1.94883533e-01, 1.81058958e-01,\n",
      "       1.49895067e-01, 1.79888246e-01, 1.70727153e-01, 1.26586740e-01,\n",
      "       1.34017104e-01, 1.13702673e-01, 1.09844762e-01, 1.08267081e-01,\n",
      "       1.07657419e-01, 9.23502450e-02, 9.95444401e-02, 8.87276811e-02,\n",
      "       8.43105457e-02, 6.46043843e-02, 6.89909132e-02, 7.19046086e-02,\n",
      "       7.91678457e-02, 7.17066017e-02, 6.87398759e-02, 5.58957330e-02,\n",
      "       4.60369285e-02, 4.78917392e-02, 4.28133193e-02, 4.85689842e-02,\n",
      "       3.94118150e-02, 4.43185243e-02, 6.08575588e-02, 6.11342657e-02,\n",
      "       3.42040288e-02, 3.47852127e-02, 4.55871023e-02, 3.19539440e-02]), 'val_accs': array([0.33074627, 0.47880597, 0.48208955, 0.54029851, 0.56      ,\n",
      "       0.62656716, 0.6761194 , 0.75641791, 0.75910448, 0.78597015,\n",
      "       0.81104478, 0.85880597, 0.83223881, 0.88477612, 0.87850746,\n",
      "       0.89223881, 0.87522388, 0.88597015, 0.88447761, 0.91462687,\n",
      "       0.90955224, 0.92119403, 0.93850746, 0.90597015, 0.89970149,\n",
      "       0.91522388, 0.90119403, 0.92179104, 0.91850746, 0.91373134,\n",
      "       0.92865672, 0.93402985, 0.94238806, 0.94835821, 0.93671642,\n",
      "       0.94477612, 0.96656716, 0.94626866, 0.96567164, 0.96089552,\n",
      "       0.96507463, 0.96925373, 0.96686567, 0.96746269, 0.96268657,\n",
      "       0.96567164, 0.95791045, 0.9638806 , 0.96119403, 0.97432836,\n",
      "       0.96328358, 0.97283582, 0.93552239, 0.97223881, 0.97343284,\n",
      "       0.96895522, 0.97791045, 0.96477612, 0.97701493, 0.96776119]), 'val_losses': array([6.50093116, 2.54139053, 2.12949935, 1.33670665, 0.98359476,\n",
      "       1.08694692, 0.98034988, 0.77762999, 0.77300651, 0.73442257,\n",
      "       0.65306174, 0.49673754, 0.57321703, 0.36970851, 0.39710094,\n",
      "       0.33358982, 0.38522438, 0.40737289, 0.39875382, 0.25910941,\n",
      "       0.25610761, 0.2377209 , 0.18963874, 0.29960235, 0.3712128 ,\n",
      "       0.26587059, 0.32272205, 0.26495807, 0.25823739, 0.2842272 ,\n",
      "       0.23818901, 0.18407175, 0.18893822, 0.15522846, 0.2565064 ,\n",
      "       0.19033141, 0.12392335, 0.21362388, 0.12971668, 0.12272158,\n",
      "       0.1422205 , 0.1135259 , 0.10763125, 0.11199327, 0.14238681,\n",
      "       0.1075482 , 0.15867285, 0.14851714, 0.15687624, 0.08788531,\n",
      "       0.14278021, 0.10217343, 0.25610924, 0.11145179, 0.10629215,\n",
      "       0.11365286, 0.07926185, 0.12644141, 0.09859759, 0.12345597])}, 'ATTACH::7::best_val_loss': 0.07926185399849912, 'ATTACH::8::history': {'train_accs': array([0.3327114 , 0.38450631, 0.3874916 , 0.40734383, 0.41129935,\n",
      "       0.41488171, 0.42898724, 0.42331517, 0.43286812, 0.44398836,\n",
      "       0.44943653, 0.44719755, 0.45689977, 0.47451302, 0.48078215,\n",
      "       0.48854392, 0.49638033, 0.50317188, 0.51578476, 0.51466527,\n",
      "       0.52862154, 0.54504067, 0.54377192, 0.55802672, 0.57578924,\n",
      "       0.58325248, 0.57541608, 0.58273005, 0.58258079, 0.59355176,\n",
      "       0.59661169, 0.60489589, 0.62303157, 0.61713561, 0.61534443,\n",
      "       0.61959848, 0.60959773, 0.61743414, 0.61885215, 0.63034555,\n",
      "       0.61929995, 0.62952459, 0.62639003, 0.63131577, 0.64139115,\n",
      "       0.64340622, 0.63765953, 0.63780879, 0.64557056, 0.64877976,\n",
      "       0.66482573, 0.66766177, 0.66281066, 0.67296067, 0.67243824,\n",
      "       0.65684006, 0.66878125, 0.66885588, 0.67460258, 0.67631913]), 'train_losses': array([85.21056067, 26.19943701, 16.75096081, 11.71852361,  9.2125953 ,\n",
      "        7.33542137,  5.9223129 ,  5.27651445,  4.53437896,  3.94967539,\n",
      "        3.48445988,  3.10875178,  2.75176844,  2.40050629,  2.17994677,\n",
      "        2.05135319,  1.7948772 ,  1.67774599,  1.58585324,  1.50126043,\n",
      "        1.37953261,  1.27102863,  1.22350582,  1.14233455,  1.07166403,\n",
      "        1.03237102,  1.00493421,  0.97855627,  0.95637269,  0.91920304,\n",
      "        0.90655008,  0.87035486,  0.84116958,  0.86443842,  0.84438458,\n",
      "        0.83286447,  0.85601292,  0.83070074,  0.81691785,  0.79400388,\n",
      "        0.81717267,  0.7974666 ,  0.78217305,  0.78194616,  0.7632271 ,\n",
      "        0.76362812,  0.76168089,  0.75682877,  0.75447483,  0.75293704,\n",
      "        0.73254899,  0.7274242 ,  0.72135011,  0.71080271,  0.71574498,\n",
      "        0.72880612,  0.71191589,  0.71376801,  0.70994455,  0.70140149]), 'val_accs': array([0.49134328, 0.42895522, 0.47940299, 0.46089552, 0.5038806 ,\n",
      "       0.50298507, 0.53522388, 0.52925373, 0.54179104, 0.52238806,\n",
      "       0.56925373, 0.53970149, 0.58298507, 0.5638806 , 0.59044776,\n",
      "       0.58179104, 0.60985075, 0.55910448, 0.55492537, 0.64358209,\n",
      "       0.64895522, 0.57820896, 0.60626866, 0.61432836, 0.55970149,\n",
      "       0.6280597 , 0.64597015, 0.63701493, 0.58328358, 0.57223881,\n",
      "       0.72776119, 0.65223881, 0.62895522, 0.74029851, 0.64059701,\n",
      "       0.76567164, 0.73910448, 0.72      , 0.61731343, 0.70776119,\n",
      "       0.62298507, 0.67402985, 0.70208955, 0.69850746, 0.74746269,\n",
      "       0.72985075, 0.71044776, 0.6519403 , 0.62208955, 0.69373134,\n",
      "       0.75074627, 0.71014925, 0.7438806 , 0.77253731, 0.70985075,\n",
      "       0.6680597 , 0.65164179, 0.63641791, 0.69373134, 0.74447761]), 'val_losses': array([11.51445828,  6.91911817,  5.01574455,  3.61580549,  2.65750735,\n",
      "        2.22991584,  2.68427607,  1.84925356,  1.69610047,  2.62057906,\n",
      "        1.18912107,  1.50134354,  1.33715435,  1.19993896,  1.02245986,\n",
      "        1.00557271,  0.92532826,  0.97926326,  1.05367794,  0.77376038,\n",
      "        0.84832766,  0.93396385,  0.77806236,  0.75871084,  0.82780792,\n",
      "        0.70099368,  0.73159781,  0.69635458,  0.75238254,  0.80522632,\n",
      "        0.64538535,  0.67891148,  0.78624255,  0.63718715,  0.71501182,\n",
      "        0.62657684,  0.64527428,  0.64720703,  0.70651043,  0.6418193 ,\n",
      "        0.70503732,  0.6869202 ,  0.62196426,  0.66372483,  0.60929752,\n",
      "        0.63848054,  0.61829022,  0.64116091,  0.68538587,  0.64097918,\n",
      "        0.59911172,  0.63982568,  0.6160803 ,  0.58877941,  0.65330071,\n",
      "        0.68285639,  0.66248756,  0.68554909,  0.67027874,  0.61369674])}, 'ATTACH::8::best_val_loss': 0.5887794093943354, 'ATTACH::9::history': {'train_accs': array([0.321218  , 0.32032241, 0.32562132, 0.32666617, 0.34211508,\n",
      "       0.3518173 , 0.36196731, 0.36778864, 0.38122248, 0.37517725,\n",
      "       0.38831256, 0.39562654, 0.4019703 , 0.41338906, 0.41607583,\n",
      "       0.42592731, 0.44480931, 0.44234644, 0.4516755 , 0.45249645,\n",
      "       0.46257183, 0.47428913, 0.49137995, 0.48428987, 0.50585865,\n",
      "       0.50384357, 0.51070975, 0.51347115, 0.52809911, 0.5294425 ,\n",
      "       0.53362191, 0.54063736, 0.54168222, 0.55213076, 0.55242929,\n",
      "       0.56086275, 0.57011717, 0.57623703, 0.58086424, 0.58870065,\n",
      "       0.58952161, 0.59705948, 0.60168669, 0.61616539, 0.6118367 ,\n",
      "       0.61989701, 0.62870363, 0.62892753, 0.63818195, 0.64221211,\n",
      "       0.64489887, 0.64848123, 0.65915367, 0.65952683, 0.66101948,\n",
      "       0.66967684, 0.6728114 , 0.68184193, 0.67758788, 0.68609598]), 'train_losses': array([55.98036787, 39.78335295, 31.81030777, 25.90791069, 21.83374297,\n",
      "       18.73866588, 16.46107346, 14.57887157, 12.9403799 , 12.01501468,\n",
      "       10.51887646,  9.4480886 ,  8.77009527,  7.88525189,  7.13153223,\n",
      "        6.66229922,  5.94214651,  5.60568286,  5.27907849,  4.84693272,\n",
      "        4.55686907,  4.30533266,  3.93278383,  3.75481609,  3.48977761,\n",
      "        3.4067432 ,  3.21421711,  3.09981817,  2.85169269,  2.75766168,\n",
      "        2.69888065,  2.53415419,  2.51661483,  2.31343619,  2.16336181,\n",
      "        2.09034153,  2.11110745,  1.99067801,  1.83177423,  1.80370458,\n",
      "        1.79990764,  1.69765369,  1.67303201,  1.58231756,  1.50077669,\n",
      "        1.46311109,  1.45539123,  1.42404311,  1.33381099,  1.30334428,\n",
      "        1.2895291 ,  1.27035508,  1.20762944,  1.22152008,  1.17895215,\n",
      "        1.13939076,  1.08832875,  1.04572433,  1.11641637,  1.04143997]), 'val_accs': array([0.44208955, 0.44208955, 0.43044776, 0.43432836, 0.44179104,\n",
      "       0.43791045, 0.41552239, 0.47402985, 0.48089552, 0.49880597,\n",
      "       0.51373134, 0.52298507, 0.54268657, 0.51791045, 0.53970149,\n",
      "       0.57343284, 0.59134328, 0.62656716, 0.66328358, 0.62119403,\n",
      "       0.67134328, 0.64835821, 0.65731343, 0.63761194, 0.61850746,\n",
      "       0.63313433, 0.64597015, 0.59671642, 0.62626866, 0.63761194,\n",
      "       0.62776119, 0.66955224, 0.67283582, 0.6361194 , 0.67223881,\n",
      "       0.66895522, 0.62746269, 0.6958209 , 0.70029851, 0.59641791,\n",
      "       0.73731343, 0.70746269, 0.70656716, 0.70955224, 0.66597015,\n",
      "       0.70865672, 0.72149254, 0.73970149, 0.68059701, 0.71432836,\n",
      "       0.75940299, 0.71910448, 0.68507463, 0.73970149, 0.70985075,\n",
      "       0.74328358, 0.72865672, 0.71820896, 0.73552239, 0.77223881]), 'val_losses': array([27.8779303 , 18.48320946, 11.55313384,  7.51568661,  5.84127683,\n",
      "        4.20769679,  3.102165  ,  2.92202936,  2.68393562,  2.71774324,\n",
      "        2.50781717,  2.11881386,  1.72626103,  1.6406434 ,  1.52298087,\n",
      "        1.38567227,  1.25901016,  1.11668815,  1.19552488,  1.23932342,\n",
      "        1.10654576,  1.12414982,  1.07307306,  1.13060575,  0.99835573,\n",
      "        1.07160601,  1.01323216,  1.37951438,  1.3369234 ,  1.07807771,\n",
      "        1.28639645,  0.89800073,  0.86143412,  1.10743862,  0.9040492 ,\n",
      "        0.99046306,  1.23245623,  0.94821158,  0.93074987,  1.4564266 ,\n",
      "        0.80423061,  0.89524631,  0.90769859,  0.822079  ,  1.07920223,\n",
      "        0.74425971,  0.83531777,  0.79179325,  0.95950923,  0.83755085,\n",
      "        0.70261776,  0.8783885 ,  0.98315701,  0.7247794 ,  0.86227811,\n",
      "        0.74947365,  0.84413079,  0.79237276,  0.70486108,  0.69939564])}, 'ATTACH::9::best_val_loss': 0.6993956442021612, 'ATTACH::10::history': {'train_accs': array([0.42630047, 0.48526009, 0.53548772, 0.63146503, 0.71162027,\n",
      "       0.76498246, 0.81222479, 0.844466  , 0.86103441, 0.88626017,\n",
      "       0.89909695, 0.91402343, 0.91894918, 0.92536756, 0.93574147,\n",
      "       0.9394731 , 0.94290619, 0.94887678, 0.95119039, 0.95716098,\n",
      "       0.96007165, 0.9613404 , 0.96559445, 0.96566908, 0.96910217,\n",
      "       0.96947533, 0.97298306, 0.97029629, 0.9751474 , 0.97089335,\n",
      "       0.97761027, 0.97328159, 0.9804463 , 0.97858049, 0.977461  ,\n",
      "       0.97775953, 0.97992387, 0.98171505, 0.98052093, 0.9829838 ,\n",
      "       0.98156579, 0.98283454, 0.98081946, 0.9804463 , 0.98484962]), 'train_losses': array([18.85942354,  5.4871001 ,  2.66796343,  1.20043329,  0.75845242,\n",
      "        0.61177837,  0.51262294,  0.44559602,  0.37710508,  0.32847861,\n",
      "        0.29311048,  0.25628851,  0.23602807,  0.21406082,  0.18933481,\n",
      "        0.18011921,  0.16839133,  0.15574475,  0.14295338,  0.13291147,\n",
      "        0.1206798 ,  0.117723  ,  0.10721439,  0.10143689,  0.09599499,\n",
      "        0.09911448,  0.07876145,  0.09112871,  0.07524273,  0.08627477,\n",
      "        0.07237779,  0.08187103,  0.05943284,  0.06913695,  0.07074302,\n",
      "        0.07166614,  0.05904075,  0.05773101,  0.06447898,  0.053226  ,\n",
      "        0.05420846,  0.05696852,  0.05367325,  0.06115545,  0.0442419 ]), 'val_accs': array([0.44208955, 0.58      , 0.62597015, 0.7041791 , 0.76835821,\n",
      "       0.83701493, 0.79253731, 0.86835821, 0.86447761, 0.87492537,\n",
      "       0.88238806, 0.91910448, 0.92716418, 0.92955224, 0.91373134,\n",
      "       0.91910448, 0.94268657, 0.96298507, 0.96447761, 0.88507463,\n",
      "       0.94597015, 0.96089552, 0.94925373, 0.97432836, 0.70776119,\n",
      "       0.96746269, 0.94955224, 0.97283582, 0.97701493, 0.95253731,\n",
      "       0.94089552, 0.9719403 , 0.97492537, 0.97850746, 0.98567164,\n",
      "       0.98358209, 0.94238806, 0.98238806, 0.97492537, 0.96298507,\n",
      "       0.97910448, 0.98537313, 0.94149254, 0.96895522, 0.96149254]), 'val_losses': array([19.80512113,  1.98155841,  1.11809706,  0.77248565,  0.53631898,\n",
      "        0.44161083,  0.53562785,  0.32755814,  0.35869856,  0.5077115 ,\n",
      "        0.3078633 ,  0.21284531,  0.20869565,  0.19240146,  0.27395691,\n",
      "        0.22828112,  0.18023169,  0.11725547,  0.10240351,  0.37006505,\n",
      "        0.14041881,  0.1068084 ,  0.14184877,  0.07893546,  1.26614316,\n",
      "        0.10507103,  0.16153951,  0.07584115,  0.06549513,  0.15592444,\n",
      "        0.19388632,  0.09366524,  0.08115814,  0.07547056,  0.04687599,\n",
      "        0.04940336,  0.18004766,  0.05662293,  0.07125634,  0.11281559,\n",
      "        0.06927415,  0.05141064,  0.16742972,  0.09997103,  0.11113523])}, 'ATTACH::10::best_val_loss': 0.04687599344151234, 'ATTACH::11::history': {'train_accs': array([0.47854317, 0.60094037, 0.65534741, 0.69878349, 0.70378386,\n",
      "       0.692589  , 0.6978879 , 0.69430555, 0.68885738, 0.70930666,\n",
      "       0.69661915, 0.6951265 , 0.68908127, 0.69691768, 0.69885812,\n",
      "       0.69012613, 0.66601985, 0.67146802]), 'train_losses': array([5.54827339, 0.86834069, 0.73101021, 0.66501233, 0.65842217,\n",
      "       0.66327226, 0.65362264, 0.66031823, 0.66781043, 0.63628828,\n",
      "       0.65807831, 0.6594579 , 0.67514473, 0.66359575, 0.65121013,\n",
      "       0.65955906, 0.69562242, 0.68910489]), 'val_accs': array([0.6158209 , 0.7558209 , 0.78029851, 0.8080597 , 0.76358209,\n",
      "       0.70865672, 0.71910448, 0.76059701, 0.60985075, 0.66985075,\n",
      "       0.71343284, 0.68268657, 0.61820896, 0.77373134, 0.62626866,\n",
      "       0.67671642, 0.67701493, 0.65492537]), 'val_losses': array([0.7942269 , 0.63111914, 0.60012572, 0.56864929, 0.58871608,\n",
      "       0.61532483, 0.6408991 , 0.55846538, 0.85762862, 0.65541464,\n",
      "       0.67140033, 0.72878551, 0.69613198, 0.57680101, 0.7997108 ,\n",
      "       0.84234754, 0.68132384, 0.77907588])}, 'ATTACH::11::best_val_loss': 0.5584653802772066, 'ATTACH::12::history': {'train_accs': array([0.5154116 , 0.62713635, 0.66534816, 0.67408016, 0.67863273,\n",
      "       0.68736473, 0.65213822, 0.66288529, 0.68676767, 0.68855885,\n",
      "       0.68714083, 0.65892977, 0.67997612, 0.67848347]), 'train_losses': array([10.31814701,  0.79369786,  0.73254462,  0.7126413 ,  0.70289815,\n",
      "        0.68795808,  0.71801612,  0.71138988,  0.67829487,  0.67635224,\n",
      "        0.66657762,  0.71702328,  0.68119885,  0.67962736]), 'val_accs': array([0.64656716, 0.62119403, 0.6958209 , 0.73671642, 0.70089552,\n",
      "       0.60268657, 0.62179104, 0.55940299, 0.65462687, 0.63462687,\n",
      "       0.59164179, 0.72477612, 0.53641791, 0.63134328]), 'val_losses': array([0.77247045, 0.75643859, 0.6772088 , 0.61474142, 0.64541167,\n",
      "       0.79298214, 0.73716272, 0.85301   , 0.76169887, 0.72795344,\n",
      "       0.75512192, 0.63030258, 0.84252002, 0.83687122])}, 'ATTACH::12::best_val_loss': 0.6147414215287166, 'ATTACH::13::history': {'train_accs': array([0.43339055, 0.44219718, 0.44219718, 0.44219718, 0.44219718,\n",
      "       0.44182402, 0.45324278, 0.47622957, 0.49048436, 0.49921636,\n",
      "       0.51309799, 0.52108366, 0.54899619, 0.57153519, 0.60235838,\n",
      "       0.61691171, 0.62243451, 0.65475035, 0.66684081, 0.69430555,\n",
      "       0.71057542, 0.73289051, 0.74624972, 0.74729457, 0.76520636,\n",
      "       0.77804314, 0.79311889, 0.81461303, 0.81438913, 0.83931637,\n",
      "       0.84095828, 0.85163072, 0.85170535, 0.86513919, 0.87170684,\n",
      "       0.87790134, 0.87708038, 0.88320024, 0.88364803, 0.8923054 ,\n",
      "       0.89626091, 0.89073811, 0.89797746, 0.89812673, 0.90223151,\n",
      "       0.89372341, 0.90760505, 0.90678409, 0.90148519, 0.91275468,\n",
      "       0.9114113 , 0.90850063, 0.91902381, 0.92424808, 0.92006866,\n",
      "       0.92335249, 0.92708411, 0.92805433, 0.93126353, 0.91081424]), 'train_losses': array([91.98495298,  1.25426856,  1.2442665 ,  1.24035266,  1.2362613 ,\n",
      "        1.22588764,  1.20248304,  1.16681284,  1.13820323,  1.11717566,\n",
      "        1.08953243,  1.06622118,  1.02569047,  0.99543536,  0.9320482 ,\n",
      "        0.915528  ,  0.90952966,  0.8557884 ,  0.83152672,  0.78143649,\n",
      "        0.74376398,  0.69646763,  0.6726764 ,  0.66325849,  0.62267077,\n",
      "        0.58588692,  0.54967573,  0.50453135,  0.50357248,  0.45039707,\n",
      "        0.45042966,  0.42380662,  0.41528259,  0.38230747,  0.37003257,\n",
      "        0.35194695,  0.34674163,  0.33209517,  0.33680289,  0.31164354,\n",
      "        0.29361831,  0.31611534,  0.29650383,  0.28986371,  0.27848506,\n",
      "        0.30328472,  0.26468705,  0.26239834,  0.27478572,  0.25281027,\n",
      "        0.25164575,  0.26045485,  0.23378241,  0.21966072,  0.2386846 ,\n",
      "        0.22210157,  0.21246343,  0.20198781,  0.19712919,  0.25010496]), 'val_accs': array([0.44208955, 0.44208955, 0.44208955, 0.44208955, 0.44089552,\n",
      "       0.44208955, 0.47492537, 0.48865672, 0.45940299, 0.50358209,\n",
      "       0.4958209 , 0.52179104, 0.52149254, 0.61731343, 0.60447761,\n",
      "       0.63731343, 0.65074627, 0.6761194 , 0.68537313, 0.73671642,\n",
      "       0.74      , 0.71701493, 0.73970149, 0.78686567, 0.78865672,\n",
      "       0.79373134, 0.83313433, 0.80656716, 0.83671642, 0.80358209,\n",
      "       0.85641791, 0.85104478, 0.86955224, 0.86358209, 0.8441791 ,\n",
      "       0.88298507, 0.87462687, 0.88955224, 0.89014925, 0.88059701,\n",
      "       0.87791045, 0.89552239, 0.90716418, 0.90208955, 0.84179104,\n",
      "       0.90119403, 0.91164179, 0.88537313, 0.90208955, 0.91134328,\n",
      "       0.91731343, 0.90597015, 0.91820896, 0.90686567, 0.9161194 ,\n",
      "       0.92358209, 0.92149254, 0.92089552, 0.91253731, 0.91253731]), 'val_losses': array([1.26389016, 1.24738788, 1.24258133, 1.23795838, 1.23153614,\n",
      "       1.21594904, 1.18170765, 1.14601261, 1.1416686 , 1.10560637,\n",
      "       1.13623522, 1.05862792, 1.07510816, 0.93107365, 0.91439511,\n",
      "       0.86925555, 0.85425665, 0.8204559 , 0.7909831 , 0.70674731,\n",
      "       0.67846604, 0.71367704, 0.67626347, 0.59230879, 0.58357457,\n",
      "       0.55388909, 0.49335059, 0.57286108, 0.46979289, 0.56955258,\n",
      "       0.43008781, 0.42378804, 0.38075807, 0.39240481, 0.45756046,\n",
      "       0.34247772, 0.35555155, 0.33567165, 0.33973743, 0.35798934,\n",
      "       0.37146068, 0.3268217 , 0.28079334, 0.28597155, 0.45326776,\n",
      "       0.2930327 , 0.26348408, 0.34161947, 0.28395932, 0.28024697,\n",
      "       0.2607715 , 0.28235138, 0.25161084, 0.28755715, 0.25663117,\n",
      "       0.24503647, 0.24810912, 0.24202084, 0.26776509, 0.28484604])}, 'ATTACH::13::best_val_loss': 0.24202084381188918, 'ATTACH::14::history': {'train_accs': array([0.36062393, 0.39465632, 0.41786701, 0.42839018, 0.44548101,\n",
      "       0.47496082, 0.49048436, 0.51018733, 0.52705426, 0.55228002,\n",
      "       0.57653556, 0.58907381, 0.62064333, 0.65363087, 0.68564818,\n",
      "       0.70945593, 0.73609971, 0.75669826, 0.77752071, 0.79326815,\n",
      "       0.80916486, 0.82565863, 0.82931562, 0.84901858, 0.8587208 ,\n",
      "       0.87454288, 0.88282708, 0.88521531, 0.88879767, 0.89775356,\n",
      "       0.90476901, 0.9196955 , 0.92081499, 0.92626315, 0.9224569 ,\n",
      "       0.93133816, 0.93730875, 0.93514441, 0.94514516, 0.94910068,\n",
      "       0.95111575, 0.94790656, 0.95126502, 0.95290693, 0.95932532,\n",
      "       0.95947459, 0.96611687, 0.96477349, 0.96328084, 0.96537055,\n",
      "       0.97178894, 0.97022166, 0.9753713 , 0.97619225, 0.98186432,\n",
      "       0.97022166, 0.9806702 , 0.97462497, 0.98283454, 0.98425256]), 'train_losses': array([43.03355452, 12.65303982,  8.24659609,  6.49435093,  5.01061974,\n",
      "        4.04014184,  3.51497552,  3.03400879,  2.66722733,  2.27270517,\n",
      "        1.97677551,  1.82632489,  1.63668422,  1.41036708,  1.2424616 ,\n",
      "        1.12052512,  0.98687566,  0.89094186,  0.83061585,  0.72430892,\n",
      "        0.67773527,  0.61467665,  0.59643018,  0.50318068,  0.48091635,\n",
      "        0.42539004,  0.41493035,  0.39266644,  0.37951874,  0.33219983,\n",
      "        0.32372545,  0.27099066,  0.26860145,  0.25417794,  0.26393233,\n",
      "        0.22631663,  0.20360182,  0.21739417,  0.19715774,  0.18225443,\n",
      "        0.17479249,  0.17493708,  0.16916176,  0.16055255,  0.13810159,\n",
      "        0.1424228 ,  0.13703968,  0.13493802,  0.12588879,  0.12908219,\n",
      "        0.10068333,  0.11479093,  0.09112098,  0.08923393,  0.07271168,\n",
      "        0.10683063,  0.07525379,  0.08548864,  0.06457387,  0.06057218]), 'val_accs': array([0.48298507, 0.51343284, 0.44746269, 0.44716418, 0.56567164,\n",
      "       0.58597015, 0.62597015, 0.59880597, 0.62298507, 0.69253731,\n",
      "       0.63223881, 0.72328358, 0.73940299, 0.7561194 , 0.78686567,\n",
      "       0.80985075, 0.79671642, 0.8441791 , 0.84686567, 0.83164179,\n",
      "       0.89044776, 0.86179104, 0.83253731, 0.84895522, 0.88447761,\n",
      "       0.86179104, 0.88119403, 0.90985075, 0.91104478, 0.87552239,\n",
      "       0.91432836, 0.90477612, 0.88985075, 0.89671642, 0.93402985,\n",
      "       0.92358209, 0.90089552, 0.92059701, 0.93343284, 0.92597015,\n",
      "       0.92298507, 0.93970149, 0.94477612, 0.92358209, 0.91970149,\n",
      "       0.93731343, 0.90835821, 0.93223881, 0.96029851, 0.92626866,\n",
      "       0.92358209, 0.94089552, 0.96089552, 0.91522388, 0.89432836,\n",
      "       0.91970149, 0.93880597, 0.94238806, 0.95761194, 0.96119403]), 'val_losses': array([5.11479625, 2.1825927 , 1.91593136, 2.11641271, 1.30993048,\n",
      "       1.21688941, 1.15661486, 1.2397424 , 1.16961987, 0.90279107,\n",
      "       1.28395357, 0.81498364, 0.86192985, 0.76076718, 0.68211821,\n",
      "       0.56607133, 0.71314431, 0.5541287 , 0.46587934, 0.52738471,\n",
      "       0.38290507, 0.44737279, 0.51779355, 0.45610107, 0.36441138,\n",
      "       0.52139854, 0.36920948, 0.29917644, 0.2946835 , 0.4525518 ,\n",
      "       0.31400843, 0.32482606, 0.3745243 , 0.37257357, 0.25928328,\n",
      "       0.25585747, 0.32305832, 0.27974398, 0.23920168, 0.26118411,\n",
      "       0.2801199 , 0.23817272, 0.24914965, 0.26469746, 0.28348331,\n",
      "       0.24607132, 0.32256159, 0.24567146, 0.21456468, 0.28535014,\n",
      "       0.26753884, 0.24853921, 0.20740713, 0.37112122, 0.39172056,\n",
      "       0.26351957, 0.24691432, 0.24272157, 0.20728221, 0.21962368])}, 'ATTACH::14::best_val_loss': 0.2072822087736272, 'ATTACH::15::history': {'train_accs': array([0.42428539, 0.48473767, 0.48279722, 0.51145608, 0.52690499,\n",
      "       0.54765281, 0.5483245 , 0.54899619, 0.56332562, 0.57929696,\n",
      "       0.57683409, 0.58519292, 0.58459587, 0.5899694 , 0.59414882,\n",
      "       0.59735801, 0.59026793, 0.5987014 , 0.60414956, 0.60310471,\n",
      "       0.60474662, 0.60780655, 0.61549369, 0.61534443, 0.62847974,\n",
      "       0.61661318, 0.62579297, 0.62265841, 0.62803194, 0.62489738,\n",
      "       0.63124114, 0.63250989, 0.63318158, 0.63280842, 0.62967386,\n",
      "       0.64161505, 0.6371371 , 0.63676394, 0.63892828, 0.63848048,\n",
      "       0.64609299, 0.64437645, 0.65295918, 0.63877901, 0.64907829,\n",
      "       0.64922755, 0.65154116, 0.64295843, 0.64057019, 0.64922755,\n",
      "       0.64773491, 0.65049631, 0.65101873, 0.64751101, 0.65273528,\n",
      "       0.6481827 , 0.64952608, 0.64900366, 0.6569147 , 0.65206359]), 'train_losses': array([6.29863545, 1.08554217, 1.01197248, 0.95446817, 0.93613511,\n",
      "       0.90303096, 0.88393943, 0.87789918, 0.84131566, 0.83534637,\n",
      "       0.82915249, 0.82424773, 0.81882558, 0.81262674, 0.80268897,\n",
      "       0.79925462, 0.79798358, 0.79464809, 0.79014207, 0.78992882,\n",
      "       0.79193648, 0.78695866, 0.7791765 , 0.77074666, 0.77248827,\n",
      "       0.76817967, 0.76163812, 0.76881826, 0.76358631, 0.7658366 ,\n",
      "       0.76177071, 0.7574488 , 0.74852039, 0.75298599, 0.75340773,\n",
      "       0.74591934, 0.75021337, 0.75023137, 0.74723793, 0.73600029,\n",
      "       0.73605453, 0.7389617 , 0.73902586, 0.75030422, 0.73475643,\n",
      "       0.73484867, 0.73678767, 0.74266805, 0.73890245, 0.73394117,\n",
      "       0.73102141, 0.73236431, 0.73072667, 0.73459286, 0.72264085,\n",
      "       0.72821372, 0.73100622, 0.7229519 , 0.71636264, 0.72122805]), 'val_accs': array([0.43671642, 0.58746269, 0.49552239, 0.54716418, 0.57761194,\n",
      "       0.64029851, 0.60149254, 0.59552239, 0.58985075, 0.6358209 ,\n",
      "       0.63850746, 0.58328358, 0.60776119, 0.63761194, 0.64298507,\n",
      "       0.66776119, 0.63701493, 0.64865672, 0.66656716, 0.63641791,\n",
      "       0.65850746, 0.59074627, 0.66567164, 0.67552239, 0.70149254,\n",
      "       0.66597015, 0.67134328, 0.6880597 , 0.71074627, 0.70328358,\n",
      "       0.68746269, 0.70507463, 0.71791045, 0.70865672, 0.69014925,\n",
      "       0.69432836, 0.7161194 , 0.69432836, 0.68985075, 0.70746269,\n",
      "       0.72477612, 0.70567164, 0.69343284, 0.72358209, 0.74626866,\n",
      "       0.71880597, 0.68597015, 0.69074627, 0.69761194, 0.72447761,\n",
      "       0.70179104, 0.70238806, 0.6838806 , 0.71820896, 0.70716418,\n",
      "       0.71970149, 0.69940299, 0.70626866, 0.70447761, 0.71044776]), 'val_losses': array([1.14028607, 1.02292436, 0.92436046, 0.86349835, 0.84683341,\n",
      "       0.82904383, 0.8148035 , 0.80816373, 0.78421611, 0.76315224,\n",
      "       0.75522979, 0.74123124, 0.74674265, 0.73504223, 0.72398013,\n",
      "       0.72420283, 0.71913711, 0.7192784 , 0.70808757, 0.72687056,\n",
      "       0.71198171, 0.74994261, 0.69588846, 0.68326904, 0.67928259,\n",
      "       0.68023854, 0.69420242, 0.66891284, 0.69720744, 0.67551938,\n",
      "       0.67507536, 0.67482398, 0.693781  , 0.6474854 , 0.6769592 ,\n",
      "       0.65827424, 0.66305621, 0.66262813, 0.64146618, 0.64255575,\n",
      "       0.64747183, 0.67366759, 0.65928064, 0.64135327, 0.64689162,\n",
      "       0.63860672, 0.68631258, 0.63913764, 0.67625638, 0.63725585,\n",
      "       0.65814453, 0.63598469, 0.6943877 , 0.62979664, 0.64878196,\n",
      "       0.6209764 , 0.62887891, 0.63052898, 0.62159287, 0.65421279])}, 'ATTACH::15::best_val_loss': 0.6209763952155611, 'ATTACH::16::history': {'train_accs': array([0.44510784, 0.52541234, 0.57967012, 0.57795358, 0.54384656,\n",
      "       0.54399582, 0.545787  , 0.53593552, 0.54295097, 0.50152996,\n",
      "       0.50152996, 0.50108217, 0.46988581, 0.48473767, 0.4873498 ,\n",
      "       0.48578252]), 'train_losses': array([18.23398373,  1.01357317,  0.87562442,  0.89037452,  0.90715124,\n",
      "        0.91223955,  0.89860728,  0.92338363,  0.8881396 ,  0.95448034,\n",
      "        0.98418051,  0.97323756,  0.99982716,  1.02313494,  0.95359728,\n",
      "        0.96237446]), 'val_accs': array([0.45164179, 0.66029851, 0.50567164, 0.52865672, 0.56537313,\n",
      "       0.60089552, 0.59134328, 0.47432836, 0.41970149, 0.48477612,\n",
      "       0.56477612, 0.55671642, 0.4558209 , 0.52865672, 0.45880597,\n",
      "       0.5241791 ]), 'val_losses': array([1.96724291, 0.85016987, 1.11441355, 0.87330931, 0.85328093,\n",
      "       0.76072476, 0.872267  , 1.09691374, 0.97038898, 0.91809543,\n",
      "       0.87549327, 0.88775241, 1.72612314, 0.9004899 , 1.21816394,\n",
      "       0.91245294])}, 'ATTACH::16::best_val_loss': 0.7607247550807782, 'ATTACH::17::history': {'train_accs': array([0.44346593, 0.48428987, 0.47936413, 0.49369356, 0.50600791,\n",
      "       0.52018807, 0.5317561 , 0.53645794, 0.5453392 , 0.55847451,\n",
      "       0.57302784, 0.58817822, 0.61676244, 0.65818345, 0.70266438,\n",
      "       0.72542727, 0.73878648, 0.76453467, 0.78386447, 0.81110531,\n",
      "       0.82521084, 0.8336443 , 0.8419285 , 0.84872005, 0.8617061 ,\n",
      "       0.87200537, 0.87797597, 0.89140981, 0.90029107, 0.90775431,\n",
      "       0.91835212, 0.92066572, 0.92880066, 0.92999478, 0.93536831,\n",
      "       0.92790507, 0.94424957, 0.95454885, 0.95313083, 0.9555937 ,\n",
      "       0.95865363, 0.96074334, 0.96857974, 0.96380327, 0.96290768,\n",
      "       0.97171431, 0.97522203, 0.97186357, 0.97701321, 0.97969998,\n",
      "       0.97246063, 0.97895365, 0.97865512, 0.98499888, 0.97566983,\n",
      "       0.97664005, 0.98693932, 0.98477498, 0.98768565, 0.98164042]), 'train_losses': array([11.38997328,  1.2257192 ,  1.19721494,  1.16241377,  1.14045651,\n",
      "        1.11848757,  1.10248166,  1.08742555,  1.06492125,  1.03909504,\n",
      "        1.01434534,  0.9769023 ,  0.93917204,  0.8889908 ,  0.82554181,\n",
      "        0.76905963,  0.72462825,  0.65267476,  0.60709684,  0.52937247,\n",
      "        0.48532487,  0.45746022,  0.43963711,  0.40748361,  0.3714128 ,\n",
      "        0.34872031,  0.33844808,  0.29533871,  0.28459345,  0.26960145,\n",
      "        0.25084755,  0.23858769,  0.20871394,  0.21729177,  0.20318077,\n",
      "        0.22445906,  0.18077531,  0.14951699,  0.15572354,  0.14378591,\n",
      "        0.13459948,  0.12645103,  0.10513603,  0.11526941,  0.11686511,\n",
      "        0.09073823,  0.08491047,  0.09393445,  0.07694865,  0.06843   ,\n",
      "        0.09232885,  0.07283392,  0.07441781,  0.05565697,  0.08366842,\n",
      "        0.07646154,  0.05004739,  0.05141543,  0.04686785,  0.06477582]), 'val_accs': array([0.46567164, 0.44149254, 0.48477612, 0.48507463, 0.50656716,\n",
      "       0.49970149, 0.54567164, 0.49373134, 0.55104478, 0.56298507,\n",
      "       0.57791045, 0.59283582, 0.63731343, 0.65761194, 0.71223881,\n",
      "       0.68447761, 0.73910448, 0.77343284, 0.78238806, 0.81641791,\n",
      "       0.80537313, 0.83791045, 0.83731343, 0.8438806 , 0.86716418,\n",
      "       0.85850746, 0.87223881, 0.8561194 , 0.88179104, 0.8880597 ,\n",
      "       0.88835821, 0.87761194, 0.90865672, 0.91104478, 0.90089552,\n",
      "       0.92      , 0.92477612, 0.9080597 , 0.91432836, 0.93044776,\n",
      "       0.92567164, 0.92985075, 0.93343284, 0.9438806 , 0.94179104,\n",
      "       0.94059701, 0.94477612, 0.93731343, 0.94716418, 0.94179104,\n",
      "       0.9441791 , 0.95134328, 0.94985075, 0.94865672, 0.93074627,\n",
      "       0.95731343, 0.96149254, 0.95373134, 0.95283582, 0.95074627]), 'val_losses': array([1.26337834, 1.23908821, 1.18382868, 1.1600533 , 1.13346963,\n",
      "       1.12540987, 1.09037006, 1.11795089, 1.05868581, 1.03152987,\n",
      "       1.01742859, 0.95481661, 0.9118124 , 0.88665065, 0.79064749,\n",
      "       0.82564003, 0.70270337, 0.63891462, 0.60882408, 0.52296466,\n",
      "       0.53200904, 0.43708357, 0.44114122, 0.41875585, 0.36321271,\n",
      "       0.38027599, 0.34559198, 0.39551245, 0.3385273 , 0.33976677,\n",
      "       0.34912219, 0.37259193, 0.31225807, 0.2880619 , 0.31462597,\n",
      "       0.25020128, 0.25621195, 0.3117175 , 0.29670583, 0.2393688 ,\n",
      "       0.24090616, 0.23737317, 0.23617103, 0.20803068, 0.21198737,\n",
      "       0.21118352, 0.20533041, 0.24123215, 0.19253564, 0.22652709,\n",
      "       0.22761742, 0.1841228 , 0.18056665, 0.20139887, 0.27027095,\n",
      "       0.1832296 , 0.17212775, 0.19506135, 0.22433014, 0.198334  ])}, 'ATTACH::17::best_val_loss': 0.17212775474164024, 'ATTACH::18::history': {'train_accs': array([0.40137324, 0.44167475, 0.46212404, 0.46921412, 0.45861631,\n",
      "       0.46055676, 0.47212479, 0.46839316, 0.4705575 , 0.47772222,\n",
      "       0.46996044, 0.47309501, 0.476752  , 0.48145384, 0.48361818,\n",
      "       0.49399209, 0.49175312, 0.49652959, 0.509441  , 0.50638107,\n",
      "       0.5069035 , 0.50287335, 0.51153071, 0.51130681, 0.51406821,\n",
      "       0.51668035, 0.52481528, 0.53235316, 0.52914397, 0.53421897,\n",
      "       0.52309874, 0.53257706, 0.53720427, 0.54399582, 0.53511456,\n",
      "       0.54526457, 0.54586163, 0.54683185, 0.54675722, 0.54907083,\n",
      "       0.55339951, 0.55310098, 0.55586238, 0.5538473 , 0.55623554,\n",
      "       0.5598179 , 0.55892231, 0.56034032, 0.56101202, 0.56250466,\n",
      "       0.56347489, 0.56414658, 0.5653407 , 0.56384805, 0.56310172,\n",
      "       0.56548996, 0.56384805, 0.56496753, 0.56526606, 0.56757967]), 'train_losses': array([6.68250517, 1.2805418 , 1.15125802, 1.08440988, 1.04117632,\n",
      "       1.01733537, 0.99359533, 0.9722749 , 0.96290981, 0.95260215,\n",
      "       0.94614286, 0.94282941, 0.93028779, 0.9270716 , 0.92429362,\n",
      "       0.90901172, 0.9022641 , 0.90487032, 0.89865412, 0.89796991,\n",
      "       0.88929647, 0.89430097, 0.88524937, 0.88502706, 0.88798865,\n",
      "       0.88127117, 0.87085186, 0.86996519, 0.8703346 , 0.86183157,\n",
      "       0.87064205, 0.8609585 , 0.86107062, 0.8486618 , 0.8555151 ,\n",
      "       0.84882923, 0.84726885, 0.85114245, 0.84031624, 0.84388739,\n",
      "       0.83560928, 0.84039847, 0.83101631, 0.83198364, 0.83442955,\n",
      "       0.83336645, 0.83182028, 0.82745189, 0.82285561, 0.81845596,\n",
      "       0.82047857, 0.8210661 , 0.81491501, 0.81998081, 0.81675407,\n",
      "       0.8115671 , 0.8116124 , 0.81203195, 0.81064055, 0.81260086]), 'val_accs': array([0.45761194, 0.45134328, 0.4838806 , 0.54238806, 0.47791045,\n",
      "       0.5119403 , 0.48656716, 0.49731343, 0.53253731, 0.49761194,\n",
      "       0.48268657, 0.49343284, 0.48059701, 0.4719403 , 0.54238806,\n",
      "       0.54298507, 0.57492537, 0.5441791 , 0.55671642, 0.52149254,\n",
      "       0.55432836, 0.55910448, 0.54537313, 0.57880597, 0.52925373,\n",
      "       0.53940299, 0.55850746, 0.59910448, 0.60477612, 0.61761194,\n",
      "       0.55462687, 0.5758209 , 0.5680597 , 0.56328358, 0.58447761,\n",
      "       0.61552239, 0.60985075, 0.56268657, 0.58925373, 0.5919403 ,\n",
      "       0.55402985, 0.60447761, 0.59522388, 0.62865672, 0.63223881,\n",
      "       0.57940299, 0.62507463, 0.63552239, 0.62089552, 0.54925373,\n",
      "       0.62298507, 0.5958209 , 0.56268657, 0.60089552, 0.61731343,\n",
      "       0.60447761, 0.58149254, 0.65373134, 0.60059701, 0.56925373]), 'val_losses': array([1.27164824, 1.09512708, 0.9953494 , 0.96927351, 0.96129592,\n",
      "       0.91597723, 0.90116872, 0.91176756, 0.88882148, 0.8908249 ,\n",
      "       0.87161727, 0.87288967, 0.85399774, 0.85263163, 0.8597025 ,\n",
      "       0.83897363, 0.83299271, 0.83472746, 0.83231183, 0.82348045,\n",
      "       0.80604692, 0.81315098, 0.81823426, 0.83340448, 0.83503146,\n",
      "       0.81498838, 0.7990299 , 0.78392632, 0.79587336, 0.78601694,\n",
      "       0.80335405, 0.79227819, 0.78842931, 0.79804954, 0.7873809 ,\n",
      "       0.76244231, 0.77405009, 0.76685877, 0.79097355, 0.75671601,\n",
      "       0.78190635, 0.75891246, 0.75665877, 0.75020139, 0.75934545,\n",
      "       0.7639213 , 0.7846261 , 0.74763235, 0.7376305 , 0.76673915,\n",
      "       0.75397185, 0.75994354, 0.75510396, 0.7359028 , 0.7389068 ,\n",
      "       0.73663868, 0.74713069, 0.73035591, 0.73396319, 0.72715077])}, 'ATTACH::18::best_val_loss': 0.7271507695895523, 'ATTACH::19::history': {'train_accs': array([0.32039704, 0.37517725, 0.37562505, 0.38681991, 0.39532801,\n",
      "       0.41637436, 0.42592731, 0.43428614, 0.43697291, 0.45286962,\n",
      "       0.45436227, 0.46988581, 0.48130457, 0.49040973, 0.49541011,\n",
      "       0.51145608, 0.51921785, 0.52548698, 0.53608478, 0.53608478,\n",
      "       0.54735428, 0.55728039, 0.55884768, 0.57108739, 0.59049183,\n",
      "       0.58907381, 0.59064109, 0.61116501, 0.61668781, 0.62183745,\n",
      "       0.62340473, 0.62497201, 0.64236137, 0.6566908 , 0.64922755,\n",
      "       0.65751175, 0.66676618, 0.67236361, 0.67736398, 0.68445406,\n",
      "       0.69139488, 0.7010971 , 0.70624673, 0.71609822, 0.71759086,\n",
      "       0.72445705, 0.72938279, 0.74177177, 0.74386148, 0.74401075,\n",
      "       0.75647436, 0.76266886, 0.76334055, 0.77050526, 0.78296888,\n",
      "       0.77946115, 0.78364057, 0.79535786, 0.79252183, 0.80200015]), 'train_losses': array([53.13452452, 34.04002188, 24.57113798, 19.12744396, 15.44612185,\n",
      "       12.44875842, 10.77072632,  9.72325302,  8.39569976,  7.45081927,\n",
      "        6.74362359,  6.09325388,  5.46966975,  5.00369746,  4.56971747,\n",
      "        4.09537841,  3.87982827,  3.65763102,  3.4668083 ,  3.19682805,\n",
      "        3.03413595,  2.84914846,  2.75270054,  2.51107182,  2.34280364,\n",
      "        2.3136347 ,  2.20915378,  1.98974279,  1.97822735,  1.82397833,\n",
      "        1.77173542,  1.782329  ,  1.65928862,  1.50675529,  1.51541873,\n",
      "        1.47227315,  1.41218622,  1.32330781,  1.29046707,  1.2086241 ,\n",
      "        1.20855076,  1.12943006,  1.10304625,  1.04061372,  1.02268641,\n",
      "        0.95962471,  0.94065757,  0.92277942,  0.89323131,  0.88967088,\n",
      "        0.82674899,  0.80300229,  0.80041389,  0.7620407 ,  0.72578732,\n",
      "        0.70721449,  0.70742463,  0.68393649,  0.66659758,  0.63649303]), 'val_accs': array([0.42      , 0.46895522, 0.44746269, 0.45462687, 0.42716418,\n",
      "       0.51402985, 0.47641791, 0.49134328, 0.50447761, 0.53880597,\n",
      "       0.56686567, 0.61164179, 0.6158209 , 0.59014925, 0.64149254,\n",
      "       0.62985075, 0.64626866, 0.67223881, 0.68268657, 0.71074627,\n",
      "       0.6441791 , 0.6519403 , 0.62746269, 0.70507463, 0.71761194,\n",
      "       0.72149254, 0.6919403 , 0.74298507, 0.68029851, 0.68507463,\n",
      "       0.73432836, 0.66656716, 0.71522388, 0.75164179, 0.69134328,\n",
      "       0.76686567, 0.76716418, 0.66746269, 0.75313433, 0.72537313,\n",
      "       0.76029851, 0.78567164, 0.79402985, 0.79731343, 0.79940299,\n",
      "       0.80507463, 0.76865672, 0.7441791 , 0.77044776, 0.80746269,\n",
      "       0.82328358, 0.76716418, 0.82835821, 0.8041791 , 0.83880597,\n",
      "       0.80029851, 0.8519403 , 0.81164179, 0.83283582, 0.85074627]), 'val_losses': array([14.28613272, 10.79238147,  9.97958076,  8.30385851,  6.71810339,\n",
      "        5.27298949,  4.36523669,  3.37391263,  2.65034111,  2.50743308,\n",
      "        1.88978714,  1.84683401,  1.54729818,  1.39826375,  1.36816402,\n",
      "        1.11789914,  1.17239011,  1.0977758 ,  0.96572016,  0.93827408,\n",
      "        1.07311739,  0.96772314,  1.03357068,  0.94266486,  0.84511131,\n",
      "        0.7680371 ,  0.81929316,  0.72139092,  0.81576139,  0.85913442,\n",
      "        0.71312649,  1.139614  ,  0.78502642,  0.64535425,  1.03516494,\n",
      "        0.61185588,  0.61196917,  0.89599192,  0.63704437,  0.77409388,\n",
      "        0.63337425,  0.56320631,  0.54496164,  0.53066527,  0.52852657,\n",
      "        0.52808467,  0.6566184 ,  0.7484084 ,  0.67425104,  0.55043028,\n",
      "        0.51744157,  0.63317355,  0.47688823,  0.52175785,  0.4726517 ,\n",
      "        0.63360628,  0.43472067,  0.54858889,  0.47183502,  0.42415344])}, 'ATTACH::19::best_val_loss': 0.4241534375432712, 'ATTACH::20::history': {'train_accs': array([0.39682066, 0.41824017, 0.44660049, 0.47914023, 0.51160534,\n",
      "       0.55571311, 0.59989551, 0.64303306, 0.68923054, 0.74393611,\n",
      "       0.78199866, 0.81088141, 0.83998806, 0.86275095, 0.88148369,\n",
      "       0.89775356, 0.90947086, 0.92506904, 0.93156206, 0.94171207,\n",
      "       0.9502948 , 0.95678782, 0.96022091, 0.96253452, 0.96768416,\n",
      "       0.97014703, 0.96947533, 0.97455034, 0.97417718, 0.97589372,\n",
      "       0.9836555 , 0.98231211, 0.98171505, 0.98335697, 0.98664079,\n",
      "       0.98679006, 0.98484962, 0.98716322, 0.98858124, 0.98790955,\n",
      "       0.98910366, 0.98962609, 0.98708859, 0.99164117, 0.98947683,\n",
      "       0.99335771, 0.99126801, 0.99029778, 0.99082021, 0.99268602,\n",
      "       0.99283529, 0.99313382, 0.99246212, 0.99156653, 0.99268602,\n",
      "       0.99462646, 0.99373088, 0.99432793, 0.99261139, 0.9972386 ]), 'train_losses': array([1.67901535e+01, 6.75295145e+00, 4.10090935e+00, 2.87596163e+00,\n",
      "       2.15962992e+00, 1.66189284e+00, 1.31858077e+00, 1.09055890e+00,\n",
      "       8.99694093e-01, 7.25284435e-01, 6.11520875e-01, 5.34086202e-01,\n",
      "       4.53151029e-01, 3.88288559e-01, 3.35641893e-01, 2.90187577e-01,\n",
      "       2.65974676e-01, 2.24507882e-01, 1.99343082e-01, 1.75015917e-01,\n",
      "       1.50340866e-01, 1.34174382e-01, 1.24242148e-01, 1.13070323e-01,\n",
      "       1.02465768e-01, 9.62254080e-02, 9.33159762e-02, 8.00551525e-02,\n",
      "       7.55762793e-02, 7.44188603e-02, 5.47213552e-02, 5.58525175e-02,\n",
      "       5.35681342e-02, 5.18380673e-02, 4.40899864e-02, 4.43137054e-02,\n",
      "       4.79264593e-02, 4.29566737e-02, 3.62927461e-02, 4.07567950e-02,\n",
      "       3.57595460e-02, 3.21413048e-02, 4.35536906e-02, 2.48272893e-02,\n",
      "       3.63843121e-02, 1.96836127e-02, 2.45582380e-02, 3.01685021e-02,\n",
      "       2.73628141e-02, 2.64254880e-02, 2.53115161e-02, 2.25265760e-02,\n",
      "       2.38362058e-02, 2.71046997e-02, 2.17669236e-02, 1.97459508e-02,\n",
      "       2.02439572e-02, 1.61912431e-02, 2.87599616e-02, 9.99554778e-03]), 'val_accs': array([0.49820896, 0.50089552, 0.5680597 , 0.48746269, 0.52895522,\n",
      "       0.5838806 , 0.69432836, 0.63343284, 0.71940299, 0.75880597,\n",
      "       0.80149254, 0.85820896, 0.87761194, 0.89641791, 0.88925373,\n",
      "       0.91791045, 0.90865672, 0.93014925, 0.90925373, 0.95283582,\n",
      "       0.93970149, 0.94597015, 0.89910448, 0.94358209, 0.94716418,\n",
      "       0.94656716, 0.93313433, 0.96955224, 0.93223881, 0.92925373,\n",
      "       0.95253731, 0.97492537, 0.97791045, 0.96328358, 0.96507463,\n",
      "       0.7919403 , 0.97402985, 0.97134328, 0.9761194 , 0.97910448,\n",
      "       0.9558209 , 0.84656716, 0.98298507, 0.96746269, 0.98447761,\n",
      "       0.97313433, 0.98358209, 0.97552239, 0.98328358, 0.98447761,\n",
      "       0.97880597, 0.98776119, 0.98208955, 0.98029851, 0.96955224,\n",
      "       0.9841791 , 0.97671642, 0.76716418, 0.97940299, 0.98298507]), 'val_losses': array([3.30778184, 2.61013198, 3.49936594, 2.72899668, 1.60949229,\n",
      "       2.40341432, 1.0950316 , 0.82614435, 0.7260457 , 0.73700064,\n",
      "       0.51438273, 0.42746691, 0.34409687, 0.31149386, 0.3179786 ,\n",
      "       0.22949705, 0.25231998, 0.19314537, 0.25547454, 0.1287983 ,\n",
      "       0.18335999, 0.18378574, 0.30423936, 0.17509993, 0.1550539 ,\n",
      "       0.15983026, 0.21492523, 0.10421032, 0.22943465, 0.21135811,\n",
      "       0.14181588, 0.08358415, 0.07028284, 0.12358521, 0.1194121 ,\n",
      "       1.02991632, 0.0794009 , 0.10273618, 0.09543722, 0.07698289,\n",
      "       0.13760468, 1.04663006, 0.06625998, 0.11450302, 0.05733383,\n",
      "       0.10386002, 0.07232377, 0.08789947, 0.06603955, 0.06951386,\n",
      "       0.08054626, 0.05234719, 0.06454813, 0.08425875, 0.12221669,\n",
      "       0.07768382, 0.10798323, 1.51687534, 0.09167517, 0.07692413])}, 'ATTACH::20::best_val_loss': 0.05234718787949532, 'ATTACH::21::history': {'train_accs': array([0.42622584, 0.46055676, 0.49257407, 0.53436824, 0.58161057,\n",
      "       0.63333085, 0.68818569, 0.7316964 , 0.77446078, 0.80311964,\n",
      "       0.83692813, 0.86290022, 0.88118516, 0.90126129, 0.91588925,\n",
      "       0.92693485, 0.94111501, 0.94208523, 0.95313083, 0.9583551 ,\n",
      "       0.95999701, 0.96596761, 0.96440033, 0.97052019, 0.97081872,\n",
      "       0.97313232, 0.97283379, 0.97865512, 0.97761027, 0.97917755,\n",
      "       0.97999851, 0.98156579, 0.98358086, 0.98350623, 0.98455109,\n",
      "       0.98529741, 0.98664079, 0.98470035, 0.98537204, 0.98559594,\n",
      "       0.98820808, 0.98611837, 0.98581984, 0.98955146, 0.9891783 ,\n",
      "       0.98805881, 0.98940219]), 'train_losses': array([12.2144193 ,  4.64706377,  2.92632093,  2.07693311,  1.59494914,\n",
      "        1.14803348,  0.94704331,  0.73602124,  0.60184941,  0.53111077,\n",
      "        0.45582734,  0.37257167,  0.32291953,  0.28038619,  0.24009987,\n",
      "        0.21252467,  0.17779843,  0.16755691,  0.13946657,  0.12730921,\n",
      "        0.11467701,  0.10615334,  0.10518985,  0.09359934,  0.08767216,\n",
      "        0.08269291,  0.08097451,  0.07017658,  0.07554198,  0.06612884,\n",
      "        0.06163393,  0.05627449,  0.05386355,  0.05377297,  0.05184743,\n",
      "        0.04793323,  0.04413987,  0.04886008,  0.04534916,  0.04820766,\n",
      "        0.03789723,  0.04262015,  0.04655284,  0.03107114,  0.03443989,\n",
      "        0.04025343,  0.03480982]), 'val_accs': array([0.48059701, 0.55164179, 0.53492537, 0.57731343, 0.76835821,\n",
      "       0.59910448, 0.77462687, 0.79552239, 0.84507463, 0.65313433,\n",
      "       0.80477612, 0.92716418, 0.92089552, 0.94686567, 0.93014925,\n",
      "       0.94447761, 0.93552239, 0.93164179, 0.95313433, 0.96686567,\n",
      "       0.97313433, 0.97343284, 0.97761194, 0.97641791, 0.9558209 ,\n",
      "       0.96865672, 0.98208955, 0.97522388, 0.97253731, 0.98149254,\n",
      "       0.98208955, 0.97910448, 0.97820896, 0.9838806 , 0.98716418,\n",
      "       0.98238806, 0.98686567, 0.98567164, 0.9758209 , 0.97522388,\n",
      "       0.9841791 , 0.98149254, 0.98298507, 0.98477612, 0.98656716,\n",
      "       0.98716418, 0.96835821]), 'val_losses': array([4.50838   , 2.85011336, 1.46088244, 1.44179172, 0.577598  ,\n",
      "       1.34822435, 0.5573955 , 0.52860016, 0.41405054, 1.24429584,\n",
      "       0.49857416, 0.20693593, 0.20638493, 0.15666275, 0.1901066 ,\n",
      "       0.13206922, 0.16332977, 0.1797913 , 0.137597  , 0.10164245,\n",
      "       0.07920288, 0.07844865, 0.0734485 , 0.07123339, 0.13960136,\n",
      "       0.09472229, 0.06175558, 0.08203254, 0.08854898, 0.0575322 ,\n",
      "       0.05969812, 0.07676308, 0.07163103, 0.05063971, 0.04750086,\n",
      "       0.05429099, 0.04260263, 0.04949803, 0.0787305 , 0.08418417,\n",
      "       0.05417643, 0.04474608, 0.05594453, 0.05263457, 0.04438768,\n",
      "       0.04688052, 0.13145472])}, 'ATTACH::21::best_val_loss': 0.04260263274020668, 'ATTACH::22::history': {'train_accs': array([0.41689678, 0.47511008, 0.50324651, 0.53414434, 0.58116277,\n",
      "       0.6035525 , 0.63012165, 0.6594522 , 0.69430555, 0.72856183,\n",
      "       0.75475782, 0.79296962, 0.82916636, 0.84371968, 0.86409434,\n",
      "       0.88514068, 0.89454437, 0.90581387, 0.91260542, 0.922233  ,\n",
      "       0.93036794, 0.93574147, 0.94104038, 0.94424957, 0.94872752,\n",
      "       0.95342936, 0.9528323 , 0.95895216, 0.95745951, 0.96305694,\n",
      "       0.96514665, 0.96738563, 0.96902754, 0.96783342, 0.97044556,\n",
      "       0.97275916, 0.97275916, 0.97604299, 0.97686395, 0.97596836,\n",
      "       0.97552056, 0.97589372, 0.97775953, 0.98014777, 0.98081946,\n",
      "       0.97992387, 0.98462572, 0.98156579, 0.98231211, 0.98141652,\n",
      "       0.98462572, 0.98395403, 0.98350623, 0.9836555 , 0.98395403,\n",
      "       0.9834316 , 0.98529741, 0.98716322, 0.98664079, 0.98664079]), 'train_losses': array([11.46439764,  4.06032837,  2.60704457,  1.82848774,  1.33115869,\n",
      "        1.0931537 ,  0.94041594,  0.82187816,  0.73104635,  0.65198717,\n",
      "        0.60565409,  0.52217938,  0.44958611,  0.43331182,  0.36957414,\n",
      "        0.32524584,  0.29352722,  0.26108722,  0.24607735,  0.21893548,\n",
      "        0.21576025,  0.18371621,  0.17460007,  0.16155507,  0.14688413,\n",
      "        0.13955724,  0.1435905 ,  0.11792356,  0.12036354,  0.11166981,\n",
      "        0.10281812,  0.09755109,  0.09934535,  0.09988443,  0.08762534,\n",
      "        0.08610436,  0.07803575,  0.07396725,  0.07492397,  0.07755136,\n",
      "        0.07283314,  0.06645389,  0.06443998,  0.05735891,  0.0576919 ,\n",
      "        0.06140845,  0.05060085,  0.05498113,  0.05410999,  0.06230423,\n",
      "        0.04725358,  0.04973224,  0.05142995,  0.05226788,  0.04675885,\n",
      "        0.04815974,  0.04717919,  0.03881852,  0.04072111,  0.04152649]), 'val_accs': array([0.54119403, 0.60686567, 0.59761194, 0.66985075, 0.65104478,\n",
      "       0.65223881, 0.69402985, 0.66089552, 0.73701493, 0.84238806,\n",
      "       0.7958209 , 0.75940299, 0.77134328, 0.80955224, 0.82447761,\n",
      "       0.90537313, 0.90358209, 0.91134328, 0.95671642, 0.94865672,\n",
      "       0.94835821, 0.9238806 , 0.94179104, 0.95373134, 0.93761194,\n",
      "       0.89432836, 0.96686567, 0.92567164, 0.9680597 , 0.97164179,\n",
      "       0.97373134, 0.97104478, 0.95313433, 0.97671642, 0.97283582,\n",
      "       0.97641791, 0.97313433, 0.95671642, 0.97940299, 0.9758209 ,\n",
      "       0.98298507, 0.97820896, 0.97910448, 0.9441791 , 0.96328358,\n",
      "       0.9758209 , 0.97671642, 0.95731343, 0.95134328, 0.96447761,\n",
      "       0.98537313, 0.96298507, 0.98537313, 0.94716418, 0.98328358,\n",
      "       0.94626866, 0.97074627, 0.95671642, 0.98298507, 0.98208955]), 'val_losses': array([2.44670669, 1.28114921, 1.89052254, 0.89016256, 0.88750314,\n",
      "       0.71413257, 0.76402978, 0.83771476, 0.60248422, 0.41471575,\n",
      "       0.45362096, 0.58913708, 0.48518292, 0.50601901, 0.42794956,\n",
      "       0.24412635, 0.25676332, 0.25221589, 0.13785186, 0.15225615,\n",
      "       0.13621938, 0.20730389, 0.16018154, 0.12626262, 0.16849866,\n",
      "       0.29400074, 0.09668299, 0.21509432, 0.09638703, 0.08753807,\n",
      "       0.08790588, 0.10055797, 0.14628652, 0.08065982, 0.08540643,\n",
      "       0.07859256, 0.08850453, 0.1353576 , 0.07802637, 0.09398574,\n",
      "       0.06130525, 0.07770578, 0.07478093, 0.16668133, 0.09151918,\n",
      "       0.08467595, 0.09107052, 0.11999125, 0.13158742, 0.10402456,\n",
      "       0.05842308, 0.12112328, 0.0539491 , 0.17741708, 0.06539181,\n",
      "       0.15742194, 0.08142282, 0.16827499, 0.05488383, 0.05813261])}, 'ATTACH::22::best_val_loss': 0.053949100220047716, 'ATTACH::23::history': {'train_accs': array([0.41428465, 0.48809613, 0.53593552, 0.5987014 , 0.65109337,\n",
      "       0.69885812, 0.74311516, 0.79565639, 0.83371893, 0.86028808,\n",
      "       0.87917009, 0.89745503, 0.91708336, 0.92365102, 0.93230838,\n",
      "       0.94014479, 0.94783193, 0.95253377, 0.95671319, 0.95939996,\n",
      "       0.95992238, 0.96141503, 0.96626614, 0.9694007 , 0.97178894,\n",
      "       0.97507277, 0.97484887, 0.97417718, 0.97462497, 0.97686395,\n",
      "       0.97775953, 0.98052093, 0.98029704, 0.98096873, 0.9832077 ,\n",
      "       0.98126726, 0.9836555 , 0.98656616, 0.98522278, 0.98484962,\n",
      "       0.98701396, 0.98559594, 0.98723785, 0.98634226, 0.98701396,\n",
      "       0.98858124, 0.98902903, 0.98955146, 0.98940219, 0.98686469]), 'train_losses': array([12.18710671,  3.87945444,  2.3994677 ,  1.57938109,  1.09170609,\n",
      "        0.84439135,  0.68921049,  0.52842997,  0.43872085,  0.37246825,\n",
      "        0.31752077,  0.29002209,  0.23633459,  0.21863523,  0.19547523,\n",
      "        0.17500996,  0.15085799,  0.14897317,  0.12888651,  0.1261439 ,\n",
      "        0.11774069,  0.11719639,  0.10280692,  0.09789683,  0.08925337,\n",
      "        0.07741566,  0.08491794,  0.08126065,  0.08028   ,  0.07080633,\n",
      "        0.06527201,  0.05981093,  0.0595616 ,  0.05415312,  0.05126191,\n",
      "        0.05894871,  0.05165403,  0.04155895,  0.04729791,  0.04657871,\n",
      "        0.04269363,  0.0423986 ,  0.04186089,  0.04170149,  0.04245058,\n",
      "        0.03906629,  0.03946784,  0.03264988,  0.03221064,  0.04134797]), 'val_accs': array([0.47970149, 0.53462687, 0.5161194 , 0.71283582, 0.70895522,\n",
      "       0.64567164, 0.64716418, 0.80268657, 0.73731343, 0.79880597,\n",
      "       0.91880597, 0.90537313, 0.90238806, 0.90985075, 0.9358209 ,\n",
      "       0.95044776, 0.93223881, 0.8719403 , 0.94746269, 0.95820896,\n",
      "       0.96119403, 0.96925373, 0.95402985, 0.96179104, 0.94716418,\n",
      "       0.93104478, 0.95373134, 0.95492537, 0.97014925, 0.98059701,\n",
      "       0.97402985, 0.97462687, 0.96746269, 0.97850746, 0.97522388,\n",
      "       0.98537313, 0.94656716, 0.97731343, 0.98089552, 0.98507463,\n",
      "       0.98149254, 0.98089552, 0.97462687, 0.98179104, 0.98238806,\n",
      "       0.98298507, 0.98656716, 0.98149254, 0.9841791 , 0.98507463]), 'val_losses': array([4.43827255, 3.31471318, 2.69974598, 0.90901708, 0.92583048,\n",
      "       0.87464907, 1.22928551, 0.50800912, 0.61486126, 0.48744636,\n",
      "       0.21629652, 0.25505638, 0.2759916 , 0.21700501, 0.18067663,\n",
      "       0.15932455, 0.19259596, 0.40745236, 0.13844892, 0.11326426,\n",
      "       0.11174748, 0.10477859, 0.1498042 , 0.11451438, 0.15416866,\n",
      "       0.2294647 , 0.15360888, 0.1386998 , 0.09790272, 0.06677578,\n",
      "       0.07547753, 0.0816288 , 0.11223462, 0.06872932, 0.08465978,\n",
      "       0.05327241, 0.1571894 , 0.0912773 , 0.06136093, 0.05063068,\n",
      "       0.06591073, 0.06530515, 0.08331427, 0.06226183, 0.05782385,\n",
      "       0.05391512, 0.05130255, 0.05692548, 0.05450818, 0.05572492])}, 'ATTACH::23::best_val_loss': 0.050630681554494954, 'ATTACH::24::history': {'train_accs': array([0.48167774, 0.58310322, 0.62721099, 0.66766177, 0.68885738,\n",
      "       0.70602284, 0.71856109, 0.72833794, 0.74154788, 0.7537876 ,\n",
      "       0.75848944, 0.76214643, 0.75796701, 0.77043063, 0.7646093 ,\n",
      "       0.76953504, 0.77558027, 0.78184939, 0.78281961, 0.79073065,\n",
      "       0.79155161]), 'train_losses': array([6.31001207, 1.05717828, 0.82097644, 0.72388604, 0.67564693,\n",
      "       0.64951649, 0.61369963, 0.58668388, 0.57706689, 0.55827734,\n",
      "       0.54226956, 0.53830033, 0.544514  , 0.52239429, 0.52697863,\n",
      "       0.51661919, 0.50738219, 0.49452425, 0.49319345, 0.48511598,\n",
      "       0.49270287]), 'val_accs': array([0.53850746, 0.63492537, 0.63223881, 0.73402985, 0.74298507,\n",
      "       0.69104478, 0.70865672, 0.62358209, 0.74776119, 0.77223881,\n",
      "       0.78746269, 0.82      , 0.7080597 , 0.79522388, 0.76985075,\n",
      "       0.74955224, 0.71044776, 0.75791045, 0.72955224, 0.79940299,\n",
      "       0.71970149]), 'val_losses': array([1.65544807, 0.72147233, 0.75032517, 0.61524634, 0.55033039,\n",
      "       0.65376548, 0.63544393, 0.94496633, 0.52279718, 0.58732699,\n",
      "       0.50076351, 0.52853865, 0.70618418, 0.54934507, 0.56561146,\n",
      "       0.68911523, 0.77538132, 0.61612941, 0.85801846, 0.55853341,\n",
      "       0.65425396])}, 'ATTACH::24::best_val_loss': 0.500763506960513, 'ATTACH::25::history': {'train_accs': array([0.3962236 , 0.42249422, 0.45510859, 0.4848123 , 0.51891932,\n",
      "       0.56354952, 0.61295619, 0.65721322, 0.69744011, 0.7261736 ,\n",
      "       0.7620718 , 0.78976043, 0.80714979, 0.83312187, 0.85349653,\n",
      "       0.86655721, 0.88864841, 0.89984327, 0.91551608, 0.92514367,\n",
      "       0.93506978, 0.93790581, 0.94611538, 0.94992164, 0.95686245,\n",
      "       0.95902679, 0.96223599, 0.96790805, 0.96746026, 0.96835585,\n",
      "       0.97313232, 0.97574446, 0.97484887, 0.97731174, 0.97917755,\n",
      "       0.98081946, 0.98037167, 0.98216285, 0.98261064, 0.98208822,\n",
      "       0.98335697, 0.98447645, 0.98552131, 0.98693932, 0.9857452 ,\n",
      "       0.98798418, 0.98798418, 0.986193  , 0.98798418]), 'train_losses': array([13.53437516,  4.8773484 ,  3.09039546,  2.21129858,  1.7213438 ,\n",
      "        1.40562742,  1.12303701,  0.97163871,  0.83922736,  0.75528546,\n",
      "        0.64737847,  0.56933436,  0.52031196,  0.45678639,  0.39886439,\n",
      "        0.37520549,  0.32124237,  0.2842733 ,  0.24373215,  0.21864914,\n",
      "        0.19394356,  0.17668074,  0.15932364,  0.15112676,  0.13228226,\n",
      "        0.12560233,  0.11392561,  0.10151648,  0.09761711,  0.09798948,\n",
      "        0.08221985,  0.07553589,  0.07508634,  0.07063071,  0.06569989,\n",
      "        0.06245679,  0.06014388,  0.05479302,  0.05238284,  0.05641637,\n",
      "        0.05271506,  0.05054716,  0.04504882,  0.04251126,  0.04450066,\n",
      "        0.03686221,  0.03522097,  0.04315162,  0.03734734]), 'val_accs': array([0.46149254, 0.53492537, 0.50328358, 0.5919403 , 0.67014925,\n",
      "       0.66089552, 0.7041791 , 0.66      , 0.65164179, 0.71731343,\n",
      "       0.85731343, 0.81671642, 0.81850746, 0.79761194, 0.83044776,\n",
      "       0.83850746, 0.89134328, 0.85701493, 0.92686567, 0.89044776,\n",
      "       0.89761194, 0.94776119, 0.95462687, 0.94208955, 0.95313433,\n",
      "       0.93641791, 0.96567164, 0.97104478, 0.90865672, 0.97343284,\n",
      "       0.96955224, 0.97761194, 0.97164179, 0.96835821, 0.96656716,\n",
      "       0.97791045, 0.96149254, 0.97761194, 0.98149254, 0.89731343,\n",
      "       0.97910448, 0.98      , 0.97522388, 0.94746269, 0.97343284,\n",
      "       0.97641791, 0.97313433, 0.98208955, 0.94477612]), 'val_losses': array([3.11901463, 1.52487742, 2.48422043, 0.90107108, 0.90086034,\n",
      "       1.00036307, 0.77505335, 0.83718986, 1.09524306, 0.87589762,\n",
      "       0.4018179 , 0.51367773, 0.58352303, 0.57439984, 0.43915771,\n",
      "       0.45076007, 0.29684235, 0.39371652, 0.19741627, 0.30529301,\n",
      "       0.31288677, 0.15014919, 0.122687  , 0.15108935, 0.11806592,\n",
      "       0.17502763, 0.10642681, 0.08600169, 0.29233492, 0.07766863,\n",
      "       0.09246484, 0.06512384, 0.08637023, 0.0990947 , 0.10389163,\n",
      "       0.07308473, 0.11061699, 0.07094017, 0.05801285, 0.45203441,\n",
      "       0.07082518, 0.06232375, 0.08584964, 0.1772694 , 0.10203253,\n",
      "       0.07673151, 0.08308349, 0.071175  , 0.19051272])}, 'ATTACH::25::best_val_loss': 0.058012851547839035, 'ATTACH::26::history': {'train_accs': array([0.34435406, 0.39405926, 0.41824017, 0.44600343, 0.46652735,\n",
      "       0.49436525, 0.51705351, 0.54310023, 0.56735577, 0.58317785,\n",
      "       0.61929995, 0.63422643, 0.66310919, 0.67505038, 0.69826106,\n",
      "       0.71318755, 0.74341369, 0.75229495, 0.77028136, 0.77744608,\n",
      "       0.79170087, 0.80311964, 0.8143145 , 0.83080827, 0.8364057 ,\n",
      "       0.84498843, 0.85707889, 0.86036271, 0.86707963, 0.87752817,\n",
      "       0.87596089, 0.87909545, 0.88573774, 0.89312635]), 'train_losses': array([27.04787483, 11.05346849,  7.57902944,  5.74230683,  4.37715336,\n",
      "        3.48586007,  2.97810685,  2.45707073,  2.12263947,  1.8881134 ,\n",
      "        1.5818717 ,  1.43897238,  1.25150163,  1.1816394 ,  1.06486067,\n",
      "        0.9661591 ,  0.84631104,  0.79652057,  0.72359619,  0.6690671 ,\n",
      "        0.63825759,  0.60052132,  0.55832589,  0.50334393,  0.49466795,\n",
      "        0.45969762,  0.43442359,  0.42226668,  0.38951533,  0.3725373 ,\n",
      "        0.36912929,  0.35122963,  0.341701  ,  0.3272571 ]), 'val_accs': array([0.46895522, 0.44835821, 0.47970149, 0.49223881, 0.57701493,\n",
      "       0.55283582, 0.63373134, 0.54985075, 0.59432836, 0.6       ,\n",
      "       0.63641791, 0.75343284, 0.71432836, 0.77074627, 0.76238806,\n",
      "       0.70507463, 0.67552239, 0.78716418, 0.76835821, 0.80776119,\n",
      "       0.82865672, 0.76776119, 0.79343284, 0.89731343, 0.80865672,\n",
      "       0.75283582, 0.76208955, 0.76895522, 0.7080597 , 0.79970149,\n",
      "       0.76626866, 0.84029851, 0.73761194, 0.86865672]), 'val_losses': array([5.54665467, 4.32193442, 2.91231395, 3.74413523, 1.71294249,\n",
      "       2.99483715, 1.63879437, 2.93274347, 1.48895469, 2.74100698,\n",
      "       1.12649009, 0.75417256, 1.00073677, 0.71956858, 0.79158464,\n",
      "       1.15848032, 1.21148981, 0.64978776, 0.71713003, 0.64858356,\n",
      "       0.48025691, 0.64305205, 0.69549551, 0.30227306, 0.72943911,\n",
      "       0.83044037, 0.77648444, 0.73856294, 1.22125116, 0.59288626,\n",
      "       0.80346921, 0.44450394, 0.71673546, 0.34124754])}, 'ATTACH::26::best_val_loss': 0.3022730637308377, 'ATTACH::27::history': {'train_accs': array([0.39435779, 0.4429435 , 0.47757295, 0.52100903, 0.57213225,\n",
      "       0.61549369, 0.64497351, 0.68773789, 0.71945668, 0.74819016,\n",
      "       0.79737294, 0.81968804, 0.85864617, 0.88581237, 0.89357415,\n",
      "       0.90730652, 0.92618852, 0.93059184, 0.94492126, 0.94626465,\n",
      "       0.95111575, 0.95992238, 0.95902679, 0.96335547, 0.97014703,\n",
      "       0.97014703, 0.96760952, 0.97798343, 0.97522203, 0.97858049,\n",
      "       0.97761027, 0.97977461, 0.97917755, 0.97865512, 0.98231211,\n",
      "       0.98417792, 0.98313307, 0.98447645, 0.9829838 , 0.98902903,\n",
      "       0.9857452 , 0.98798418, 0.98753638, 0.98746175, 0.98671543,\n",
      "       0.98761102, 0.99126801, 0.98932756, 0.98649153, 0.99037242,\n",
      "       0.98970072, 0.98999925, 0.99059631, 0.9914919 , 0.9919397 ,\n",
      "       0.98887977, 0.99417867, 0.99119337, 0.99089484, 0.99231286]), 'train_losses': array([2.00413105e+01, 7.82951524e+00, 4.92202091e+00, 3.49186617e+00,\n",
      "       2.50696466e+00, 1.77973719e+00, 1.51995633e+00, 1.14497736e+00,\n",
      "       9.42150754e-01, 7.93557220e-01, 6.04865121e-01, 5.31840481e-01,\n",
      "       4.35012948e-01, 3.27880305e-01, 3.09572268e-01, 2.73026400e-01,\n",
      "       2.23081638e-01, 2.30250500e-01, 1.69014996e-01, 1.73821769e-01,\n",
      "       1.48571605e-01, 1.20600977e-01, 1.30985038e-01, 1.07593524e-01,\n",
      "       8.79956768e-02, 8.97869163e-02, 1.01614049e-01, 7.36970436e-02,\n",
      "       7.58975669e-02, 6.51726721e-02, 7.03603366e-02, 6.04970884e-02,\n",
      "       6.51073406e-02, 6.32055092e-02, 5.24729558e-02, 4.67449656e-02,\n",
      "       5.27831248e-02, 5.16694213e-02, 5.45061961e-02, 3.41137652e-02,\n",
      "       4.33878049e-02, 3.59257612e-02, 3.89075602e-02, 3.75587695e-02,\n",
      "       4.05708716e-02, 3.96899629e-02, 3.20203070e-02, 3.66185904e-02,\n",
      "       3.87065865e-02, 3.13133244e-02, 3.15479995e-02, 3.50274791e-02,\n",
      "       2.94865914e-02, 2.59100388e-02, 2.60173617e-02, 3.60831279e-02,\n",
      "       1.99240343e-02, 2.77139204e-02, 2.73618188e-02, 2.46497469e-02]), 'val_accs': array([0.46626866, 0.43850746, 0.55820896, 0.50567164, 0.61044776,\n",
      "       0.66298507, 0.73253731, 0.74656716, 0.66119403, 0.82895522,\n",
      "       0.83402985, 0.88268657, 0.87373134, 0.92656716, 0.93104478,\n",
      "       0.94746269, 0.86477612, 0.93343284, 0.95283582, 0.93880597,\n",
      "       0.95074627, 0.96298507, 0.89492537, 0.97104478, 0.97492537,\n",
      "       0.96328358, 0.96268657, 0.97850746, 0.9680597 , 0.98208955,\n",
      "       0.96985075, 0.97671642, 0.97671642, 0.94626866, 0.97134328,\n",
      "       0.9758209 , 0.97761194, 0.97970149, 0.97850746, 0.98358209,\n",
      "       0.96507463, 0.97910448, 0.96686567, 0.98507463, 0.98567164,\n",
      "       0.97253731, 0.98567164, 0.9761194 , 0.98447761, 0.98776119,\n",
      "       0.98358209, 0.97910448, 0.98567164, 0.98537313, 0.98477612,\n",
      "       0.98686567, 0.98119403, 0.98626866, 0.97552239, 0.98567164]), 'val_losses': array([8.22300711, 4.16564029, 3.39987363, 2.64743551, 2.07405776,\n",
      "       2.13279512, 0.79289882, 0.81462875, 1.22433338, 0.51300162,\n",
      "       0.45421076, 0.31205063, 0.35639754, 0.22062684, 0.21337769,\n",
      "       0.15615353, 0.45239392, 0.19324816, 0.13443323, 0.17459516,\n",
      "       0.15056545, 0.10562098, 0.35126276, 0.09284344, 0.08136458,\n",
      "       0.1255781 , 0.12835227, 0.0698713 , 0.09921154, 0.0669708 ,\n",
      "       0.09570422, 0.08318637, 0.06855877, 0.16942797, 0.0953236 ,\n",
      "       0.08595686, 0.07597178, 0.0929488 , 0.08346958, 0.06366995,\n",
      "       0.14095358, 0.08687359, 0.09737858, 0.05650035, 0.05464904,\n",
      "       0.09864013, 0.05465759, 0.08979097, 0.06106823, 0.05266043,\n",
      "       0.05490826, 0.0776275 , 0.06505806, 0.04802121, 0.06232404,\n",
      "       0.05050574, 0.06637558, 0.05630408, 0.09423067, 0.05617588])}, 'ATTACH::27::best_val_loss': 0.04802120743736402, 'ATTACH::28::history': {'train_accs': array([0.45354131, 0.5568326 , 0.62430032, 0.6702739 , 0.71050078,\n",
      "       0.73841331, 0.76849019, 0.77647586, 0.79528323, 0.80274647,\n",
      "       0.81162773, 0.80476155, 0.81483693, 0.81364281, 0.8090156 ,\n",
      "       0.81394134, 0.81438913, 0.81632958, 0.80953803, 0.80700052]), 'train_losses': array([9.26518507, 1.24981478, 0.82662962, 0.72681479, 0.63925313,\n",
      "       0.5883142 , 0.53725837, 0.50178054, 0.48209122, 0.46944443,\n",
      "       0.45060392, 0.46721661, 0.44548283, 0.45407916, 0.44217656,\n",
      "       0.44847588, 0.44342136, 0.44105478, 0.46009615, 0.48180317]), 'val_accs': array([0.59104478, 0.66895522, 0.60029851, 0.59373134, 0.77970149,\n",
      "       0.78865672, 0.75343284, 0.6758209 , 0.69462687, 0.83970149,\n",
      "       0.74776119, 0.77970149, 0.78477612, 0.79402985, 0.74268657,\n",
      "       0.78477612, 0.75850746, 0.72149254, 0.79791045, 0.76597015]), 'val_losses': array([1.00872081, 0.96307077, 0.81873363, 0.83519005, 0.49517578,\n",
      "       0.48377746, 0.74940616, 0.97260124, 0.71746935, 0.46130807,\n",
      "       0.55001756, 0.5532726 , 0.52785753, 0.50109676, 0.60030323,\n",
      "       0.51035842, 0.6351222 , 0.75834415, 0.5160717 , 0.56845612])}, 'ATTACH::28::best_val_loss': 0.46130806853522116, 'ATTACH::29::history': {'train_accs': array([0.5347414 , 0.58989477, 0.59623853, 0.62556907, 0.65542205,\n",
      "       0.66146727, 0.66825883, 0.66348235, 0.67661766, 0.67161728,\n",
      "       0.67445332, 0.69572356, 0.68520039, 0.67616986, 0.67766251,\n",
      "       0.6870662 , 0.69348459, 0.70035077, 0.69639525]), 'train_losses': array([3.60733782, 0.89425793, 0.85621645, 0.78794652, 0.76206945,\n",
      "       0.74769239, 0.73130926, 0.7487807 , 0.72843378, 0.7525106 ,\n",
      "       0.72366996, 0.70717963, 0.71092455, 0.71045481, 0.7055348 ,\n",
      "       0.70768116, 0.68797897, 0.67074874, 0.69387098]), 'val_accs': array([0.69164179, 0.65014925, 0.59074627, 0.69074627, 0.56656716,\n",
      "       0.6480597 , 0.56925373, 0.65373134, 0.71014925, 0.59970149,\n",
      "       0.64238806, 0.64776119, 0.69820896, 0.63940299, 0.5841791 ,\n",
      "       0.62835821, 0.61283582, 0.64358209, 0.71402985]), 'val_losses': array([0.74151191, 0.74876979, 0.90846515, 0.67393157, 0.81614447,\n",
      "       0.74664076, 0.90144419, 0.75608686, 0.6217607 , 0.82392019,\n",
      "       0.80593688, 0.76904189, 0.75162831, 0.727778  , 0.89543667,\n",
      "       0.9266557 , 0.78355279, 0.78648548, 0.66837279])}, 'ATTACH::29::best_val_loss': 0.6217606982544287}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def to_json_safe(obj):\n",
    "    \"\"\"Recursively convert objects to JSON-serializable types.\"\"\"\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    if isinstance(obj, (np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    if isinstance(obj, (np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.isoformat()\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: to_json_safe(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, (list, tuple)):\n",
    "        return [to_json_safe(v) for v in obj]\n",
    "    return obj\n",
    "\n",
    "\n",
    "def serialize_trial(trial):\n",
    "    \"\"\"Extract only JSON-safe and meaningful parts of a Hyperopt Trial.\"\"\"\n",
    "    print(trial.attachments)\n",
    "    return {\n",
    "        \"attachments\": to_json_safe(trial.attachments)\n",
    "    }\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# DROP-IN REPLACEMENT STARTS HERE\n",
    "# ===============================\n",
    "\n",
    "json_results = []\n",
    "\n",
    "for entry in trial_results:\n",
    "    json_results.append({\n",
    "        \"params\": to_json_safe(entry.get(\"params\")),\n",
    "        \"trial\": serialize_trial(entry.get(\"trials\")),\n",
    "    })\n",
    "\n",
    "with open(\"all_trials.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {\"results\": json_results},\n",
    "        f,\n",
    "        indent=4,\n",
    "        ensure_ascii=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1faee892-23fa-430f-8bde-2e9ff855827a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization complete. Best trial found at index 1.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA/AAAAJ1CAYAAAB+VRvRAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjYsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvq6yFwwAAAAlwSFlzAAAPYQAAD2EBqD+naQABAABJREFUeJzs3Xl4G9W5P/DvzEij1ZYSOyvOYid22EIIlLAvgZBAWUIKDS330kBb2t+lFFqgXGhLISyl9AJlabrQUrjQ0nJp2UvDdtkKhD2hbInj2NmInThOZGtf5vz+8J3BsiVbmpHtkfL9PE8eyJEsnVfzWpN3zplzJCGEABERERERERHZmjzaHSAiIiIiIiKiobGAJyIiIiIiIioDLOCJiIiIiIiIygALeCIiIiIiIqIywAKeiIiIiIiIqAywgCciIiIiIiIqAyzgiYiIiIiIiMoAC3giIiIiIiKiMsACnoiIiIiIiKgMsIAnooojSRKOO+44S6/x0ksvQZIkXHvttSXpE9nLtddeC0mS8NJLL412V0ZVvjw/7rjjIEmS5dcppfvuuw+SJOG+++4btvcgKtZ5550HSZLQ1tZmtLW1tUGSJJx33nkj1o9SvWexv/tENPJYwBPRsJAkqag/NLTp06dnfWaKoqC2thYLFy7E448/PtrdoxL74Q9/CEmScNNNNw36PE3TMHXqVCiKgs2bN49Q70pvNIoeK/QLCj/72c9GuytUpvQLX4X+sXphmogqg2O0O0BElemaa64Z0Hb77bcjFArlfKyUPvnkE3i9XkuvMW/ePHzyySeora0tUa9KQ1EU/PjHPwYAJJNJfPrpp3jiiSfw3HPP4ZZbbsFll102yj0sDxdddBG+8pWvYOrUqaPdlby+/vWv46abbsK9996Lq666Ku/znnvuOWzevBknnXQSpkyZUpL3vv/++xGNRkvyWqWyZMkSHHbYYZg0adJod4WoJKZPnz7gfLh7927ccccdmDZt2oCLWdOnT8/7WnvttRc++eQTBAKBYegpEdkJC3giGha5ptLed999CIVCwz4tfe+997b8Gl6vtySvU2oOh2PA5/fss8/ipJNOwk9+8hP8x3/8h+WLF3uC2tpa212c6W/mzJk49thj8fLLL+PVV1/F0UcfnfN5f/jDHwAA3/jGN0r23na8sBEIBFicUEWZPn36gO/ztrY23HHHHTkfG4zT6bTlOYuISo9T6IloVPWdNvvJJ59gyZIlqKmpybqn8NFHH8VXv/pVzJw5E16vF4FAAEcffTT+9re/5XzNXFMN9fsUW1tbceedd2LvvfeGy+XCtGnTsHz5cmialvX8fPf0Tp8+HdOnT0c4HMYll1yCyZMnw+Vy4YADDsBf//rXvDGeffbZGDt2LPx+P4499li88sorJbsPe+HChZg1axai0Sg++ugjAJ/fxxiPx/HjH/8YM2bMgNPpzIrntddewymnnIKxY8fC7XZj7733xjXXXJN35PWRRx7BF77wBXg8HkyYMAEXXHABdu3aZXwmfemf94YNG3Drrbdi3333hcvlyhpR2r59O77//e9j5syZcLlcqK2txZlnnokPP/xwwHs3Nzfj/PPPR319PVwuF8aOHYs5c+bge9/7HoQQxvO2bduGSy65BI2NjfB4PAgGg9hnn33w//7f/0MoFDKeN9hn/+STT2L+/PkIBALweDyYM2cObrvtNqTT6azn9c3d9evXY8mSJRgzZgx8Ph8WLFiANWvW5DtkBdOLcr1I76+rqwuPP/44amtrcfrppyOZTOKuu+7CokWLMGXKFLhcLowfPx5f+tKX8P777xf8vvnug43FYrjyyisxZcoUuN1u7L///vjd736X93UK/d297777UF9fDwD47//+76xpw/oxGuwe+GJyWf9+6OjowLJly1BbWwuPx4PDDjtsWNdEKKaP7733Hs466yxMnToVLpcL48aNwyGHHIIbb7wx63mF/l7k89lnn+Gaa67BYYcdhvHjx8PlcmH69Om48MILsX379pw/k0wm8Ytf/AKHHHIIqqqq4Pf7se++++LSSy/Frl27jOfp3wu7d+/GRRddhClTpsDhcGQdv0J/1wDgxRdfxMknn2x8506YMAFHH3007r77blOfXT7r1q3DFVdcgYMOOgg1NTVwu91oamrClVdeiXA4XNBrDIe++f/kk0/iyCOPRFVVlfHdm+8WlHfffRcXXXQR9t9/f+Nznj17Nn72s58hlUoV9N6apuH3v/895s2bh7Fjx8Lj8aCurg6nnXbaHr+OCNFo4Ag8EdnC+vXrcdhhh2H27Nk477zzsHPnTqiqCgC46qqroKoqjjrqKEyaNAk7duzAE088gbPOOgt33nknvvvd7xb8Pj/4wQ/w8ssv49RTT8WiRYvw2GOP4dprr0UymSz4H3ipVAoLFy7Erl27cOaZZyIajeIvf/kLli5dipUrV2LhwoXGc7du3YojjjgC27Ztw0knnYS5c+di7dq1OPHEE3H88ccX9yEVoH/RdeaZZ2LNmjU46aSTEAwGjSLp4Ycfxle/+lW4XC6cffbZGD9+PJ599llcd911eOaZZ/DSSy/B7XYbr/OHP/wB3/jGN1BdXY2vfe1rCAQCePrpp3HiiScilUrB6XTm7M93v/tdrFq1CqeccgpOO+00jB8/HgDQ0tKC4447Dlu2bMHChQtxxhlnYPv27fjb3/6GZ555Bi+88AIOPfRQAL1Fxrx58xCJRHDKKafg7LPPRiQSQXNzM371q1/hlltugcPhQDQaxZFHHom2tjYsXLgQS5YsQTKZRGtrKx544AFcfvnlQ47g3nbbbbjsssswduxYnHPOOfD5fHjiiSdw2WWX4dVXX8Ujjzwy4DNua2vDYYcdhv322w9f//rX0dLSgscffxzz58/HJ598ggkTJhjPve+++3D++edj2bJlBS3GdtZZZ+G73/0uHn74Ydx1113w+/1Zjz/44INIJBK48MILoaoq2tvb8b3vfQ9HH300vvjFL2LMmDHYsGEDnnjiCfzjH//AK6+8gkMOOWTI981F0zScfvrpeP755zF79mycc8452LlzJ77//e9j/vz5OX+m0N/dAw88EJdccgnuuOMOzJkzB2eccYbxGoNNGwaKz2Wgd5ryUUcdhUAggHPPPRfbt2/HQw89hEWLFuHdd9/F/vvvb+ozKkUfV69ejSOOOAKKomDx4sWYNm0adu/ejY8//hh33303fvSjHwEo/PdiMK+88gpuvfVWnHDCCTj00EPhdDrx/vvv49e//jWeeeYZvPfee1m/M7FYDCeeeCJee+01NDY24vzzz4fL5UJzczN++9vf4mtf+xrGjBljPD+RSOD4449HOBzG6aefDofDYfw+FPO79ve//x2nnXYagsEgFi9ebOTSmjVr8MADD+Bb3/pWUZ/dYB555BHcc889mD9/Po477jhomoZVq1bh5ptvxssvv4xXXnkl7/fdSHj44Yfx7LPP4tRTT8WFF16I7u7uQZ//u9/9Dk8++SSOOeYYfPGLX0Q0GsVLL72Eq666Cm+//XbeC+F9XXXVVfj5z3+OGTNm4JxzzkFVVRW2bt2Kf/7zn3j++ed5bz7RSBNERCNk2rRpov/XTmtrqwAgAIif/OQnOX+upaVlQFtPT4+YPXu2CAQCIhKJZD0GQBx77LFZbcuWLRMARH19vfjss8+M9h07dohgMCiqqqpEIpEw2l988UUBQFxzzTU5Y1i8eHHW859//nkBQCxatCjr+f/+7/8uAIgbb7wxq/2ee+4x4n7xxRdzxt3ftGnThMvlGtD+/PPPC0mShM/nE9FoVAghxLHHHisAiAMPPFDs3Lkz6/mhUEgEAgHhcrnEmjVrjPZMJiPOPvtsAUBcd911RvuuXbuE3+8XPp9PrFu3zmhPpVLi+OOPFwDEtGnTst5D/7zr6urExo0bB/T5iCOOEIqiiJUrV2a1r127VlRVVYnZs2cbbXfeeacAIG6//fYBr9M3tieeeEIAEN/73vcGPK+np0fE43Hj79dcc82Az379+vXC4XCI8ePHi02bNhnt8XhcHHXUUQKAuP/++432vrn7s5/9LOv9fvzjHwsA4qabbspqv/feewUAsWzZsgF9zOf//b//JwCI3//+9wMemzt3rgAgPvzwQ6OvW7ZsGfC8Dz/8UPj9frFgwYKs9nx5rudPrr6fdNJJIp1OG+0ffPCBUFU15+sU87urf575Phv9/e+9916jrdhcFkIYx+zCCy8UmUzGaP/9738vAIhvf/vbOd8/X3/6H+P+iu3jpZdeKgCIxx57bMBrdXZ2Gv9f6O/FYDo6OkRPT8+A9v/+7/8WAMQNN9yQ1X7ZZZcJAOLcc8/NygEhhNi9e3fWa+nflYsWLTK+l3TF/q596UtfEgDE6tWrB/S172dS6Gc3mC1btmR9t+uWL18uAIg//vGPWe36d11ra6vRNlQuD0b/2f7nMD3fZFkWzz33XN6f6/+eGzduHHCsNE0TX//61wUA8c9//jPrsVy/+2PHjhWTJ08ecK4VovBcI6LS4RR6IrKFiRMn5h0daWhoGNDm9/tx3nnnIRQK4e233y74fa6++uqsRbBqa2uxePFi9PT0YO3atQW/zi9+8QtjhgAAnHDCCZg2bVpWXxKJBB5++GGMHz9+wOJy559/PmbNmlXw++nS6TSuvfZaXHvttfjRj36Es846CyeddBKEELj++uvh8Xiynr98+XKMHTs2q+3xxx9HKBTC17/+dRxwwAFGuyzL+PnPfz5gmuvjjz+OcDiMb3zjG2hsbDTaHQ4HbrjhhkH7+4Mf/GDA/dTvv/8+Xn/9dSxbtgyLFi3KeqypqQkXXHAB/vWvfw2YSt8/NgADYsv3PL/fD5fLNWhfH3zwQaTTaVx22WVZi8G5XC7cfPPNAJBz1Ly+vh4/+MEPstr0qe/9c3PJkiX45JNPhlxZPtdr9Z9Gv2bNGrz//vuYN28e9ttvP6Ove+2114DX2G+//TB//ny88sorBU+b7e/+++8HANx4441QFMVonz17Ns4999ycP1PK391cis1lnc/nw8033wxZ/vyfQcuWLYPD4bDcp1L1MVce19TUFPS8XL8XuYwfP37ArA4AOPfcc1FdXY3nn3/eaEun07j77rsRCARwxx13ZOUA0LtGQa7X+vnPfz6gj2Z/16x8Jrmel8tee+2V9d2uu+iiiwAg6zMZDYsXL8aCBQsKfr6+Q0VfkiThO9/5DoDC41FVdcDrAIXnGhGVDgt4IrKFOXPm5PxHE9B7r/Sll16KffbZB16v17g3Vi+KP/vss4Lf5+CDDx7QVldXB6B3Wm0h+k5F7/86fV9j7dq1SCQS+MIXvjCgeJQkCUcccUTB/dZlMhksX74cy5cvx89+9jO89NJLOP744/H444/j+9///oDnz5s3b0Cbfi90rmmPU6dORUNDAzZs2ICenh4AMO7lPuqoowY8/9BDDx10mm6u91+1ahUAoKOjw7gY0ffPp59+CgDGf0877TT4fD585zvfwdlnn417770XGzZsGPC6xxxzDCZNmoSf/exnOOWUU/DrX/8aH3/8cUH3AgODfy6HH3443G43Vq9ePeCxAw88MKsQBPLnVCAQwN57713USupf+MIXMGfOHLz++utZF5nuueceAAMXr1u9ejXOOeccTJ06FaqqGr8vTz75JJLJJDo7Owt+777WrFkDn8+Hgw46aMBj+RbYK+Xvbi7F5rKuqalpQLGpT+8u9HtguPq4dOlSyLKMJUuW4Otf/zr+/Oc/Y+vWrQN+ttDfi6E88sgjWLRoEcaNGweHwwFJkiDLMrq7u7OOz6effoqenh4ccsghWdPkB+N2uzF79uwB7cX+rn3lK18BABx22GG46KKL8Oijj+bM40I/u8EIIfCHP/wBxxxzDMaOHQtFUSBJknEBwGrOWpXrO3UwyWQSt912G+bNm4fq6mrIsgxJkoxzYSHxfOUrX0FbWxv2339/XH311fjf//1fxGIxU/0nIut4DzwR2ULf+4T76urqwiGHHIJNmzbhyCOPxIIFCxAMBqEoClavXo3HH38ciUSi4Peprq4e0KYXoJlMpqDXyHcftcPhyFoMT783Ub/vu798MQ/G5XIhHo8X/Pxc76H3K9/7T5o0CevWrUN3dzeqqqoGjUOW5UFXc8/1Hl1dXQB672v9+9//nvdnI5EIgN57oFetWoVrr70WTz/9NP7nf/4HQO9uA9dddx2+/OUvA+g9LqtWrcJPfvITPPnkk3j66acBAFOmTMGVV16JCy+8MO97AYN/LpIkYcKECTmLgVLk1FC+8Y1v4OKLL8Yf/vAH3HzzzUgmk3jwwQfh9XqN4gYAXn/9dWNthYULF6KxsRF+vx+SJOGxxx7DmjVrivp96SsUCuXdpi7fcS7l724uxeayLtcxA3qPW6mOmdk+HnrooXjppZfw05/+FA8++CDuvfdeAMAhhxyCm2++2VhvoNDfi8HceuutuPzyyzFu3DgsXLgQdXV1xuj17bffnnV89EUgc83wyGf8+PE5F0Ms9nfty1/+Mh577DHcdttt+M1vfoMVK1ZAkiTMnz8ft956Kw488EAAKPizG8zFF1+MX/7yl5gyZQpOP/10TJo0ybgAu3z5css5a1Wx542zzjoLTz75JJqamoz1F5xOp7FdXSHx3HHHHaivr8e9996LG264ATfccAPcbjeWLl2KW2+91fY7ehBVGhbwRGQLuf6RB/SOMm7atAnXX3+9sf+57mc/+xkef/zxkeieKXqRkG81546OjmHvQ67PVe9Xvvdvb2/Pet5gcWiahs7Ozrz/qB/s/e+66y5jWupQ9t9/f/z1r39FKpXCu+++i3/84x+48847cfbZZ2Py5Mk48sgjAfSOaN53333QNA0ffPABnn32Wdx55534zne+gzFjxuCrX/1q3vfo+7lMmzYt6zEhBDo6OvIWfsPt3/7t3/CDH/wA999/P2688UY8/vjj2LlzJ5YtW5bVpxtvvBGJRAKvvvrqgBkTq1atsrQyfiAQwI4dO3I+liuXRuJ3t9hcHg1m+nj00UfjH//4B2KxGN588008+eST+NWvfoVTTjkFH374oXFrQqG/F7mk02lcf/31mDRpElavXp11gU4IgZ///OdZzw8GgwBQ1Ih2vu91M79rixcvNm53eu2114zF5k466SR8+umnRv8K/exy2b59O1asWIEDDjgAb7zxRtaWnO3t7Vi+fHnBsQ+XfJ9pLm+//TaefPJJLFq0CH//+9+zpsCvWrUKd9xxR0Gv43A4cPnll+Pyyy/HZ599hpdffhn33nsv7r//frS3t+OZZ54pOg4iMo9T6InI1lpaWgD0/uOtv1dffXWku1OUWbNmweVy4d133x0wyiGEwBtvvDEq/Zo7dy4A5Nz+Z/PmzWhpaUFDQ4MxYjlnzhwAvdtg9ffWW2/l3PJpMPrq8mbidzqdOOyww7B8+XLceeedEELgqaeeGvA8WZZx4IEH4oorrsCf//xnAMATTzwx6GsP9rm8+eabiMfjxkjfSBs7diyWLFmC9vZ2PP3003n3fm9pacHYsWMHFO/RaBTvvfeepT7MmTMHkUgk5+vk+l0s9ndXLy6KGQEvNpdHg5U+ejweHHfccbj11lvxwx/+ELFYDM8999yA5xX6e9FXZ2cnQqEQDj/88AGza955550BU6RnzZqF6upqvP3221nbxZlh5XetqqoKJ510Eu6++26cd9556OjowJtvvjngeYV+dn1t2LABQggsWLAgq3gH7H++yUX/HTzllFMG3L9uNp7Jkyfjq1/9KlauXImZM2fi+eef53R6ohHGAp6IbE0fnfnnP/+Z1f7ggw8aU6TtyuVy4ayzzkJHRwduv/32rMfuv/9+4x7vkbZ48WIEAgHce++9xr7xQO9Fhf/8z/9EOp3O2kt48eLF8Pv9uOeee4x/EAK9I3hXX3110e8/b948HHroofjzn/+Mhx56aMDjmqbh5ZdfNv7+7rvv5twqSR/R1Lff+uijj3KOcvZ/Xj7nnHMOHA4Hbrvttqz7QpPJJP7zP/8TAAbssVysUCiETz/9FNu2bSv6Z/Vi/aabbsKzzz6LpqamAfeeT5s2Dbt27co6rplMBpdffnne0fNC6QvV/ehHP8oqsv/1r3/hgQceGPD8Yn93x4wZA0mSsHnz5oL7VGwuj4Zi+/jGG2/kvE2mfx4X+nuRz/jx4+HxePDee+9l7UW/a9eunFtzOhwOfPvb30YoFMIll1wy4EJLKBQqeJ/0Yn/XXnnllZwXdvRZQXqshX52+eg5+/rrr2fdDrVlyxZcddVVhYRmK/l+Bz/66KOCF9JMJBJ4/fXXB7RHIhGEw2E4nc4Ba4AQ0fDiFHoisrVzzz0XN998M7773e/ixRdfxLRp07BmzRq88MIL+NKXvoRHHnlktLs4qJtuugnPP/88rrzySrz88svGPvBPPfUUTjrpJKxcuXLE//FTXV2N3/3ud/jqV7+KQw89FGeffTbGjRuH559/Hu+++y7mzZuXtap6MBjEbbfdhm9961s4+OCD8ZWvfMXYB97lcmHy5MlFx/DnP/8Z8+fPx1e+8hXcfvvtOOigg+DxeLBp0ya88cYb2LFjh/EP8QceeAC//e1vccwxx2DGjBmorq7Gxx9/jKeffhpjx47F+eefDwB47rnn8IMf/ABHHnkkmpqaUFNTY+yB7na7jVWX85kxYwZuvvlmXHbZZTjggAOwdOlS+Hw+PPnkk1i7di0WL16Mf//3fy/y08726KOPFrUPfF8nnHCCcd8zAHz9618f8Jzvfve7ePbZZ3HUUUdh6dKlcLvdeOmll7B161Ycd9xxOUc8C7Vs2TI8+OCDWLlyJebOnYuTTz4ZXV1d+POf/4yFCxcOGPEt9nfX7/fjkEMOwSuvvIJzzz0XjY2NkGUZ55577oBp1rpic3k4PPzww3kvxp1xxhk444wziurjzTffjBdffBHHHHMM6uvr4Xa78d577+GFF15AQ0MDlixZAqDw34t8ZFnGhRdeiFtvvRVz5szBaaedhu7ubvzjH//AtGnTMHny5AE/c91112HVqlV44IEHsGrVKpx88slwuVzYsGEDVq5ciX/+858FzVIp9nft4osvxmeffYajjjoK06dPhyRJ+Oc//4m33noLhx12mDHjpNDPLp9JkybhzDPPxN/+9jd84QtfwAknnICOjg489dRTOOGEE7IuYJaDefPmYd68efif//kfbNu2DYcddhg2bdqEJ554Aqeccgr++te/DvkasVjM+E49+OCDMXXqVITDYTz11FNob2/H5ZdfPuQOH0RUYqOyeR0R7ZEG2wd+sP1yV69eLRYuXCjGjBkjqqqqxLHHHiuef/75nPtCCzH4PvB99+rV5doTfLB94Pvvea7LtX+uEEJs2LBBfPnLXxaBQEB4vV5x9NFHi5dffllcdNFFAoB4//3388be/71z7QNfTF/6euWVV8TJJ58sgsGgUFVVNDU1iauvvlqEw+Gcz3/44YfF3LlzhcvlEuPHjxff/OY3xc6dO4Xf7xdz5szJeu5gn7euq6tL/PjHPxb777+/8Hg8wu/3i8bGRnHOOeeIRx55xHjeqlWrxLe//W2x//77i2AwKDwej2hsbBQXXXRR1h7zH3/8sbjkkkvE3LlzRU1NjXC5XKKhoUEsW7ZMfPTRR1nvneuY6x5//HFx7LHHiqqqKuFyucTs2bPFrbfeKlKpVNbzhsrdXHloZh/4vvS9qBVFEZ999lnO5/z1r38VBx10kPB6vaK2tlYsXbpUtLS05DwmxewDL4QQkUhEXHHFFWKvvfYSLpdL7LvvvuLuu+/O+zrF/u6uXbtWfPGLXxTBYFBIkpR1jPL9jBDF5XKu46Ib7Pe7P70/g/3p+3kU2seVK1eKr33ta2LWrFmiqqpK+P1+se+++4of/vCHYseOHcbzCv29GEwymRQ33nijaGxsFC6XS0ydOlVcdtlloqenJ+9nEY/HxS233CIOPPBA4/d23333FZdddpnYtWtXUZ9lob9rf/nLX8TSpUvFjBkzhNfrFYFAQMyZM0fcfPPNWXvPF/rZDaanp0dcdtllYvr06cLlconGxkZx/fXXi2QyWfC5ZTj3gc+V/4O95/bt28XXv/51MXnyZOF2u8Xs2bPFihUrxIYNG3I+v//vfjKZFDfffLNYuHChqKurE6qqigkTJohjjjlGPPjgg0LTtKJjJCJrJCEK3F+HiIhK6qijjsIbb7yBUCiUc//kcrB+/Xo0NjZi6dKlOafDExEREVHp8KYVIqJhlut+5z/+8Y947bXXsGDBgrIo3nft2jVgIb5YLGbsPX/GGWeMQq+IiIiI9iwcgSciGmY1NTWYO3cu9t13X2MP7JdeeglVVVV47bXXMHv27NHu4pAee+wxfOMb38DChQsxdepUdHZ24n//93/R1taG448/Hs899xwXMiIiIiIaZizgiYiG2Y9+9CM8+eST2LRpEyKRCMaNG4f58+fj6quvxt577z3a3StIc3Mzrr76arz++uvGauYzZ87E2Wefjcsvv3zI1Z2JiIiIyDoW8ERERERERERlgPMdiYiIiIiIiMoAC3giIiIiIiKiMuAY7Q7YjaZp+Oyzz1BVVQVJkka7O0RERERERFThhBDo6enB5MmTB10YmAV8P5999hmmTJky2t0gIiIiIiKiPczmzZtRV1eX93EW8P1UVVUB6P3gqqurR7k3+WUyGbS0tGDGjBlQFGW0u0NlhLlDZjF3yArmD5nF3CGzmDtkxUjnT3d3N6ZMmWLUo/mwgO9HnzZfXV1t+wLe7/ejurqaX0hUFOYOmcXcISuYP2QWc4fMYu6QFaOVP0Pdxs1F7MqUJEkIBAK8T5+Kxtwhs5g7ZAXzh8xi7pBZzB2ywq75w33g++nu7kYgEEAoFLL1CDwRERERERFVhkLrUI7AlylN07Bt2zZomjbaXaEyw9whs5g7ZAXzh8xi7pBZzB2ywq75w3vg88hkMshkMgB6p0/IsgxN09B3wkK+dlmWIUlS3nb9dfu2AxiQHPnaFUWBpmnYtWsXampqoCiK0RchRNbzi+37aMaUr++MqbQxZTIZI3ecTmdFxFRI3xmT9ZgymQx2796N8ePHV0xMg7UzptLGlO+8Vc4xVeJxsmNMhZy3hjsmSZKQTCaRTqdLElMh7eV2nOwYUyaTQVdXF6qrqyFJUkXE1LfvlXKc7BpTKpXCzp074fP5TJ23+sekKAocDofxWv1jKnRiPAv4/7NixQqsWLHC+CBbWlrg9/sBAIFAAJMmTUJHRwdCoZDxM7W1taitrcXWrVsRiUSM9okTJyIYDKKtrQ3JZNJor6urg9/vR0tLS1aS1NfXw+FwoLm5OatPjY2NSKfTaG1tNdpkWUZTUxOi0Si6urqwfv16yLIMVVXR0NCAUCiE9vZ24/k+nw9TpkxBV1cXOjs7jXY7xhSJRLBlyxajnTENT0zpdNrInRkzZlRETJV4nOwYk6ZpiMfjAFAxMQGVd5zsGlN3d3fWeasSYqrE42THmDRNM3Jn1qxZIx5TMBhEIpFAd3d31j+wFUWBLMtIp9M521OpVNbxcDh6/9nd9yLAYO1Op9O4gKGTJAkOhyNveyaTyfocZVmGoih52/P1vZJiEkJg48aNyGQyFRNTJR4nu8aUSqXQ1tZWkpj0x6ZOnYoxY8YM+N6rqalBIXgPfD/6vQf61TrAnleQ0uk01q1bh5kzZ3IEnjEVFVMmk8H69esxc+ZMjsAzpqJHMlpaWtDU1JR3JKPcYhqsnTGVNqZ8561yjqkSj5MdYyrkvDVcMWmahpaWFjgcDowbNw5OpxOSJGX9TK5/SudrL0axrz1a7cUY6T4KIZBMJuFyuYrqTzHsdjwYU25m3lPTNCSTSaiqavzem+27EAKpVAo7duxAJpNBY2PjgOeHw2EEg8Eh74FnAd9PuSxip1+NHjt2rHGSIyoEc4fMYu6QFcwfMms0cycej6O1tRXTpk2D1+sd0fcm64QQSKfTcDgcWRdeiAoxHPkTjUaxceNG1NfXw+12Zz1WaB3KKfRlSpZl1NbWjnY3qAwxd8gs5g5Zwfwhs+yQO7zoVJ4kSTJmbRAVazjypxTfJfw2KlOapmHz5s0DppARDYW5Q2Yxd8gK5g+Zxdwhs/Qp9JxwTGbYNX9YwJcpIQQikYjtEorsj7lDZjF3yArmD5nF3CEr+q+vQFQMO+YPC3giIiIiIrKF++67D8FgsKifmT59Om6//fZBn5NMJjFz5ky8/vrr5juXg5n+UuX5+OOPUVdXl7X7xnBhAU9EREREVEHOO+88SJJk/KmpqcFJJ52EDz74oGTvce211+LAAw8c9DnTp0/P6kf/P+edd96Anzn77LOxbt26kvVT95vf/Ab19fU44ogjjDZJkvDYY4+V/L1K7aOPPsKZZ55pfJ5DXazIp5ALHaNpxYoVmD59OtxuNw499FC89dZbgz6/0M9l69at+Pd//3fU1NTA4/Fg9uzZeOedd4zHOzo6cN5552Hy5Mnwer046aSTsrblbGtry5vDDz/8MABg3333xWGHHYbbbrvN+gcxBBbwZUqWZUycOJGLqlDRmDtkFnOHrGD+kFnMHXNOOukkbNu2Ddu2bcMLL7wAh8OBU089dUT78Pbbbxt9+Nvf/gYAWLt2rdF2xx13ZD0/lUrB4/Fg/PjxJeuD0+mEEAK//OUv8Y1vfKNkrzuSotEoGhoa8LOf/QwTJ04c7e4Mi4ceegiXXnoprrnmGrz33nuYM2cOFi1ahO3bt+f9mUI+l127duHII4+E0+nEP/7xD3z88ce49dZbMWbMGAC9t+icccYZ2LBhAx5//HG8//77mDZtGhYsWIBIJAKn04kpU6YYOav/Wb58Ofx+P04++WTjvc4//3z8+te/HrAHfanxm7BMSZKEYDDILTGoaMwdMou5Q1Ywf8gsO+VOdzyFt9u6Ru1PdzxVcF9dLhcmTpyIiRMn4sADD8SVV16JzZs3Y8eOHcZzNm/ejKVLlyIYDGLs2LFYvHgx2trajMdfeuklzJs3Dz6fD8FgEEceeSQ2btyI++67D8uXL8eaNWuMkcj77rtvQB/GjRtn9GHs2LEAgPHjx2PixImIx+MIBoN46KGHcOyxx8LtduNPf/rTgCnpLS0tWLx4MSZMmAC/349DDjkEzz//fEGfgSRJcDgceO+999DS0oJTTjkl73P1UdZHHnkE8+fPh9frxZw5c/DGG29kPe++++7D1KlT4fV6sWTJEuzcuXPAaz3++OM46KCD4Ha70dDQgOXLlxtF3XXXXYfJkydn/dwpp5yC+fPn512o8ZBDDsF//dd/4Stf+UrePe1L4de//jVmzJgBVVUxa9YsPPDAA8ZjQghce+21mDp1KlwuFyZPnoyLL77YePxXv/oVGhsb4Xa7MWHCBJx11llFvfdtt92GCy64AOeffz723Xdf/OY3v4HX68Uf/vCHvD9TyOdy8803Y8qUKbj33nsxb9481NfXY+HChZgxYwYAoLm5GatWrcKvf/1rHHLIIZg1axZ+/etfIxaL4S9/+QscDgccDoeRx/qfRx99FEuXLoXf7zfe68QTT0RXVxdefvnlomIvFreRK1OapqGtrQ3Tp0/nFWkqCnOHzGLukBXMHzLLTrmztr0HX/7NG0M/cZg8/P8OxyHTxxb9c+FwGH/84x8xc+ZM1NTUAOgd7V60aBEOP/xwvPrqq3A4HLjhhhuMqfayLOOMM87ABRdcgD//+c9IJpN46623IEkSzj77bHz44YdYuXKlUUwHAgFTMV155ZW49dZbMXfuXLjdbjzzzDMD+v7FL34RN954I1wuF+6//36cdtppWLt2LaZOnTroa+uriL/yyitoampCVVXVkP350Y9+hFtuuQWNjY340Y9+hK9+9atYv349HA4H3nzzTXzjG9/ATTfdhDPOOAMrV67ENddck/Xzr776Kr72ta/hzjvvxNFHH42WlhZ861vfAgBcc801+NGPfoSVK1fim9/8Jh599FGsWLECr7/+OtasWTOq+f3oo4/ikksuwe23344FCxbgqaeewvnnn4+6ujrMnz8ff/vb3/CLX/wCf/nLX7Dffvuhvb0da9asAQC88847uPjii/HAAw/giCOOQFdXF1599VXjte+77z6cf/75eReiTCaTePfdd3HVVVcZbbIsY8GCBQMuoBTriSeewKJFi/DlL38ZL7/8Mvbaay9ceOGFuOCCCwAAiUQCALL2ZJdlGS6XC//85z9x7rnnQlXVrAuI7777LlavXo0VK1ZkvZeqqjjwwAPx6quv4oQTTrDU78GwgC9Tdt3WgOyPuUNmMXfICuYPmcXcMeepp54yRgcjkQgmTZqEp556yigSH3roIWiaht///vdGcXLvvfciGAzipZdewhe+8AWEQiGceuqpxmjlPvvsY7y+3+83Riat+N73vocvfelLeR+fM2cO5syZY/z9+uuvx6OPPoonnngCF1100ZCvr2kaNm7ciMmTJxfUn8svv9wYqV++fDn2228/rF+/HnvvvTfuuOMOnHTSSbjiiisAAE1NTXj99dexcuVK4+eXL1+OK6+8EsuWLQMANDQ04Prrr8cVV1yBa665Boqi4I9//KMxK+LOO+/E73//+yEvRgy3W265Beeddx4uvPBCAMCll16KVatW4ZZbbsH8+fOxadMmTJw4EQsWLIDT6cTUqVMxb948AMCmTZvg8/lw6qmnoqqqCtOmTcPcuXON1w4EApg1a1be9+7s7EQmk8GECROy2idMmIBPP/3UUlwbNmzAr3/9a1x66aX44Q9/iLfffhsXX3wxVFXFsmXLsPfee2Pq1Km46qqr8Nvf/hY+nw+/+MUvsGXLFmzbti3nrIh77rkH++yzT9Z6CrrJkydj48aNlvo8FF4CJyIiIiKqMPPnz8fq1auxevVqvPXWW1i0aBFOPvlko7hYs2YN1q9fj6qqKvj9fvj9fowdOxbxeBwtLS0YO3YszjvvPCxatAinnXYa7rjjDmzbtq3k/fzCF74w6OPhcBiXX3459tlnHwSDQfj9fnzyySfYtGlTwe8Ri8WyRlgHc8ABBxj/P2nSJAAw7sP+5JNPcOihh2Y9//DDD8/6+5o1a3DdddcZn6nf78cFF1yAbdu2IRqNAugt6m+55RbcfPPNOP3003HOOecUHMtw+eSTT3DkkUdmtR155JH45JNPAABf/vKXEYvF0NDQgAsuuACPPvqocVvAiSeeiGnTpqGhoQHnnnsu/vSnPxmxAsCSJUssF+JmaZqGgw46CD/96U8xd+5cfOtb38IFF1yA3/zmNwB610h45JFHsG7dOowdOxZerxcvvvgiTj755JwzImKxGB588MG86yl4PJ6s2IcDR+CJiIiIiAowa2IVHv5/hw/9xGF8/0L5fD7MnDnT+Pvvf/97BAIB/O53v8MNN9yAcDiMgw8+GH/6058G/Oy4ceMA9I7IX3zxxVi5ciUeeugh/PjHP8Zzzz2Hww47zHowffo5mMsvvxzPPfccbrnlFsycORMejwdnnXUWkslkwe9RW1uLDz/8sKDnOp1O4//1mQn57k3PJRwOY/ny5TlnFfS9iPDKK69AURS0tbUhnU7D4bB3WTZlyhSsXbsWzz//PJ577jlceOGF+K//+i+8/PLLqKqqwnvvvYeXXnoJzz77LH7yk5/g2muvxdtvv13QFnu1tbVQFAUdHR1Z7R0dHZZneEyaNAn77rtvVts+++xjLKoIAAcffDBWr16NUCiEZDKJcePG4dBDD8XBBx884PX++te/IhqN4mtf+1rO9+vq6jJmrAwXe2cK5SXLMurq6kb9XjAqP8wdMou5Q1Ywf8gsO+VOtdtp6h50O5AkCbIsIxaLAQAOOuggPPTQQxg/fjyqq6vz/tzcuXMxd+5cXHXVVTj88MPx4IMP4rDDDoOqqshkMsPe79deew3nnXcelixZAqC3QO670N5QVFXF3Llz8Zvf/AZCCEuLIe6zzz548803s9pWrVqV9feDDjoIa9euzbp40t9DDz2ERx55BC+99BKWLl2K66+/HsuXLzfdr1LYZ5998NprrxlT/4Hez75v8evxeHDaaafhtNNOw3e+8x3svffe+Ne//oWDDjoIDocDCxYswIIFC3DNNdcgGAzif//3fwe9PUKnqioOPvhgvPDCCzjjjDMA9F40eeGFFwq6TWIwRx55JNauXZvVtm7dOkybNm3Ac/V1HJqbm/HOO+/guuuug6qqWc+55557cPrppxsXufr78MMPi17Ar1gs4MuUJElZqx4SFYq5Q2Yxd8gK5g+ZxdwxJ5FIoL29HUDvVlq//OUvEQ6HcdpppwEA/u3f/g3/9V//hcWLF+O6665DXV0dNm7ciEceeQRXXHEFUqkU7r77bpx++umYPHky1q5di+bmZmPkcfr06WhtbcXq1atRV1eHqqqqYVkhvbGxEY888ghOO+00SJKEq6++uuARcUmSoCgKjj/+eITDYXz00UfYf//9Tffl4osvxpFHHolbbrkFixcvxjPPPJN1/zsA/OQnP8Gpp56KqVOn4qyzzoIsy1izZg0+/PBD3HDDDdiyZQv+4z/+AzfffDOOOuoo3HvvvTj11FNx8skn553ZkEwm8fHHHxv/v3XrVqxevRp+v3/QCwW56D/b17Rp0/CDH/wAS5cuxdy5c7FgwQI8+eSTeOSRR4xFCu+77z5kMhkceuih8Hq9+OMf/wiPx4Np06bhqaeewoYNG3DMMcdgzJgxePrpp6FpmnHf+6OPPoqrrrpq0Gn0l156KZYtW4YvfOELmDdvHm6//XZEIhGcf/75xnO+9rWvYa+99sJNN91U8Ofy/e9/H0cccQR++tOfYunSpXjrrbdw99134+677zZe9+GHH8a4ceMwdepU/Otf/8Ill1yCM844A4sWLcrq4/r16/HKK6/g6aefzhlDW1sbtm7digULFhRyKMwTlCUUCgkAIhQKjXZXBhWJRMSnn34q0un0aHeFykw6nRZr165l7lDRmDtkBfOHzBrN3InFYuLjjz8WsVhsxN/bimXLlgkAxp+qqipxyCGHiL/+9a9Zz9u2bZv42te+Jmpra4XL5RINDQ3iggsuEKFQSLS3t4szzjhDTJo0SaiqKqZNmyZ+8pOfiEwmI4QQIh6PizPPPFMEg0EBQNx7772D9unFF18UAMSuXbuEEEK0trYKAOL999/Pet69994rAoGA8ffW1lYxf/584fF4xJQpU8Qvf/lLceyxx4pLLrnEeM60adPEL37xiwHvqWmaiMViQtM0sXTpUnHllVdmPQ5APProo3n7s2vXLgFAvPjii0bbPffcI+rq6oTH4xGnnXaauOWWW7L6K4QQK1euFEcccYTweDyiurpazJs3T9x9991C0zRxwgkniEWLFglN04znf/e73xUzZswQPT09OT87vW/9/xx77LFZn9tQpd20adNyvs4DDzwghBDiV7/6lWhoaBBOp1M0NTWJ+++/3/jZRx99VBx66KGiurpa+Hw+cdhhh4nnn39eCCHEq6++Ko499lgxZswY4fF4xAEHHCAeeuihovomhBB33XWXmDp1qlBVVcybN0+sWrUq6/Fjjz1WLFu2rKjPRQghnnzySbH//vsLl8sl9t57b3H33XdnPX7HHXeIuro64XQ6xdSpU8WPf/xjkUgksvJHCCGuuuoqMWXKFON3oL+f/vSnYtGiRYPGONh3SqF1qCQEl/Tsq7u7G4FAAKFQaNDpRKNt165daG9vR1NTExRFGe3uUBnJZDJobm5GY2Mjc4eKwtwhK5g/ZNZo5k48Hkdrayvq6+sLXgSN7EMIgXg8DrfbjX/961848cQT0dLSUpEzOq655hq8/PLLeOmll0a7KxWjb/4MdetFMplEY2MjHnzwwQGLAfY12HdKoXXo6N9MRKYoisLtVIiIiIiICnDAAQfg5ptvRmtr62h3ZVj84x//wM9//vPR7sYea9OmTfjhD384aPFeKrwHvkzZYSEXIiIiIqJycd555412F4bNW2+9Ndpd2KPNnDmz6PUIzGIVWKacTidqa2tHuxtUhmRZRn19PS8CUdGYO2QF84fMYu6QFcOxsB7tOeyYP/wmLFOyLEOW5RHZvoMqj933GiX7Yu6QFcwfMou5Q2ZZ2TaOyI75wwK+TEmShO3btyOdTo92V6jMaJqG5ubmgrdgIdIxd8gK5g+ZZYfc4bpD5Ssej492F6iMlTp/SvFdwgK+TOlXgzgCT0RERDQ8nE4nACAajY5yT4ioEujfJfp3ixmcj1TmOJJBRERENDwURUEwGMT27dsBAF6v15ZTaik3IQQSiQQAe06FJnsrZf4IIRCNRrF9+3YEg0FLW2KygC9jkiRxBJ6IiIhoGE2cOBEAjCKeyocQAul0Gg6HgwU8FW048icYDBrfKWZJgjf1ZOnu7kYgEEBXVxeqq6sB9BbKsixD07Ss+xbytcuyDEmS8rb3L7r1VVX7j6bna1cUBZqmIRqNIh6PIxgMGovaCSGynl9s30czpnx9Z0yljUl/b1mWjat/5R5TIX1nTNZjEkJACAGHw2H8f7nHNFg7YyptTJqmIZ1OG+9VCTFV4nGyY0yFnLdGIqZUKoVkMlmSmAppL7fjZMeY9P/q71sJMfXte6UcJ7vGlMlkkMlkTJ+3+sfkdDqhKEremMLhMILBIEKhkFGH5sIR+P+zYsUKrFixwvggW1pa4Pf7AQCBQACTJk1CR0cHQqGQ8TO1tbWora3F1q1bEYlEjPaJEyciGAyira0t64u+rq4Ofr8fLS0tWUlSX18Ph8OB5ubmrD41NjYinU6jtbXVaJNlGU1NTYhEIti4cSMkScKOHTvgcrnQ0NCAUCiE9vZ24/k+nw9TpkxBV1cXOjs7jXa7xrRlyxajXVVVxjQMMelfRoqioKGhoSJiqsTjZMeYhBBGe6XEBFTecbJzTFu3boWiKJAkqWJiqsTjZLeYhBDGeaupqWnUYmpvb88Z04YNG3LGtG7duqKO08aNGwfEFA6Hc8a0e/funDF1dnbmjGnbtm05Y9q8eXNFxySEQE1NDcaNG4fW1taKiAmovONk15h27NiBjo4O47xV6pj6f+/V1NSgEByB76dcRuDT6TTWrl2LcePGoaqqCi6Xa4+7KsaYzMWUyWSwfv16zJw501hAo9xjKqTvjMl6TJlMBi0tLWhqaoIkSRUR02DtjKm0MaXTaaxbtw4zZ840/jFU7jFV4nGyY0yFnLfKLaZC2hmT9ZgKOW+VW0x9+14px8muMaVSKTQ3N5s+bxUbE0fgLVIUZcDiAnpC9Fdse75FC4pplyQJiqIYU1n199LbrfZxNGMqtJ0xmY9JlmXjiyhf3/O12zUmK+2MqfA+6jlTSTGZbWdMxcekf/f0fbzcY6rE42THmAo5b5VbTIW0MybrfR/qvFWOMQ3VzphKF9Nwnrf6v6eeq0PJ/WpUNmRZ5kJ2REREREREewAW8GVMvyLUf4oI0VDyXQkkGgpzh6xg/pBZzB0yi7lDVtgxf3gPfD/6PfBD3Xsw2vSiPZlMIhqNYsyYMQVPuyAiIiIiIiL7KLQOtd8lBSpIKBRCV1dX3oUaiPIRQiAcDoPX7qhYzB2ygvlDZjF3yCzmDllh1/xhAV+mnE4ntm/fbvyd98FToTRNw5YtW3jRh4rG3CErmD9kFnOHzGLukBV2zR8W8GVKVVUAvYV7rm0IiIiIiIiIqLKwgC9T+rYDyWQSiqKwgCciIiIiIqpwLODLlCRJUFUV6XSaI/BUFD13uOghFYu5Q1Ywf8gs5g6ZxdwhK+yaP1yFvp9yWYUe6L0vY/fu3cZWcsFg0HYJRkRERERERIPjKvQVTgiB7u5uOJ1OpNNpCCFst8AC2ZMQArt377bdippkf8wdsoL5Q2Yxd8gs5g5ZYdf8YQFfpjRNQ3t7O5xOJ4QQyGQynEZPBdFzhxd8qFjMHbKC+UNmMXfILOYOWWHX/GEBX+YcDoexiJ3dkouIiIiIiIhKhwV8mZMkCS6XC5qmIZ1Oj3Z3iIiIiIiIaJiwgC9TkiTB5/MZBbwkSUgmk6PdLSoDfXOHqBjMHbKC+UNmMXfILOYOWWHX/OEq9P2U0yr0fXV2diKVSmHSpEmj3RUiIiIiIiIqAlehr3CapqGzs9O4793lciGdTnMaPQ2pf+4QFYq5Q1Ywf8gs5g6ZxdwhK+yaPyzgy1Q0GsWOHTuMbQ3cbjcAIBaLjWa3qAwIIdDZ2Wm7LTHI/pg7ZAXzh8xi7pBZzB2ywq75wwK+DOnbxun/BXpXo3c4HEgkEqPcOyIiIiIiIhoOLODLkL6gAgCEw2FkMhlIkgRVVTmNnoiIiIiIqEKxgC9TsiwjGAxCkiSEw2Fomgan0wkhBEfhaVCSJCEQCNhuRU2yP+YOWcH8IbOYO2QWc4essGv+cBX6fsptFfpMJoPu7m4oigJFURAOh6GqqlHcExERERERkb1xFfoKp2katm3bBkmS4Pf7kU6nkUwmIcsyNE1DKpUa7S6STem5Y7cVNcn+mDtkBfOHzGLukFnMHbLCrvnDAr5MCSEQCoUghIDT6YTf70cmk0Emk4Esy5xGT3n1zR2iYjB3yArmD5nF3CGzmDtkhV3zhwV8hVBVFT6fD6lUyhiBt9vVIiIiIiIiIjKPBXwF8Xg8UFUVyWQSmUwGyWRytLtEREREREREJcICvkxJkoTa2toBC9V5PB4oioJ0Oo1oNDpKvSM7y5c7RENh7pAVzB8yi7lDZjF3yAq75g9Xoe+n3Fah7y8ajSKZTELTNESjUYwbNw4ul2u0u0VERERERER5cBX6CqdpGjZv3jzgPnd9Ffrq6mrIsozdu3fzXnjKki93iIbC3CErmD9kFnOHzGLukBV2zR8W8GVKCIFIJDJgVURFUYz/r66uRjqdRnd3t+1WT6TRky93iIbC3CErmD9kFnOHzGLukBV2zR8W8BVGL+A1TYPH44HL5UIqlUI4HLZd8hEREREREVHhWMBXGFmWIUkSMpkMFEWBqqpwOp1IpVJc1I6IiIiIiKiMsYAvU7IsY+LEiZDlgYdQURRkMhkAvfvD66PxiUQCsVhspLtKNjNY7hANhrlDVjB/yCzmDpnF3CEr7Jo/9uoNFUySJASDwZzbGvQv4HUejwexWAyJRGLE+kn2M1juEA2GuUNWMH/ILOYOmcXcISvsmj+O0e6AXWUyGaMIliTJWN29733k+dr1aez52vXX7dsOIOeK8rna9QK9tbUV06ZNM15XlmXj/dLpNNLpNGRZhqqqSCQSqKqqQjqdRk9PDwDA5XLZKiYhRFZ735hytRfad8aUHZOmadi4cSOmTZsGh8NRETEV0nfGZD0mTdOwadMm1NfXA0BFxDRYO2MqbUz5zlvlHFMlHic7xlTIeavcYiqknTFZj6mQ81a5xdS375VynOwaUzqdRltbm+nzVrExFbpeGQv4/7NixQqsWLHC+CBbWlrg9/sBAIFAAJMmTUJHRwdCoZDxM7W1taitrcXWrVsRiUSM9okTJyIYDKKtrQ3JZNJor6urg9/vR0tLS1aS1NfXw+FwoLm5OatPjY2NSKfTaG1tNdpkWUZTUxMikQja29uRTCYhy71FekNDA0KhELZt2wYhBLZv3w6/34+JEyeiq6sL27ZtA9CbHOFwGFOmTMH27dttFdOWLVuM9r4xtbe3G+0+nw9TpkxBV1cXOjs7jXa7Hie7xZROp9HV1YVkMokZM2ZUREyVeJzsGJOmaYjH4xBC4LPPPquImIDKO052j0k/b1VSTJV4nOwUk6Zpxnlr1qxZFRFTJR4nO8akaRqSySSEENi4cWNFxARU3nGya0w7d+7MOm8Nd0w1NTUohCS4NHmW7u5uBAIBdHV1obq6GoA9ryCl02msW7cOM2fOhKIoWVeQ0uk0QqEQ/H4/VFWFJEnYtWsXnE4nvF4vNE1DOByGLMvGRQo7xMQrfSMTUyaTwfr16zFz5kw4nc6KiKmQvjMm6zFlMhm0tLSgqakJkiRVREyDtTOm0saU77xVzjFV4nGyY0yFnLfKLaZC2hmT9ZgKOW+VW0x9+14px8muMaVSKTQ3N5s+bxUbUzgcRjAYRCgUMurQXDgCn4eiKFl7qgOfJ0R/xbb3f10z7Xqi9O+nJElwOp1Gkup9cLvdSCQSxs9UVVWhp6cHsVhsQBE/mjEV016q47EnxqTngSRJefuer92uMVlpZ0yF91HPmUqKyWw7Yyo+plznrXKPqRKPkx1jKuS8VW4xFdLOmKz3fajzVjnGNFQ7YypdTMN53ur/nnquDiX3q5HtybKMurq6QROi75Ukl8sFIQRSqRQAwOFwwOfzIZlMcmX6PcxQuUOUD3OHrGD+kFnMHTKLuUNW2DV/7NUbKpgkSfD7/Xmv1MiynDUtQ1EUOByOrBXoVVWF2+1GLBbLui+DKttQuUOUD3OHrGD+kFnMHTKLuUNW2DV/WMCXqUwmg3Xr1g24d0KnKMqAx1RVRSqVyhqZ93g8cDqdiEQieV+LKstQuUOUD3OHrGD+kFnMHTKLuUNW2DV/WMCXsf6LM/SVa6EGfU/4vqPwkiTB5/NBlmWEw+FBX5MqB48zmcXcISuYP2QWc4fMYu6QFXbMHxbwFUpfFKHvFSNZ7t1urv90eVnuXY1e0zREIpGC9yAkIiIiIiKikcMCvkLpiy3kmkavbyXWl6Io8Pl8SKVSiMfjI9ZPIiIiIiIiKgwL+DIlyzLq6+vzroqob4fQv1B3Op2QJCnnonWqqsLj8XBRuwo3VO4Q5cPcISuYP2QWc4fMYu6QFXbNH3v1horicDgGfTxXAa/vE5+vQPd4PFBVFZFIBOl0umR9JXsZKneI8mHukBXMHzKLuUNmMXfICjvmDwv4MqVpGpqbm4dcyC7X4/mm0eu4qF1lKyR3iHJh7pAVzB8yi7lDZjF3yAq75g8L+AomyzI0TRuQdINNowc+3/NQCMFF7YiIiIiIiGyCBXwF01ei71/ADzWNXv9Zv9+PVCqFWCw2rP0kIiIiIiKiobGAr2C5tpLTDTWNHugdqfd6vYjH41l7xxMREREREdHIkwTnR2fp7u5GIBBAKBRCdXX1aHcnLyEENE2DLMuQJCnv83bv3g1VVeH1egf8/O7du+F2u+HxeAZ9r3A4jGQyierqalsu5EDFKTR3iPpj7pAVzB8yi7lDZjF3yIqRzp9C61COwJexQlaJz7USPfD5NPpUKjXka/h8PjgcDi5qV0G4wwCZxdwhK5g/ZBZzh8xi7pAVdswfFvBlStM0tLa2DllQ5yvggd4p8ul0etBp9ED2onbxeNx0n8keCs0dov6YO2QF84fMYu6QWcwdssKu+cMCvsLpW8nlulNCVVUAKGgUXpZluN1uxOPxIQt+IiIiIiIiKj0W8BVusIXsClmNvi+32w1ZlrkqPRERERER0ShgAV/GZHnow6c/J9+ouaqqBU2jB3oLfo/Hg2Qyacv7QahwheQOUS7MHbKC+UNmMXfILOYOWWHH/OEq9P2Uyyr0xdi1a1fe1eY1TcPu3bvh9XrhdruHfC0hBLq7uyFJUsV8PkRERERERKOJq9BXOCEEwuFwznvb+xtsITtZlouaRq+PwqfT6YJ/huylmNwh6ou5Q1Ywf8gs5g6ZxdwhK+yaPyzgy5SmadiyZUtBqyIOVsADn0+jL3SFRVVV4XQ6EYvFbJfQNLRicoeoL+YOWcH8IbOYO2QWc4essGv+sIDfA+gFfL5i2+l0AkBRI+oejweZTAaJRKIkfSQiIiIiIqLBsYDfA+gr0ee7elTsNHoAcDgcUFWVo/BEREREREQjhAV8mZIkCaqqQpKkIZ872FZyumKn0QO9o/BCCMTj8YJ/hkZfMblD1Bdzh6xg/pBZzB0yi7lDVtg1f7gKfT+VuAo9MPhK9EDxq9HrotEoEokEAoGALbdZICIiIiIisjuuQl/hhBDYvXt3wdPXh1rITp9Gn0qliuqHXuzHYrGifo5GT7G5Q6Rj7pAVzB8yi7lDZjF3yAq75g8L+DKlaRra29sLnvIuy/KQz9UL+GKm0cuyDI/Hg0QiMegFArKPYnOHSMfcISuYP2QWc4fMYu6QFXbNHxbwe4ihRuCB3vvgARQ9Cu9yuSDLMqLRqOn+ERERERER0eBYwO8hFEWBEGLIafQOh6Oo1eiB3gUePB4PUqlU0cU/ERERERERFYYFfJmSJAk+n6/gVRGH2kpOp6pq0dPogd5ReIfDwW3lykCxuUOkY+6QFcwfMou5Q2Yxd8gKu+YPV6Hvp1JXoRdCYNeuXUOuMq+vRu/z+eByuYp6j1QqhZ6eHlM/S0REREREtKfiKvQVTtM0dHZ2FjxSLklSQffBm51GD/Qugud0OjkKb3PF5g6RjrlDVjB/yCzmDpnF3CEr7Jo/LODLlBACnZ2dRRXKhRTwgPlp9ADg9XqhaRoSiUTRP0sjw0zuEAHMHbKG+UNmMXfILOYOWWHX/GEBvwcppoAHil+NXn8Pl8uFWCxmu6tVRERERERE5YwF/B5ElmUIIYYsrK1MowcAj8cDAIjH46Z+noiIiIiIiAZiAV+mJElCIBAoalVEfSX64Z5GL8sy3G434vF4Qe9FI8tM7hABzB2yhvlDZjF3yCzmDllh1/zhKvT9VOoq9MDnK9EXskp8JpNBKBQyvaK8EAK7d++G0+mE3+8322UiIiIiIqKKx1XoK5ymadi2bVtRI+SSJEGW5YJGxRVFgcPhMHUfvP5eHo8HyWQS6XTa1GvQ8DCTO0QAc4esYf6QWcwdMou5Q1bYNX9YwJcpIQRCoVDRqyIWupAd0LstXDKZNL3yosvlgqIoiMVipn6ehofZ3CFi7pAVzB8yi7lDZjF3yAq75g8L+D2MoihIp9MFJaKV1eiBz0fhU6mU6QXxiIiIiIiIqBcL+D2My+WCEKKgFeIVRYGiKJaKb1VV4XA4EI1GbXf1ioiIiIiIqJywgC9TkiShtra26FURFUUxVogv5H4OfTV6K8W3z+eDpmmcSm8TZnOHiLlDVjB/yCzmDpnF3CEr7Jo/LODLlCzLqK2thSwXfwjdbjeAwvZpV1UVQgjT0+iB3osGHo8H8XicC9rZgJXcoT0bc4esYP6QWcwdMou5Q1bYNX/s1RsqmKZp2Lx587Dv016KafRA70UDRVEQiUQ4lX6UWckd2rMxd8gK5g+Zxdwhs5g7ZIVd84cFfJkSQlgqht1uNyRJKngU3uo0ekmS4PP5kMlkCnpPGj5Wc4f2XMwdsoL5Q2Yxd8gs5g5ZYdf8YQG/h9JXiE8kEkOOwpdiGj0AOByOgkf+iYiIiIiIKBsL+D2Yy+WCLMuIRqODPq9U0+gBwOPxQJKkId+TiIiIiIiIsrGAL1OyLGPixImWFlXou0/7UIvLlWIavf6ePp8PqVQKiUTC0muROaXIHdozMXfICuYPmcXcIbOYO2SFXfPHXr2hgkmShGAwaHlbA1VVoSjKkCPiTqezJNPo9ddSVRXRaNR2i0LsCUqVO7TnYe6QFcwfMou5Q2Yxd8gKu+YPC/gypWkaNmzYYLkA1kfh0+n0oMW5w+Eo2TR6APB6vQDAqfSjoFS5Q3se5g5Zwfwhs5g7ZBZzh6ywa/6wgC9TQggkk8mSrIqoqiocDgei0eigr6eqKpLJZEmSWJZleL1eJJPJkl0UoMKUMndoz8LcISuYP2QWc4fMYu6QFXbNHxbwBKB3cblMJjNoMa2qKgCUZBo90LuIntPpHPLCAREREREREbGAp//jdDrhdDoRi8XyFtOKosDhcJR0xNzr9UIIwan0REREREREQ2ABX6ZkWUZdXV1JV0X0eDzQNG3Q1eFdLhdSqVTJ7gVRFMXYj36olfCpNIYjd2jPwNwhK5g/ZBZzh8xi7pAVds0fe/WGCiZJEvx+f0lXRXQ4HFBVFfF4PO8ovNPpBICSjsK7XC44HA5EIhFOpR8Bw5E7tGdg7pAVzB8yi7lDZjF3yAq75g8L+DKVyWSwbt06ZDKZkr6uPgofj8dzPi7LMlRVLeke7pIkwev1IpPJ5H1fKp3hyh2qfMwdsoL5Q2Yxd8gs5g5ZYdf8qcgCfsmSJRgzZgzOOuus0e7KsBqOLQ0URYHb7UY8Hs/7+qqqIpPJlHTKu8PhgNvtRiwWs90vSSWy23YYVD6YO2QF84fMYu6QWcwdssKO+VORBfwll1yC+++/f7S7UbbcbjcA5B0NdzqdkCSp5Nu/eTweKIrCqfREREREREQ5VGQBf9xxx6Gqqmq0u1G2ZFkedBRekiRjT/hSFtr6VPp0Ol3SKfpERERERESVwHYF/CuvvILTTjsNkydPhiRJeOyxxwY8Z8WKFZg+fTrcbjcOPfRQvPXWWyPf0VEmyzLq6+uHbVVEl8sFSZIQi8VyPq6qKjRNK/nK8U6nEy6XC7FYzJZTVirBcOcOVS7mDlnB/CGzmDtkFnOHrLBr/tirNwAikQjmzJmDFStW5Hz8oYcewqWXXoprrrkG7733HubMmYNFixZh+/btI9zT0edwOIbttWVZNrZ3y3VPutPphCzLJZ9GD/ROpZckCZFIpOSvTb2GM3eosjF3yArmD5nF3CGzmDtkhR3zx3Y9Ovnkk3HyySfnffy2227DBRdcgPPPPx8A8Jvf/AZ///vf8Yc//AFXXnll0e+XSCSypmt3d3cD6F11UC9cJUmCLMvQNC1ryni+dlmWIUlS3vb+BbF+Vaf/iHO+dkVRjFURZ86cCUVRjL4IIbKeX2zf+7brCRuJRFBVVTUgJofDgUQiAa/XW3DfB4upb99dLhei0SiSySScTmfJYhrp45TveJTyOBUbUyaTwfr16zFz5kxjW8Byj6mQvjMm6zFlMhm0tLSgqakJkiRVREyDtTOm0saU77xVzjFV4nGyY0yFnLfKLaZC2hmT9ZgKOW+VW0x9+14px8muMaXTaTQ3N5s+bxUbU6G3JtuugB9MMpnEu+++i6uuuspok2UZCxYswBtvvGHqNW+66SYsX758QHtLSwv8fj8AIBAIYNKkSejo6EAoFDKeU1tbi9raWmzdujVrtHjixIkIBoNoa2vLGqGuq6uD3+9HS0tLVpLU19fD4XCgubk5qw+NjY1Ip9NobW3NirepqQnRaBRdXV1Yv349ZLl3a7eGhgaEQiG0t7cbz/f5fJgyZQq6urrQ2dlptBcakxACQghMmDABNTU1WTEJIRAMBpFKpdDW1mY5pkgkgi1bthjtiqIYv2wdHR0li2kkj1P/mIbrOBUTUzqdNnJnxowZFRFTJR4nO8akaZ9vMVkpMQGVd5zsGlN3d3fWeasSYqrE42THmDRNM3Jn1qxZFRFTJR4nO8akaZoRR6XEBFTecbJzTH3PW8MdU01NDQohCRsv9y1JEh599FGcccYZAIDPPvsMe+21F15//XUcfvjhxvOuuOIKvPzyy3jzzTcBAAsWLMCaNWsQiUQwduxYPPzww1nP7yvXCLx+wKqrq41+2O0KUjqdHvYReL147+npgaIoqK6uHvD8cDgMWZbh9Xotx9S/70IIdHd3w+12w+VylSymvn2x25W+kYiJI/CMyWxMHIFnTFZiynfeKueYKvE42TEmjsAzJo7AV/ZxsmtMqVRqREfgw+EwgsEgQqGQUYfmUlYj8IV6/vnnC36uy+XKKg51+uhvX3pC9Fdse//XNdOuJ0r/fkqSlPP5Vvru8/kQDoeRSqWME6dOn+qu98dqTP3bPR4PYrEYXC7XsB2P4T5OxbSPVEx67kiSlLfv+drtGpOVdsZUeB/1nKmkmMy2M6biY8p13ir3mCrxONkxpkLOW+UWUyHtjMl634c6b5VjTEO1M6bSxTSc563+76nn6lByv5pN1dbWQlGUrOnUANDR0YGJEyeOUq9GhyzLaGxszJsQpaSqKhwOR84V6VVVBYBhWcwO6N2TXpblvKvhU/FGMneosjB3yArmD5nF3CGzmDtkhV3zx169GYKqqjj44IPxwgsvGG2apuGFF17IO0W+kpV6C7fBeDwepNPpAYW6LMtwOp3DVsBLkgSPx4NkMolUKjUs77EnGsncocrC3CErmD9kFnOHzGLukBV2zB/bFfDhcBirV6/G6tWrAQCtra1YvXo1Nm3aBAC49NJL8bvf/Q7//d//jU8++QT/8R//gUgkYqxKv6fQNA2tra0D7u8YLk6nE06nE7FYbMAKiaqqIp1O59xurhT0GQDRaLTg1Rkpv5HOHaoczB2ygvlDZjF3yCzmDllh1/yx3T3w77zzDubPn2/8/dJLLwUALFu2DPfddx/OPvts7NixAz/5yU/Q3t6OAw88ECtXrsSECRNGq8t7DI/Hg+7ubiSTyax1A1RVNbZ883g8JX9fSZLg9XpzvjcREREREdGewnYF/HHHHTfkKOtFF12Eiy66aIR6RDqHw2GMwquqaiy0IEmSMY1+OAp4/b1VVR3w3kRERERERHsK202hp8KNxoIKHo8HmqZlbb0H9I7CZzKZYb1PRH9vLmhnnd0W46DywdwhK5g/ZBZzh8xi7pAVdswfW+8DPxq6u7sRCARsvw/8aO7NGA6HkU6nUV1dbWzpkk6nEQqFoKoqvF7vsMUUi8WQSCQQDAYH7OfJ48SYGBNjYkyMiTExJsbEmBgTYyrHmPbofeDNWLFiBVasWGF8kC0tLfD7/QCAQCCASZMmoaOjA6FQyPiZ2tpa1NbWYuvWrYhEIkb7xIkTEQwG0dbWlrU6e11dHfx+P1paWrKSpL6+Hg6HA83NzVl9amxsRDqdRmtrq9EmyzKampoQDofR2toKp9MJSZKgqioaGhoQCoXQ3t5uPN/n82HKlCno6upCZ2en0W4lpkQiASEEOjo6MGXKFPj9fmzYsAGZTAZCCEiShIaGhqJjikQi2LJli9GeKyYhBFRVhcvlQjweL1lMw3WcColpuI5TvpgymQxSqRScTuewHaeRjqkSj5MdYxJCwO/3o66urmJiAirvONk1pt27d2Pz5s3GeasSYqrE42THmIQQxnmrqampImKqxONkx5iEEMbzKyUmoPKOk11j2rlzJ7Zt22act4Y7ppqaGhSCI/D9lMsIfDqdxrp16zBz5kxjFHwkryDpi9aNGTMGsiwb0+d7enrg9/uNheaG46pYMplELBaD3++Hoigli6l/e6Ve6ctkMli/fj1mzpwJp9NZETEV0nfGZD2mTCaDlpYWNDU1QZKkiohpsHbGVNqY8p23yjmmSjxOdoypkPNWucVUSDtjsh5TIeetcoupb98r5TjZNaZUKoXm5mbT561iY+IIvEWKomQVh8DnCdFfse39X9dMu54o/fspSVLO55eq73q71+tFKpVCIpGAx+Mx+hGPx5FOp+F2u03FVEi72+1GKpVCLBZDdXU1JCl7QTu7Hadi2kt9nPL1Uc8d/bOrhJistDOmwvuo50wlxWS2nTEVH1Ou81a5x1SJx8mOMRVy3iq3mAppZ0zW+z7UeascYxqqnTGVLqbhPG/1f8/+NU0+uV+NaAiKosDtdiMej2ddsVJVFalUKusqU6lJUu+2cplMJms6ChERERERUSVjAV+mJEka9e3U9FH2eDxutKmqCiHEsBfWfbeV410gxbFD7lB5Yu6QFcwfMou5Q2Yxd8gKu+YP74HvR78Hfqh7D6hXNBpFIpFAIBAwpod0d3dDkiRUVVUN63tnMhl0d3fD5XLB6/UO63sRERERERENl0LrUI7AlykhBHbv3j3qo8/5RuFTqdSAxSNKre80/v6LQFB+dskdKj/MHbKC+UNmMXfILOYOWWHX/GEBX6Y0TUN7e/uwF8lDkWV5QBGtqioAjMj96W63G7IsIxaLDft7VQq75A6VH+YOWcH8IbOYO2QWc4essGv+sIAny9xuNyRJMkbhZVmG0+kckQJekiR4PB4kk0mkUqlhfz8iIiIiIqLRwm3k8tD3ywbsvY9h/z6O1t6M+oJyTqcTiqJAVVWEw2Ekk8mc2y6Ucr9JRVEgyzIikQgCgQCEELY7TsXGZLa9kJgymYyRO5USUyF9Z0yl2Qde//9KiWmwdsY0PDGZPbfaOSazfWdMhcVUyHmr3GIqpJ0xlWYf+KHOW+UWU9++M6bhj8nKeavYmAqdqs8C/v+sWLECK1asMD7IlpYW+P1+AEAgEMCkSZPQ0dGBUChk/ExtbS1qa2uxdetWRCIRo33ixIkIBoNoa2vLGoWuq6uD3+9HS0tLVpLU19fD4XCgubk5q0+NjY1Ip9NobW012mRZRlNTE2KxGHp6etDS0mIU0A0NDQiFQmhvbzee7/P5MGXKFHR1daGzs9NoL3VMW7ZsQSaTwfbt2yHLMqZPn258jn1Xbhwspkgkgi1bthjtxcQkhIDb7YbH40FXV5dtjpOVmIbjOLW0tCCTyRi509DQUBExVeJxsmNM+sUxSZIqJiag8o6TXWPq7u7OOm9VQkyVeJzsGJMQwsidpqamioipEo+THWMSQhjFUqXEBFTecbJrTLt27co6bw13TDU1NSgEV6HvR1/9r6ury1j9r1KuIA33VbFEIoFoNIrq6mo4nU5EIhEkk0kEAoERiSkajSKTyQxY/Z7HiTExJsbEmBgTY2JMjIkxMSbGZOeYwuEwgsHgkKvQs4Dvp1y2kdM0DV1dXRg7dqyRqKNNCIFQKARFUVBVVYVUKoWenh5UV1fD4Rj+yR7cVq4wdswdKg/MHbKC+UNmMXfILOYOWTHS+cNt5CqcEAKdnZ0F3ysxEiSpd0G5VCqFdDoNh8MBWZaRSCRG5P25rVxh7Jg7VB6YO2QF84fMYu6QWcwdssKu+cMCnkpKVVUoioJYLGbcm59MJkcs8bmtHBERERERVSoW8FRSfUfhU6kUVFWFEGLEtnjT3z+ZTI7YyD8REREREdFIYAFfpvSVEPuu8G4X+lZysVgMDocDiqIgHo+P2Ci8y+WCy+UyFtGjbHbOHbI35g5Zwfwhs5g7ZBZzh6ywa/5wEbt+ymURO7tLJpMIh8PGivA9PT3wer1wu90j8v5CCKOAr6qqgtPpHJH3JSIiIiIiKhYXsatwmqZh27ZtA7ZIsAtVVeFwOBCLxeB0OuFyuRCLxUasv/oew06nE+FweMSm8JcDu+cO2Rdzh6xg/pBZzB0yi7lDVtg1f1jAlyl9yzY7T6DweDxIp9NIJpPweDwAevdqHymSJMHv90NRFITDYaTT6RF7bzsrh9whe2LukBXMHzKLuUNmMXfICrvmDwt4GjZOpxNOp9NYkd7r9SKZTI7oaLgkSaiqqoKiKOjp6eH2ckREREREVLZYwNOw8ng8yGQySCaTxrT6aDQ6oley9JF4WZZZxBMRERERUdlyjHYH7CqTyRiFniRJkGUZmqZlFZ752mVZhiRJedv7F5Cy3Hsdpf/9FfnaFUUBAIwdO9Z4TO+LECLr+cX2vdQxSZIERVEQjUbhcDjg8XjQ3d2NaDSataCdoih5+16qmHw+H7q7uxEKhVBVVQWHwzHsx2m4YzJznDRNM3KnUmIqpO+MyXpMmqahpqZm0L6XW0yDtTOm0sYE5D5vlXNMlXic7BhTIeetcoupkHbGZD2mQs5b5RZT374zpuGNSQhh6bxVbEyFDnCygP8/K1aswIoVK4wPsqWlBX6/HwAQCAQwadIkdHR0IBQKGT9TW1uL2tpabN26FZFIxGifOHEigsEg2trasrYxq6urg9/vR0tLS1aS1NfXw+FwoLm5OatPjY2NSKfTaG1tNdpkWUZTUxNisRi6urrQ1dUFoHfRuIaGBoRCIbS3txvP9/l8mDJlCrq6utDZ2Wm0j2RMQggIIdDe3o6mpiYoioKNGzdCkiQj4ZuamhCJRLBlyxbjNUod07Zt2xAOh42+TJo0CWPGjBnW4zTcMVk5Tl1dXRUXE1B5x8mOMcmyjM2bN1dUTJV4nOwWU09PT9Z5qxJiqsTjZOeYurq6Ki4moPKOkx1jkmUZGzZsqKiYKvE42S2m3bt3Z523hjummpoaFILbyPWjL9/f1dVlLN9vxytImUwGW7ZsweTJk43XtfNVse7ubsiyjOrqagghsHv3biiKYlwkGckrfZlMBj09PXA4HEZ/7HKlbySOk6Zp+OyzzzB58mQ4HI6KiKmQvjOm0ozAb9u2DXV1dQBQETEN1s6YShtTvvNWOcdUicfJjjEVct4qt5gKaWdMpRmBH+q8VW4x9e17pRwnu8aUTqexdetW0+etYmMKh8MIBoNDbiPHEfg8FEUxpqrr9ITor9j2/q9rtj0Wi0GW5azHJUnK+fxS9d1sTB6PB9FoFJqmGYV7OBxGJpOBqqqD9r3UMSmKgkAggJ6eHvT09KCqqirnz5TiOI1UTIW26++p544kSXn7nq/drjFZaWdMhfdRX8MiX1/KMSaz7YypuJiA3Oetco6pEo+TXWMq5LxVbjEV0s6YrPd9qPNWOcY0VDtjKk1MkiQN63mr/3vq329Dyf1qRCXmcrkgSRISiQSA3ikrTqdzxBe00zkcDvj9fmQyGWNaPRERERERkZ2xgKcRIUkSXC4XEomEUSx7vV5omoZYLDYqfXI6nfD7/UilUohEIiziiYiIiIjI1ljAlylZlo1FOcqFy+WCEMIYhVcUBR6PB/F4fNS2dtOL+GQyiWg0Oip9GGnlmDtkD8wdsoL5Q2Yxd8gs5g5ZYdf8sVdvqGCSJCEYDBZ8r4QdKIoCVVWNAh4A3G43ZFnOWrFxpKmqCp/Ph0QisUcU8eWYO2QPzB2ygvlDZjF3yCzmDllh1/xhAV+mNE3Dhg0bBqywaHculwuZTAapVApA7y+Gz+dDOp3OKuxHo19erxfxeHzUpvSPlHLNHRp9zB2ygvlDZjF3yCzmDllh1/xhAV+mhBBIJpNld9+20+mEoiiIx+NZbaqqGqvUjxa32w2Px4NYLFbRRXy55g6NPuYOWcH8IbOYO2QWc4essGv+sICnEed2u5FKpbLue/d6vQAw6oWzx+Mxivi+FxmIiIiIiIhGGwt4GnGqqkKSpKwCWZZleDweJBIJpNPpUexdbxHvdrsRjUZZxBMRERERkW2wgC9Tsiyjrq7OdqsiFkKSJLjdbiSTyawp8y6XC4qi2GJLN6/XW7FFfDnnDo0u5g5Zwfwhs5g7ZBZzh6ywa/7YqzdUMEmS4Pf7bbcqYqH0LeWSyaTRpi9ol8lkRnVBO13fIt4O/SmVcs8dGj3MHbKC+UNmMXfILOYOWWHX/HGMdgfsKpPJGPdoS5IEWZahaVrWyHC+dlmWIUlS3vb+e57rV3X6L+CWr11RFKTTaaxfvx4NDQ1QFMXoixAi6/nF9n2kYhJCwOFwIBqNwuFwwOFwQAgBSZLgdDoRiUSy2kcrJn3V/J6eHggh4HK5ijpO+fo+mjFlMhls2LABDQ0NcDqdgx6ncompkL4zJusxZTIZtLa2YubMmZAkqSJiGqydMZU2pnznrXKOqRKPkx1jKuS8VW4xFdLOmKzHVMh5q9xi6tv3SjlOdo0plUqhpaXF9Hmr2JgKnYHMAv7/rFixAitWrDA+yJaWFvj9fgBAIBDApEmT0NHRgVAoZPxMbW0tamtrsXXr1qx9zCdOnIhgMIi2trasEea6ujr4/X60tLRkJUl9fT0cDgeam5uz+tTY2Ih0Oo3W1lajTZZlNDU1IRqNorOzE5qmQZZlqKqKhoYGhEIhtLe3G8/3+XyYMmUKurq60NnZabTbISYhBIQQ2LFjB2bNmoVIJIItW7YY7V1dXWhsbBz1mPRfJk3TUFVVhY0bNxZ8nPSYdHY4Tul0Gl1dXdA0DTNmzCg69+wYk9XfJ8ZUWEyaphm3lFRKTEDlHSe7xtTd3Z113qqEmCrxONkxJk3TjPPWrFmzKiKmSjxOdoxJ0zQjjkqJCai842T3mPTz1nDHVFNTg0JIYrRvNraZ7u5uBAIBdHV1obq6GoA9ryCl02msW7cOM2fOLMsReL29p6cHABAMBrP6nkgkEIvFUF1dDYfDMeoxCSEQj8eRSCTg9XqhquqQsdr16mUmk8H69esxc+ZMjsAzpqJHMlpaWtDU1MQReMZUdEz5zlvlHFMlHic7xlTIeavcYiqknTGVZgR+qPNWucXUt++VcpzsGlMqlUJzc7Pp81axMYXDYQSDQYRCIaMOzYUj8HkoigJFUbLa9ITor9j2/q9rpl1PlP79lCQp5/Ot9v3x1Vvx1AfbcOoBk7D4wL0s9b1vu8fjQSQSQTqdhsPhMNq9Xi/S6TSi0Siqq6uHJaah2vu/p8/nA9C71Z2iKFlFfK7nA/mPx3Adp6Ha9ffUc0eSpLx9z9du15istDOmwvuo50wlxWS2nTEVH1Ou81a5x1SJx8mOMRVy3iq3mAppZ0zW+z7UeascYxqqnTGVLqbhPG/1f089V4fCEfh+9BH4oa58jDYheheA07dkG07PfNSObz/wLgBAloBXrpiPujHekry2EAKhUAhOp9MokHXpdBrd3d3w+/0DiuXRIoRAJBJBMpm0Vb+KMZK5Q5WFuUNWMH/ILOYOmcXcIStGOn8KrUNzXw6gsuBwDP8Eii27ovjBw2uMv2sCWL15d8leX5IkuFwuJBKJAVNd9EXs7LSNmyT1rpTvdDoRDoeRSqVGu0umjETuUGVi7pAVzB8yi7lDZjF3yAo75g8L+DKlaZqxsMtwSWU0XPzn99EdT2e1t3VG8vyEOS6XCwBybtXm8XiQTqdtVShLUu+WEuVaxI9E7lBlYu6QFcwfMou5Q2Yxd8gKu+YPC3jK67bn1uG9TbsHtLd2Rkv6PrIsG6Pw/e/ocDqdUBTFVqPwwOdFvMPhKMsinoiIiIiIyg8LeMrp5XU78OuXWnI+1raztCPwAIz91ftusaDzeDxIpVJIp9M5fnL06EW8oigIh8O26x8REREREVUWFvBlKhKJDBitLpWO7jgufWh1VlvDuM8XmCv1FHrg8/vdc02jdzqdkGXZdqPwQG8RX1VVBUVR0NPTwyKeiIiIiIiGDQv4MjZp0qS82xKYldEEvveX1dgZ+XwkfNnh0/CVQ6YYf98ZSaI7Xvop4263G+l0ekARLEkSPB4PksnkgP0S7aDvSHw5FPGyLKOxsbHkuUOVj7lDVjB/yCzmDpnF3CEr7Jo/9uoNFczlcg1LQfvL/12PNzbsNP6+3+RqXPXFfTC9JnuLt+EYhR9spF1VVduOwgO9v+B+vx+yLKOnp8eWFxr6svtFBrIv5g5Zwfwhs5g7ZBZzh6ywY/6wgC9TiqKgs7OzpAXtqg07cccL64y/+1QFvzznILidCuprswv41mEo4CVJgtvtRjKZHLDao/5YIpGwbXEsyzKqqqpsX8RrmobW1lbbrahJ9sfcISuYP2QWc4fMYu6QFXbNHxbwZUqSJEiShGQyWZJ74XeGE7jkL+9D6/NSP/3SbKNwnzLWC0n6/LG2Eq9Er1NVFZIk5bwX3uVy5X3MLvQiXpIkWxfxRERERERUfljAlzlN0yxP7dA0gcseXoOO7s8L47O/MAWLD9zL+LvbqWBywGP8fThWogd6C2BVVRGPxwdcmOg7Cm+3K2F96UU8ABbxRERERERUMizgy5iiKJBlOefWa8X4/T834KW1O4y/N47349rT9xvwvL7T6IdjCr3O7XZDCJEzLpfLBQC2HoUHBhbxdrvgYLfFOKh8MHfICuYPmcXcIbOYO2SFHfPHfj2igiiKgqamJmN1drPT6N/btAs/X7nW+LvbKWPFvx0Ej6oMeO70Wq/x/8M1Ag/0xuZ0OnPe3y/LMlwuV84RertRFMWWRbyeO4oy8BgTDYa5Q1Ywf8gs5g6ZxdwhK+yaP47R7oBdZTIZY+qzJEmQZRmapmUVjfnaZVmGJEl52/tPqdav7PQv8PK1K4oCTdMQDofhcrmQyWSQSCSMkeu+zx+s7z3xDL774PtI97nx/ZrT9kXThKqcfe+7Ev3uaAo7e2IY63eXLKa+fXc6nYhEIkilUnA4HFnP16fYx2IxY0R+qFhH6zjJsgyv14twOIxQKISqqio4HI6ijlOpYxJCIBqNwuv1Gl9IZo9T376MZkyF9J0xWY9Jz52qqioIISoipsHaGVNpY9LPW16v11jHpdxjqsTjZMeYCjlvlVtMhbQzJusxCSEQi8Xg9/vznrfKLaa+fa+U42TXmDKZDCKRiOnzVrExFTo4yQL+/6xYsQIrVqwwPsiWlhb4/X4AQCAQwKRJk9DR0YFQKGT8TG1tLWpra7F161ZEIp+PSE+cOBHBYBBtbW1Z08Dr6urg9/vR0tKSlST19fVwOBxobm7O6lNjYyPS6TRaW1uNNlmWMW6v6dCSUXzy0YcYO3YsAGDXrl1obGxEKBRCe3u78Xyfz4cpU6agq6sLnZ2dRnt1dTWue2Ebtu6OGW3H1fuxoL53lD1XTP1Xon919VosmDuzJDE1NTUhEolgy5YtWe0ulwuRSGRATGPGjMHOnTsRDoch/d/qenY7Tn1j0k8aO3bswMyZM9Hd3V3QcRqOmNLpNLq6ujB27FjMmDHD8nFSVRUNDQ0F556djxNjGjwmTdMQj8cxZ84cfPbZZxURE1B5x8muMe3evRsff/wxxo4dC1mWKyKmSjxOdoxJ0zTjvDVr1qyKiKkSj5MdY9I0DclkErNnz8bGjRsrIiag8o6TXWPq7OxEc3Ozcd4a7phqampQCEnYfR7yCOvu7kYgEEBXVxeqq6sB2O8K0g8f/QiPr9mKphoVR+89GQfVVWFWjYrJE2qN9x2qjw+s2oRrn/zY+Pu0Gi8ev/AIVHuceWPa0BnBCbe+bLTd+uUD8KWD6obtqlgymUQsFkN1dbVRpOsxCSGwe/dueDweYxTebsepf0yZTAbhcBgOhwM+X/bFkJG8epnJZLB+/XrMnDkTTqfTUkx9+7InXZHdU2PKZDJoaWlBU1OT8XtY7jEN1s6YShtTOp3GunXrMHPmTCiKUhExVeJxsmNMhZy3yi2mQtoZk/WYCjlvlVtMffteKcfJrjGlUik0NzebPm8VG1M4HEYwGEQoFDLq0Fw4Ap+HoigD7nfQE6K/Ytvz3UdRaPvbbV2IpzR80B7HB+0bAAAOWcK+k6pwaEMN5tXX4JDpYxD0qjn78tFnIfz06U+Nv6uKjBXnHISgz5Xz+bopY7yQJRhbzW3sihmFtdWYgN7k79uurzgfj8eN2RB9qaqKVCoFj8eTVeDb5TgB2TEpioLq6mr09PQgEokY281Z6aPZmGRZNr6I8vU9X3v/4zRU+0jFZKWdMRXeRz1nKikms+2MqfiY9O+evo+Xe0yVeJzsGFMh561yi6mQdsZkve9DnbfKMaah2hlT6WIazvNW//fsXxfkwwK+zOzoSWBDjhXg05rAB1u78cHWbvzu1d4pI00T/Dhk+ljMq+/9MyngQTiRxkUPvo9k5vOrTD/84t7Yf6/AkO+tOmTUjfFiU1fvHvBtw7gSPdCbxG63G9FoFJlMZkCSezwedHd3I5VKQVXVPK9iLw6HA1VVVejp6UFPT0/OIn446VcYnU7niL4vVQZJkqCqKnOHTGH+kFnMHTKLuUNW2DV/OIW+H30K/VBTF0ZLOJHG8x934K22Lrzd2oXm7eGCf7ZujAdVbic+2dZttC3cdwJ+e+7BBSfm1/7wFl5Z17vl3AF1ATxx0VHFBVAkIQRCoRCcTueAaedA7/ECYMtjNZhUKmVMp/f7/SX5YtDvs9enG+rTdfT/7zt9R5Z77+Ox2xcSEREREdGeqNA6lCPwZcbvcuCMuXth8YGTEQqFkHF48M7GXXirtQurWnbg0/YIMnmuyWzZFQPw+aJ1ewU9+K+z5hRVxNXXePHK//1/a2cEQohhLQL7jsK73e6co/A9PT1IpVLGfXHlwOl0wu/3IxwOG4X8YPJdZ9MLc/1PX/p0VVmW4XA4jP8HgB07dsDlcsHj8ZQmINoj6BfUePGHzGD+kFnMHTKLuUNW2DV/WMCXKU3T0N7ejsbGRizabyIW7TcR0WgUu3qiaAlpeKdtF95q68L7m3YjkdYG/LwiS7jzq3MR8BZX9E7vsxJ9TzyNrkgSNX7XID9hncvlQiwWQzweHzAK73Q6oSgK4vF4WRXwwOdFfCQSQTqdznpssC+Jvo/pRXrfAr3vn1wymQy6u7vh9Xrhdrtt9YVE9qZ/71RVVeW954woH+YPmcXcIbOYO2SFXfOHBXwFUVUVHmcch02vxjFN4wEAybSGf23djbdad+Htti6809aFjCawfPH+OHjamKLfY3q/reTadkaGvYDXR+FjsVjeUfhwOIx0Oj3kSLbdOJ1OBIPBEX9ffUXMRCIBt9s94u9PRERERETFK69qhwblcDigKAqSyaSxqJvqkHHwtLE4eNpY/AdmWJ7yXl+TXcBv2BHBwdPGWup3IdxuN+LxeN5ReFmW865WTwPpi3LE43G4XC6OwhMRERERlYHcc2zJ9iRJgs/nG1B4uVwuJJPJAfdD9/05K+rGeOCQP3+Ntp3DuxK9Th+FTyQSOe/19ng8SCaTA/ZTpIH03PF4PMYoPFEh8n3vEBWC+UNmMXfILOYOWWHX/DFVwK9evRp//vOfs9qeeeYZHHPMMTj00ENxxx13lKRzlJ8sy5gyZcqA+5z1kfdkMjks7+tQZEwd6zX+3tYZHZb3yUUfKY7H4wMeU1XVGIWnwem543Q6jVF4bkZBhcj3vUNUCOYPmcXcIbOYO2SFXfPHVG+uuOIKPPTQQ8bfW1tbsWTJErS29u4/fumll+Luu+8uTQ8pJ03T0NnZOWA0WpZlOJ3OYSvggez74FuHeS/4vmRZHnQUPt9jlK1v7uij8MOZL1Q58n3vEBWC+UNmMXfILOYOWWHX/DFVwK9ZswZHHfX5/t/3338/FEXB+++/jzfffBNnnXUWfvOb35SskzSQEAKdnZ05R05VVUU6nR626eTT+9wH37YzMqKjty5X74J5uUbaBxuhp8/1zR1FUaCqKmKxGEfhaUiDfe8QDYX5Q2Yxd8gs5g5ZYdf8MVXAh0Ih1NTUGH9/+umnceKJJ6K2thYAcOKJJ2L9+vWl6SEVTVVVSJI0bKOq9bWfT6GPJjPY0TNy91DLsgyXy8VR+BJyu90chSciIiIiKgOmCvhJkybhk08+AQBs27YN7777LhYuXGg8Hg6HbXevwJ5EX2E8kUgMyxWj/lvJjeQ0egDGtme5Fl/TR+i5MFvhHA4HR+GJiIiIiMqAqW3kFi9ejLvuugvxeBxvvvkmXC4XlixZYjy+Zs0aNDQ0lKyTNJAkSQgEAnlXRdQL+HQ6DafTWdL3nl4zcC/4Qxtq8jy79PRReH0LtL4Xi/o+5na7bbdqpB3kyh23243u7m4kk0njIghRf0N97xANhvlDZjF3yCzmDllh1/wxNUx+ww034Etf+hIeeOABbN++Hffddx8mTJgAAOju7sZf//rXrBF5Kj1ZljFp0qS8Mx30vdGHY1r05KAHqvL5+7aO4Er0OrfbDSFE3lF4IQTvhc8jV+44HA44nU6uSE+DGup7h2gwzB8yi7lDZjF3yAq75o+pEXi/348//elPeR/bsmULvF5vzsepNDRNQ0dHByZMmJA3qfSRaK/XW9IrR4osYWqNF+u3hwEAbSM8hR74fEX6XCPtiqJwFH4Q+XLH4/FwFJ4GVcj3DlE+zB8yi7lDZjF3yAq75k9Je5JMJhGLxRAIBEo+bZuyCSEQCoUGHS1VVRVCiGEZhe+/Ev1o0Efhc420D/bYni5f7nAUnoZSyPcOUT7MHzKLuUNmMXfICrvmj6kC/i9/+Qu+//3vZ7UtX74cfr8fwWAQS5YsQTgcLkkHyTxFUeBwOIalgO+7En3bzgg0beQTu+/97v1/sfqOwnNF+sJ5PB5kMhmuSE9EREREZEOmptDfeuutmDt3rvH3119/HcuXL8cpp5yCffbZB3fddRduvPFG3HTTTSXr6EjLZDLGPuqSJEGWZWiallUo5muXZRmSJOVt778/uz4lo3+hma9dURQIIaBp2oA+6u06p9OJWCyGdDqdNZXcakxTx3qMx+IpDZ/tjmJSwF2SmPr3JV+7pmlGfNFoFB6PJ6vvqqoiHo8jFovB5/ON6nEqJqZCjoeV3MtkMkbu9O+7JElQFAWxWAxOpzPnhRE7xlTI8Si342THmDKZjPH/lRLTYO2MaXhiMntutXNMZvvOmAqLabDzVrnGVEg7Y7IeUyHnrXKLqW/fGdPwx2TlvFVsTIWO9Jsq4FtaWrBs2TLj7w8++CAmTpyIRx99FA6HA5qm4W9/+1tZFfArVqzAihUrjA+ypaUFfr8fABAIBDBp0iR0dHQgFAoZP1NbW4va2lps3boVkcjn08gnTpyIYDCItra2rJHMuro6+P1+tLS0ZCVJfX09HA4Hmpubs/rU2NiIdDqN1tZWo02WZTQ1NSEWiyEej6OlpcXYNq6hoQGhUAjt7e3G871eL/x+P3bs2JHVd6sxOePZC9e9/q9mHDDh8wLeTEyRSARbtmwx2vPF5PP5MGXKFHR1daGzsxNCCAghEAwGMXny5KyYhBDw+XzweDyjcpzMxlSq45QrpkwmY+ROQ0PDgJiEEBg3bhyi0WhW3+0cUyUeJzvGpF8YkySpYmICKu842TWm7u7urPNWJcRUicfJjjHpt8S1tLSgqampImKqxONkx5iEEPB4PJAkqWJiAirvONk1pl27dmWdt4Y7ppqawnb1koSJSf0+nw933HEHvvnNbwIAmpqacPTRR+Oee+4BAPzhD3/ARRddhGh05Fcnt6q7uxuBQABdXV2orq4GUP5XkKLRKNLpNKqqqobse6ExfbY7hqP/62Xj8RsW74evzpsyYjH17WMmk0F3dze8Xi+8Xm9W3zVNMx7TV6fPF1P/9j356mUkEoGmafD7/QMWCCzXmCrxODEmxsSYGBNjYkyMiTExpsqIKRwOIxgMIhQKGXVoLqZG4Ovr6/H888/jm9/8Jt555x2sX78eN954o/F4R0eHMXpdrhRFgaIoWW16QvRXbHv/1zXTLoTAZ599hr322ivrfSRJGvB8VVWRTCYhhIDDkX3Izca01xgfXA4ZiXRvsm/aFcvZz2JiytX3wdr1viiKArfbjWQyaUyj7/s+Ho/H2DN+sNex0vdSxzRc7YqiQNM0bN26FXvttZdRnPfvi9frRXd3NzRNg6qqBfV9NGOy2s6YCutj39yplJistDOm4mLKd94q55gq8TjZMaZCzluD9d2OMRXazpis9V3TNGzZsmXQ81a5xVRIO2MqTUwAhvW81f89+w6aDSb3qw3h29/+Nv7nf/4HBxxwABYuXIi6ujqceuqpxuOvvfYa9ttvPzMvTQUSQiASiRR0r4TT6YQkSSVdmEyWpayV6FtHYSu5vjweDzRNy7kvvNvdO7U/12N7okJyR1+RPhaLFXw/DlW+Yr53iPpj/pBZzB0yi7lDVtg1f0wV8N/97nfx29/+FjNmzMDixYvx7LPPwuPpXdSsq6sL7e3t+Ld/+7eSdpTMkyQJLpcLiUSipAk4ve9K9KNcwCuKYixa1z9GWZa5Ir0JbrcbmUwGqVRqtLtCREREREQwOYUeAC644AJccMEFA9rHjh2Ld955x1KnqPT04jaVSg2YEm3W9NrPR+A3dkWhaQKyXNjUj+HgdrvR3d2NZDIJl8s14LFEIoF4PA6v15vnFagvp9MJh8OBWCxWspwhIiIiIiLzTBfwuo8//hgbN24EAEybNg377ruv5U7R0GRZxsSJE/PeU9Gfw+GAoihIJpMlK8bq+0yhT6Y1fBaKoW7M6BXHDocDqqoaBWff+0j0UfhEIgG3213w51aJiskdj8eDnp6ekuYNla9iv3eI+mL+kFnMHTKLuUNW2DV/TPfm8ccfx4wZMzB79myceuqpOPXUUzF79mzMnDkTTzzxRCn7SDlIkoRgMFjwYgcA4HK5kEwmSzaNvO8IPAC0dY7+rgNutxuapuW831+/Fz4ej490t2ylmNzpOwpPZOZ7h0jH/CGzmDtkFnOHrLBr/pgq4J9++mmceeaZAICf/vSnePTRR/Hoo4/ipz/9KYQQ+NKXvoSVK1eWtKOUTdM0bNiwoahiXB9BLdVidvX9CvjWnaN7Hzzw+eJrg90Ln0gk9uh74YvNHY/Hg0wmU9JFEKk8mfneIdIxf8gs5g6ZxdwhK+yaP6am0F9//fU44IAD8Oqrr8Ln+7yIO/3003HRRRfhqKOOwvLly3HSSSeVrKOUTQhhbA1XKFmW4XQ6kUwmjdFoK8ZXueBVFUSTvXsYjvZCdjqPx4Pu7m7E43FjcUUd74UvPnf0Ufh4PM5p9Hs4M987RDrmD5nF3CGzmDtkhV3zx9QI/AcffIBly5ZlFe86n8+H8847Dx988IHlzlHpud1upNPpkmypJkkSptloKzmdw+GAx+NBLBYbMGosyzLcbjdXpC+Sx+NBOp02PQpvty8+IiIiIqJyZGoE3u12o6urK+/jXV1dJRnhpdJzOp3GQm9Op9Pyogz1tV58sq0bgH1G4IHPL1REIhEoigJFUYzH9C3l9uRR+GL1HYUHegvyYv4An19YcTqdoxkKEREREVHZMlW9HX/88bjjjjvwxhtvDHjszTffxJ133okFCxZY7hzlJ8sy6urqTBXgXq8XQoiSLOY2vc8I/KauKNIZe4xqS5IEn88HSZIQDoezRoD39FF4s7mjj8KHw2FEIhFEo1HE43Ekk0mk02njs5Rl2dgRwO12w+v1GjnX09ODnp4epNPp4QiNhpmV7x0i5g+Zxdwhs5g7ZIVd88fUCPzPf/5zHH744TjqqKMwb948zJo1CwCwdu1avPXWWxg/fjxuvvnmknaUskmSBL/fb+pn9QJW327N4TC/m2DflejTmsDW3bGsafWjSZZl+P1+9PT0IBKJZH1e+ih8LBbLeStIJTObO06nE8Fg0HiNYlfkdLvdSCaTiMVi6O7uhtPphMfjsZR/NLKsfO8QMX/ILOYOmcXcISvsmj+mLifU19fjgw8+wMUXX4xdu3bhoYcewkMPPYRdu3bhkksuwZo1azB9+vQSd5X6ymQyWLduHTKZjKmfd7vdUBQF0ai1rd8a+q9Eb6Np9EDvtG2v14tkMpk140C/iJFIJEx/huXKSu7IsgxZlk1vp6GqKqqrq+Hz+aBpGrq7uxEOh/e4Y1CurH7v0J6N+UNmMXfILOYOWWHX/DE9H2D8+PH4xS9+gU8//RSxWAyxWAyffvopbrvtNqRSKbz++uul7CflYGX6tyRJ8Hq9lhe0G7gXvL0KeKB3tN3lciEajWZN3Xa73ZAkaY/cF340bx2QJAkul8so5NPpNEKhEAv5MrEn3nZCpcP8IbOYO2QWc4essGP+DMuE/vvuuw9HH330cLw0lZC+oF00GjWdnDU+FVWuz6dAt+20NqI/XLxeLxwOB8LhsBGrJEl77Ci8HeiFfCAQMC4mhUIhRCKRoo6HEMK4EBWNRtHT04NQKITdu3cjlUoNYwRERERERCPLXnfk04jTV2E3OwotSVLWKLzdptDr9HtYhBBZi9rtyaPwdqFfSNEL+WQyiVAoNODCUq5Cfffu3di1axe6u7sRiUSMbe70HRY4qk9ERERElYSrR5UpWZZRX19veVVEWZbh8XgQjUZNL2g3vdaHf20NAQDadtqzgAeyF7WLxWLwer2QJMmIX18XoNKVKndKTS/k+27zl0gk4HA4kMlksop5WZahKApUVTW2CdTvz9dpmmasel9dXW27eMuRXXOHygPzh8xi7pBZzB2ywq75wwK+jJVq9W6Xy2WMalZVVRW9QFl9zed7qW/ZFUMqo8Gp2CvRdU6nE16vF9Fo1NjqzOVyIRaLIR6P7zEr0tt55Xf9ooqel+l0OqtQVxSloBzte8FGL+LNLr5Hn7Nz7pD9MX/ILOYOmcXcISvsmD/2rLJoSJqmobm5uSQLK/Rd0E6fglyMvlPoM5rA5i573gevc7vdUFXVuNdaLxj3lHvhS5k7w0mfHVJVVQWv1wuXywWHw1FUEa4oCvx+PzRNy7p1gswpl9whe2L+kFnMHTKLuUNW2DV/Cr6k8MgjjxT8oh999JGpztDo6bugnX7/cKEGrES/M4KGcfbbM7Evn89nbGFWXV2dtS+8Hfd7JPMcDocxEh+NRveYWRZEREREVHkKLuDPOussSJJU8AgWp6qWH6/Xi1AohFgsVlSRU1/Tfy94e4/AA58vaqcvfub3++F2uxGNRpHJZPaIe+H3JE6nEz6fD5FIxBjZJyIiIiIqNwUX8C+++OJw9oNsoO+Cdvp05UKM8akIeJwIxXq37LLjXvC5KIoCn8+HcDiMeDxujMKHQiGoqgpVVeF0OnkxqkK4XC5omoZYLAZZluFyuUa7S0RERERERZEEbwrN0t3djUAggFAohOrq6tHuTl5CCGiaBlmWS1pgCiHQ3d0NSZKKWtBu8YrXsGbzbgDA0Y21eOAbh5asT8MtGo0iHo+jqqoKiqIgmUwa98PrhZ6+iFolGK7cKRfhcBjJZBJVVVVwOp2j3Z2ysqfnDlnD/CGzmDtkFnOHrBjp/Cm0DuUidmUsnU6X/DXNLmjXdyV6u+4Fn4/H44HD4UA4HAYAY0/y6upqOJ1OY1S+p6cHiUSiIhZCG47cKRc+nw9Op5N7xJu0J+cOWcf8IbOYO2QWc4essGP+2G9dfJvIZDLGP+4lSYIsy9A0Lat4y9euX6XJ196/aNAXjOu/wmG+dkVRkMlk0NLSgpkzZxrbasmybFwpGqqPg7U7nU6jwNH31h4qpmljPy/gP9sdQzyVhsuhFBVTvr6XIqbBjpMQAh6PBz09Peju7jb2DO+7L3kymUQqlTKKfH37OX3/cTMxZTIZpFIppNNp4yJCqWIaLPf65o4+Al0Ox6mUv0/68Q6FQggGg8brl3NMfQ3XccpkMtiwYQOampoGrIlSrjEN1s6YShtTvvNWOcdUicfJjjEVct4qt5gKaWdM1mMq5LxVbjH17XulHCe7xpROpy2dt4qNqdBBQhbw/2fFihVYsWKF8UG2tLQYq5EHAgFMmjQJHR0dCIVCxs/U1taitrYWW7duRSTy+ajzxIkTEQwG0dbWljWKXVdXB7/fj5aWlqwkqa+vh8PhQHNzc1afGhsbkU6n0draarTJsoympiZEo1F0dXVh/fr1kGUZqqqioaEBoVAI7e3txvN9Ph+mTJmCrq4udHZ2Gu1DxbRr1y6Ew2Fs374dkiQNGZMn02O0aQLY0NGNWZMCRcUUiUSwZcsWo73UMQ11nIQQEEIgkUigqqoKmzdvzvpFqq+vhyRJaGlpyWqfPn06ZFlGW1vbkDE5nU5MnToVXV1d2Llzp9HudrtRX19f8pj6H6eWlhak02kjd2bMmFF07o32ccoVk5nfJ/0Y6hdhtm7dWvYxAcN7nDRNQzweB4CKiQmovONk15i6u7uzzluVEFMlHic7xqRpmpE7s2bNqoiYKvE42TEmTdOMOColJqDyjpOdY+p73hrumGpqalAI3gPfj37vQVdXl3HvgR2vIKXTaaxbt25YRuD19lgshlgshqqqKqiqOmhM72/swpJfv2G0333uwThx3wlld6UvnU4jlUohlUoZ98DrMxL6fwapVArJZBLpdNqYueB2u40Y+8aUyWSMUXwhhPF8VVWhaRoikQjcbjc8Hs+IjMCvX79+jx6B16XTaUSjUSiKAq/XC0mSyj4mYHhH4FtaWjgCz5hMxZTvvFXOMVXicbJjTIWct8otpkLaGVNpRuCHOm+VW0x9+14px8muMaVSKTQ3N4/YCHw4HEYwGBzyHniOwOehKMqARcv0hOiv2PZ8i6EV0y5JEhwOx4B+SpKU8/lm+q7fC59IJKCq6qDPbxhflfX3jTujefsyWEzFtJfqePRtVxQFLpcLQghjHQC9oI/FYsbK9E6nEw6HAx6PB5qmIZFIIB6PIxwOw+Vywe12Q9N6r/r2LfL11e0dDodRLOoxRiIRKIoCt9s9bLHqn6OeO3ofyu04DdXHQtv10Xd9fYP+2yeWY0y64TpO+s9WUkxm2xlT8THlOm+Ve0yVeJzsGFMh561yi6mQdsZkve9DnbfKMaah2hlT6WIazvNW//fsWxsMhiPw/ZTLKvQjJZ1Oo7u7G16vN2dh2ddB1z+Hrkjv9JBzDp2Kny6ZPRJdHHZCCGMEPZlMQtO0rBF0fas5TeudXhyJRJBOp41fev1++aG2pOu7Gj5XRx9ZiUQCkUgEHo+He8QTERER0YgrtA41PQKfyWTwzDPPYMOGDdi1a9eAm+4lScLVV19t9uVpCEIIRCIR+Hy+gq/WmOFwOOByuYzR53xXkABgeo3XKODLZS/4QuijRg6Hw5iVoE+fD4fDxuOyLCOZTBpX8YQQkOXPt6EbisfjQSaTQTgcRnV1dd4rilaNVO6UE+4RXxjmDlnB/CGzmDtkFnOHrLBr/pgq4N955x2ceeaZ2LJlS97V8ljADy9N07BlyxY0NjYOW6Gn83g8SCaTiMViA6YY9zW91of3Nu0GUFkFfH96Ma8X3Po0+3Q6DbfbbewZn06nEYvFEA6HjeLf4cj/KydJEnw+H3p6etDT02Oshl9qI5k75UQ/npFIxLjdgbIxd8gK5g+Zxdwhs5g7ZIVd88dUdXDhhRciFovhscceQ1dXFzRNG/CH+ytXDlmW4fF4kEgkBj2u9TWfF/efheKIJSs/BxRFgcfjQXV1NQKBADweT9b9elVVVaiqqoIQAt3d3UPuPS7LMvx+P4QQCIfDBW8nQaXh8/mgqirC4TDC4fCABVCIiIiIiEaTqQL+gw8+wH/+53/itNNOQzAYLHGXyI5cLhckScraAqG/6bXZo/Mbuyp3FL4YTqcT1dXV8Pl8SKfTCIVCiEajeYtDRVHg9/uNFdJp5EiSBL/fD5/Ph1QqhVAohEQiMdrdIiIiIiICYHIKfV1dHUcGR5k+xXek7sfQF21LpVJ5F/mq71fAt3VGsPdELgQI9H5++r3w8Xgc8XgciUQCbrcbbrfb2NpE/wPAGAlOp9NwOp3GFhT6f4UQxlT+wabm5+rLSOZOOdIXHYxGo4hEIkgmk/B6vbaaPjUamDtkBfOHzGLukFnMHbLCrvljahX63/3ud7jlllvw9ttvV9xK7VyFPj99pe5gMJjz3uxwIo39r3nG+Pt/nrQ3/uO4GSPZxbKhL5iWSCSML4Vcv4r6FnQej8dYxV7fO1KfEZHJZKCqatb0fSqdZDKJaDQKIQQ8Ho8xG4WIiIiIqFSGdRX6np4e+P1+zJw5E1/5ylcwZcqUnPvYff/73zfz8lQAIQRCoRACgcCIFRP61mapVCrnKt1+lwO1fhc6w71Tjit5ITurZFmGz+eD2+02ivi+xbn+XwDGtnQ+n2/A75nb7TYWGAyFQgUV8qORO+VMVVU4HA7EYjFEo1Ekk8mcx2JPwNwhK5g/ZBZzh8xi7pAVds0fUwX85Zdfbvz/L3/5y5zPYQE/vDRNQ3t7O6qqqkaskND3NU8mk3m32aqv9RoFfOtOFvBDURQFXq930Of4/X50d3fnXJm+79T8RCKBeDyOUCgEl8sFj8eTc6bEaOROudMvuKiqikgkglAoBK/Xu8eNxjN3yArmD5nF3CGzmDtkhV3zx1QB39raWup+UJlQVdWYTpyrcKmv9eHttl0AOAJfKvrCavoq9lVVVQM+e0mS4Ha74XK5ct5jPxzb0e2JnE4nAoFA1mj8UNsDEhERERGViql/dU6bNq3U/aAy0Xcafa59svuuRL+9J4FIIg2fi8WNVfrK9D09PYhGo/D5fDmfJ0mScZ+2PiKvF/Iul4uFfAlIkgSv12uMxnd3d8Pj8RiLERIRERERDRdLlVUkEsHLL7+MjRs3Augt7I899ti8xQWVjiRJ8Pl8I14wKIoCRVHyFvB994IHgLadEew3OTBS3atoTqcTPp8PkUgEiqLA7Xbnfa4sy0YhH4/HEYvFEI/H4Xa7oapq0bmjaRoymQw0TTNupdjTi1WHw4Hq6mrj89Xvja/k0fjR+t6hysD8IbOYO2QWc4essGv+mP6X5l133YUf//jHCIfDWatnV1VV4cYbb8RFF11Ukg5SbrIsY8qUKaPy3k6nE4lEAl6vd0BC998Lvq0zygK+hFwuFzKZDKLRKGRZznkRpS9ZluH1euF2uxGLxYyV78ePH593NF4v1jOZDNLpNNLpdM496xVFgcPhyPqv3b7ghps+48HpdCISiaCnpweBQKBiZzqM5vcOlT/mD5nF3CGzmDtkhV3zx1QBf//99+OSSy7B4Ycfjosvvhj77LMPAOCTTz7BXXfdhUsuuQSBQADnnntuSTtLn9M0DV1dXRg7duyIFwv6XuaZTGbAaOP0HCPwVFoejweZTAaRSASaphkr2Pf903cVeyB71ftoNIqOjg6MGzcObrcb6XTaKNb1wh3oLU4VRYGqqkaRLsvygOI+kUgY79O3oNefvycU9Q6HA1VVVQiFQohEIqiqqhrtLg2L0fzeofLH/CGzmDtkFnOHrLBr/pgq4G+77TYcc8wxeOGFF7JW5DvggANw1lln4YQTTsCtt97KAn4YCSHQ2dmJMWPGjPh76yOtyWRyQAHvURVMrHajvTsOAGjlQnYlpy9q19PTg1gslnP/+L7P7b93PNC7FaQ+gq8fQ73odrvdxq0SuYpv/Xn6TgRCiKyCvm9Rr18E0F/XTl9+paZfJAmHw0gkEnl3aihno/m9Q+WP+UNmMXfILOYOWWHX/DFVwK9duxa33HJLzuX0FUXBl7/85ayt5qiySJIEVVWRSqVyPj691msU8FyJfnhIkoTq6moAvV8uff9omjagre+fdDoNAMb0e6vboUmSZBT1On0avj6qn0gkkMlkKnZkWqeqqrFTg9PprOgLFkREREQ08kwV8IFAAG1tbXkfb2trM4oLqkz6ffCZTGbAhZz6Wh9WbegCwCn0I6HvyHohMpkMOjo6MHbsWOO++P4FuFWyLEOWZWPXgkQigUgkkjNfKo3X6634qfRERERENDpMDQ+dcsopuOuuu/CXv/xlwGMPPfQQfvnLX+K0006z3DnKT5IkBAKBUbu/uO92cv31vQ++M5xETzz3SD2NDj13ZFmG3++Hoijo6ekx7n0fDqqqQpZlxOPxYXsPu9Cn0qdSqaz1ASrBaH/vUHlj/pBZzB0yi7lDVtg1fyQx2A20eezYsQPHHnss1q5di4kTJ6KxsREA0NzcjPb2duy99954+eWXUVtbW/IOD7fu7m4EAgGEQiHOIhhCT08PhBADPqdnPmrHtx941/j7kxcdhdl1XInerjRNyzqWwzXtWx/tDwaDe8TU8nA4jFQqVdGr0hMRERFRaRRah5r6V+W4cePw3nvv4bbbbsPs2bPR0dGBjo4OzJ49G7/4xS/w7rvvlmXxXk40TcO2bdtybu81UlRVzbnFWH2/reRaOY3eVvrnjj4SD/RelBmunNLvs98TRuGB3qn0ABCJVE7+2+F7h8oX84fMYu6QWcwdssKu+WP6ple3241LLrkEl1xySSn7QwUSQiAUCmH8+PGj1oe+0+j7rrg9dawXkgToczu4kJ295ModRVFQVVWF7u5uhMNhVFVVlXy6kCzLcLlcSCQS8Hg8tpuOVGqVuCq9Hb53qHwxf8gs5g6ZxdwhK+yaP5zXSabJsgyHwzHgPni3U8HkgMf4Owv48qAX8ZlMBuFweNDt6cxyuVwQQlTcveH59F2V3m5Xb4mIiIio/BQ0Aj9//nzIsoxnnnkGDocDxx9//JA/I0kSXnjhBcsdJHtzOp2Ix+MQQmSNqE6v9WLr7hgATqEvJw6Hwxg1jkaj8Pl8Q/9QERRFgaqqiMfjlrauKyf6qvTRaNS4VYGIiIiIyIyCRuD1vaV1Q+0z3f/5VHqSJKG2tnbUCyCn05m1t7iu70r0HIG3l6FyR1VV+Hw+JBIJRKPRkr+/2+2GpmlIJpMlf2070qfSJ5PJso/ZLt87VJ6YP2QWc4fMYu6QFXbNn4JG4F966aVB/04jT5ZlWywU6HA4IMsyksmkcU88kL2Q3a5oCqFoCgGvM9dL0AgrJHf0qe7RaBSyLMPtdpfs/R0OhzFzoxLuCy+EPpU+EokYvzPlyC7fO1SemD9kFnOHzGLukBV2zR9T/4p85ZVXsGPHjryPd3Z24pVXXjHdKRqapmnYvHmzLWY6qKo64D74viPwAKfR20mhueN2u+F2uxGNRkt+z7rb7UYmkxmQN5VMX5V+OGY1jBQ7fe9Q+WH+kFnMHTKLuUNW2DV/TBXw8+fPx3PPPZf38RdeeAHz58833SkamhACkUhkWBYaK5bT6YSmaVnT6Kf330quMzzS3aI8iskdr9cLl8uFSCRS0unfTqcTiqLsMVvKAb1Xcb1eb1lPpbfT9w6VH+YPmcXcIbOYO2SFXfPHVAE/VBCJRAKKopjqEBUmk8nYJpkcDgckScoaTZ061gu5z+0irZ3lO+q4p/N6vVBVFeFwuKQj5h6PB6lUasD6CZXM5XLB6XQiEonY7mpuoezyvUNERES0Jyp4H/hNmzahra3N+Punn36ac5r87t278dvf/hbTpk0rSQcpt3DYPiPakiTB6XQimUzC4+ndPk51yNhrjAebu3pXoudCduVLkiT4fD5omoZwOIzq6uqSXKBzOp2QZRnxeHyPWp3d5/OV7ar0qVQKQghkMhlepCUiIiIaBQUX8Pfeey+WL18OSZIgSRJuvPFG3HjjjQOeJ4SAoij47W9/W9KOUjan04lgMGibxbD0Ar7vP+yn1/g+L+B5D7xtyLKMiRMnFpU7kiTB7/ejp6fHKOKtrsgpSZJxj/2eVBDqU+n12xJUVTX1OvpuH7Isj9jqqEIIVFdXI5PJjMj7UWUx891DBDB3yDzmDllh1/wpuIBfunQp9t9/fwghsHTpUlx88cU4+uijs56jj9QdeOCBmDBhQsk7S59TFAUej8c22xroK9CnUimjEKuv9eHV5k4AQGtnZMBe8TQ6JElCMBgs+uf07dC6u7tLtke8y+VCLBZDIpEwFnnbE7hcLiSTyaJWpddHvtPptHHrgRACDocDfr9/RE4uQgh4vV4W8GSK2e8eIuYOmcXcISvsmj8FF/D77LMP9tlnHwC9o/HHHnsspk+fPlz9oiHIsozt27ejqqrKFiOXsizD6XQilUoZW471XYm+J55GVySJGv+esW2YnWmahra2NkyfPr3oos/hcMDn8xmFp9Vt4PRR+Hg8DrfbbbsrnMNpqKn0esGuF+t6wQ70Hgf984pGo+jp6YHf7x/274J0Oo3Ozk6MGzduWN+HKpOV7x7aszF3yCzmDllh1/wpuIDva9myZaXuBxVJkiTjH/V2KOCB3lH4aDRqTOut77cSfdvOCAt4GxBCIJlMml6MzOVyIZVKIRqNwuFwWM6/vqPw+hoKe4L+U+mdTqfxO51Kpf4/e+8dJslVnY2/lTqnCRtmZ4M2C6VVFkJEiSSiwSCQZMAmfnzGgMkOBINtDNgEf6x/gMFgkokGiQySkATKOUs7Oxtndmd2Zme6q6u6q7vS74/WuV3d06G6u3qmZqbe55lnt2t6uu+pe+ree9J7GFElx3HMYBdFkZFGEkRRhKIokGUZyWQSotjVsu4KlAFgmiZ7zgMEcIte154AqxeB7gToFoHuBOgFftWfrk96mqbhxz/+Me69917kcrkFjMocx+FrX/tazwNcKpimydJEOY4Dz/OwLKtmAptdp5rUZtfr00/pEFx/D5tdFwQBPM/Dtm3ous4O9HTN+f5Ox96LTPR3pVIJkUgEW4ZqU6LHTyg4d/NAU5majX0pZXIzH63myY8ykfFFc9aNTJFIBLquQ5ZlRmrXi0ySJKFYLCISicC27VUzT9ROL5/PM2Od53nwPI9QKMQcJOQksSyrZjz0XnIE5HI5xONxdh+9lsnZMYCcDvUyrcR5CmTyVqZu91Y/y9Tt2AOZ3MnkZt9abjK5uR7I1LtMzq5NK0Um59gDmfovUy/7VqcyuXUUdGXAHz58GM95znNw6NAhZDIZ5HI5DA4OIpvNwjRNDA8PLzt25b1792Lv3r3sRo6PjzMZ0uk0RkZGMD09jVwux/5meHgYw8PDmJychKpWSdrWr1+PTCaDQ4cO1fR73rhxIxKJBMbHx2uUZOvWrRBFEWNjYzVj2rlzJwzDwMGDB9k1nuexa9cuFItFKIqCw4cPQxAEhEIhbNu2DblcDlNTU+z98XgcmzZtwtzcHGZnZ9n1fslEcm3fvh0b0mHwHGA9qYv3jh3Fn5472lQmVVUxMTHBrvtFpl7myY8yGYaBubk57N+/H9u3b+9aJjK0Z2dnsXPnzp5ksm0b8XgcsVgMMzMzq2qeSqUSu75hwwakUimMjY11LBPNB8dx2LZtG3Rd91Qm2txKpRJ4nsfx48drxr7S5ymQqXeZZFlmaw/P8ytCppU4T36UybIspju7d+9eETKtxHnyo0yWZTE5VopMwMqbJz/L5Ny3+i3T0NAQ3ICzu8gJuOKKK3D99dfjV7/6FbZt24a1a9fiuuuuwyWXXIJ///d/xxe/+EVcd9112LlzZ6cfveSQZRnpdBpzc3NIpVIA/OlBsiwLMzMzCIfDSCaTvvGKUSr0wMAAOI7D8z57E/bPVBT5GTuG8M03XdRUpsDTtzgy2baNQqGAWCxWE9ntRqZSqYRisYhEIoFQKNSTTNQbPZlMLhjLapynbmSybRvFYpFxUTg5CnqVyTAM5PN5CIIAURRhmmbNXAXzFMjUTibLqrSijMVi4DhuRci0EufJjzK52beWm0xurgcy9S4T7YuJRII5upe7TM6xr5R58qtMpmlCVdWu961OZVIUhQXHyQ5thK4M+OHhYbz97W/HJz7xCczNzWF4eBi/+93vcNlllwEA3vjGN2J6ehq/+MUvOv3oJQcZ8O1unB9AxrKf2BENw2C1uJIk4QM/egA/uLvi6UqGRTzw0eeD5wMm+pUERVGg63rP/eFJd8gZEKB7FItFFItFhMNhtun0ilKphHw+z7oPFItF5qgLECBAgAABAgQI0Bvc2qFdMRAVCgXGQE/9oJ1pBBdffDH++Mc/dvPRAVzCNE0cOXIEhmEs8DAtJag+X9d1AGA17wCQLxkYO6G4+hzbtqGqKvL5fF/GuZphmib27dvnWSuweDwOnuehKIrr2p1GIII2TdM8GddqRjQaRTweR6lU6nlebNtmxnuxWMShQ4dYtoSzJj5AgHbweu0JsHoQ6E6AbhHoToBe4Ff96cqA37x5M6sfEEURo6OjuP3229nvH330UdZKLED/QIa7n5SK4yrEXFTncd6WgZrf33N4vu1n2LYNRVFQKpWg6zpzBvgZ1O5rucBLpw/HcUgkErAsq6bupxtEIhHGwt4tbNuGpmmQZdlXz8ZiIxwOI5FIsNT3TufcsiwUi0Vks1moqgrbttm6LggCNE1j1wMEcAs/OZwDLC8EuhOgWwS6E6AX+FF/ujLgL730UlxzzTXs9Z//+Z/jc5/7HN7ylrfgTW96E/bu3YuXvvSlng0yQGv4zUiRJAmWVWFs3L4mgVSkypV475HWBrxlWcjn8zAMg/W1dhJl+RWqqkJR3GUXrEQIgoBYLIZyudzTfIVCIWYcdgqKFFNvdcuyVr0RHwqFkEwmYZom8vm8q3uh6zoURUE2m4WmaQiFQkin0wiHwwiHw+A4DslkEpFIhPWg9+PmFiBAgAABAgQIsBLRFQv9hz70Idx1110olUoIh8P427/9Wxw7dgw/+tGPIAgCrrrqKnz2s5/1eqwB6sBxHARB8J2BIkkSOI5DuVxGNBrFOZsHcNO+GQDAvS0i8KZpQlEURmQmiiIsy6rpLe9HUO9uAL4eZ78RDodhGAYKhQIjOusGkUgEqqrCNE3XNfXlchnFYhGmaUKSJCQSCfA8j3w+vyj90f0MURSRSqVa3gvbrvQ51TSN3fdYLIZQKFRDBkN/x3Ecq4U3TZO1rwu4CwIECBAgQIAAAfqLrkjsVjKWC4kdHbjL5TJs2/bdWKmvdSqVwheuG8PnrtvHfnffh5+HgXjtQd8wDBbBTiaTzHCzbRvZbBaRSATRaHTxBOgAFNm0LGtZELCR7oRCIc8JyGzbhizLAKr8GN18Ri6XgyiKbdtR6rqOYrEIwzAgiiJisViNcUoZHU6n0GoFMYCbpolEIgFJkmCaJkqlEkqlEmzbhiRJiEQiEEWxZu5s28b8/DwjxAuFQiw1P5lMQtM0xnwfjUYDYrsADdHPtSfAykagOwG6RaA7AXrBYutPX0nsAvgDoij6MgIPgB3wLctaUAd/39HaKLyu68jn8+A4bgGTORkLmqb5staW6rXJaPHjXDRCvwxZL+rhOY5DJBJBuVxumppNxiM5ihKJBFKp1AK5eJ5HMplk0fjVTLpG90IURRaNz+VyLJMqnU6z7hH1mxTNA8/z7B7Tv+QcicViAfdAgLZYzU60AL0h0J0A3SLQnQC9wI/642pEH//4xzv+YI7j8OEPf7jjvwvgDpZlYWxsDFu2bGG9C/2Uui1JEoCKcb5nUxocB5D9fc/heVx66joAldRnRVEgSRJjM69HJBJBqVRCuVyu6WvtB2iaBp7nEQqFUC6Xl4WBSLqzc+fOntq+NQOlX6uqyozDThEKhVAsFqFpGmKxGLtumiaKxSLK5TJ4nmdp2628omS4ksG/miPx5GApFAowTdPV/QOqPBscx9XojiAI0HUd4XCYRe5VVYUsy4jFYr57XgMsLfq99gRYuQh0J0C3CHQnQC/wq/64OsV+7GMfW3CNDnz1UVGO42DbdmDALxJImUzT9JUBT5G6crmMZDKJ3euSeHyq0hLu3sNZABXjt1AoIBQKIR6PNzUiBEGAJEldG4P9gmmaKJfLLK1YEATGvr/aQfXwqqp2VQ/P8zzC4TBKpRJjPi8WiyiVSuB5nhmHbtOZAiO+Cmf9ultQBL7+fouiWNMxgOrtC4UCVFWFruuIxWK+WpsCBAgQIECAAAGWM1ydqizLqvk5evQozjzzTFx55ZW48847kcvlkMvlcMcdd+C1r30t9uzZg6NHj/Z77AEAdjD2Y8qqJEkwDAO2beNcRxr9/UezkBUFhUIBkUikpfFOIIPQTxFuTdPAcRxzKgiCwJ6RAEAsFoMgCF23GotEIqylYC6XY6SI6XQakUik41qkIJ2+e1iWBUEQGhrw9TpPDoJEIgFd1yHLcnCvAwQIECBAgAABPEJXYZG//Mu/xM6dO/Htb38b559/PpLJJJLJJC644AJ85zvfwfbt2/GXf/mXXo81QAP4lYkeqKRB27YNXddx3uaqAV/UTTx8dB7RaJRFr9tBkiTwPN9Ve7F+wLIsFh2m8VNE149zsRTotR6eovCmabIa7V4J0gIjvjs0y/AhnW90H0OhEFKpFHiehyzLrEuAYRis3aCmaSgWiyxirygKq83PZrOYn5/H3Nwc5ufnWdp/gAABAgQIECDAakZXBvwNN9yASy+9tOnvL7vsMlx//fVdDypAe/A8j507d4Lned8a8M4a2XPriOwenyl1xCpPke5WxGaLifroO1CZk+VAZOfUnX5DEATE43HGdUBdE9wiFoshk8l4moYdGPGdgzg26nVHEATwPN/0HgqCgGQyiWg0imKxiFwuB1mWoSgKVFVFoVBgDPamacK2bVZ+EwqFEIlEWLlEqVRif0+s+f2QU9M0pqsBvMVirj0BVhYC3QnQLQLdCdAL/Ko/XRWBRiIR3HbbbXj729/e8Pe33norq1sN0D8YhoFQKMSMZD9CkiSUy2VsHkhiICpivlg56D94LN/xZ4XDYVYHvZQt5Sj6Hg6Hax5oyoZYDgYh6c5iIBQKIRaLoVQqQVEUcBwHSZIQCoUaMp470a+WHUFNfGegFHpgoe6IothS5zmOQzQaRSgUYo4AjuPYj1tEo1HWOpOM/1AohHA43PXc2bYN0zSh6zrK5TJzvvE8j3K5vCzaQi43LObaE2BlIdCdAN0i0J0AvcCP+tOVO+Hqq6/Gd77zHbzzne/E2NgYq4EcGxvDX/3VX+G73/0urr76aq/HGsABy7Jw8OBBdrAmJnq/QZIk1ov7zNEku37PkfkWf9UYxPber+ibW9D3N3JS+TUbwgmn7iwWIpEI0uk0q183TROKoiCbzUJRlCWZUzLiBUEIIvEtYFkWi4w30h0y4NvNH5FRUtS+U+cMZbwkk0mmR1Rjn8vloGmaK52mnq6qqrKIvqZpLFskk8kgnU4jFAoFkXiPsRRrT4CVgUB3AnSLQHcC9AK/6k9XYYtPfepTmJ2dxRe/+EXs3buXRSHpoHfllVfiU5/6lKcDDdAcfmWiByqHezqoX7RtDW7eXzHcj84VcSKvYW2ys0yNSCQCWZah6/qSeMNs24amaQui7wRRFJkx2q/o8XKGIAiIRqOIRqOMxV/XdaiqClVVayLzi6HLPM8jkUhAUZQgEt8Ezh7wjeCsg6f2kf0G6REZ8eVyGYVCYUFUnp5BirLTD30G6ZrzvQRi6lcUBfF43FcdMAIECBAgQIAAqxddnVRDoRC+9a1v4f3vfz9++ctf4vDhwwCALVu24PLLL8eePXs8HWSA1nAy0S/WAdotOI5DKpUCx3E475TayPS9h7N44RnrO/o8URSZkbwUBnyr6DtQ60wJDMHWqDfmyRAjwrvFMuYDI741nGnljaLsxE6/mAY8geM4hEIhlp5P5Hj5fB48z7NOGCSDKIqIxWIsE6DdZ5MRTzoZGPEBAgQIECBAgKVGT6fUs846C2eddZZXYwnQIcio8TMTPVA1avdszEDgOZhWxQi478h8xwY8UDlEq6oK0zTbHsK9BEXfiXegEZx1wn42Av2WqUGEh5Re7zTmKXU6Eon0bdz1RrzTUKuPzDbKrKBr3fS89zssywLHceB5vmGWD8dxbevgFwM8zyMSidRE5XVdhyiKiEQiXTmCyIjnOC4w4j2C39aeAMsHge4E6BaB7gToBX7Un5V10lxFEAQBu3btqnntVwOeEA0JOG0khYcmcwCAew53XgcPVDJAiL2aImSLAWLAb0XQ6HdnCrBQd/wGpzFPrODUcowM+X44bsiIV1UVuq4viDa3e00Ih8OIRqO+XPC7gZPArpnuiKLomxaPQCV7w6tsAI7jEIvFAASR+F7h97UngH8R6E6AbhHoToBe4Ff9cWXAU/sgqi90Q0BEKZUB+gPbtqGqKosO+ZmJ3onztgwwA/7ByRzKhoWQ2HlULBKJQNO0RTOUKPpO9bKt4IdoZCvU646fwfM8YrEYIpEIM+KpA0A/DHkitusGtm2jVCqhWCxC13XEYjHfsZZ2A2fUvZnuiKLIGN0XMytmsVBvxLcqownQHMtp7QngLwS6E6BbBLoToBf4VX9cGfAf+chHWJqk83WApYNlWZiYmMDOnTtZxHI5HKDP2ZzBN26t/L9sWHj0uIyzN2U6/hxqKVculxflIE19qukQ3wqCIPiayK5ed5YDeJ5npGVkJBMPQjQa9YUc5FgKhUJQVRWKorAWen6Lxtu2zZjl28GyLLb2N9MdZ+mIH+aiH3Cm0xcKBQAIjPgOsRzXngD+QKA7AbpFoDsBeoFf9ceVAf+xj32s5esASw8neZqfFKwe524eqHl9z+H5rgx4Z0u5xThEa5oGURRdpeWSsRMQ2XkPMpLD4TCLyOdyOUiShGg06ov7TVH8UqmEQqGAXC6HWCzmi7RrZ5YAUGFab5UlQO0p260pPM9DEAQYhuELOfsJcuJ5acRTK1Y/6G+AAAECBAgQwN/wV1goQNegsga/9Smsx8aBKNYmqwf8e7voB08Ih8OM8Kyf0HUdhmG4Pqg7nSkB+gMy5NPpNOLxOEzThCzLyOfzviklCYfDSKfTkCQJqqoin88vqU5Qz/RCocBKQRRFaVm73q6FnBN+Lx3xElTWQVwc3cCyLMaYn81mIcsySqWSxyMNECBAgAABAqw0uHL3f/Ob3+zqw1//+td39XcB2oPaJ1GKNtXB+/0AzXEczt08gF8/MgUAuLdLIjsArBUU1ab3C5qmsZ7RbuCcCz9GI+t1ZzmDGOpDoRDK5TI0TUM+n4coiohGo0veVpGI8ahPuSzLiEajCIfDi3b/TdNEoVBgjOypVIrVrBeLRRQKBVYeUj8mZws5oLXuUHtHy7J8VzLQD3QTiadWd85+9NTazjAMqKq6IjsZEFbS2hNgcRHoToBuEehOgF7gV/3h7GZUyg50cxjjOG5ZRiBlWUY6nUYul0MqlVrq4XQERVFgWZbvx/2fNx/AP/3yMfb61g9dig2ZaFefpWkaCoUC0ul0X0oHDMOALMuIx+MdGePLZS5WGmzbhq7r0DSN9SWPxWK+KCuxLIvV7ouiiHg83tdxEYO/pmmMDLCRE6pUKkFVVYiiiEQiUbPe0/M1MDDQdvMyTRO5XA6JRGJFkPe5BUXho9EootGF6xgZ7eVymTlYiSWfSGGBiu7m83m2bqwGJ0iAAAECBAgQoAq3dqgrN//Bgwc9G1gAb2DbNnK5HNLpNDtYi6KIYrHoW/I0wrlbMjWv7z0y37UBT2R2pVLJFcFcpyDjp1ODhLoC+HEuGunOSgF5SikiT1Fv6g++lPLyPM9qzlVVRS6XY8R8Xo6rvs693XeEw2EIgoB8Pg9ZlpFIJGpI6wRBYH/bSncEQQDP8zAMY1UZ8JS54LzfVNpTb7TH4/Gm/eiJJE+WZaiqikQiseKez27WHsoUoSwRPzjjAiw+VvK+FaC/CHQnQC/wq/64MuC3bNnS73EE6BCWZWFqagrJZJIdaHied006tZQ4fUMaIYFH2azU1957OIuXnLWhq8+iFOpSqYRoNOrpw2WaJsrlcsPU4nagFGU/zkUj3VmJCIVCkCQJxWKRdSyIxWJLnlYvSRLS6XTNuOLxuCdp0+VymRk7nfSjp9R6RVGQz+eZo8HZQg5orzurqQ7eCYq803xS9lk7o70egiAgkUggn8+jWCz2xSm5lOh07bEsC4qiwDAM8DzfN6dXAP9jtexbAbxHoDsBeoFf9WdlFtqtUiwXJvqIJOD00RTuO5IFANzTA5EdUIkgapqGcrnsac05Rd+7+czV0FZrOYD6d4dCIRQKBeTzeV+0dnOOS1VVyLLMuBOI0d35b7uxNqtz7wSCIDAjXlEUxGKxjpnRRVFEoVDwZeZJv0EORCK8dGu014M6KhSLRYii6ItsBtu2YRgGDMOAruuwLKtpSYZXME0T+Xwetm0jmUyyDDM/OeMCBAgQIECApUDXBvzU1BS+9rWv4d5770Uul1vAfs5xHK6//vqeBxjAPSjVdTlwD5y3eYAZ8I8ey0HTTUSk7gxdQRAgSRI0TfPMgCeG6G6j+mR0LYe5WA0QRRHJZJKl1VMkbzHJ5JqNK5VKQdd1mKYJ0zRhWRYMw6hZUzmOa2rYa5qGUqnECPN6Mao4jkMikWDkdqVSCZlMpiN5gNXbQtGrlpaUhk+kdovtBLRtm5UBkOFOThlRFCEIAhRF6ZszTNd1KIrCWjKS/H50xgVwD0pF7bfzJ0CAAAFWOro6YT344IN49rOfjWKxiN27d+Ohhx7Caaedhmw2i8nJSWzfvh2bNm3yeqwBHKB6yUZ1qMvBaDx3ywDwxwq3gm7aeGgyhwtOGez688LhMEu19MJw0DSNped3C7/ORTPdWemg+aS0+kKhwCJ5S2lsUt1+PciIoh7h9P9yubzAuKc+817MKX0ex3FQVRXFYhHhcJi1qmylO+RE9Oo57AWGYaBYLDJOhOWm71QPrygKUqlUX8dPukYR9nqDnTIKnHwIpVKJcUxQmUA7uFl7iDiRyg/qjfNGzjgyCJfbHK82UPZGuVzu2IBfrftWgN4R6E6AXuBX/enqhPWhD30IiUQC999/P2KxGNauXYsvfOELuPTSS/HDH/4Qb3/72/Gd73zH67EGcIDn+YZOkuXQSg4AztsyUPP63sPzPRnwlK6qaRoSiURPY6PoOxkt3UIUxa57RPcTzXRntcBJJkcGSCf14osFMp4agfgVyFDuR4SWUrkB1JDbtdIdv7SzpPRroGI0FItFRCKRGtZ3v4OyIZykdl6CDCky2qkhDRnsoihCFMWW5IeiKEJVVeTzeUQikbYZS63WHtu2WdZHu89yOuMKhQJUVUWpVOp7Z4cAvaFcLgMAa6HYCVb7vhWgewS6E6AX+FV/ujrJ3HLLLXjb296GzZs3s8MQRYRe/epX4+qrr8b73/9+70YZYAEsy8Ls7OyC0gWK+rroDrikWJeKYNTBPH9PD/3ggcqBLhKJLIhOdoNSqQSg93RYQRBYZMtPaKY7qw2SJCGVSiEWi6FcLkOWZTb3fgel1FMWQT9ABHYU/c3n89A0ra3uiKLY1QHdK1iWhXw+D57nkU6nGR8ARWsLhcKy0X1BEBCPx1Eulz1zBtq2DU3Tau5FJBJBMpnEwMAAUqkUotEoJElqG3EQBAHJZBLRaBSapkGW5ZbrXbO1h8jqqJuIW+JQKhtJJpMsPZs4GAL4C+QwkiSpq30x2LcCdItAdwL0Ar/qT1cGvGVZWLduHQAgk8lAEATMzc2x35955pm45557vBlhgIawbRuzs7MLDioUffCbojXCuY4o/L1Hsj0fuiiFshcjjA63vUbfgdp6YD+hme6sRpDjJ51O10QT/TZnjUBpzv0yli3LYlkAqVQKkiRBURTMzMy0vD/UgWEp7iEZ7wBYT3vqb5/JZFjHimw2y0pu/I5QKIRIJMJICnsBZSYUCgWEQiFkMpmODPZG4DgO0WiU9auVZbmps6HR2kNjMgwDyWSyK8cpOePIkZDL5frm2ArQHUh3qbNCp7oc7FsBukWgOwF6gV/1pysLZevWraw3PM/z2Lp1K6677jr2+1tvvbUj4qMA3sHJRO93nLs5w/4/q5QwMV/s6fOoX3upVOr6QdM0DbZte0KGRzXDy2EuVjsokpdIJGCaJnK5HIrFou8WbCfoAEyGvNdwdrOgGrBIJMIindlslrU7o9pWoOq4Wmzj2LZtKIoCy7IatnvheR6xWAzpdBqxWAyGYUCWZeTz+SXNGHADMrAVRalZT2zbRjabbToXzvcVi0VGOJtMJhvWl/cCcvQ4SebaOZJ1XYcsywDAnETdghwJ6XSakeyRPgRYepTLZVbus9RZOgECBAiw3NFVDfzzn/98/PCHP8Q//dM/AQDe/va3473vfS8OHDgA27Zx44034r3vfa+nAw1Qi2aHdjdGo19aPNXXwd9zeB6bBnvre0wRNl3Xm5Lk1JOCOf8l492rOsrV2hd7ucKvveMbgVrGUQ2z16zOlmXVGHhkIJExT1F2cnoBYBFvy7KgadqiEYuR8W6aZtterTzPIxKJIBwOs9T0fD4PQRBYnbwf1kcn6J5TPXwymWTZRpQC32wugGpZUDQa7bqzRifjlCQJqqoil8sxvol6lEolqKralKyuW1BaP5Hs+aXjxGqGZVnQdZ1F36lrjF/OIgECBAiw3ODagJ+fn8fAQMXg+ru/+ztceeWV0HUdkiTh3e9+N1RVxY9//GMIgoAPf/jD+Nu//du+DToAUCgUEIlEkMvlmFebfpq1L7MsC6qqsoM/ecJFUWSG/2LiKSMpRCQeml6JkNx7ZB5/cs5oT59J8miaxvgA6g31egZvum/EsuylsSYIgu9SOTmOQzqdDg5OTeBkdaeUer+1qyK9TiQSKBQKMAyjLwZ8/bPAcRxLRad74STUczKZl0olFsWn57Lb3uitYNs2W9eoX7gbEBFaOByGruvQNI2x7jv7uHfynBQKBeZE8BqUJSLLMgqFAmKxGOsQQIYRADYHhmEwUjiSlThCnOt/P9aBUCjESlIURWFj5DgOqVSKOcec171GfccJagsatC9bfNAeSPfeWWbTyfMa7FsBukGgOwF6gV/1h7Nd5l6Gw2G86EUvwtVXX42XvvSlnvXb9htkWUY6nUYul2M1fX6Es07Xtm32Q1EZ27ZZJEoQBBYRo4McGQD0GWTI0kF7sXoPX/Hl23DnwQp/wukbUvjFO5/R82dSZIfg7KHdqI92P1Eul6EoCjKZjG+MvwDuYds2a1cFwDftqqjV1sDAAFRVhWmaSKfTnn2+bduYn59HPB7vaq3XNA2KoiAej9cY907niFcoFArQNK3rsTphGAY0TatxujVaO+j/9ZBlGZZl9bWEjNY3yr6glHEndF2HqqosOh8Khdh6T3Nh2zYEQeh7izpN01AsFlkJg6ZpLBrbK1GoW5AzwzAMSJKEWCwWsNWjypXT771JlmVwHMccW1T6Qd0GAgQIECBABW7tUNcR+Fe96lW49tprce211yKZTOKVr3wlrr76alx66aVLfphdjeA4DrlcbkGkh4in6CBq2zY7sAmCgFgsxtilKfpCETTTNGvYjumw6jTqvd7oz908wAz4x6fyUEsG4uHe+keTgUVOiaU0nJ2cBH4x4C3LwvT0NNatW+ebMfkVzdpVLXXveMp+4jgOkiSxyKpX89nsYO9Wdyh6LQgCO6BblsXuIZUm9GpEFYtFaJrmmVOACO9oPXRm79R3uGjkGHTyEhCcDlbn6/rfUYlCuzmkjIFsNotUKlVzDy3LQrFYRKlUYv3S6fdOfaV9gQjt4vF4j3euOSibQVVVyLIMWZaxYcOGRQ0CUH2+s3d8JBJBJBJZtWsglRg4O030A+TAc+oYkWN2Ul4W7FsBukWgOwF6gV/1x/UJ9Dvf+Q6KxSJ++tOf4rvf/S6+853v4L//+7+xbt06XHnllbjqqqtw3nnn9XOsARyIRqMoFovYuHEjOI6rSRGnukgAjNCIDPByucyiS3RopB/nYZR+T6mw9e+jf+t/nL93A2cdvGnZeGAii6dtH+7p3nAc55s0SboXFPnxA4iEbO3atUs9lGUDSl/WdZ31jnfT97ofoOeS0qadpHFe6T1l5tRvVm51RxCEBVwcdA/JiJJluafaZIrsRqNRzyO5tJ7VP7NOZ6fTuKd1ljI1bNtuudE711N6TZ9J9e2tQA4DwzCY44buq23bzKHRqo86RaILhQIkSerrmkl16cViESdOnFgy5xdxXGiaBk3TmDPOD1k1iwXSU2rpput6Xzg0CHTeqH+WqLzBbR18sG8F6BaB7gToBX7Vn4520Wg0iiuvvBJXXnkl5ufn8YMf/ADf/e538fnPfx6f//znsXPnTvzZn/0ZrrrqKmzbtq1fYw5QBzps0qGImNgBIB6P10TA6NBJ9Wf0f/qdswVb/cbqNPgB1PzOeSBtZNw7DXvn/8/eVJv2e9+R3g14P4GyAAIm+pUBaldFxiNFkhfTYURRXjoQO6O/Xo3Di9RaYpquT5Gl+miqTaZ72IlRR8ZqOBxe1BRcJ2dGPah0ybIsRuTmXCsbrZtO6LqOfD4PVVURj8ebvo9aXSaTSRiGAUVRmAHfKSFcJBJh6fbEhdIv+MVApkyHUCiEYrHom6yaxQDNtW3brORElmUUi8W+GvCNuC+oDt5Pzu0AAQIEWC7oercaGBjA2972NrztbW/D5OQkvvvd7+J//ud/8JGPfAQf/ehHcdFFF+HWW2/1cqwBXIC867quIx6PtyVeoANn/b/NmNqdKZ+NUkHpu+g6HVydv6s39jdmwpjIVhwHt+8/gdedt5ZlA9S/t9GPV7jh8Wl84uePYUMmgn9/7TkYSniT4hm0zFlZcBoAhUIBiqIsKsmdrusLjEhJkjztdmBZFnv+ukWrCBvP88yAoNRqtxkNuq7X3HO/gKLuztKjTiBJEhKJBBRFYWzujUCOglgsxvre0/u7SUt3stsnEom+GdrksPFLa0ZBEBZk1fiNrNIrUBtBTdMgiiLi8ThbP6LRKGul6LUhTZwLjXSZ1pfAgA8QIECAzuGJu3l0dBTvf//78cIXvhAf+chHcM011+COO+7w4qMDNAHHcRgeHq45bJXLZaiqyoiinBGgVp/jtg61kfFeb/A3+3G+l6L/dO20tRFmwD8wIWNmZmbBuOujV/XOADpwNYv2A9V6dKfMdOAen1Hx9m/fi5Jh4eCsio9d+zA+/5qzG46h0etW91kQBGia5mmNci9opDsBOkezdlX9JuZqFGkXRZGVznihY804GzrRHTdM01SbTBkNVBrQ7EBPEWeKNPtJh4lnhJyd3SAUCiEej0NVVfA8vyC7gAwxanVJJHS9EHKSMyWfz0PTtL5kNJRKJRSLRcYx4BcjHqhm1dAYnfXxftKvbmGaJlRVhWEYbH1yykXdV4rFoueGdLlcblrSRmUcjbJ0GiHYtwJ0i0B3AvQCv+pPzwb8kSNHWPT94Ycfhm3beNrTnoarr77ai/EFaAKe5zE8XEk1t22btcmhg22xWPS8/7gXEe/6qL1lWbhkt4nf7ssBAOSSiYKYxJaByAKjv9G/QLU0oFFmAH1PK5QNC++45jBKRvXQ/bMHp/Cynfuwc031YFEvf32PbPq3/rplWYx7wOksaXY/W113/tvod27eC1RSZwuFguv57GXevV70lnIRbXU/S6USS0ftV208kalJklRT7kJEUaqqukqFdfYKb4RyuQxRFBu2QEylUq7WFjLeyeBsBSK7KxaLmJ+fRygUWkAwZpomFEWBIAgIhUK+y2ohrhDiIenWoCaCL+o04pzPUqkEwzAQiURq5qZbh4ETgiBAURT2f69AeilJEiKRCAzDYK36/AQieS2VSlAUBYVCAaFQqGXXAb+jVCqx7jNUItDo2RVFEYVCgTlZvAJ1H2i1XpRKJWbot0M6na7pnhMggFsEuhMAWMjF4QZOe8tP6Gqlnp2dZfXvt912G2zbxqmnnoqPf/zjuPrqq3HKKad4PMwA9bAsC5OTk1i3bh2KxSIsy6ppyyMIAku19JPXqN44FQQBF2xdA+Bxdu3xmRLO3NI9WYTTQeD8l4z8ekfAv/xmPw7MlRZ8zjfuPYnPvWJXjaOAPq+Rk6D++5y/I5IrQRAWzEknBnmz93cKRVG6TpdtVZvb6vetPqudk6Vbmal8o1t0+r1knFDv+HaH/k4/X9d1dhiu/9tSqdS0Dr5RpgwAFsmtBxGb1RvJRObiticqHc7dpnbTOPP5PLuH9MyQIRKJRJDP52v+zg9rHN0zcrL06kA1DANzc3MIh8Msm6FYLEIQBEaW5yVs22bz5VX0mdqXUoaAqqqYm5tDNBrtW/93L8BxHIvIO51djbhd/AAqq6H1huaSsl+o3r8ZqF3m/Py8Z44V6ogQDoebOphIP4D2TiNqPZfJZHxz3wMsDwS6E4AwODjY8d+QvTU6OuorR65rA15VVfzkJz/Bd7/7XVx//fXQdR0jIyN497vfjauvvhrnnntuP8cZoA6WZUFRFESjUZYC6NwAne3L/E7Ms3t9EvGQALVc8YzeeySLV5+/qevPaxZ9bnRA+P3jJ/Cdu481/JxbDuZwSBVx4dbGD3yzdlCNrlFdazQabVqKANQ6CZp9Z/3/650ijQiznP9aloVsNot0Ol2zGLn5znbv7eV3Xvze7Xu6ea8bCILAouNkTHv5/Nm2zTpK1EMURZb67iSpdBrsQMUQob7gFM2nlpL0Hc4ylHpQ5N/NQYhSyjvJ9CBCTmfWCrXH9GtacyPOj15BjhhnZB/oLoLgBhxXaZlI7Oy9tnkjpwtQlYWykagVnp/3JueYnZwwTqdWI6OesFh6ahgGyuUyeJ5HJBKpIaN1rj/tdJKeOa9anpIDi9ajRqDnxTCMtt9J+uSn8guvQWcBPxkJKwHkKPJLGWOA5QXbthn5p5/gevdcu3YtNE1DIpHAVVddxXrABw/D4sOpTJFIpGEt6HIy4AWew9mbM7hl/0kAwL2H5xfle2fyJbz/Rw/UXPvbF52KT/7qcdBz+ulfP44f/p+LO0p1b4RGvXBbod4J0Oz/bh0I9JoOoUAl7dvLVNmViG6dAfSM6rrOmNJb6Yqb76GMkUY9zylVXVEU9llEpOb8cRrldCCmnuFE3kUHb2cPcXq/ZVmYmZlhddftQKzXiUSio72CZNB1nUVBk8mkL/cb27ZZOzdqk2cYBhKJhCefraoqM6qSyWTfWfcjkQiLnHbrLKBxh8PhmrknngBaB724R4sNco45f5xZXeTEc8NB0yuovj0ajdaUkcViMUSj0Y6fOSpR8YIckjJonJ9FJLuSJLE1jJxT7fZGegai0eiK3beos0k4HPZNK9yVANKdWCy2YnVntcBvRvRSwrVl99znPhdXX301Xvayl/mudm21gUjYKKLb6JBA0QAvaiMXA+dtHmAG/L4TeciajlSkf8y0lmXjfT98ALNKtY70jZdsxVufuR2PHc/jJ/dNAgDuPjyPGx4/gcuesq6n7+u0pKFdDXsvME0TMzMznn/uSkQ35QAEImcrFArMuOvFAKUoFdWG67rOIv1OR40oiiwzp933UV2yqqo17csoSl8vE0XTm2UB1IMIHOm7OgUZAH6PSlFmBBlNbur+3SIUCuHkyZMol8vIZDJ9P9hT/3giF+v0wEuGIM/zSKfTNQ5k0p9EIoFCocAcS8sd5KAl1nVd11ndeb8MBtu2mZGcSCQwOzsLTdMwNDTUdaYKx3HMwO5l3IZhMJZ/p76SXpCzgc4vxN3R6hkn3Vmpjmdax4knIhKJBEa8R1jpuhNgdcL1ieiaa67BFVdcERjvPkE8HsfIyEjLDU8QBM+J7PqFc7YMsP/bNnD/kWxfv+8btx7CTfuqRuxTRlL44OW7AQB//dxdkITq4eczv3kCltWb148OqV4SqFCKcaceSZ7n2+pOAG8QiUSQTCZhWRZkWe6JeE3XdWa4y7LMSM7oOwYGBpBMJiGKIsLhsOv5pRIcURQZeRfQ2HnB8zzWr1/v+rOppVq36xDV5jYi0/MTnG33KIXeK+cpOWxFUWR8J/0G1ad3kzZIbUwTicQC45z0h4w1cu4sd5DDKxqNIplMIplMwjRNyLJcQzbpJTRNY7XvsiwjHA6z81m3jl9y3vQ6ZiKlczrt6DmOx+OMsNJJmtdujeh07VlOsCwLqqqyrhySJLHOAQF6x0rWnQD9h1/1x1+jCeAaHMe1JeSg2tHlgHM3DdS8vvdI/9LoHz0m419+VSXNC4s8/v21ZyMsVjyzm4diuPLCzez3j0/l8bMHG9fJu4WzpMELECO3oiiYn59nLaDcfL4b3QngHchA5nmezVOnoFRZZ6vIRCKBVCpVE21vxjLdDpSeHYvFoGkaq4WtRze60+2YyuUy609eKBRqygP8BmfdsJNIzAuQMTUwMMC4T/p9H6i1nGEYHelrsVhEqVRCPB5vmHFB+kOZJOVyednsUZ2AnvlQKARVVZmzzSuUy2Xk83kYhsHaw2UyGcRiMRSLxa6/i6KU1JKyl/E5M3jqU+ej0Sgz4skJ0c65uZL3rUKhANu2WQZUIpGAIAie681qxUrWnQD9h1/1JzDglyksy8KBAwdabrJEIOXXQ68T6ZiEHWur9ZD39KkOvlg28c7v3YeyWb1vH37Jadi5LlnzvndcugNRqZpq9W+/3Yey0f2BhqJoXnnUidgqlUqxFGPqRZ7L5VAoFGAYRsO5d6M7AbwFGcjUvs+tEUaG+/z8PGMHT6VSSCaTDVPcJUmq4TnoFNT/GkDD6GE3ukPkem7XIcowoHTbVCqFeDzOjBY/6i3VdgNVA96LcRLzPNXEUmR3MYx4Kq9w25KUWNuj0WjT8gGn/pD+9itCvdQgJ0gikfA0Gl8ulzE7Owtd1xGLxZBOp1kqOpVw9NKlgOau27ESH4Qz/Zt4LJz18NSTnkqM2unYSt23KDPBWW5BRjw5fVeazIuNlao7ARYHftWfwIBfpqDU0laHOK+jvv3GuZsz7P/3H8n2nLbeCP/4i0ex/4TCXj/vtHW4+qLNC963NhnBG59+Cnt9ZK6A7999tKfvJodKr6A+05FIBKIoMqMuk8kwzz31JM9msyxy6ySz6yb1PkBvoJrYRCLBjNRm+qDrOvL5PHK5HGNij8fjLEW+Geh3vaTqk2OI0jidadTd6I7bFFnDMFj7OCKtc5YEpFIpZgj5aU0jhvJ+GPDEm0FOFVEUmf70o5VcPaLRKCutaDXnRFZIaeTN4NQfSjtf6czioVCo5nnq1iAzTRP5fB4nT1a4YoaHhxcQO5IRr+t612sAz/OsG0E380KM+M61qFQqNeRToNa3uq4zlvBmWIn7FqXOh0KhBU4vnucZySOtiQG6w0rUnQCLB7/qT2DAL1M4Sauawcn8uxxwnqMOPl8yMOYwtL3Abx+ZwnfuOMJer0uF8ak/PatpWsxbn7kd6Wg1DfTfrx9Dodx9BL3TSGQzUGSk0YZPhEaZTAbJZBLhcBiGYdSk2pNREGBpQAd6oBLlpvpu2670bs7lcuyQH4/HWcpxo4h7PYhgrpeaczJGE4kEi3zLstz1ZxLzfbO/p4gyGebxeJwZPE5QfSjHcW35BGzbdmUUeAH6fGf0zNn2rVs4o+9Ow0eSJMTjcZRKpb4b8RzHIR6PM1b5RqD5o3F1gkgkwg5HKxlkjFE0PpfLuY5wU/o5/Y0gCBgaGmqa5RAOhyGKIkvL7gY0L51G4Wkuaa2izDBywjVCLBZDLBZDuVyGoni75/sd9Ew1Y/2ndXixSmcCBAiwfBAY8MsQxPLbDsREv1wM+HM3968Ofiqn4QM/fpC95jjgs1ecjcF4c5bXdFTC25+9nb2eyZfwjVsPdT0GQRB6JreifqbtSMqIQIhSLNPpNDsk0MFOUZSeIrUBuocgCMxIJS6DXC4HVVVZun06nUY4HGYp8W4ZiSVJYozGnYJ0k3SLIt9A45R6598R+zalUlO0ker+5+fna8o7TNNkhokzHZiItBpBEAQWla/nEyCjXVVVZLNZ5PN5FAqFvteR0mc7jWwvOoDUR9+dCIfDjK+g30Rw1FasXC43LKnI5/MsXbzTGkFqubZS0+jr0Uk03unQK5VKzJHTqI1kPWKxWE0/+E7RbRS+Pn2e6vHb6UYikUA4HIaqqiuG2LAdqINIPB5vuZc7s26aOdECBAiw+rD8+7esQnAch1AohEwmU5O62QhepW0vBravSSAVESFrlUjdPYfna8jkuoVl2XjPD+5HtlA1Vt/6zG24ZMdw2799w8Wn4Ou3HMS0XDkIfenGcVx94RakY523xKJ5ohY73aDVob7ddwuCgEgkwvp9UxsiQRAQDodbGk4BvAfVOZIRRjXH9bpBTha37bZEUWS9qjtt0dXIGCVnQ6FQQLFYxODgIHMCUfp4I9nIgUh6R2RVpVKJEXCRQdJJiz2KZhYKBVYeQuz8lJpNNeMcx7Hofjwe70tbJiKwcz47vRrwzaLvTkQiERadJdZvGofzp9G1Tp/zcDjM0vaphSCtHwBczx/P89i4cWPNe8PhMBRFgWEYvmopR8+k122nSH+JkDKXyy0wyulek+MuGo0y8ko3Pdop4l0sFtu2Z2sGIrOjvuRu4EyfJwJEt60IE4lETReM+j2uke4sV5Dz0m2/d8puIUJPNzoQoIqVpDsBFh9+1R//7JYBOkI0GoVhGFBVFel0umXEarmkJ/I8h3M2D7D2bl5F4L/yhwO4dfwke33maBrvfd5uV38bDQl452U78Xc/eRgAIGsGvnTzOD74wlM7HkevGREUkemkRVgjCIKAwcFBAGBpxmSckSEf9EpdPDiJ4xpB13WIothwzhsZj6IospT1Tg0iMjrr1xNKpZYkiUXI6KBOek3jqB8PUIk8UqSW/pYItwzDQDabhSRJ7KeV/tm2DcMwWJp6LpeDJEmsZKRe5mQyCVVVoSgKY7/2Ek4COwLJ1S3cOuqc95CcNuRYaRU5JSNeFEXXkfNYLMa6AiSTSSiKAsuykEwmXa0XlmWhWCwCqBjH5NwRBIG1lKOa36UGlScQiWI/Dm6hUIiluquqCl3XEYlEWBcIKhkRRRGapsEwjAU1760QjUZRLpdRKBS6uq+CICAUCjEnQDsdofT5cDjMSi7IeecGoihCFEWEQiEUCgVwHFfjOCCH53IH3RviK3ALuq+kl0FLZ/dYKboTYGngV/3xlzshgGtYloXjx48zT24zUCu55VI75ayDPzCjYl7tzfnw4EQW//qbJ9jrWEjAF157NkKie9W/4vxNOGWo6vH++i0HcULuLs2vl/pkau3T68Ztmib27dsH0zSZ4UOpy84abD+Sdqw2UFp4oyiNbduQZZkZRQQyzLopj3D2M28EQRAwNTXFouZEQkXGSLO/JaOaopqDg4MYHBxEJpNZUN7RrJOCYRjs96Sf8XgcQ0NDCIVCzKivB0U8qW2V17WkzQz4biPwbqLvBGr7lUgkkEwmkUqlkE6nMTAwgMHBQQwMDCCdTrPOBcRrQHPWjkyxXqZEIgHDMBgHQaNe741Af6NpGiYmJpjRKssyI2rM5XKQZRmFQoGlFy/F3uVsedaq9t8LOGvjaS4Mw2A8ELRfFAoFRCKRhq35Wn02GfHd7jmU5eFmLaEMGCImdJM67wTJRtkDqqrWlAA4963ljFKpxOa4kTOm1bpBzl7KugngDitFdwIsDfyqP0EEfhnDtm1Eo1F2KG50yHcy0fspPbEZ6uvg7zs6j0tPXdfVZ6klA+/63v0wHGz2H3vZ6di2pjNPmiTweM/zd+Od/3MfAEDTLfz7DWP4xz85s+Mxuel32wi2bUPTNIRCIU+i4/WHBEplpgOfpmlQFIWl13ebhhmgN5CuNDq404GZWhA5QVE727Y7Spd29jNvhm4MU47jkEwmmXPBCWd5B9XSU721pmk1pHBUPkQOA5ItFApBURTk8/mGRiVFuwRBYIYjdWzoBU7Sv3qmWiIa7TRdvdMymVbfQe0rm4HS1+l+tDMQRVFkjpBmvd7rQanixO0wMzPDeupSCQYZ+OSkbLQ+dVveQ3/XqIyg0Q+1PIvH44wAVNO0vkY8SZ8Nw4AkSWzMzkh2N5kj5JQtFArs+esEoihCkiQWhW8F6pZB95C6pLiFk4CTol3kPKFIvN/aOHWKds6YYrGIYrGIdDrd9LmNRqOM1C6ZTHbk1FnNWO66E2Bp4Uf98b9FF6AlwuEw6xXdKM12uRnwezalwXMA2dz3Hs52bcD/w88ewcHZavTkxWeO4NXnbezqs15y5gi+dOM4Hj0uAwC+d+dRvOUZ27BlqDPWZTroN4ratUK5XPYk+t4OlLZINa90+KMDHNXJk3Hi/AHQ8Dr9jqJ+AdxD13WWalwPqnsmA8j5fNOhu9Pn3rKsvq0Tbg6axLYfCoVYqrwzsuc02p2gdGMy4pvVu5MjqhOjtRUoZV3XdRSLRbbJm6bJrkUiEdfOr06i7/T+XC6HcDjclYFHpIBEqEYZFa0QjUZdORJJFnI8xuPxmkMQlV0AYPNdLpeRTqcBgJE30r+9ROKdZQX165ITpmmy8WazWYRCIUiShEKhwAj3+gXSfSfoGW5VJtcKVDNPGStua9mdiEQiyOfz0HW9qfz0DFDknKL/nYJKdJw948mIXw7nl1Zo54whglqg4sRrVudO5UxkxKdSqaDkLUCAVYjlvSI2wc9//nO8973vhWVZ+OAHP4g3v/nNSz2kviIejzMG62QyWfM7qkn1W+pHMyQjEnatS+LxqQo50j2Hu6uD//mDx/CDuyfY6w3pCP75FWd2HcXheQ7vf+Fu/MXX7wIAGJaNz/5uH77w2nM6+hw6hHRqwFOWxWIeYqgemfrOa5rWktW4niDLSaLlPBgHcI9mh2bLslAul1kGDtXJEygSVn+9HdqRYi4mqJOCW6OJIrzt6t3J2O/EaG0E6uFcLBaZ4ysSiYDjOGiahmw2y1i1ySnhjK42QqfRd4pYk4Otm7mjNO5iscjKFtqlPrf7Hro3uq675h2oJ02jzIx+wmnME6N+LBZjrfNKpRJM02Q92Kml42KAMqFisVhP94Gy86gsoNPx075TLBabPovkZCOnWzfRfmAhAafTiPeau2KxQc4YaoXZ6PdAxZlVKpUQjUZbZtYkEgnW5aNfPA0BAgTwL1acAW8YBt7znvfg97//PdLpNM477zy84hWvwNDQ0FIPzVPwPI+tW7cywqh4PN401W85MdEDlTp4MuAfmMjCMC2IgvvNaWK+gL/534fYa54DPv/ac7pijnfi2bvW4MJTBnHnoTkAwLUPHMPbnrkdp21Iuf6MbojsyuUyTNP0jHnWqTtu3x+NRhGJRFgtZSNG61YHNurx6zemaT+DWjI1S58HKoc90zSZMU9w1sG7PfhShLKVXnSqO4sNWgsFQWAR8Ua9ybsxWgmmaaJYLKJcLjMHS71hR7WqdO9J/8nQp2wCJ7qJvpdKJUiSxLhQ6h24bkGRWkEQUCgUeioxIGPXtu0FKb6t9Ici3NQmczHgXL/omUqn02xuIpEIKwEgPRkcHOz7Guasw/ci6yoWiyGXyzGHQKeIRCIt12/KBiLnS7eZCvTZ9D1O1v1CoYDR0VHfrj2t4GTkb3T/DMNgUXdJklAul9tmTJDDUpZl5PP5jggOVxv8vm8F8Df8qj/+Go0HuPPOO3H66adjdHQUiUQCl19+OX77298u9bD6AudGQOnN5OV1YrkZ8M46+ELZZMZ8PXTTwoEZBdc/No2v/uEA/u4nD+Gq/7wdL/1/f0Req5L2vOM5O3Dh1sGex8VxHD7wwip7vW0D//rbJ1r8RWMIgtARqZCmaawW0St0cwB1RkSJsMzZoqoVKPKzWvo9ewFd19k9rwexVBNTNEUJnZAkqYYErh3qe8A3g98dMFTvHovFGGFUs/dRtLVcLrfsyQ1U5iOfzyOXy8EwDMYZQW0ZnXA+F9FolBHJhcNhlMtlRt7mXK87jb7rug7TNFlrSF3Xeya2CofDSCaTjCCxU74Oko3jONbvvB6t9IdKwnph8O8GlMVQ38WAHC4DAwPIZDIol8uYnZ2Foih93VNVVWV1+F6AWMuJzb5TUGeIesJMoJoNRBwVvUTKab1z6h09p1TPv9z2ENu2oSgKRFFs+mwXi0XGN0OOLDdykhFP6fQB6Wxz+H3fCuBv+FF/fGfA33zzzXjpS1+KDRs2gOM4/PSnP13wnr179+KUU05BJBLBRRddhDvvvJP97tixYxgdHWWvR0dHMTk5uRhDX1RYloWxsbGaAycdJOsXcmKi9yMJQyM4megB4IbHT+DW/bP49u2H8YmfP4o3fuMuPOdfb8SpH/41Lv23m/Cm/74b//iLx/CdO47g1vGTmHf0ez93cwbvvGynZ2M7/5RBXHbq2pqx3fVkRN4tOnGo6LoOwzA8rX1vpDv9BpGPBcz27kHp7/XOEWKFpnIEMpLqjS1nNMsNGvWAr8dS6E63oOhVO/nD4TBSqRRrSee8j1SXTVEuiuin02lGutfsftUz0VNKcDqdZq3XNE1j7OudRN+BisFPh31nrXavzxeVGAiCgHw+z+pyW4HaWymKAkmSmtblttMfMhTdfKeXoE4uzYxPSlkeHBwEx3GsW0c/DHli4G/GUt4tIpEIy7DoFNTtgJxGTtAeRXXrvY6ZiOycekzfPzMzwzINlwvomWyW4UOZPM6U+XA4DMMwXK3dgiCw7hDL6b4sJpbTvhXAf/Cr/vjOgFdVFXv27MHevXsb/v773/8+3vOe9+CjH/0o7r33XuzZswcveMELcOLEiUUeqf9A6aOU4kmgg5TflK8ZtgzFMBiv1kp/9nf7cNVX78Df//RhfO2PB3HD4ydwcFaFabU+qKYiIj7/mnM6Sr93g/e9YDec+/Cnf/14R4dmURRdO1Q0Tes7edJigfrYBu1v2oOI6dqlzwPVqFX9fXXWwbv9TjfZFMsFTlbrdmhktJJxrSgKACCRSLB2i0Tk2IkB7xyXJElIJBLIZDI1B3u3jjoiyXO+PxaL1RBh9QKK7IXDYdbyrdkaR9E/ShFOJBI9McZTlsJi7VeUueDG+HRmXFBJkZeGPJVCUJmFl6DoOHV56BREAEnlI4qiMH4UcjJ7MWZq31d/P2ltojZqjbIB/AbqpkEdMOpBZTP1XYQ6zVijUgtN05bNOS9AgAC9wXc5AZdffjkuv/zypr//7Gc/i7e85S34i7/4CwDAl770JfziF7/Af/3Xf+FDH/oQNmzYUBNxn5ycxIUXXtj08+pTsmS5wjLuTEklIq76nrTNrlP6ZLPr9RsTHRrqF95m14nJnNh5nWOhNCwna66zxZGzXtmvMgHAuZsyuO7xzpwyAzEJW4fjT/7E8KIzRjCaCcOyLE9l2rU2jpedNYJrHjgOALjr0Dx+//gJPGvXcEuZgGpbJ5oPMtAa3XeqbU4kEg0/p1uZnMzOvc6TcyxuxkjRtXA43FfdMwwD+XyeMWb3U6ZurreTicjJKFvDOXZN09jYnPe1UCgsYK0XBIHVUraTifqot1r3nGzgS71GuJknnucZh4SbeYrH4yySDFScTolEgn0HfQ7xWDhZ5+vHTu9v9ZxRCYQzPY/Wq1a6R1FUyq6ie0xkZYIgsK4kvcyTM2pbHxWme+DMTKD1rN08tZoPuhfE59LP/YnGz/M8I1BrNk80dicreyqVQqlUqmHbj0ajjPyzkzXCNE12/iBnp9fPExEDKoqCgYGBBe9v9zyJoghZlmt0i5jVqfzBi3my7UrbVCohcHYkIEdCoVCAaZo1WRN+WssB1JzDGq0RRECaSqUW6B7xQVBkvp1M1HmkVCox/fFapqVcy3uZJzf71nKTyTn2QKb+y9Ru3/JSJrcBQd8Z8K1QLpdxzz334G/+5m/YNZ7n8dznPhe33XYbAODCCy/Eww8/jMnJSaTTafzqV7/Chz/84aaf+clPfhL/8A//sOD6+Pg460WaTqcxMjKC6elp5HI59p7h4WEMDw9jcnKStToBgPXr1yOTyeDQoUM1nu6NGzcikUhgfHy8Rkm2bt0KURQxNjZWM4adO3fCMAwcPHiwRt5du3ahUChgbm4O+/fvB89X2s9s27YNuVwOx48fh23bOHHiBBKJBDZv3oxCoYATJ04wA96PMqmqiomJCnP8mUM2rlswK0BY5LA5E8auDQNYH+MwIBnYmJIwmpKwce0gRkZGcPz4ceRyOZTnJjE21x+ZXr5dws8fBMwnn7NP/+ZxrMda8E/e30YyAWDzpGlazXzE43Fs2rQJc3NzmJ2dBVBZeGKxGAYHBzE1NeXZPBmGwXRn+/btPc2TU6ZcLoepqSl2vZFMVOMbj8cxMzPTN93TNI2NhSI3/ZIJqDxPAwMDOHHiBFRVZfParUyHDx+GZVnsO0j39u3bxw72U1NTTKbDhw/Dtm3MzMxAEAQm0+TkJHt/OBxuKROlctPG12iNoEMngCVfI9zMk6IomJubw/T0NDiOa7vuHTt2rCbaPDIyAlEUceDAgQUySZKEmZkZzMzMsPl2ykQOFp7nPZXp5MmT7DtPnDhRI1M2m4Vt25iensbatWs9e57i8TjGx8cBVJ+nLVu2IJ/PY3Z2ll1rJ5MsyzX7VrPnKRaLgeM4ZLNZZtQ658lL3Wv0PLWbJ9u2mWFWLpcxPT3NrofDYYyMjEBVVZw8eZJ9TivdS6fTOHr0KDRNA8dxmJ6e7tvzRLodjUZhmqZr3Vu3bh3m5uaQz+fZfIuiyAxq0g8v5onkcu5PlmUx3dm9ezcMw8CRI0dc614na7kXZ6OhoSGIoojp6emaA3mjtXx2drbpPMXjcRiG4UqmcDgMnq+UUbrVPT+c9/o9T5ZlMTkWSybbtiFJEnbs2LHourdc58nPMjn3rX7L5JZ0nbN9XJDKcRx+8pOf4E/+5E8AVOvbb731Vlx88cXsfR/4wAdw00034Y477gAAXHvttXjf+94Hy7LwgQ98AG9961ubfkejCDxNWCqVYuPwmwfJsioptvSZjSIc+XweoVCIMZXats2cEn6Uyen9siwbP7n/GA6eLGBjJootQ1FsHYpjXSrMImtL7en76LWP4tt3HGG//9wVZ+FlezY0lck5FjqUUoShfowUjYnFYiw11iuZaEw8X43ULpZH1rYr5FjE0t0v72Uul0M0GmUGZygUYqm9/fAym6bJuCckSWIGSDcyAcDc3FxNb2+6Tqzp6XSaPQc0T3SwJvZw2670ZiZGcTpoN5Npbm6ORRBbzZ9t2yxa6XfPuWEYyGazrC+8l2sERV2pb3n92EulEgqFAjKZTNPnrBuZKH2YWkfVj51Y0xOJBKvT92KeDMOAqqowTZMRjZG+kAOjnUzN9q36sZA+OzNoup2nZjKZpolsNlvTLaAT3aPWmvF4nGUN0P0n4lFaB5qN3bIq5HmGYUAQBEQiEfZZ/XyeKA2+vqVZI92z7Uqat5OTo1wuI5lM4tixY+A4DqOjow0/p9t5ohKWgYGBmvfX71u0HtJaSevSUkcMy+UyCoUCEonEAvIrmg9FUdgc1K/lBFVVYVlWTYS+lUy2bbNzn7O8xu/nvVYyeTFP9C99bz9lojKYcrkMnudbZros9RnWrUxL/TwttUzOTLpW+5ZXMimKgkwmg1wux+zQRlhWEXi3eNnLXoaXvexlrt4bDocbtupo1IOWFKIenV5vVjPZyXWO41gUoH7jpLEnEgmoqsoYq8vlsu9lqqb+AldcsLnh33Y7Rq9leudlO/Gjeyeg6ZUH/vPX78dL9oxCctTcO2Vyot18UO17/cHSC5lok3HqTrfz5OZ6/Vio33M0Gm04zl7nSVEUdu84rlJvTL3Bm7XG6kUm27ZrvlNRFJRKpRoG6U5kolR2Sl+u/12jNk10+C8UCjUGA6Vn08G3mUy0qVDJTStZyYO81GuEm+uiKLJDvfP3XjxPZMg2+l5nCjtt0l7IRPc/Eoks0AEaI9U5U6mKV/MkSRLS6TTbU4iFvxFaydRo36ofizO63eg7vJCpWCxCFMWG5GJu5iMWi7HDupO0j3SOWs9RCYZzjLZtMyOV4yrcNc1ahvXjeaKobrFYbNh6kGTVdb2GEZ9Kn6hEiUpANE1jwQE3Y28nEzHOO5+xRvsWtT6krCeay172p16vO5+9ZnNKHBbUNq7RPQAqz3I+n2f3wY1MdO8a7a9+Oe+Rw2qx5onWTUEQ+ro/OR35iUQChUKBlUEshu71MvZm15f6eXIzxk6vdyMT6Wyrfavb6/XfWb8fNYPvSOxaYXh4GIIgsHQ1AqWbrSZYloWDBw8u8C45QUQ4tLlZ1vJhol8OWJuK4I2XbGWvD58s4M3/fTe+fNM4bh2fhaw1Jw+jLIpG80HRu0gk4vpB7gRudKef6CeZHfEGOO8dtcairBSvWaOpFpOi3PF4nEVfuwFxVdQv6nRwbkYU1YqNvh2RHelCsw3G+b6l1J1u4Eb+bmBZzQnsgOYRgF5A5G7t+qR7SWjnBGV4ZDKZrtqFdaI/VFPdj7kjpndnhLwbkMFYT/JHmW9OjgBCuVxmLQTD4TAjRlxMcFyF0E7Xdda+0AnbtqGqKuMHoBaIQEWviSU9FAohlUqxvuVegSL9zrlvpjvhcJi1gmxFtrgYsCyLOXNbtQAsFArgeb4taWU3XRnoM/3KSF/f1WMx0O99i7JUqAQtlUohEokgHA4HxIIrAH499yyrCHwoFMJ5552H66+/nqXVW5aF66+/Hu94xzuWdnA+RSwWgyzLbJN2EioF6B1ve+Z2fPv2w5Cf7Dt/074Z3LRvhv3+lKEYztyYwZmjKZwxmsYZo2mkIhJLq2s0HxSVWexD3WKBomtEtuMlmt07am2lKApLKfeC2Z9YhikSBFQdFG4PafXQdR2SJC0wLOiA3GzcFNHQdb1GfopkOqPw9XDTQm65grJdqN7UC9Ba2kp/+2HAl0olFuFtBcoGKRaLCIVCns/rYuwhZLyUSiVPu3BQ2nooFOqZNZ3neSQSCciyjEKhUGO0UWeDfD7PyqHIcSBJEmsluFQgtn9VVZmTn7IDSqUSM/IbOZIjkQi7h5TxoaoqyzrxApIkuW6BSSSdzuhnP5zfrUDp6wBafr+u69B13fUYiZSYsg/agRwsFATw03nPNE2oqsrmVpblJX8OeoVlVYgcqSuIs6QoGo2yMt1uHJ4BArSC7wx4RVGwf/9+9vrgwYO4//77MTg4iM2bN+M973kP3vCGN+D888/HhRdeiM9//vNQVZWx0geoBc9XiIJkWYZhGE1TsQJ0h3RMwl9duhP/9MvHGv7+0MkCDp0s4GcPHGPXtg7HccaGFLYPhnD2lkGcu3UNUpHKnFD0vVXq60oAZYa4PZS4Ad27+ppcgiAISCaTUBQF+Xy+Zdqq2+8rFAqshY8TVHtMKe1uv8eyKvwVjYx+6lrQSi8ondVprDr7wTczWMjIXOxD72LAKb9Xax/dr1a6S/fSKwOe+kI3SlVuBCpVKRQKDdOk/Q56bgqFQkvnU6eg58OrAzXVutNa4HzGqJRtbm4OiqIgFouxTB0/gPqH07pDLOZUfkPGJlDb2YHKQkiOeDyOXC7HSga8AJU+uZ17ynqg9T2ZTC7aekZlVJZlIZlMNh0vOXaJ/M8NwuEwm5dYLObqb+jZ1zTN9d/0G8RrQU4vylYgI76dU9KP0HUdiqKA4zgkk8mGZU3U3m+ln+kCLD5898TcfffdeM5znsNev+c97wEAvOENb8A3vvENvOY1r8HMzAw+8pGPYGpqCmeffTZ+/etfY926dUs15CWD28VAkiREo1HMz88v6B0coHe86elbEQsLuPGJGTw8mcPxXOvUtYOzKg7OEkPlYXAc8NoLNuOfX3EGy5To9xwt9UZC7a46OZS0g5vMBZ6v9Lem3tbUhqibg56zNrQRYrEYS0V1HnZbgQ7L9QcB0zRhGEbLtEygcl+JcIq+j+r+nNfqQengbu7DUutOpyC5vDTg3WQsOAltvAC1D3QrA8dxiMVijCzLL0ZjJ/pDxotXhgjVJzszZrwA9YSnlmr02dRijjgqAH85yYgjhO4Lx3EYGBhgUXcq83L+0FpE9e9AZU5jsRjjR/BC1xo5HtvpjiRJSCQSzIiv5x/oF2jNbURa5wTV9bcipqoH7WmtnNP18FsUnvZB27aZY6WRQ72fa5SX94BS5jVNgyRJNe0160EGfBCFX95Y6meoEXzNQr8UkGUZ6XS6LfvfcoNtV9rK2baNdevW+eoQsdIwky/h4ckcHprM4cGJHB6ezGFKbl+P9rU3nI9z14dYHfVKBxnwmUymZ320LAu5XA7hcNj1Qb9YLLI00EZEVq1QKpUY03erQwcdXIi1uZ3xlc/nYdv2grVH0zTGaN5uI8nlcoyci6CqKgzDqGFMr/9eAMsyUusGXstHhzdiGG4GqonsNSppWRay2SxisVjHzj3ifUin08ty3fdqnaAUZ3q+vL4X1GEDqESkC4UCcxrFYjHwfKW9Fxl6fnCoEJkeORmcjPrdgOSjLhm9IpvNIhQKdey8IZI9cth2MxZijHaSUDYCrc3tnk3ao8jJ0AlM00Qul+soa6ybPbFfIDK3Rnugc4/sZn1bbFAZgGEYiMVirHyjFWgN8+q5CLCy4dYO9V0E3i+gtgGAP9sgUPpRfbuqVu0RIpEIIw+JRqOQJMlXMq2UdhWDMRHP3DmE55y6lo19VinhoUkZD0/m8PCxfEOj/ks37seXrjgVkiT1Vfcojc8ZhVqKeZIkiW3srVp8uZGJiGIkSapp0dRKJjpAa5rGalTpPa1kogOHKIoQBAGmabaUNR6Ps7aAFKFpJJNtV9q+UX9mJ6gOmGqvW82TIAiMvZs+m1hULcti3+scI0XgnZ/dSCbSnWQy2bQdjx/XCI7jWPSL1s9enieKQDpl0nQTn71uP/Kajr96znZsyETZe5vppFuZisUiADAW8E7WvUgkglwuB1VVGTP1Us1Ts32rlUyiKLLWeRRN7GaNKJVKzJEGYMEYu5XJeZ1Yw2VZZhkQZLBQbTalDTsP/0ux59I6Rll5zkhtt3suRX0VRUEqlepZJuKvcPKKtNq3aJ5o3aV7TQ4behadf2eaZk2GAclPP1QiQeu2c+zE0k/tDuvXZqdMxWIRlmWxdbkT3aOyBeqc4HbdkySJOampG8lin42opR6tPbQGO8dORjvV+juNYi/OexQxTyQSTfctNzKVy2UUi0VWmkr3lOap2XxQJhHdB7+fYQkr6Vzei0zktOlk3+pFJrdx9cCAfxJ79+7F3r172Y0cHx9nXtJ0Oo2RkRFMT08jl8uxvxkeHsbw8DAmJyehqiq7vn79emQyGRw6dKiGmXXjxo1IJBIYHx+vUZKtW7dCFEWMjY3VjGnnzp0wDAMHDx5k13iex65du6AoCh588EEMDg6C53mEQiFs27YNuVwOU1NT7P3xeJz1tZ+ZmYFlWezwMDg4CFmWWQ3PUsukqiomJibYdTcyzc7Osut+nCenTBt5YNv2EN79vPORzWbxyIGj+NIdJ/GHw5Ux3XU4i/sOz2FjrEqC1w+ZDMPA3NwcBgcHsX379iWbp9nZWczPz7PP7lam/fv3MyN6Zmam43natm0bZmdnMTk5yRbnZjLFYjFkMhnWlo6em3bzlMvloCgKpqenwXEcRkZGFshk2zYymQxSqVSN7tm2jaGhISSTSVcycRyHNWvWQJZlNnbbtlk9brFYXDBPFDF0zmsjmSyrwmy+Z88eHDt2bNmsEXRo0zQNo6OjPT9PAwMDC2T64u0z+PnjlQjs4ek5/MNlI+x7k8lk1zLRZ0SjUQwODuLkyZMdrXtTU1OM3KuZ7i3WPGWzWTz66KNs33K7ltN4UqkUBgcHMTMzU9MysZM1YnZ2tq+6F41GsW7dOuTzeRw6dGiBTPl8Hrlcjs3HmjVrFnV/ou9du3YtdF3H3Nwcu49e7Lm2XW05d+LEiZ5kGh0dhWEY2LdvH2zbZvvW7t27Xc2TZVXq58kwpEwcoOIMGxgYYGz7QLXLwpo1azA7O8syNoDKM79+/Xo2T/RcDg4OYnBwEAcPHmwqEznO6fO70b3jx4+zzwiHwx2te6qqYvPmzYt+NqIxzs7OMoOq1fO0YcMGzM7OMgcYOWJ6Pe9ZloVyuYwzzzwThw8f7lgmp9G/ceNGhEKhjtaIfD7PMmBJB5bjGdYp00o5l7uRaXZ2FmNjY2zf6rdMQ0NDcIMghb4OlLowNzfHUhf86EGiTW3Hjh2sxrOdV4xYPyld2DAMVnNNzODEzLscvGLL3dNnmibuGz+OV3/9Qfb7y09fhy9edU5fZTJNE/v378eOHTtYdGip5olS0VOpVMMIhxuZCoVCTS/mbp8namtDtXiNZKJatvp+8m5kdfaIJXmd76eU24GBgdrI7pPfmclkFnhmG8lk2xVCpXpyvXw+z9J5699PTj1nemMjmUzTxPj4OHbt2sWyEdzO01KuEbZtI5vNIh6PIxqN9vQ8NbpfJ2QNz/rXm1A2K+8JiTzu//vLYJs6NE3D4OBg1zIREV06ne4oa8p5nRy3giCwaORSzFOzfaudTLZdaTtJ9df02cTITxHGZtE1WZah6zpSqRSLZnolU7f7E6WuRyIRxOPxpmP3cp4okmSaJktXbjTGXvdcWsvqieQ6lQkAKx0RBKHtvuWcJ8uqsoPTNUrH53mecRW40T1ag0VRZBHUfD4PjuNYWnQrmRRFYbXv3Wa9URaXs+bazTwRAR6V+yzW2YjuPxnhTj1o9TxRBgc5nOl57eW852bfaiYTZRBYViV7gurYO10j6PxNJQ0r4Qy7Es/lja7ruo6xsbGO961uZVIUBZlMJkih7xZOIhoCKUQ9Or3ejDynk+ukKPXj5LiF/aNpLMRgzXEcO9jH43Houo5yucz+5XmeGfL19XD9lqmT617Nx1LJJIoidq1P4Vk7h3DT2EkAwG8encZEVsOWodoaeK9lIt2hTXWp5okIXtwQFTW6Tgf7SCSyoKa0E5lEUUQmk4GiKDVpfJQmD4A5vBp9lxtZeb7iuZVlGaqqLqjNNE2zhniOQPem2Wc3kkmSJOi6XlMHHwqFGvaYdxLnuZk/0hm/PU/txu4sTenleXIS2NF3fuO2I8x4B4CyYeGhY3mcPZpgB0Z65jqVidoCktHSzdh5nmfp20RmuFTz1GjfciOTcy8iRn7as4g5XZIk9kN/q+s6Y++vr79dyv2JUnApUtOMCMuLeaJ1kmrd27F+97rnUuebYrHYkHeiE5nI0Uk6227fouAEZWhkMhmIosgIBetJDN3IlEgkEIlEWLTesiqlHc6a5mafQ46Ten6BTnWPnAfO/u5u5ikWi0HX9ZZEkF6v5bZdKXeg7KNOnhtia1cUhfHMdDPGTvetRmMxTZO1g3U6X5q9v9UaQWTSpVKp47F3e32p99x+XF8qmbrdt9xcr/9Op7OrFQI2hWUKjuNY3XAnf9OozjUUCiGRSCCTyTCSkVKpBFmWkc1mUSgUmBc7gLcQBAGvu2A9e23ZwFf/cLDFX/SObnSnn2Ohuslu9Iv6m3tBfMNxldS2aDQKXdchyzJyuRw0TWORK0EQemKS5fkKqZJlWewgCFTrMBuxzzsNe7egLBpnH2U6CNd7e2kMrRwEBD/pTqcQRbFhhK9T1DPQ5wo6vn374QXvu+PgHLun3a6duq43bSvYKUKhEOOdWKq13Cv9EUURkUgEyWSS7VvED6CqKrLZLGtrRm27emkZ2S+Ew2EkEgmUy2WWneM1KHWcov2U/dNP8DzPDEenwdINRFGEruuudMeyLGb4SZKEVCoFSZJY0IKMMGI+r4/OtRsHZRSUSqUF62sjkCHbSdu4ViAuAGcabjs4HeWdyNsLnKz8zQyrVqC5s+0K8WS7+9wO3aw7lMnWyHjvFrSOO50wXkLX9UWb49UEv557AgN+mYLnK7W7bg7dTtQb8E6QlzAej7NDEUXt8vk8stks8vk8Y9R0kjMF6A6iKOKc0STO2FCNUvzwnqOYU91v0J3Ctm1s3ry5Y93pF+hQQpFgt6DURkmSPDuQchyHaDSKdDrN0vcKhQKrufXCCKAIGB02nbLXG/AUMe+0/RkRHTnvqbMtkxOWZbHMnHbodt3xA8iA7/WA4yQtBIBv3X4Iannhmnqnw4Dv9js1TWPlTV6Ayif6dYBsh37oDz0fsVgMqVQKmUyG1RlT2r2fu3pQ/3JKsSWHYa+w7QpxF9Xbp1IpRsK0GKAsPspo6hbOspFWukNOV8o8atRCjtbeRCIBwzCYg9btOYaem6GhIYRCISiKwvq/NwLpX7etSutB2ZGdOkWIFG4xnntn//le1i1nuU8+n+/4fOBEN+sOPYeddqhpNw4KWDTTGdM0O3LQEDRNQz6f79gxFaA9/Hru8ddoArgG1XR2akCLosjSD1vBeShKp9NIpVLMe1gul6GqKmRZxvz8fGDY9wAytN72jK3smqZb+OZthzz/LueBbmpqaskO8fUQBIGlOHYCik72o7cqeVyTySTbwIkNO5fLMUbhbiGKIhKJBKuPLJfLLFLkRLPrbsYvSVLNQYDneRbNcsI0TdcbU7frjh/QzIHRKYhFGQCKZRNfv+VQw/fdc3gelFXfja6YpskYwr0CZZAUi0VPjMROsRj6Q0YOOaIHBgY8iZ71E5IksegurTGUQdDNfroUUfdGIIdBoVDo+jMo6CDLMk6cOLHgWaIodz6fhyAISKfTbR2toVAI6XSaORio1WIrOA3TaDTK9gZd15HL5RbsX7TfUuaLVwiHw2xtcIvFisJT6UIoFPJk3aLotyiKyOfzXWdzdLruGIaBYrGIaDTq+XPTKgpPma9U6uQWxJNCwRBZlhdlfbesCjngcjwPdAK/nnsCA36ZwrIsTE1NdbwYh8NhCILAWFTdwFl/RSmLAwMDSKVSjPALCAz7bkB1dC/asxGnDFXr075522EUG0T1ugVFd+hAkc/noSiKb4z4cDjMDHK30DSNkVj1C5ZloVgsIhaLYXh4mNWOaprG9LvbDYyyXahGt/6QZxhGV+nzBGpr5LynjQx4isC7Qbfrjh9ABIdeGvA/uPsoTjqyZc7bnGH/L+omHj4mM/KaTqFpGnMkeQlqF9aLUdUtlkJ//Jb22AyiKLIMgng8DkEQakrZyNHX6t4tddS9Hs5U+k73Gqo/pjVW0zTMzs4il8uxNc2ZtRCLxRpG3VuNLR6Ps5aY5JhttJZTu7hwOFxjmIbDYUYuSfXxNDYylr12MBNXSbdReGpJSTAMwxMjjDLK6L56BSptC4fDUFW1qzNLJ+sOpc5TmY7XIGeKMwpP30mlH4IgLJinZiDdJKclOQJlWe55r2v3veRsyGazjBxzJcKv557AgF9lIEZQ0zRdLxDNPofqCmOxWMeGPRF5aZrG0sxWq3EvCAIEnsObnrGNXZtTy/jRPUd7/myqCaW2LKlUiqXzRSIR1l95qUH1RW4PJWT09iP67gTVC1MUXpIkJBIJpNNpxGIxtvESV0SnGxhtuhQ1dKLb9HkCRe6dUfhGveQty/J9hNIrUAZSt6B7JwgCdNPCV24+wH43EJPw988/peb9dxyYY6y0nYAiG5FIxHPji+M4ZlR1k6oZoL+gNFsnLw1FXWmtyefzC1Lt/RJ1r0coFGJ9sNutj1TeQRkIpVKJZSeQAUrGtizLkOVK28Z0Ot31s0L11pSZQmn4BLrvlJFYDyKIpIwqcpTTPPRjbQ2Hw22dOY3GSYYjnbdIXtIrivx2ehajfdC2bSQSib6sWfF4nJ1ZeuVVaIX6Pb8foAwRIvDN5XI1pR+xWIw5VlrBMAymm+Q0oTIRQRB6Lj1oBkrXpwwJ0kd6LrvlNArQGZZ+dQ+w6KBoOkVjvdzkybCv/0xq60I1qNQOqN7zS1EyYnukn+USRekFrz5vIz73u32s/v2rfzyIqy7aAoHvTnZKqwIqqYx0AKJDVDQaZZ5ey7L6FqmhtDo6QDTSNyeZnZt6Qa9rgxuhVCqhXC43jOqQLJFIhOlxqVRCqVRqy/Bcj3A43DDls1wu90ScQs+iruvM0UHj0nW9pp2K32q7+gUqg7Btu6v7SgdmQRDwsweOYTJbdX697sJRnHnKOmzMRDCRrUSJbh+fxZ+dt7ZjA57WxX4Rr1Far6IoTI9Xiw4sJziZ9YFqWYWu62xtpz2yXC6zw3s/18VuQA4j6r7hfPaIA6RUKtVwgUSjUeaEpDR1+qxcLgdVVVnEsVfdJe4TInnM5/Ms2k6GSjuDjs5SZLzTZ/YDoVCItYfr5DsojZ50h/YG6lpCfEeU+UMytVsri8UiaxvYT2cwOc1VVWWdlbwE7ePOLgX9ADnpqOyDHPn0ncTrUywWm5bQkWOJHEjO9xBZLpE1JhIJTzK56N6Tc5nOamRT0P1TVZWl9FPWbwDvERjwyxTkkez2cB+JRNiGSkQh/QS1Yag3bJw9Wyndl6JPdOilBYIWNT9EFfqBiCTg9RdvweevGwMAHD5ZwG8emcKLzhzp6HMo9ZDaRVHPUYJTd6LRKHieZyUVXnqdnb2OBUFg6Y6Umla/MYVCIWiaxsbdDNQ+itrL9AN0D+kQ0wqkk3TYUxSl54MM9bruddMNhUJQVbWGqI6M+kgk0hEDPdD7urPUoLXDNM2u1pFq6q6JvTeMsevxsIA3PWsXRFHERduGMXHvBIBKHXxRKyEccn/QpOemXevAXpFIJKBpGutxTYZ8P+d2uevPUoOMderfTsY8Pc9ekaV5DZp3qmMOh8PM8UnOKlEUEYvFGuo9GZGURk2Zf7quQ1GUjlLnW4GY5qnlHLUndOskIEOfHG/9mgsnmV0nzyyt/7lcjmVOkhFMRphzXkqlEvuuZsEer0jr3IKIOGmfdbOOu1l3LMtCoVCAJEl9SZ13wumII66d+rFFo1EWQa8/B5D8ABr+LVAtPVBVFYqiIBaL9SQXOQwsy2roEKDnk7KFKKBBRMPUCtWP61M7+HXfWpmWkAdw1o4S4zAxoRKaXaeDcrPr9WlkzZiKm12niPSGDRtYSieNhQzidmMkhaTaFfLiLpVMZJzXX7csq6bXL31GKBSqidC3k3UxZKJ5IDgjnPVjaXb9zy7ajC/dNA5Nr/zuyzeN4/Iz1sO27bYyUXsZqp2lDZX+1ikT6Y5t26zlGNUwOT3BjXTPjUxkAFuWVRNhprlUFIUt+BRlJgcPGfzN5klVVXYQoe/qZJ7ayeT8jmg0uqDtYisdo/7HsiyzFjrd6B4xI9OcdSsTfbemaSzKSveYyCxt24YgCK6fm9HR0SVb9zp9nhrJRGUFzmfV7RpBDqmbxuYwPlutIb/6ws1IhCrjuHDrAH78pAGvlE08elzGzqHwAkdaM5moFRAdhNzI1M26R88+cTqQwUKHdbo/Xs4TAIyMjCzYt7ySye+656VMVPpCBr1zH/KjTBR1pAg1rWtEeEZdIpwlPs6xEys5RSup5KpQKCCbzTLD2QuZaDyUpUhjcDtPTn3vl+6Fw2HmgKMxtlv3KP2c47ias1P92KPRKGunSs4MZztG+ltK4XYabr3I5HaeotEo6wBQH31udg82btzY8nlSFIWdf3rZc9vJVC6XUSwWWfo5lQPSOGjslIlK7Wtpjybj3TRNpNPptjpJzj4yvqPRaMcyUdYGRfvpXNlsngCws119Zgdd7/ZstBRrOYCG9la/9ie35QeBAf8k9u7di71797IbOT4+ziJ86XQaIyMjmJ6eZmlcADA8PIzh4WFMTk5CVVV2ff369chkMjh06FBNDcvGjRuRSCQwPj5eoyRbt26FKIoYG6tGdABg586dMAwDBw9W+4LzPI9du3ZBURSMjY0xj3soFMK2bdsYwzghHo9j06ZNmJubw+zsLLtOMhUKBUxPT7MFfillUlUVExMT7DrJpKoqpqammFJHIhEMDQ1hbm6OeSEBIJVKYWRkBCdOnOjbPAmCUCOTbdvYsGEDyuUyZmdn2UYiCEJLmZrNE0oKnrc9gZ89Xqnte2AihzsPzmFztNxSJjLG0+k0BgYGcPz48ZraJ6dMxH8QjUaxbds2iKKIw4cP1zgJdu3aBdM0O5qnXC6H48ePs/sSDoexZcsWZLNZHD1aredPp9MYHh7GsWPH2HyQ7qVSKUxMTODYsWPsXjrniWqrOK5aj+617pFBu3XrVuTz+Y6ep5mZmRq20jVr1mDNmjUd6V48Hmf3a2ZmpmeZ6N5EIhFs27YNxWIR09PTOHHiBJu/wcFBnDx5sqFMznWPjL6tW7d2LNPY2FjNxrR161ZIkuTJGtHJuhePxzEzM1PD/dBujTh48CDTPdu28fU75th7JJ7Ds0YsjI2NYevWrbhgy0CNPLfsm8aOp1bGMjc3V2MYN5KJ53msX7+erXtuZPJif9qwYQN4nmdrAe0J9bpX1C0ck3U8/8LTYVnt1wjbtiFJEtavX49sNosTJ06wfavfMi3G/tTpnruaZVq3bh1EUcTU1BTjouA4zpVM5IiMRqPYtWsXdF3HoUOHahzUW7duhWEYnsg0MzPj+3kql8uYnp5mRkYzmVKpFBKJBE6ePMmcw7lcDkNDQ+y84FwPSaaJiYmaMsfh4WHYtl1zHgOAbdu2wbKsRdU9ImicmJhgJQvN5sm2bUSjUWzevLnhPEmShMnJSQDe7LmNZDp+/HiNA2LLli04efIkZmZmcOLECXAct+B5It0ulUpYv349JiYm2JmXnDBudc+2bQwNDcE0TUxMTNQ4PZrJtHPnTpw8eZKdFcgA7+Z5mpychCzL7B6k02msX7++5hwI+HPdm52dxcTEBNu3+r2WDw0NwQ04262pv0ogyzLS6TTm5uaQSqUA+DMaYBgG9u3bhx07drCIfDeecyK+sSwLqVQKPM8vG68YpSFRSrVlWezQSd49eq8z8lX/nkYyAdU2ZZZl1UQGaCz02ZIkged5lnrG8zxLZ6yX1c08HT6p4rLP3gzrySFddupa/Ofrz2s4f1RfXi6XIYoi4vF4Td/cRvNkmib279+PHTt2sJQ3Ggt51KnNWT1azRP9rWEYLMWxXWTXMAxWaw5UjEwi/6H759Q9+vxUKsX03ivdo9Yx5XIZ0WgU8Xi860iUk1yGWI7dPk9EakOe7l5ksiyLRakymQzLcpmfn2cp9KZpIpPJuFrfTNPE+Pg4du3axaK47WSibAyaJ2dEjuaQIkPBMiEAALIYSURBVAw8z7N02X5FDClqlU6n276fog8UtTJNEw9Na3jjtx9k77nygk34xz85nclk2zYu/uQNmM5XMoaee+oafOql21nKK5FhkfzOsZPeUGrrUkR2KeJmGAZbU0hvZpUSXv3lO3BkroBLdgzhW2+8cEG0gNp+URouGWqiKILneRw5cgQ7d+6s2beCCHwgUzuZWu1b9JxS201nKUG3MlGKM+3vfpynUqmEfD5fsx/Wj5H4EnieZ23RbNvG3Nwcc67Zts3KF9rJBICVupmmiUQiwbLhFlv3yBFBkWE623WybwGVc78gCDXs+TR2OpvQObIbmXRdRz6fZ/c5HA6zMVJGSjqdbngmpch5JpNhxM9EEt2p7lHZLK3rTmdyvUw0NsMwEIlEaspCelkjLMtiewMAxsFQP3Y/rXu6rmNsbGyBvdWvdU9RFGQyGeRyOWaHNkIQgW8CZ3oRwWkE9nK9WW1sJ9dJUerHSYfhTsaSTCYZcyQtYEslUyfX62Ung5CMekoPbuejontJixkZ6vXf70wfo3tfD4pu0wYXjUYb1jG3mqeta5K4/IwR/OKhSjT7+sdPYHxGxc51yRpZKe0VAGuzUr8gN7pn9HsnOSBdp3tK5CeNarkbjZ2MRPJO1i/IjcDz1do6MjTpvpETwPk5ZBjHYrGaurdedY883HQvifG4maztZKLv4LhKWl6hUGjaVqfR5+i6znTN6SXvRCbn2Im4iOq+6b6TYeXUCTdjbKdjzo2T0rKptpQcOuRIoh9nlKd+baMfirJ1skY0GmMr3XFu4mS48zzPDky5XA5fv32y+n4O+D/P3r5gDb5o2xCufeAYAODuw1nYADsEUU0ilVg4x07lI1Qr2I3u9Xqd0mEp1TOfzyMUCiEajeILN4zjyFyldOCW/Sfx+JSC0zZUDhh0MCP+DQA1dc1k2NeXPi2GTEuxPwUyeS9Ts30LqOyB1PGGni/ns95OpvqAAK1HRBRHe4Kf5omeq/o6aVrH6IxALOU8X3X60zknnU4z57UgCAs+pxEoxb4bmWzbZu36nI4RMnyana2ajSWZTLKuDM6gg5t9y7ZtVs7XiEeBDF7nZzjPf85/nb+vl7VYLDKjuX7di8VizKBtdP5OJBKM3Z0cJvW17G51j8ZK55J6men99BxxHNe0m0UvZyNyshUKBRQKhYb8HX5b9xrZW/1a9+rPfc0QGPABIAgCe5iInXg5wmkQOkFGvNsfjuNq6ryaPXSNIAgCEokE2xAVRYEgCB0TvLz1mduYAQ8AX7n5AD79qrPYQk8H5EYkdb2CjK18Pt/UiCdYVqV2vhlhnltQdIDaxMzPz7MoMRHeUW2/l8zchmGw2v9wOMxI/bwA1WoSY64btmDbtntmn68Hz1ei2uVymd27SCQCVVWhqiqLxHupQ2TIOSMOzgMUzy9kEG5k2Ne3NHIe9pr9tLpv9HxTVgBFoyzLqvmh7yKdpL95fErBH/afZJ/34rM2YMvQQufMhVsHmQGfLeo4MFvEniedbKIosp7RzoMLGcBL2bfbCdoLyJC/78BxfP/O2taWD03MY9tgaEGkvRkZWYAA/QQZtIqiQJblGkK5elAnHDLa6bkn56mzHzdludUbYEsNjuOYg9a5bjiz1WgNM02T7dWCUEm9pswoMhzJ8eEFY3kjOMdFBnsjONfy+v+TE5pAzglVVVlpoFtQ54NGJIV0tiFj05mNaVlWw72p3qgvlUo1c9BoXac5bNZ2kBxW+Xweg4ODPRPsEWmeM0hDsjdz+vQDPM8zItVCoQDDMHz3fPkdgQG/TEGRTq8OehRxUVXV08/1AyitajFBRrCu6yyKRQdbN6ypezZlcNHWQdxxsFJr+5P7JvHmi9ZjTULq+YDsRncEoUIYlM/nIctyQ7ZX0hcAnm36HFepi3UeqMhDTi3dvJhLZ3SYZO1Hd4NwOMy+izbqVvCKfb4e5AAhA5Z6H5fLZZimiWw266qdWDvdsSyLHXYpVdzthtzOsK83sule1WfY1B/6nAY6vZcMTpKXfuhw2OigaJomvn7HZM13vf1Z2xvK8tRtgzWv75uQcdaWYSZnIpFgWSvUUYGI9fp1eO4G5DALhULY+79PwKy71/cdmsXzdiTZYY/SjVt93krbXwIsDtzqDq3nhUIBqqqyrC3btheU3NH7icyx/pkHwLJvVFVFLpdraYwtBYgYkBjpiZmf46rs+dT6y5lNRGsjRUD7bcRTuaZt2zX7rTO1un6tpnXemdpM8+u8/859luf5BU6bRrpD2ZL1mX4EZz942g/qQWNtZtzzPO+qrSM5YSg93gkqVeg0mNQKtP87z3ccxzEHz2LqOBFGOp1uftoDAf/uW4EBv0zB8zxGRjprL9YKZDjJstwy5TdAZ6AevhTFkmWZeXSbGYyUKv76CzcwA96wbPzwgRP4mxed1rOH0q3u0Obj7CVKdblOI60fXlqKEFMdei6Xg67rzAht1k/eDVpFh/sBYn2lOsRWmxMdsrx2JlDf4PpUS3IGOWu9KdW70Zy20h0ySAG0jHx1CjKom6FRBN15jZwCTsNekiRW++8WB2cUXPdENfr+7N1rWPp4PbavSWAoHsJJtVLnd89EHq+rq60jHVZVFbJcIa1sdt+XGrcfmMPv980uuL5vVsPAwIDr58frfSvA6kEnukPnGXq+nCU6lMJLnW/cPG+SJCGdTrN0aIrG+6GlLa1nRLJJ0dNoNMpaeVGqdv1eR0z2iqIgGo0iFosB8N6INwwD+XyenSkapSG3O9dQyr8sy4zM0AnaZyn1u74UwKk7tl3tJU8yO0G8PO3aEzqdvfWwrConUztQthc5U+heOFn+KUrv1XmFymfJcCaHjhuHg9cQRRGpVIqVv/itJaZf9y3/nRQCuIJlWTh+/HjTFKRuQKn0lFYUwDuEQiGkUinWokSWZdYKBACLDqiqimw2i3w+j6dtTWPHmqoj5fv3HEPR6J1zshPdoQ1XFEXk83nmhNB1HfF43HV/3E5BEQIyaHmeRyaTQSwWY/3kc7kcM+jdgCIAVNaQSqUWzctMGROKojR9tpzp816DykGcDKike3TYS6fTLJrT7N420h3LspDP51m6IX3OYoFSF6nXLB1EE4kEY1+OxWKsjlWSJMa70Mn6+dVbDjFiSQD4v8/e0XJMF26tRuHvPSIvIKoBqgcXmvPFvG9uYVk2/vmXjzX83eNT+Zp70v6zvN+3AqwOdKM74XAYqVQK4XAY8XgcmUyGdaLoNION4yqtWYlUioIdbveffoJS5MnA43ke+XyeEbKm0+mGex05OoivRZZlZpAqilKzX3SLcrnMSOJaleO1A8dxrFc9tSGsB5UqUuYFoV53NE1jKdv194TKqxqVY3aCduVc9SDDXNM0Ng5FUVjGltPR7hWcc8LzlbZ2S1VCS3JGo1FomsbItf0Av+5bgQG/TEFRSa83D6o3VlXVd8q63EHpqOl0mhmiuVwO+Xye/avrOjt0DGQyeJsjRTevGfjenUd6HkenukMELxTFpYW+n8YGedApgkDRcjqMkGecegA7Wc7rQVGJXC7HSGB6OUh0Kw9FbJyOGycoVbBf6WPUk5XmnZ5vOsRSNCKTySAUCqFQKCww5Ot1p1QqIZfLwTAMJBKJthELv4AiJs10ph4nZA0/faDaXua8LQO44JSBFn+BGgN+Vi3j4Kza8H2kG5lMxpf1fz978BgemnS0yklU9bNQNpvK1Qj92rcCrHx0qzuUZeRVdgs53cjQIIf2UkKSJHZ2I54cOmu0i2RSOjUZb1S3bZomcrlcT0Y8Rfep7tqL+08Ramqf6wSdVQRBQD6frwmQkO4QP1GjLEiKzJOzZjFBUXi6/9QujtLbeZ5nqfZens3pPEedDJYSdA+SySQsy2KZl0sNv+5b/j9pBVh0kKfP2Rc0gHeg9Fky5KnmOZVKsSgzbSwvP3sU61JVQ/lrfzwI3Vx8xwoZGalUatGMX2ddm/PwRcZ9Mplk0V4yJGVZrkmX1HUdsiwzgph0Ot03A/mPY7O45v5JaHpjo9DJdNvIu1wul1m3g36A2gVRZILSy+sPd1QnmU6nIYoiM+TJkQJUsxmcUXe/1a21AhEDOaM0rVB57qqb99uftb1tdOWirbW9XO8+km35fr+kCzqh6SY+/esn2OuIxONTf3pWzXseOZar/7MAAVY0yNCgulhaCxfzgE9rORnJlDkZCoXY2aITg5lY0ulvJUmCruuYmZnpOBpKtfXO+nqv1jcyrqkFbaPfE1cOtWBzjovaqDXioyHjudeyQPoeKilzCzrnyLIMy7IWOD1ozF5G4f0Icio5Mz/9Zjz7AYEBH2ABiDXd2Z87gPeoN+QbGW4hkcdfXLKVvT6e0/CzJ9mtFxuUwrZYhoYoisxR0Iz8jXQ1k8nUkPBQRkM+n2etUPrJ7v3dO47gz752B971vfvxsWsfafo+KkkAUHMo6mf6PIHqPemZNk2z5SGFOiqQZ56Y023bZtGN5RR1r4coiq4M+FxBx7dvP8xe716XxKWnrm37d7vXJ5GKVJ/puw/7z4PfDt+87RAms1VH7pufvg3P2LkGklB9jh6eDAz4AKsTlIJMrcB6jVi3Qr3Bns1mWRq/ZVks4t6r8UmR3kwmg+HhYUiShGw2i5MnTy5ITW82TkVRGDN+P/ZdKpcqFosNHQu0zxLrPa27lHrfKnWeMhm6BZWUUVvcTvSBHENEQNiozSkFLFZ6hqwzpZ46Oq10mTvF8jt1BQBQedCHh4f7ZpBQrShtDr1guR1a/YarLtqMRLhqCHzl5gM93dN+646XoANAu4g/lSc4UwEty0IsFmvIoO8l5tUy/uVX1RrhH90zgZn8wsgAodHhglLb+11/5kyjd9s6jjoqpFIpFp2nw+JyirrXgwz4ds/St24/BLVczar4P8/eBp5v/+wIPIcLTnHUwU/Iy2otzBbK+OIN+9nroXgIb3vWNoREHrvXJ9n1hydl15+5nNaeAP6CX3WHHPHk6FQUhZVJNeqS4RZOgz2fzy8w2CORCJLJJAYGBpiD2uvMuFAohDVr1iCVSrEae1mWIctyTUYWteClskCqvRdFEbquQ9d1lMtlVhJXKpVcly81A5HYNYtyk3OF+t4PDAywNp3194mcDsQD1S3IeDdNE8lkknUv6ETWcDiMTCbT9MyyWqLwQG1KPXEfLUVKvV/XnqWn0AzQFXiex/DwcF+/Ix6PI5fLoVAoIJFIdPS3hmHU9FgNh8OLXlO0UpCKSLjywk34zz8cBFAhjrp5bBbP2rWmq89bDN3xCsQW3AmoHdBi4fPX7YOsVaMShmXjf++dqOEvqAcdLohUj4jY+j1uaidHfdY7ubeiKCKdTi9o4bNcQT3gTdNset+LZRNfv+UQe71xIIqXnrXB9XdctG0Q1z9+AgAwJZdx5KSKU9Yk2/yVP/D/bthfo9fvfu5OJCMVfTljQ5oZ7o8cy7H2hO2wnNaeAP6C33WH1nTqxlEfeSVG8nY/ZAg7nYuU8k1722Kuv5TBRtlb4XCY9ZSnNrJAxXglo5IY09uVYVILv1Ao1LHzgThbqMtAI2eyKIpIJBJQFIWdJRpx91Bkvpe9jcrKADBnjiAIkGUZqqqyWnY3aPU+ypCg0sKlrltfDFCZHnVEIpJj5w/Q+hmj33cDv649gQHfBHTABap9heu9qc2uU11ps+v13jiKgtVHuptdFwQBpmliYmICGzZsYJ/L83xNv8xuxl5/ndp50cLXTCbysjo3HjJKeJ5nrU2cvT/rZWo2dq9lWsx58kqmP3/aFnz9lkMwnqR8/vJN43jWrjVdyWRZFo4dO4YNGzY0nY/FkGklzNP+Ewq+fcdCYsHv33UUb7pkC2PQbzR2IleiTT8SibDnpl8y0eHPyeJLkXg380FsrBs3bgSAZTNPja7TWJ2dDurf/4O7j7BWcADw5qefAg6263k6f3Om5ve3jc9g02DVkenX5+nIXAHfvO0Q+93W4Thefd4ok/e0kaoTQtYMHJ5VmFytZGq2b63kNSKQyRuZ3OxbfpCJjEZ6D6WcU5SaYJrmgmv0mrpqiKLIDE4aO61diz1P0WgUtm2jVCohFouxnvP0PjIoKcJNn01OCfo+mieKyBcKhZozZr1TudXYKZ1cURRmfNe/n5wE09PT2LJly4J7ZpomNE1jLPCkg508TxTl5ziOlfKZpsleU+YERc97nSfKji0Wi0gkEqtmjUgmk6yMo/65qR876Vr9dw4ODi6QtZ1MhmFgcnKy632r03XPbdZOYMA/ib1792Lv3r3sRo6Pj7OoczqdxsjICKanp5HLOdh4h4cxPDyMycnJGk/k+vXrkclkcOjQoRov7MaNG5FIJDA+Pl6jJFu3boUoihgbG6sZ086dO2EYBg4ePMiu8TyPXbt2QVVVHD16lPWyDIVC2LZtG3K5HKamqmzJ8XgcmzZtwtzcHGZnq318O5HJsiyUSiWMjIwwmUjB1qxZA0mScOzYsZpFevPmzQiHw9i/fz/blE6cOIFdu3a1lGliYoJd76dMizlPXsjEazKevTWO68YrzKS3jp/Ew5M5pC25Y5kMw8Dc3BxUVcX27duXTKaVME8fvu44zAZ9tA7MqrjmlofwrNM3tZRpcnKSpSJyHIdNmzb1XaZyuYzjx48DqBL+uJ0ny6pEWUZHR3Hs2LFlM0/NZKLxrF27dsE8GZaNL904Wf3bCI9z0hrGxsZcyxS2bEREDtqT7R9///AEzsmU+yoToZfn6Z9uOF5D2vfBF+zCoQPj7HXKrE3fvO6eJ/D0UxKuZHLuW6thjQhk8kYmy7LYvrV79+5lI9OhQ4d6nidFUXwh0+joKCRJYmMk43xoaAihUAjHjx+viXK2kknX9ZpzoyiKGBoaYh1j6HPa7U/EBzA9PQ2O45qeYcvlMjiOq5HJtm0MDAwgFothYmKixnByO0/0N6Ojo+A4DuPj1XWS5sk0TRw+fJg5GLyYJ9uu9GyPRqM4fPjwqlkjjh071lCmffv21ci0bds2GIaBI0eqARaO4zA0NNTx83Ty5Mmafavf697QUC0BbjNw9nIqylsEyLKMdDqNubk51u/Tj55zwzCwb98+7Nixo8bb2Q+vmGVVanWJBIsi7TQW6qdKv280dk3TUCwWWaskv3v6/Oi9fPy4jBf9v1vY7166ZwO+8Jo9Hctkmib279+PHTt2MG/3cvXILuU83bTvBP7iG/ew3+3ZmMaDkznQEF917ig+/aqzXMlEBvxiyFQqlVjUP51OsywZN/NhmibGx8exa9eumshKI5nqr/vtebLtClOyruvIZDIL3v/T+4/hvT98kP3tO599Ct71vFM7lunPv34X/rD/JABg80AUv3/fs/oqU7vr7ebp3kMn8cov3c6uX3DKAL7/1qfWvFfTTZz18euY8+rtz9qG9z1/V1uZmu1bK3WNCGTyTiY3+9Zyk8nNdb/JBFTOyYZhsFav4XAY8Xh8QeSwU5kAsBp5wzDAcZWOM41SxZ1jp3T9ZDIJSZJc71u0/qfTadTDzTxRBgGxpte/3zlGYpancoRe58m2K2SykiSxzlH187fSdG+pZNJ1HWNjY13vW53KpCgKMpkMcrkc06tGCCLwTUD1K06QQtSj0+vNalY6uU6KUj9OjuMavr+XsRPTN/XHpHosSZLa1t/Q74mlVdM01qezkUydXPdqPvo9T17JdNpoBs/atQY37ZsBAPzyoeP4wAt216Tktvsc+k7SHdo0l0omL64vxTxZNvDPv6y21xJ4Dp959R584ueP4g9jFe/zLx+ewsdefgYSYd5XMjnrDZ3pim7HSDrjJ5m61b1wOFxDilPd9G18+eYD7Ho8JOD1F5+y4DvcjP2p24eZAX9kvohZVce6VLWrgp+eJ9u28S+/ro1k/N2LT1vw3rggYPuaOPZNVzKCHj2ed7UPNdu3VuIaEcjkvUxu9q3lJpOb636TKZVKQVEU6LqOWCzWttd8J2OPRqOIRqMwTZP1tCeeGDLmnZwlPM+ztnKaprFa+Hb7Vrlchq7riMfjLc+xzX5H59loNFrDtN9sPpLJJGRZZmfgRmN0vt/N9Wg0ylLzG/G4rETdWyqZ+rlvNXJOuUHAQr9MwfM81q9f31QhvAaxTmcyGSSTSUQikbbGuxMcV2GTLJfLPbOPrma87Znb2P9Ny8bX/niwxbsbY7F1ZyXif+46irETCnt91YWbsWtdEq+5YBO7Viib+PkStfxrBY7jWJSiU6w03aFDT31rpBseP8GMUwB41dnrMJBo3MqwHS7cOljz+o6Dc119zmLgd49O485D1fG95KwRnL0p0/C9Z2yoRq2IyK4dVpr+BFg8BLrjH3Acx1qM9qs9qyBUGOGJOJWcrcSC70xF5rhKWjrVs9ejXncsy6qpu+8Etl3p8V4sFhGNRl33uKdAGDklvAD1jW9HFhigN/h17fHXaAK4BsdxyGQyfVk4m8Hp+e4GwWLTOy7ePoQzRqspNd+/6yjuOHASVoM67GZYCt1ZScgVdXz2t9XoezIi4q+fV0kfft5p6zAQq0a1v3/30UUfnxvEYrGOO0sAK093yLPuNOBt28Z/3FhtnxYSeFx1/kjXm/dZG9MIi9W/vePAye4H3EfopoV/+fXj7LUkcPjAC05t+v7TR6sG/KxSxokWrRMJK01/AiweAt3xFygbczFAhK/pdJrtW4qiIJfLMQ4Zygx1ErQ6x+rUHapPJsI5tyDjnYj8Om05Fw6Hu2ot1wwUGHOWtQbwHn5dewIDfpnCsiwcOHBgQX2Hn+GMwgeLTXfgOA5ve2a1PVlRN/Gar9yOSz51A/7x54/igaPZtpGw5ag7fsIXbxjDfKGadv3OS3diMF5J2wuLAv7knFH2u/uOZLFvOr/oY2wHSgfrFCtRd6gfPOHOg3O490iWvf6TPeuwPt06RbQVwqKAcxxs9Hf6NAL/vTuP4MBMlXjn9Refgs1DzVt/nrGhtjbv4clck3dWsRL1J8DiINCdAJRGn0qlkEwmwfM8VFVFLpeDpmmIRCLsmhNO3SmVSix1vhOnrGVVeryXy2UkEgnGKN8pKGNBVVXXbOOtQCVxSxEYa8T+vhLh17UnMOCXKWzbrmGDXy5YysVmpeDyM9ZjS93B+nhOw1f/eBAv33sLnvWZG/HpXz+OR4/JDfVjueqOH3BwVsU3bj3EXp8yFMMbnnZKzXucafRAJUtipWAl6g4Z8CTT/3dTlUWY54A3XDTalbPDiYu2Vlllx04oOKm0j1YvJvKajs9fV2UmTkVE/NWlO1r+zWkLDHi57fesRP0JsDgIdCeAE5IkIZlMIpVKQRRFFAoF5HI5CIIAXddRKlXXWNIdavdGkXC3IOPdNE0kk8mO/rYePF/pvkE1+73CGYV38rn0G4ZhQJZlZLPZRf3epYBf157AgA+wqOA4jvUPDaLw3UEUePx/V5+Hp24bRKOg4JG5Av7jxnG86N//gOd+9iZ87nf7sN9Rrx2ge3zyl4/VtNf6mxc9BSGxdhk9dX0KezZW04t/ct8kykZ/PLe6aXVUPhFgIZx18IdPqrjxiRn2uxeftQEbUiEPDPjaOvi7DvkrCv/lmw7U9Lt/x6U7kIm1PqQmIxK2DldTUB8+1j4CHyBAgABeQhRFJBIJpNNphEIh6LqOcrmMbDa7oDSKerV3kvpumiZkWWZt2+r71HcDSZJYur8X5+DFDIzZto1isQhZrjhsRVGEqqq+i06vBgQGfIBFBy02hUJhqYeybHHahhS+99aLcduHLsOHX3JaU6Kp8RkVX7h+DM/97E24/At/wN7f78eRueC+d4Nbx2fx20en2euLtw3h+aeta/jeKxxR+Dm1jOsem274vl7wjVsOYvff/wqXffYmTMwHc9otiNvDMAx8ry5b4o1P2wLbtnsmrzln8wAkoeptu/2Afwz4qZyGr/6xyrg/moni9Ref4upvT3dE4R9xkUIfIECAAP2AIAiIx+PIZDJIpVLQdR0zMzM19eaGYbhOnTcMA6qqQpZlcByHZDLpac1/NBqFKIpQFMWTyG40GmVj7pcxTVF3IvBLpVI1nAR+i1CvdAQG/DIFz/PYuHGj71gR3YA8oIZhrPjUm35jfTqCNz19K376l5fgDx94Dj74wlNrDtVOPHZcxmd+8wSe828344O/O4H7jwYHbrcwLRuf+Plj7DXHAX//kqc0rYt+6Z4NiEjVZ9PrNPrHjsv4+M8fhWVX0vo/du2jnn5+MyzndacZiIypWCrjhw7SwdNGUjh9pHI46TUCHw0JOGtjhr32Ux38v/32CWh69cD3gRfuRkRyJ+/pDib6YzmtbWnAStSfAIuDQHcCuAGlqA8PDzPS5Hw+j0wmg2g02jKCbpomisUicrkcZFmGrusIh8NIJpM97wH1IOZ8IsbrFaFQiHFM5XI5FItFzwz5+qh7KpVirQN5nkcikYBhGCu2NNava4+/RhPANaiNh99YEd0iFApVDs0r9IFfCmwajOHtz96OX7zzGbj+vc/CXz93F3asbcw0/sBkHlf+5x01BkuA5vjRPUfx2PFqje8V522qMV7qkYpIeNGZI+z1zWMzOJb1Rtdt28bHf1Yx3gnXPTa9KGnZy33daQZRFHH94zOYVapp5FdetJkdgLw4vDnT6B+bkpErLL3z8rHjMn507wR7feZoGi89a4Prv3d2xACAR461roNfqfoToP8IdCdAJ4hEIojFYiwyTy3v6mHbNkqlEvL5PCPDEwQByWQS6XQasVisb4abs7Wcs2a/W1DbvXA4zBwRxWKxp8h4o6h7fSYCdQnQNM0TOfwGv649gQG/TGGaJvbt27ese6pTFN6rnpgBqti+JoF3PXcnfvfXz8Sv3/0MvOM5OxYQ35VNC+//0YP4518+BjOoo24KpWTgM7/Zx17HQwLe+4Jdbf/uNedX0+htG/jRPRMt3u0ev354Crc1aEX2qV893vcUtpWw7jSCKIr48f1T7HVUEvDyszfAsqye22cSnP3gbRu4+/DSR+E/+avH4VSZv33RU8Dz7mWtd2K1M+BXqv4E6D8C3QnQKWKxGGzbhmEYmJycZA5Z27ah6zoURUE2m2WM8JSCn0gkIEnSohhsRKhXKBQ80W2e5xGLxZDJZBAKhZghr2laR+eDVlH3RohEIkyOlcZv5de1JzDglzGWO2mEJEksCh/UzvQHHMfh1PUpvO8Fu3Hj+56Nn73j6bj8jNq67a/cfABv+ebdyGtLHxH0I/7j9/sx60gN/r/P2YG1yfYtZC7cOlhD8vWDu4/2TDin6Sb+6ZePNfzd3Yfncd1jJ3r6fDdY7utOIxzLlXD7oWpJyUv3jCAVkWCapmfRl/NPGYTTNl7qNPo/jM3g5n1Vwr7nPmUtLt4+1OIvFmIwHsJopkoI5YbIbiXqT4DFQaA7ATqBIAiIRqPQNA2madakyOfzeRiGgUgkgnQ6jVQqhXA4vCRRVq9bywHVUoJ0Ol3D0l8qldp+h5uoeyMQv4CiKCvuWfWjPN4xMqww0MMOgNV5WJZVo/jNrvM8D47jml6v9+LQAbFeQZpdFwSB9V+sH2N9X8ZOx77YMoXDYdZbMxQKNRz7cpOJ4Jwnv8h02kgCn3/1mVgjlvHN++fZ7294/ARe+R+34iuvOxebB2PLSqZ+ztPRORVf/eNBdn00E8Wbnr7V9dhfdd4oi95PzBfxx/0zuGT7UNcyffmmcUzMV1PxX75nA3758HHGjP/pXz+OZ+0cQkgS+zJPpmmy//tpnnrVve/fXZsd8ZrzN8I0TRiGgXA47IlM8ZCAp6xP4JHjlY4Qtz+ZRbEUz5NlA//scAQJPIcPvvBUAOh4nk4bSWLyyfIQ6gXfTqZu91Y/rhErcd3zo0ymabJ/V4pMbq4HMlXHSMTHzvrrdjJRJN2yLMiyDJ7nIUkSI5Fbaplo7NFoFIqiQNM0hMNhT+cpGo0iFAqhVCpBURTwPM8i5k6ZTNOEpmmsjCCVSkEQhI5kisVikGUZ+XyeGfTOsRuGgVKpBMMwIEkSK6ftVKbFnqde9q1OZXLrxAkM+Cexd+9e7N27l93I8fFxxq6YTqcxMjKC6elp5HLVCMPw8DCGh4cxOTlZQ0Kxfv16ZDIZHDp0qCY9fOPGjUgkEhgfH69Rkq1bt0IURYyNVfvwAsDOnTthGAYOHqwaEDzPY9euXSgUCpibm8P+/fvB8zxCoRC2bduGXC6HqalqKmg8HsemTZswNzeH2dlZdt1PMgGVaHy5XMbk5CS7tpxlonlSVRUTE1XjwA8yGYaB52+0sDmzFv92yyyKTxJYjZ1Q8PIv/hF/9+z1eNUzz1pWMvVrnj5x7UM1LeDeeuEaRCQBs7OzrmS6bGsKn+U5VqLwX79/DGutdV3JNKMa+I8b97NrA1EBH3/5aUhFBHzrjgqXwdgJBV/+zX34q5dc0Jd5siyL9a710zz1IpNh2TUkg1sHQogVT2DfvhNIJpOIxWKeybR7QMAjxyvXHprMQS0ZsHVt0Z+nGw4oeOx4nl17wY4kRuKVyFOnMq2PVDN3Dp8sQNZ08EapoUyyLNfsWythjViJ654fZbIsi+nO7t27V4RMK3Ge+inTiRMn2Gd0IhM5Y+PxOI4dO1ZDnrzUMjnnKR6Pg+M4zM3NIZ+vrs9ezlMoFML4+Di7xnEcdu3aBU3TcPjwYXaN53kMDAxAUZSOZMrlcpiZmYFt2+A4DplMBuvXr8exY8dYOj4AZDIZCIKAiYkJlEollvkwMjLiS91z7lv9fp6GhtxlwnF2kLtcA1mWkU6nMTc3h1SqQtDjR++lZVkolUrMu7icPbLU+iIWi9UwhC5nmQD/es6p/kuSJDwxreIt37wbx3Iae6/Ic/iHl5+Oqy7cvGxkcjMfnc7TvUeyeNWXbmPXztuSwQ/e+lT2/LmV6a3fuoeltodEHrd/6DkYiIc7lund37sf1zxwjF379J+eiVefvwmzSgnP/syNUMuVv1mfjuDG9z0bYZH3fJ5IdyKRCGzb9sU89SrTbx6Zxv/97n3sPR958al4w9NOgWEYUBQF6XSafUevMv3i/qN4x/cfYte/+cYL8Yydw4v6PHEchxd+4Q/YN13JBIiFBNzwnmdiXTralUzXPzqFN3/rXnb9e299Ki7aOth07I32reW6RqzEdc+vMjn3LSKVXO4yubkeyFS5bpomstksbNtmKd2d7FuUTdVs3/LDPAGAqqowDIMZtPR+juMajp3kazZ/zcZOa7FlWSwCLkkSEokEe756kalYLKJQKCAUCrH3iqKIcDjM5o7neZRKJWiaBl3XwfOVzIBIJMLkqpdpqXSvXC53vW91+jwpioJMJoNcLsfs0EYIIvBNIAjCAuZhUoh6dHq9GaNxJ9c5jkMoFGIK4Lze6P1ejb0fMgmCAF3XWeqQUx5gecpEaDb2pZSJPKM8z+P00TSuecfT8bZv3Y17j2QBVCKSf/eThzE2reDvX/wUiELtZ/lRpl6v14/dsmx84ue1rdk+8pLT2Xs6GeNrLtjMDPiyYeFnD07hDU87peXn1I/x7kNzNcb7nk0ZvOq8TeA4DmuSEbz1mdvxuesqqfpTOQ3fvO0Q3vrM7Z7PE+mO2/e3kqmb6/3QPWf6fFjk8LKz1kMQBHaIakVi16lMT90+DA4AbeF3HpzDM3etWdTn6Y9js8x4B4A3PO0UrM9UCS47lemsTQM1rx+ezOGp24Y62reW4xrR7nogk7cyOfct0p3lLpPb64FM1d+RIUtj82rf8ss8JRIJluVWj07jrfTcNPveUCgEwzCgaRq7r8ViEaIoMqOe5/mOZTUMA5ZlsVbR6XQa0Wi04fvD4TDC4XBN+n6pVEIoFEIkElnwN0s1T/3ct+q/s9l5Y8HnuXpXAN/BsiyWVrYSEI1GmUcwQH9RrztrkmH8z1ufileeO1rzvm/cegh/8Y27fNHuarHx0/sn8cBENTXqleeMYs+mTFef9Zzda7AmGWavO+0Jb1k2/uFntc6Ej770NDjZwt/8jK0YToTY672/H0eu6P28rbR15+hcATePVYncnn/qMGJS5b5Sra3bzdQNBuJh7FhTNZbvOLiwm0C/8fVbqimFIs/h9Rdv6enz1qYiGE5U9bsVE/1K058Ai4dAd1Y3nPXHnbKcLyfd4flKHXmjn3g83vFPIpFo+UMp7sPDw0gkEhBFkWXF5nI5ZLNZ5PN5FItF6Lre1IlgWRYjCFSUioN4cHAQsVjMFRkstfvLZDKIRCKsn30+n68peVgK+FV/AgM+gC8gCAJreRFUdSw+wqKAf3v1HvzN5afCaa/8YWwWr/iPW3BgRmn+xysMhbKBT//6CfY6Kgl4/wt3d/15osDjT8/dyF4/elxmZF9u8KN7JvDQZK0z4dzNtVHPeFjEOy/byV7nijq+fNM4ArTGD+4+Cudyc8V5o+xwaJqmJ/3fneB5HudtqqbEPXA0B01fvNY0B2dVXP94tVPB5WeOYCQdbfEX7uDsB/+ICyb6AAECBOgElmWxDJ6lNuhWIgRBQDgcZsz1AwMDSCaTLJ1d0zTk83nMz89DlmUUCgWUy2WUy2Xk83lks1lomgZJkpBKpZBKpRCJRJBMJmFZVk1NeCvwPI9oNIpMJoN4PA7LspDP5yHLsisG/dWEwIAP4BtEo1HYtt00fShAf8FxHN72rO346uvPRzxUNVwOzKr4k7234A+OSOVKxpdvOoApuaqDb3vWtp6NnCvO31jz2m0UXtZ0fPo3j7PXsZCAD15+asP3vvaCzdg8WI3u/tctBzEtB89SMximhR/cXZ2HXesSOP+UQcYT0Q8DnuM4nOsw4MumhfueLF1ZDPz3rYdqXr/xklM8+dwzHP3g959QUCz7q19ugAABljcoiitJEqvhDtA/cBzH2PqTySQymQxSqRRisUqXonK5DEVRWMs46j0fj8drWOVFUUQsFkO5XO7obM9xHMLhMFKpFCMUp6wAVVVRKBRQKBRQLBZZ2n2pVEK5XIau69B1HYZhsI5ijfgGljsCAz6Ab0AeQE3TgsV5CXHZU9bhf//vJdg0WDVaZc3An3/9Lvz3rYcWdRG0bRuabqLX/umNYFo21JKBk0oJk9kixmcU3HVoDl++uRq5Xp+K4K3P3Nbzd21bk8CFpwyy1z+9f9JV5PWLN+zHrFJlLf3L5+zAulTjHvQhkcf7XlDNFNB0C5+/bqzhewMAv39iBtNytWTnygs3Q5IkRnhkWZbnBjwAnL8lU/N6sfrBy5qOHzocFmdvyuCcukyObuGMwFs28NhU8zT6AAECBOgU5FAl47DTNPoAvYHjOIiiiEgkwlLvyahPp9OIRCJNy82ozr1QKHQ8b5R1kUgkEIlEoOs6VFWFpmnMKVAsFqGqKlRVhaIoyOfzLGqfy+VYKcD8/PyKMuIDErtlCp7nsXPnzrZ1JcsN0WiUedKi0d5TOwMshBvd2b0+iWv+8un4P9++hxkYpmXjo9c+gp/eP4k9GzM4bSSF0zaksGtdEiGxdz20LBsHZlU8PJnDQ5M5PDyZw6PHZORLlQWf5wBJ4J/84SAKPEICD1HgIAk8RJ5DSKz8Kwk8BJ5D2bCgGSY03YKmV/4t6SY0w2S901vhg5fvRizkzTJ5xQWbcOehyr3MawZ+/fAU/uSc0abvPzCj1NQrbx6M4U1P39ryO15y5gi+fNM4q0P+wd1H8eZnbMX2NQkPJFj8dWdsOo+3fPNuyJqBz7zqLFz2lHWeffb/3HmE/T8s8njFOaPMYCcujn4Y8GuSEWwdiuHgyUpP40od/M7Wf+QBfnDXUdapAADe2EaXOsHpjgg8ADwymVtQ5gGs3H0rQP8R6M7qhmmakCSJkarpus76mLdDoDv9ATHJuwHVwiuKglQq1fbvnCR4FEkHUMNqz3Ec62dPLPJORv5G/++G08av+hMY8MsYhmG4XsCWC6iNBDHS++2BWSlwozuD8RC+/aaL8JFrHsb3HCnf9x3J1qT9SgKHnWuTOH1DxaA/fUMaTxlJIhmRGnxqBaZl48CMgoeeNNYfmZTxyLFcjYFRD8sGSoaFkrE42Rl7Nqbx8j3NDexO8aIz1+Nj1z4C5UmHxPfvOtrSgP/HXzxW42T4uxc/BRGptUHJ8xw+dPmpeN3X7gRQuc//+psn8P/92XkeSFDBYq07hmnhXd+7H4eeNHTf+T/34VfveiY2D8Xa/GV7TGaLuPGJai34i88cQSZWkUkURdartR8GPMdxOH9zihnw9x6ZR9mwPHGCNYNp2fiGI31+fSqCy89Y79nnbxyIIh2VGHHiw5PNI/Arcd8KsDgIdGd1gtKfaT0morVOEOjO0oLjOCQSCciyDEVRkEwma4xpynwjg53ml8omotEoaz9HLSXL5TJLpZckCeFwmDl5vIYf9Scw4JcpLMvCwYMHsXPnzr4cMpcSkUiE9YaMxXo/rAeoRSe6ExJ5fPKVZ2LXuiT+8RePolEmu27aePS4jEePy8A91etbhmI43WHQz6k6HqbI+nEZBR/XycZDAv7pFWfWML33ilhIxEv3bGCR39sOnMThkyq2DMUXvPf3T5zADQ6ysafvGMbzT3MXfX7GzjW4ZMcQbtlfYTj/1cNTuO/IvCfp0ou57nzr9sMVnXoSatnEe35wP77/tosh9DgvP7jraI0uX3nRZvZ/Ohx6zUBP4Hke52xM4Yf3TQGolDo8NJnFeVsG2/xl97jusWlMzBfZ69ddvAWS4N0hh+M4nDGaYjr3cBMiu5W8bwXoLwLdWb2gkkoyzERRZD3M3Rhrge74AzzPIx6PM1Z7IiQkwx2o1t5TLX2rdqShUAiWZTEyPUVRWO18KBSqqcXvBX7Vn8CAD+A7UBS+WCwiEokEUfglBsdxeOPTt+K8LQP44T1H8cgxGY8fz6PYpob78MkCDp8s4JcPTXX8nSGBx6kjSZy+IY1ThmKwbEA3LRimhbJpwzAt6KYF3bKhGxYMy0b5yd/rpg3dtGBaNkIij4goICLxiEgCIpKAsETXnNer18ISj9NH0kjHmmcQdIvXXLCpJnX7h3dP1NStA5Ve8c4e9ALP4SMvPa0jY/KDLzwVL/viLez1v/zqcXzvrU/ti0HaD5yQNXz2t/sWXL/78Dy+cvMBvP3Z27v+7Hryuh1rEzh/S9W5QZt+vzbqigFfW9Jwx8G5vhrw//XHailGWORx1YWbW7y7O5yxIc0M+H3T+b5nFQSoYP+JPL580wGcMZrG6y/esmye8QAB3ILSp50ReMCfUdEArSFJEmKxGAqFAus/T0R3zv7zbkH2QiQSgWEYKJfLLABIvFqUYr/SEBjwAXwJIrNTVRWhUAiCIHRUbxPAe+zZlGG90E3LxsFZFY8cq9SpP3pcxiPHZMyp5dYf0gAhkcdTRlI4czSFM0fTOH1D2rO6er9hz8Y0dq9L4onpPIBKi7i/ft6umojyN287hAMz1ZYrr3vqFuxal+zoe87amMGLzxrBLx48DqBiIN64bwbP2b3WAyn6j3/+5WOM+wAAOA6s3dtnf/cEnr17DZ4ykmry161x074ZHM9V2XCvvHBzjdGzGAb8umQYmwejODJXiYrfcWAO//fZffk6PHIshzscRHmvOGcUA3HvD72nbajOh27a2Dedxxmj6RZ/EaBXzKllvPpLt2G+oOOH90xAFDhcfdGWpR5WgACeghjoaZ2m82BgwC9PUGCOjHevnI7kAIhGo9B1HaVSiaXYU8RekqQV4+QMDPhljJVszPI8z7x09T0/ncY8/QQGfmfo9T4JPIcdaxPYsTaBl59dqeO2bRtTsoZHJsmgz+GRY3JN6m5E4nHayJOG+mgaZ46msWNtwtN0Xj+D4zhcccEmFmGfkjXcvG8Gzzm1YljPKiV8wcEcPxCT8O7ndkdw9r7n78ZvHp6C8WSu+Kd//QSetXNNz2UB/X7Gbh2fxU/vP8ZenzGawuueugUf/PFDACrG4V9//35c845LEBY7N7KdGRAhkccr63gIyKPfr4Mh3b8LTxnEkblJAMA9h+dhmBbEPjwH37jlUM3rv7jEO/I6J+qN9UeO5Roa8MEa7R0+85vHMV+o7o+fv24Mrzhn1DPiTb8h0J3ViUYtPTutgw90x1/op+OlUYp9qVSCqqrIZDJdfaYf9WdlrvKrAIIgYNeuXUs9jL6CWk9Qz0/nj2marNWTsy0Ex3HMkI/FYr6qV/EL+qU7HMdhJB3FSDqK5zrqtXNFHftP5JEIS9i+Jt4XI2U54RXnjOJfflUlqPv+XUeZAf+vv3miJvL8nufvZuRqnWLrcByvvXATvn17xWB97LiMax841pI4rx36ve6UDQsfueYR9prjgE+8/AycvSmD6x87gd8+Og0AeHwqj8/+bh/+5vKndPT5x3PFGm6By89Y3zAa3U/uDToInL8lgx/dWzHglZKBR4/LOGtjxtPvmlVKuMbhDLlkxxB2r+8sm8Mttg7FEQ8JjIjy4UkZr7mg9j2rYd9aLNx/NFtDLgoAM/kSvn7LIfzlc3Ys0aj6h0B3Vi9M01xg8ImiiEKh4IpZPNCd1Qtnij0x13cKv+rP6j5JL2PYtg1FUVZUT8Nm4HkeoigiFAohEokgFoshmUwinU5jYGCA9aJMJBKIRqOQJAmmaUKW5QXR+wCLrzvpqITztgxi9/rkqjfegQq7//NPqzKAX/fYNGaVEh6ezOH7jtrsU9cnceUFm3r6rndethNRB3P9v/72CZSM7skD+607/3XLQew/obDXr71gE87ZPACO4/DJV56J4UT1EPeVmw/grkOd9VD/wV0TteR1fagFbwc6QJy/uTY63Y9+8N+94wjKZrVrwxv7FH0HKh0QnGn0jYjsVtO+1U+Ylo2PXPMwGt3GL904jvkuSpn8jkB3VieoZVh9MEaSKhw1bqLwge4EALqPovtVf4LT9DKFZVmYmJhg7JyrGY0M/FQqBUEQkM/nWU/nABUEurP0uMJhmBuWjf+9dwIfu/aRmgP5R156Ws8Oj7XJCN78jKrRNjFfxHfvONLiL1qjn7ozmS0uKB/4wAtOZa+HEmF88pVnsde2DbznB/eztnztYFo2vn9XVfZtw3FctLV/xHHNQFlCI6kQRtIRdv32A94a8GXDwrduP8xenzIU6zsHgrMf/GPHZRhmrZ4Ea483+N5dR/DgRNVBssGhR/mSgS/dNL4Uw+orAt1ZnahnoCcIggCO41wZ8IHuBOgFftWfIIW+CUzTZMyXdOBqlq5df53IGZpdp891XgewQDmaXRcEgXkl68dI19uN0c8y1Y+lW5mohl5RFFiWhXA4vOxlanfdjUymaTLdWSkyuRm7n2S6ZPsQNqQjOPYkmdrnrxuraav3wtPX4aJTBphsvcj0pktOwbdvP8xqZf/fDfvxirM3IBkROx67aZrs/17P08d/9khNZ4MPvGA3UhGhhoH4uU9Zi1efN4of3lNJPT86V8Q//vxRfPKVZ7adp98/McPuNwC85oKN7G+WQvdM08RFWwdZvf9dh+ag6wYEgfdE937+wDHM5KvOy9dfvAW2bcG2+yfTaSPV9HxNtzA2LWPXuuSC56nbvXU1rRHNZMoWDXzmN0+w1/GQgO+95SK84Rt34+BshfzyG7cewuueuhkj6ciykMnNPLnZt5abTG6ur3aZnO3FnN9LBGjlcrkmvb7bfSuYp0CmVjL1sm91KpPbSH9gwD+JvXv3Yu/evexGjo+PI5GotPpJp9MYGRnB9PQ0crmq13t4eBjDw8OYnJyEqlZZo9evX49MJoNDhw6hXK6msm3cuBGJRALj4+M1SrJ161aIooixsWr0CQB27twJwzBw8GC1BRDP89i1axcKhQLm5uawf/9+8DyPUCiEbdu2IZfLYWqq2rYrHo9j06ZNmJubw+zsLLvuR5lUVcXExAS77pVM6XQaHMdhamqKta1Y7jL1Mk+GYTDd2b59+4qQaTnO06vO34R/v74yRqfxHhJ5XHVatGb8vcp0xekpfPmuSouvObWMz1x7D153zmDHMlmWBU2rGMFeztPdEwX85pFp9rtzNqWxJ1Vg8jpleu3uEG5+QsS0Uom8fO+uo7h4cwK7k9XvbCTT135/nP1eEjicnS6xz19s3bOsSv/iC7cOMQM+V9Rx3d2PYPe6ZM+6Z9s2/vPm6v2MSTzOTmsYGxvr6/OUNmtLlq6/bwzc9iR7nmRZrtm3gjWiKtPExATrY8xxXFOZvvpAAVkHcd2VZ2VQmJ3Auy/dhnf9oEL0WDIs/ONP7sG7L1nru3Wv23myLIvpzu7du1eETE6slHnyWqZIJIJ0Oo1sNrtApoGBARw/fhzHjh1j57pGMlmWxeTwg0wrcZ5WukzOfavfMg0NDcENONtvSf1LDFmWkU6nMTc3h1SqUs/nRw+SaZo4ePAgtmzZwj53NXrF3Mqk6zoURYEgCIjH4+B5ftnL1O08WZaFw4cPY8uWLaxl1nKXyc3Y/SbTxHwRz/zM71G/Av/VpTvw7stqSah6lalkWHje5/6AyWylI0AsJOD3730mhhPhjsZuWRaOHDmCrVsraflezFOxpOOF/34LjswVKtc54GfveDpOXV/bK90p050H53DV1+5k924oEcKv3vl0DD1JSFc/9qmchmf+600wnyyAf8lZI/jCa/a0lLUXmdrNU7FYhK7rmDMkXPZvN7H3ffQlT8EbnnZKz7p39+F5vOYrd7D3vPGSLfi7Fz2lrzIBgGFaOOvj16FkWDXfS2Nstm8FawQPRVGgaRoSiQQkSWoo04MTObzyS7cxvd+5NoGfveNpkITKe1++9xY8NClXZOGAX7/r6di5LrVkMnk5T272reUmk5vrq10mIqpLJBILxm5ZFnK5HBKJBNOJbvetYJ4CmZpdNwwDhw4d6nrf6lQmRVGQyWSQy+WYHdoIQQS+CQRBWECaQQpRj06vN2NG7+S6IAjYsWMh0yzHcQ3f79XY+ylTs7F7IVM4HGaHpEKhgEQiwTy2y1Wmbq6TXtfrznKXqdfrSyHTpsEYLtk+jD/ur3qr16ciePuzt3suU0wQ8N7n78J7fvAAgErEf++NB/Dxl5/R0dgFQcD27dsbvq/d5zQb+1f+eIgZ7wDw+otPwelN+oeTTBfvWIO3PGMbvnLzAQDASaWMj1zzKP6/PzuXPdfOsfzvfceY8Q4AV1202ZN57XaeRFGEruvYNhzHcCKMWaWS6n734Sz+4ul8jazdjPG/b6vWvnMc8OdP2+Z6P+tF9wRBwKnrk3jgyfrsR47lF/y+0b7VSqZ903m86b/vQqFk4l+v2IPn7F67ItcISg23rFrCLnq/adn46M8erXH4/cPLT0ckJLHXH3zhU/BnX6s4biwb+Pz1+/EfV5+3ZDJ5OU9u963lJJPb66tZJsuyIIpiw7E4DaZW65ubfSuYp0CmZtdFUex43+rkev13Os8wrdD40wL4HrZtI5vNuq6VCFBhLSVvlizLHfUQXUkIdMc/uKKOZf5vXnRq33o4v/zsUZzqaCH23TuO4NCs2uIvFsJr3Tl8UsV/3Fgl3FqTDOM9z3fXruU9z9uF3euq8vz6kSn875Nt2ZwwLbum3dYpQzFcvM1dilq/QN5+27ZriPTuOHiypy4BADAxX8CvH66mAT73Keuweah/bfHq4XS+PHpMhuVwnHSqP7Zt430/fABH54o4qZbxV9+9D4dPdqazywHETcJxzUm5vn/X0RriupecNYKnbR+uec/Tdw7jkh1V3f7lQ1N44Gi2L2NebAT71upEI+OcwHGcq37wge4E6AV+1Z/AgF+msCwLU1NTC9JDArSGIAhIJpMQBAGyLK9KhvpAd/yDF585ghefOYKQwOPKCzfjZXs29O27BJ7DB19YZXU3LBsv/eIf8clfPobjuaKrz/BSd2zbxseufQRlo/pZf/eipyAVkVr8VRURScBnX7MHklD1Vn/s2kdYmQDhD2MzNdeuvHCzaw93v0Dfb9s2LnQY8LNKGX/+X3chV+y+/eW3bjtc0yqvn63jGuEMBxN9vmTUZFd0qj83PH6ixmhVSgbe9b37oZsra+0ioq5IJALDMBYcFOfVMj79m8fZ61hIwN+/+LSGn+Xs3ACg5u+WM4J9a/WByOeaRS4BMAO+lXEV6E6AXuBX/QkM+ACrDjzPI5msECupqopi0Z3xEiCA1xB4DnuvPhePfeKF+OQrz+y7Yfns3WtqDMa8ZuDLNx/AMz71e7zre/fh4cmFvbv7hd88Mo3fPzHDXj912yBefnZnDozTN6Tx7udWI/b5koH3/eCBmqjv/9x5hP1fEjj86Xkbexi1N3DW511+5nokwtWsi9sOnMQVX7oNx7Kdr0uFslEj76nrk3jqtsEWf+E9zhitrdlr1A/eDWzbxueu27fg+v1HszXtBlcCyuUyJElq2tv60795ooa47l2X7cR6R+s4J/ZsyuBFZ65nr2/ZfxJ/HJtt+N4AAfwMMpiaReCBigFv2/aCOuIAAVY6AgM+wKoEx3FIJBKIRqMoFotQFMV36TEBVg8EfnEiwhzH4TOvOgubB2tTqg3LxjX3H8NL/t8f8dqv3IbrHp2uMYK9RqFs4OM/e4S9FnkOn3j5GV05MP7Ps7bjvC0D7PVtB07i67ceAgCckDVc99gJ9rvnn74ew4lw9wP3CE4Dfm0ygq++4XzW1g8AnpjO45X/cSsen5I7+twf3zsJWasaf298+tZFzzbYtS4J0aHPD092JgPhd49ON/3bvTfux+0HTnb1uX6DZVkwDAOSJDXsbf3A0Sy+d1fVKbNjbQJ/0Sar4r3P312zpnzq148H+1uAZYf6NqqNQOR1q7UkMsDqRWDAL1NwHId4PL7kqaDLHdFoFPF4HOVyGfl83ncpMv9/e3ceH0V9/w/8NbP3nROScISEhEsOuUFAQPDAo/WoWo9613prbWvbb39We1p7eBa12hZtq/WqWrUeICJ4cAiI3BAg4U5CSLL3OTO/P8JndjfZ3eyd2fB+Ph48NJPNZj7Z987O+3O8P7lAsXNyqy41Ydm9p+OPl06IWhPPrNnXjpv+sR4LH12JF9fuhzdii7tsxc6TH++J2pP9xjk1qB/Y81ySoeI5/OnSCTBqw6M0D3+wEw0tTry24VB08bppQ9M/6SziOE6uSgsAM2pL8Z9bT0NVxKhqs8OHS59ejS/2JDd6KooSnv88vGVOqUmb0yUZ8eg1qqjXclvECHyy8SNJEh6LGGXXqjjcf97oiO8D339lEzo9gVg/XlBY4qHRaMBxHDQajTylXhQl/Py/W6MK1/3yG6dAq0586za83IzLpoRnmmw5bMd7W5oT/ITy0efWyUcQBLlTKx62Dp69Z+I9hmKHpEup8UMJfIHieR5DhgxJ2DNJkqPT6WCxWCAIApxOZ7+fikWxQ3RqFb41eTDev3sO/nXjdMwdUd7jMfuOufGzN7fitN8txyNLd+GY05+V2NnT6sRfP90nf11p0+OuM+rTfj4AGFZmws8iErxASMT3X90UNXI5tKTvi9dFYtWTmREDLXjjtllRnSpOfwjXLlmH/27qWZyvu1UNx7D3WLjA21XTh0KviT/1NJfGVoWn0W874pBHf5ONnw+3tWD70fDo+xXThuLGObW4dFK4Q+Ko3Yf/e3NLwY8sBwKBqF1vItf0vrL+oFzRHzhRuK6uLN5TRblrQT10EYn+H5fuKujaAfS5dfJhCXxvNBpNwhF4ih2SCaXGj7LOhiRNFEW0tbWdFCPG+cAq1EuS1O9H4il2CMNxHGbXl+GFG6Zh6fdPx+VThkCriv5Y6PAE8cTHezDrdx/jR699jbU7D6QdO5Ik4f63tiEohJOuBy4YA5Mu88r7V04binkjwx0RWw87cLA9vI7829OGgM/TUoVkdE/gAaDCpsert8yMqiQeFCTc/fImPLNyb8JkdcnnTfL/a1Qcrp5RnfD3d9/zNptOiUjg290BHD0x2yKZa48oSngsYu27Vs3jtvldW/j84sJxGFZikL/33pZmvLr+YI/nKBSSJCEYDMpr34HwlOBjDg8e/iC6cF1kJ1VvKm0GXHfaMPnrxjY3Xt9wKPOT7iPpfm61uwNYu+84fMH+3THfHyWqQB+pt3XwdM9DMqHU+KEEvkBJkoS2traCH31QElahHkC/XhNPsUNiGTHQgoe/NR6f/+QM3HVGHYqN0dXgA4KI1zYcwuXPb8F5T3yGX7yzDR9ua05pGvPbXx/B6oi1y3NHlOPsUyoS/ETyOI7D7y8ZjyJjzyr2ap7DtxRQvC5SrAQeAKx6DZZcNw0XTRwUdfx37+/Eg29vi1oSwOxpdWHl7nBBwPPHV2GANXaRM8blcsHhcOTkOjA2Yis5AHJxxGSuPR9sa8bOZqf89ZXThmLgibYYtWo8/u1To9bYP/j2duw95srm6ecNG2nXarXyMTZl+E9Ld/coXFdpM8R6mrhunTc8qrbCYx/tLthENp3PrcOdXpz5yEpc/uwaXPP3dTHfO0SZRFGEKIpJjXqyTq940+jpnodkQqnxQwk8IRFUKhXMZjMEQejXSTwh8XTtxT4Sq3+6AL+5aCxqy0w9HrOj2Yklnzfhe//cgIm/WoZzH/8Uv3xnO5Zua4bdE/smyuEL4tf/2yF/rVXz+MU3TsnqurIBVj1+c+G4HsfPHDMQAyyJE9p8i5fAA11/m0cum4Bb5w2POv7C6v247cUNPZKw579ojPr6+lnDEv5uQRAQDAYhiiI8Hk/Cx6ZjdKUVkS/r1iPJFbITRQmPLQuPvuvUPG7r9jeYMLQE319QK3/tDQq4699fwR/q28Q0JIj4+mAnnv+8Ef9csx/NETUe4gkGg+A4LmqUkeM47Gr14tUNR+Rjw8tNvRaui6XIqMUtc8N/vxaHH8+fKPB4Mnj8o9047u7qYFzX2I73tx7t4zMiyUqmAj2T7H7whPQnmc9bJKSfUavVMJvNcDqdcLvdMJvNfX1KhOSdXqPCVdOrccXUoVixqxXPfboPa/a193icJAHbjzqw/agDf/+8ERwHjKm0YkZtKWbUlmLasBLYjBo8umw3jjn98s/dOnc4hsXoHMjUeeMrsWx7Fd7aFE6ArlBI8bpIPM9DkiRIkhSzE4PjOPz4nFGosunxwNvb5L3dP9zWgiufW4O/XTsVxSYt7J4g/rMhvEZ+SnUxxg8uSvi7fT4fOI6DXq+H1+uFTqeTR7GywaRTo7bMJK/J35bk9oT/23IUu1vDo+lXz6iOOZPglnkj8GnDcaxp6ux6/iMO/GnpbvzfuclPMc+ULyhg08FOfNnYjnVN7di4vwPuiIKPD769DWeMGoArpw3F6SPKY+40EQwGodVqo15/UZTw26V7Edl1/Mtvju21cF08188ahue/aJLfe0+t2IMrpg6FLcZMlf7kYLsHb2yMrh3x5PI9OHdspaKW0pDY2HT4ZBJ4oOu+LRAo/KKWhCSLEvgCxXEcbDab4qoi9hcajQYmkwlutxsejwdGo7H3HyoQFDskFTzPYcHogVgweiC2HOrA62v3YmuLH18fsiMUY0qqJHUlVNuOOPC3z8IJ/Y6IomRDS4w9Rpez6RffGItDHV6s39+B88dXYnaShb/yKXIruUQ3qd+ZOQwDrXrc9fJX8AW7RqU2HujEJU9/gRdumIb3thyFN2JEvreRWlEUEQgEoNfrodfrEQwG4Xa7YbVas3pNGDvIJifwbC/4RNceodvad72Gjxo9jqRS8fjjpeNx/p+/QKe3a9Tt2VX7MKe+DHPqexZkzAaHL4gN+zu6EvbGdmw+ZEcgQVE4QZSwbHsLlm1vQZVNj8unDsVlUwfL0+AFQYAgCDAYoqfFv7r+IDZHbJ933vhKzMogfo1aNe5aUI/739p6oh0h/GXVXtx3zqi0nzMTPp9P3jIvFal+bj31yZ4e16ddLU4s3d6Mc8ZWpvS7Sf4JggCe55N+vdVqNXw+X8zCd3TPQzKh1PjhJJojHMXhcMBms8Fut8Nqtfb+A6Rf8/l8cgKv12d/Cq4gCPD7/dDr9YqrcElIIp5ACBv2d2DNvuNYvfc4NsdJ6GNZct1UzB81IKfnJ4gSnL4gbAaN4j54ga73vt1uh8ViiSpiFs/GAx248fkv0RGxRKHMrAXPcWg9MbpaZdNj1X3zoVbFv5awa1pRURF4nkcoFILD4YDBYOiRTGbiuVX78Jv3wksmvvzZQpRbdHEf/9+vDuPuVzbJX393Tg1+dt6YhL/j3Y1NuOPVbfLX5RYdPrh7DkrN8X9Pso67/PiyqR1rG9vxZVM7th9xINMl1DwHnDFqAK6YNhQzqq3w+7woLi6W47PTE8D8P34iv8ZGjQrLfzg35bXv3QUFEQsfWYn9x7uWS+g1PFb9aH6vdRKyLVex1t2hDg/m//GTqGKZzOhKK967a7YirwkkzOnsqoPB6hL1RhRFdHZ2wmQyQafL/P1PSF9JNg+lEfgCJYoiWlpaMHDgQEr8ckiv18vrRHmejyo2lCl2Iw10TaW0WCx5eS0pdki6ImPHqFVjTn25POLp9ocT+jX74if0Z40ZmPPkHejaH77ImL33a7ZFjsAnY9LQYvzn1tNw3ZIvcaC967rR5oqeMnrNacMSJu+SJMHn80Gr1cq/X61Ww2AwwOv1QqvVpjwyGs8pg6JvPLYdseP0+rKY157uo+8GjQrfizP6HmnRhCG4rKENr37VAgA45vTjvtc346/XTkk7QdtyyI4nP27Ash0tSHZ4o7bMhGk1JZhWU4Kpw0pwqMOLl9YdwIdbm6NG6UUJ+GhHKz7a0YqBFi0uPrUC35llQFVRVzL7hw93RXXQ3Dx7SMbJOwBoVDx+cNZI3PXvrwAAvqCIx5c34DcX9awXkUs+X1ddgHTWKqfyufX0J3ujkvfxg23YfGI7vh1HHfhoRyvOHDMw5XMg+SOKYlIdmwzP81CpVAiFQj0SeLrnIZlQavxQAl+gJEmC3W7HgAG5vxE+2RkMBoiiCJfLlfRoWSKiKMLtdiMYDEKn00Gn08HpdMLpdOYliafYIelKFDsmnRqnjyjH6SPCCf36iIR+x1EHRlVY8euLxub7tBWJ4zhwHJdSoczacjP+c+tpuPGFL+WEhNFreHx76pCEP88K13WfTaTX6xEIBOB2u2GxWLIyOnlKVXQl+m1HHJhTVxozfv771SE0Hg8X07vmtGqUJTGKrlKp8NNFo7DhoAN727q2DFy+sxX/WrMf35k5LKXz3bC/HU9+vAef7DqW8HEcB4yusEYl7N1nFgwpMWLm8FIcd/nxxsbD+Pe6A9jX5o56TIszgKc/PYC/fHYA80YOwOn1ZXhp3QH5+7VlRlwxaWDcGgmpOn9cJZ75ZC+2n1jK8vKXB3HTnFrU5KAORSxs6QbP83G3+0ok2c+tI53eqK0Fxw+24dnvTMHpf1iBQKirM+WJ5Q1YOHoAjcIrFNsSLtWRdLVaHbMSPd3zkEwoNX4ogSekFxzHwWQyRSXx6RZ8YutNJUmC2WyWR/QtFgucTqf8/HRjQQqdSafG3BHlmDsiN2uS+4NElejjKbfo8PLNM3DHS1/h452t8vFLJg3udcaBz+eDWq3ucf3iOA5GoxFOp1Ne0pMpm0GDISUGHGzvSqy3xilkFxK6RoMZo1aF752efH2EIosJv/vGSFz1wtcInBh1/fX/dmBaTSlGViSefitJElbvO44nl++J2t4wkkbFYfzgIkwdVoLpNSWYVF0MmyG5TtxSsw7fPb0WN82pwdrGdvx73QG8v6XnqPzHO1ujXksAeOD8MdCoupY4ZNppDHTVsrjvnJG4bsmXALpmPTyybDeevGJixs+dDL/fD47jYDAY4Ha7k94iLFV/WRk9+n7XGfWosOnx7alD8I/V+wEAWw7b8cmuY3mZCURSl0oF+kgajQZ+vz9nsUWIklCEE5IEjuNgNpvB8zxcLlfKIwiSJMHj8cDpdEKlUsFms0VNx1er1bBYLBAEAU6nk7avI+QkkE4CD3QVJnv2O5Nx3WnDoOY51JaZcPfC+oQ/EwqFEAqF4ibnGo0GOp0OXq83rRHSWMZGjMKzQnbdvbnxIPafSPIB4LrThqHElPzSB47jcGrNANwzv1o+5g+JuOvfX8Xd81ySJHyyqxWXPrMaVz63NmbyvnD0QPzrxunY/MDZ+M+tp+Eni0Zh/qgBSSfv3c9xRm0pHv/2RKz5vwW478zhGFYSf2r8eeMqcfrIrhHibG6NNXdEOWbUlshfv/P1kbgdK9kkSRL8fj+0Wq3cGZGLLb9aHD78+8vw6PspVVYsGN2VpN8ydzg0qnDH+OPLG+hzVqFSrUDPsI5J2k6OnAwogS9QHMehrKyMRmrziOd5uaCKy+VK+sZbEAQ4HA74fD4YDAa5I6C7yCQ+l3vQU+yQdFHsZBfHcWkl8ACgVvF48BunYPODZ2H5D+b2us+9z+cDz/MJR3MNBgM4jsva3vBjB4UT+IPtXjh8oaj4CYYEPLF8j/wYs06N786p7fE8vdFoNPjO9KGYM7xYPrarxYnfvb8z6nGSJGHptmZ8c/HnuG7Jl1i/vyPq+xzXlTy/d9cc/PXaKZhdXwaDNjs1AZhiowZXTBqI/90+Ha/cPAMXnloVtUWcWafGz84bLe9tHWtKcLo4jutRff73H+7K2vPHw5Zu6HQ68DwvF09MRTLXnmdW7pWnyQPAXQvq5cdXFRnwrcnhJSabDnbi04a2FFtC8kEQBHAcl/IoOlsH3/09k+7nliRJONThgT+UnQ5NUpiUet9DU+jjYFu8AJAvJKIoRiVV8Y6zrS/iHe8+uhGvmFG84yqVChzHobi4WF4rxM5FkqSox6d67n3ZpnjnrqQ2RU41dTgcMJvNci9xrDb5fD643W7wPA+TyQSNRiOve43VJp7n5SmGkc+f7Tax2GGP6W+vE7Upd20qKSnpd23qq9eJ53kEg8Go50m1TXp17HOMfLwgCHIHIhOrTTzPQ6/Xw+VyQa1WRxW7S+d1Gl1hjvre9qMOzKgpkT+3XlnXhIOdPvn718wcCqteJU+BTeV1MhqNeOCcWnz7+S1oc3cV93v+iyacPqIMs4eX4oNtzXjqk33Y2exEdzwHfGNCFW6bNxz1Ay3y3yxWm9J9ndhx9nqr1WpMqS7C1GHFeOCCU/DWV4dwsMODSycPxkCLFpIkQaPRwO12IxQKyTePmcbehEFWnDl6AJbt6Jqyv2r3Mfzi7a04bXgpJg4tRplFn/X3EysCyz4ru8d9sm1K9LnV6vDhpbXhGgKjKyxYMLJMjiVJknDL6cPw2vqDcnHNx5c3YNbw8IyEdNpayNe9o3YffvbWNoiShF99YwyGlBgV0aZgMCj/nnRej1Ao1ON4aWlpSm0COHz/1U14++ujGFxswH9vm4kiY/zrYX/8fKI2hTuRYuVbuWpTsoN3lMCfsHjxYixevFj+Q+7duxdmc9fNh81mQ2VlJVpaWmC3h6eblZWVoaysDIcPH4bbHS5QU1FRgaKiIjQ1NSEQCFcJHjx4MMxmM/bu3RsVJDU1NVCr1WhoCK8DBID6+nqEQiE0NjbKx3iex4gRI+ByubB9+3Z5716tVova2lrY7XY0NzfLjzeZTBgyZAja29vR1hbubVZim9xuNw4dOiQfV2qb9u3bF3UTMWLECAiCENUmjuMwaNAgOJ1OdHZ2yseSbRN7fpPJhKFDh2a1TWxGgNVqRW1tbc5fp+PHj8uvE8dxFHsF3CYWl6NGjeo3bQL67nUaOnQofD4fmpub5QQtF21iM3o4jkNlZWXCNh04cEAeHeU4LqNrhMEXPcq6fk8L1Mf3wWq1IiRKWLwi/FoaNTzmV4poaGhI+3WC34V7Zhbj/33UIn/vh69thkkt4aC950i2igMW1llw+bhiVFk1GF5uQiAQyGnsNTc3w+Vy4dixY/LITllZGRZWa+AuUwOOZjQ4mlFRUQGz2Yzjx4+jtbVVjo9sxN5lo/RYvhPytnhLvtiPJV90rQ+vLTNhbKUJNRYJo8v1GFqkgcVszkrsAUBRURFaW1vlgnbJtkmSJPlza8SIET3a9NyXx+GPGH2/ZJQRe/bsiXqdvG3NOGO4GUsbujpxNuzvwNKvm1BjDMeG0q4Rubzu/ebT4/h0bycA4Ecvr8evzqxURJuOHDkiPzbVNhUVFUGr1eLQoUPyTCJJksDzPOrr65Nu0ydHebz99VEAwKEOL576cBMuHVt8Un0+UZu62tTW1obGxkY538p1m0pLS5EM2ge+G7b/Xnt7u7z/nhJ7kEKhEHbv3o26ujp5RP5k6xXr6zYFg0G4XC7o9Xq5yB077vF45NH6yIJRqbSJFbzT6XQ99szNpE2CIGDPnj2oq6uTp9Pm6nUCuvZzDQaD4HkeVqtVHumj2Cu8NgmCgL1792LEiBHyTJJCb1Oi47luE6t5wd4XuWiTIAjo7OyEVquF0WhMqk2iKMLhcECj0cjLhtJ9nWY9vALNjq596r8xvhK3nKpHXV0dXlzThF+8F77punP+cNxzYh1/uq8T65x8dEUT/rHuCOLRqnhcNnUwbp5dg0HFhqjnSaZNkeeS6uvU2dkpJya9tQkAOjo6oNVq5c+AbMXe/725Da9EVGuPx6JXY9LQIkyuLsHEITaMH2yDWadOqq2iGN5xxWq1yvcqfr9fjnuVSpVUmxJ9brW5/Jj7x5XwBbu+HjHQjP/dMQs8z/V4nfYf9+DMxz6FcKL3YnpNCV66aVrMc1fCNSJX171jTj9m/f4T+e/Ac8Dqn8xHmVnX521qb2+HXq+HXq9P+fUQRRFOpxNGo1GOk2Q+tyLbtP2IA5c8syaq2OSM2hK8eOO0k+rzidoUnjXV0NDQI9/KVZtcLheKiopoH/h0qVSqHgU0WEB0l+rxeIU5UjnOAqX7eXIcF/Px2Tr3XLcpleN93Sb2Rna73VCpVNDr9fB6vXKl53hr3YHk2sRubFwul9wZwJLiTNvEYoc9Xy5eJ/ZBKooiLBaLPBWUFdGi2CvMNrGY6U9tSvd499/JCsCx2Vu9PZ6NTMY6/2y1iU2/NhqNUc+XqE0qlQpms1lOvhLtD9/b6zF2kA3Njq7p2tuOOsBPMsIXCOHZz8LTnS16NW46fXjGn7nsvG+bPQRfHnBgR7Mr6vt6DY+rplfj5tNrMdAav2ZArmKPdY50fy0StUmj0UAUxR6Pz/T99MA3xqDYpMWq3cews9khj8Z35/SFsHJ3G1bu7hpV4zlgVIUVU4YV45qZw1A3IP7nHNAVfwaDIaojW6PRyD8TeV69nXu8z62/f75fTt6BrrXvGk3PnRZUKhVqB1jwzVOr8MbGwwCAtY3tWL+/E9Nro0e9+vu1/N0tzXLyDnTNxli6vTVq+8W+aBO7Jmo0mqSuV92Ps/um7u+Z3j632GO9AQHff21zVPIOAOubOuAJirDoVVGPT6ZNSvl8Suc4tanrXGLlW7lqU/f7/HioiB0hGWCj416vN6pQXbb2c9dqtTCZTPD7/VkrLJUP3ZN3tt+91+vt0SNKSH8gSRJ8Ph8CgUDSa9jijRJkk8/n63EznAydTgeNRgOPx5N0e2KJ3A9+X5sbnoCAV7/cj6MnRuUB4MbZNWlVd49Fo9HAbNTjN+fXofRENXuTVoVb5w3HZz8+A/efPyZh8p5LrLhWKtvCaTQahEKhjF6DWIxaNX6yaBTeu3sONj94Nl68aTruPXME5o4oh0Uff2xHlLpqGfxj9X6c/+SneH/L0biPZe+F7vt5sxvibFQLP+7yy9vDAUD9ADPOHVuZ8Gdun18HPuIe+cmP98R/cD/FOjAivfN1/NcyX9KtQB9JrVanHVu/eW879rS6ehwPiRI+30NFD4ly0Ah8geJ5HhUVFVlJEklmDAaDXHjFarWmvUd8POzmx+12yyNpmch17ERW0WdTJIGuv5Pf74ff7++xJIAUBrruxBcMBuUkK9m9u1lPe64SeFYoLN1rhtFohMPhgNfrTfs5TqkKTwGUJOBoQIfnvgivS7Tq1bhhdk1azx2P0WhETWkQb31vCg46BZxSaYPNmJ0OgkwEg0G5kGmyIrfGysZ+8LGYdWrMqivDrLoyAIAoSthzzIWN+zuwYX8HNhzowL5j7h4/5wuKuPXFjfjhWSNw+/y6qHZJUnjruFjXi1STrHjXnr9+1ghvxHaBd5xRB55P/PcdXm7GBROq8N9NXcssPtvThg372zG5uiThz/UXO446sP2oo8fxdU3tOGr3otLWd5/PkYUN06VWq+WORzZbNZnPrY+2t+Bfa8IzgwZYdOj0BuWdDT7ZdQzn9NI5RPofpd73KOtsSNI4jkNRUVFKNwIkd4xGI2w2W9aTd0an08FoNMLn82U8Ep/L2Incx95isfSYbqTX6+Hz+WgUvkDRdSc+v98PtVoNjuOS3vorcs1cLvh8PqhUqrQTP5VKBYPBAJ/Pl/aIVuRWcgDw+KqDaHGGC/l8d04trPrsJqY837Wbh0ktYVp1kSKSd1EU5QQ+FWzKeD73tuZ5DiMGWvDtaUPxh0sn4OMfzMPG+8/E366dgtvmDce4bq/pH5fuxr2vfg1fRCIdCoUgCEKP0XdGpVJBEISkZxbEuvZ0uAP4xxdN8te15SacP74qqee7Y34dIi9jkdsZ9ndvftVz9J353+a+HYVnU98z+Yxh77HIYpy9fW61Ony47z+bo449ctmpmBGxtOKTXceyPhOGKJ9S73sogS9Qoihi3759lAidRPR6vZzEe73etJ8nV7HDkneO43ok7wxb/+7z+Xp8jygfXXdiY8mZVquVpzwnK1cJvCAICAaD8nsuXTqdDmq1Gm63O62b10qbHiUnprIDwO7WcAdkkVGD62YNy+j84tHpdFCpVGmfd7axmEg1gec4LqMpwdlSYtJiweiBuO+cUXjr9ln43um1Ud9/86vDuOqva9Hm6loa0VvnkVqt7lE0KpFY156/fdYIdyDcaXDnGXVQ9TL6ztQPtODcceHR1JW7j2HTwc6kfraQhQQxKoEfN8iGQUXhEfd3vo5f/DEfBEHIeKSze6dXb59boijhB699jXZ3uGPx5tNrMbu+DPNGlMvHmh2+mNtQkv5Nqfc9lMAXKEmSUlprSfoHvV4vr7lPNwnOReyEQiE4HI6EyTsQPQrfc99VonR03YmNbQ0TmcAn+2GfqwTe5/OB47q2rswEW7bD9pJPRSgUgtfrxcgBsafff3dOLSxZHn1nOI6DyWSSOxb7+uYrEAjELI6bjFytg0+Xiufw03NH4/eXjIdGFU6YN+zvwDf//Dm2H+5EMBiMO/oOhNc4J9sx0f3aY/cE8XzE6PuwUiMuSHL0nbnzjLqor59c3hDnkfnB9pnOpc/3HscxZ7j+xLcmD8b548MdGV8fsmP/8Z7LJfJFEISM1r8zarVangnV2+fWki+a8GlDeH37mEorfnDWCADA/FEDoh67YldrxudGCotS73sogSekwBgMBuj1eng8HjgcDrjdbni9XgQCgT65yQuFQnA6nVCpVEkV79Pr9eB5PqNZBIQoSeRa38g1y8lg29dkkyiKCAQC8jZMmVKr1fIuG4kSDFYLxOPxoLOzEw6HA36/H6dU9twKp9iowbWnDcv43BJRq9WwWq1yEt9XnYbs75LuUgY2Wt3Xo/DdXTZ1CP5543QURSxRONzpxbf+sgaf7u1ImMCzQnbpviZ/+7wRLn/473HHGfVQq1K7pR1VYcU5p1TIXy/f2Yqth+0JfiK3/H4/7HZ7TuP0jY3h/a/VPIcLJlThggnRHR99NQrPZmRkI4FPttNr+xEHHn5/p/y1XsPjiSsmQqfuOoeaMhOqS8MdkJ/sOpbxuRGSDZTAE1KAjEajvKdzKBSCz+eDy+WCw+FAR0cHOjo64HA44HK54PV64ff7EQwGsz4KFQwG5eQ90bZ5kTiOg16vlzscCClk3df6sm2Mkl0Hn4sReL/fH7P6dyYMBkPMKeksOXW73ejs7ITT6UQgEJD3kC8qKsLEYWU9nu/m04fL+4nnEkviJUmC0+nsk2sOSyTSnQ3RF+vgkzWjthT/vX0Whpeb5GOegIB7/rMTf/usMWECle7SALs3iCWfhwshDi0x4sJTUxt9Z+5cED0K/0QfjsL7/V0j48leO1Ll9AXx4bZm+ev5owagxKTFKVVW1JSFX7++qkafjQr0TDIdqb6ggLtf/ipqy7j7zx+DugHR24DOHxkehd+wvwN2b25eH0JSQQl8geJ5HoMHD1ZcVUSSP3q9HmazGTabDcXFxSgqKoLVaoXJZJJHuUVRhN/vh9vthtPpRGdnJ+x2O4qLi+Hz+aIqZ6cqEAjA6XRCrVanvG2eTqejUfgCRNednvx+f9TIO4CU1sGzEfhsJfGs+jd7j2ULm0ofCoXg9/sRCATgcrnkpJ1NmbZarbDZbDCZTHLF9bGDokfgS0xaXDOzOmvn1huVSgWr1QqO4+RzzadgMBh3j+FkKGUdfDzVpSa8cdsszKkPd9RIAH79vx346Rtb5Cre3anV6qQL2UVee57/vAlOX8To+/y6lEffmVOqbFg4eqD89dLtLdh+pGeF9lxjHYE8z8tLcrLt/a3N8AXDr8UlkwYB6IqvCyKm0e9qcWJXH6z1zkYFeiay0yve59Zv39uBhogt484cMxBXThva47nmjgyvgxdoO7mTjlLve5R1NiRpHMfBbDYrrioi6TssiWB705vNZlitVhQVFaG4uBhWqxVmsxlGoxEmk0kePWej9V6vN+kp+OzmXaPRpBWHHMfBYDAgGAzm/WaapI+uO9HY2jitVhv1N2GJSTJJObspyNY0ejbTJpuj74xarYZarYbD4ZCnpOv1evk6YzQa5Ur8kYaWGDG4OFwo69a5w2HKw+h7JJ7nYbFYoFar4XQ65dHOfEhn+7julLYOvjubQYMl103F5ZOit9l6+cuDuObva9Hh7pmUqlSqpNd9s2uPyx/C3z7bJx8fXGzARScS0XTd1W0U/s8r8j8KzzoCDQaDnMxnW+T0eZtBE7W+u/s0+nc3538avSiK8tKKTEV2esX63Fq+owX/WL1f/nqARYeHLxkf8z06s7YUOnX4nFbspHXwJxOl3vdQAl+gBEHA7t27qRAYSQr7MGNFto4cOQKLxQKbzQaj0QiO4+Dz+eBwOOQRtXiF5vx+P1wuF7RabUYXNa1WC5VKRaPwBYSuO9FYYZvuyTJb65xM51S294L3+Xxyop0NrJOCTZFns3b0ej1sNhsMBkOvv4vjODx5xUScOXoArj61BNfN7DnKlQ88z8NsNkOr1cLtdudlNwxBECAIQsbFBNk6eEW/9yQRP144DD8/b1RUNfg1+9px0VOfY0/EaCcQnuacTJvYtef5zxvhiBh9v31+HTRpjr4z4wcXYX7EKOt7W5rzOgLNalbodDq5MzDbo/CHOjxYs69d/vqCCZXyOm+gqyr/qAqL/PU7Xx/Je2dRNirQR2IJfCgUivrcanX6cN/r0VvG/emyCVG7ZUTSa1SYOTxiO7ndtJ3cyUSp9z2UwBewvq6qSwoXix2VSgW9Xi+vVbVardDr9ZAkCR6PB3a7HZ2dnXC5XPD7/fD5fHC73dBqtTCZTBn1SEZOyc3VlEGSfXTdCQsEAlCr1T2mRvM8D5VKldSUZ3bDmo2/K7tZzXTrOLb0hi27cblcCIVC8hR5s9kMv9+f0g3NxKHFeObqSbj61GLwSW71lQtsNIUVAvV4PDm9GWedOOkWsGPYlGAlz1jy+XzgeR7Xz67FkuumwqIPd+w0Hffgoqc+x2cR1b7ZsoJklwZ0jb43yV8PKjLgkkmDs3Ludy6oj/r6zyvyty98ZEcgx3HQaDRZ/0x8q9ve7xfH+LtFjsI3Hfdg6+H8LiXIVgV6JrLTi11fRVHCj17bjOMRM0Juml2DOfXl8Z4GQPQ6+GNOP7b1wTIL0neUeN9DCTwhBEB4lN5gMMBqtaK4uBgWiwVarRaCIMDtdsPj8UCn02WcvDMajQZqtRper5d6tElBYXu/x5uqrtFokh6B5zguKzcILIFKJ1kUBAFer1eehcOK1bGRdjZbR6PRwGg0KmqP9XSwQqCsUzJX7cjG9HlA+evgu+98cPqIcrx526yoCt5OXwjXLlmHJ5Y3YNsROwRRgkqlSroj6N2ddnRGFBC7dd5waNXZuY2dNLQ4ag3/u5uP9JgxkCt+vx8ajUbuzGOfudka8ZMkCW9sDCfwNWUmTBxS1ONx3bfheyeP0+hZop3tBB6ILmT3wuomrNwdriQ/utKKH50zstfnmjcyOsGPfA5C+gIl8ISQmNhIgNFohM1miyqSl821QGyPaRqFJ4XE7/fL75FY1Go1RFFM6iY8G5XoU906LnLLN7vdDrvdLu8dbzKZ5Pc7qz4fiT2G7YChRCwhSNSJwgqBsoKc2R5lYZ08mY6+MyyBV2KnCaspELlUoG6AGW/dNgvTa0rkY4Io4ZFlu3HeE59h/IMf4rsvbsaTK/bhk52tCat7u/0h/Gdrp/x1hVWPS6dkZ/SduStiFF6SgMV5GIXvvosFALnDJ1ufiZsOdmJfW3hv94snDop5jRhaasSEiMT+3a+PQBTzE2uRswKzpXun165mJx6K2DJOp+bxxLdPjVpKEE91qQm1EZX6aR086Wv5rSJDsobnedTU1CiuKiJRvnRjJ1vFZbpja/O9Xm+PYmCZkCRJcUVHCh1dd8LYVmnx/hZs9CcYDPZ6U5qNBJ4l38kWr/N6vfLPaLVaGAyGlEaK2Wwdr9crz6TpTS7jh+2THvmPJbpWqzXu+Wm1WlgsFrhcLjidzpR31EiEJQ7ZSuA1Gg28Xi8EQchajYNsSLTzQbFJi3/eOB33v7UVr6w/GPU9d0DA6sYOrG7swHOrD4PjgPoBZkyuLsakocWYXF2MmrKuDuN/f3kIdn/4PXLrvOFJJV6pmDqsBDNrS7F633EAwH83HcZdC+qjtljrTpIkuPwhHHcF0Obyo80VwHG3H6UmHc4aM7DX5SKseF1kjEROozcYDAl+OjmRo+8AcOHE+EX/Lhhfia8PdgIAjth92HigA1OGlcR9fLZkswJ9JI1GA5/Ph8rBQ3Hpc19G7Yjw/84fg/qBlgQ/HW3uyHK5I2TjgQ7YPUHYjNl5bxPlUup9j3I+AUjKlPQBTgqL0mLHYDDAbrfD7/dnvH6Xrd/3+/1yVf5sdgyc7JQWO32BjZoZjca4j2G7QiS7Dj6TqdGRCVSyo+/svWYwGNJ+b+j1ernAHdumrTfZih9RFHsk7EB41E2v10OtVsPlcsm1CuLRaDSwWq1wOp1wOBywWCxZGQkMBAJQqVRZG1WMXAevpPdhbzsfaNU8fnfJOIypsuLJjxvQ5oo9sixJwO4WF3a3uPDvdV3JfrFRg8nVxdh4oFN+3ACLDpdPHZL1dgBdo/AsgRcl4KH3duDccZXh5Nzlx3F3V7LOknZ/nG3yFo2twBNXTIxbZI8ViIz1vtVqtXC5XBlPK/eHhKip8NNrSjCkJHzd6t7Rff74KvzmvR1gkzze+fpIXhJ4URTBcVzWkyQ2E+qRZXuwuyW8JGLh6AG4enpqxTTnjxyAJZ83AeiKjVUNx3pU7yf9k5Kut4zyzogkRRRFNDQ0oL6+PqtTjkj/p8TYUalU0Ol08ih8uh/ioVAIbrcboijK2/GwtftarRY6nU6RF+JCocTY6Qux9n6PRa1Ww+/39zobJNMRePY7kh19Z6PTmXZssan0bBvKRB0aQGbxIwhCVLIeOWKnVqvlLexYkstotVp5JDNRW9le8SyJN5vNGY2csyUK2dzOT6nr4JPZ+YDjOFx72jBcM7Ma+9rc2LC/A18d6MCG/R1RiVV3HZ4gPtoRPV35lrnDodfk5vozo7YE04aVYF1TV8X2pdtbsHR7S1rP9f7WZtzx0kY8ecWkmGv14+1iAURPo89kFH7FzmPo9ISXJnQv+ud2d40om81mAECFTY+pw0qwrrGr/f/bchT3nz8G6gwr/feGVaDPdke7Wq3Gp3s78M+14dkf5Qm2jEtkWk0JDBoVvMGua8+KXa2UwJ8ElHrfQ3eyhBBFMBgM8Pv98Pv9Kd+wSJIEn88Hr9cr34izCy1bX8+em3UWZNJRQE5ebNQsmbXmbPpmb1OeeZ6XEz6VSpVSXLLRdLYtYzICgUBSHRDJ6D6VPlvTxRlJkuByueS17CqVKmqEvbc2a7Va+P1+hEKhXs+N7RXPptMbjcakZzV0JwiC3EmSTWq1Gj6fTzFLhFjHiskUf5p5JI7jMLzcjOHlZlw2pWsU/UhbJzYd6MTONj82HOjApgOdcAdi144oM2txZYojp6ngOA53LajH1X9bm5Xn+3BbC+54aSP+fGXPJJ4Vr4sVw9maRh+597tOzWPRuAr568jaM5Ej/RdMqJIT+DZXAGv2tWN2RIG/XMh2ATvmQLsHv/xgX9SxP106AaXm1DvW9BoVThteiuUn1r+v2n0Moij16a4a5ORFCTwhRBF4noder4fP54u5ljIeURTlba5iTQlWqVQwGAzQ6/UIBoPw+/3y9lFsVD7bSQfpv9ioWTKJWWQV5ETJMrtxdTq79p5mU0nZP5bUR/5jgsFgr9P5I0VO280W9t5yu92w2WxZSyzZe1sQBJhMpoQ1B+JRq9XgeV6uWdAblsSza4TP54PBYEh5tkIgEJC3Scsmpa2Dj6yjkK4Ssx4zhlmxaGIxOI6DIErY1ezEhgMd2Li/a5T+QLsHah74zYVjczb6zsyqK8W54yrw3pbmqOMGjQplFi1KTTqUmbUoM+tQKv9XhzKTFmUWHQ51eHDbixvhC3bNqlm6vQW3vbgRi6+aKK/bZx0fbOQ7FjaNvrfrRzzt7gBW7ArPXjj7lApY9OH3AHvtAER1FJw7tgIPvr0NwokCdu98fSQvCXy2P4d3Njvwnb+tQ3vEDIQbZ9fg9BGJt4xLZN6oAXIC3+YKYOsRO8YPLsr0VAlJWd9f/Qkh5AS9Xi/vN59MQsLW33IcB4vFkvAGgN1karVaeZ9rv98vr1NlyTyNypNE4u39HgsbRQsGgwlrO6jVahQVFUEUxah/7CafdRpEPi9L5EVRhEqlSvrmN3L6fLZETqX3eDxJj8YmIoqiXBneYrGknayy973f74fRaEwqCWft0ev18Hq9cLvdUYl8MrK1fVx3SloHn+rOB/GwdrBOCRXPYUyVFWOqrPjOjGoAQJvTi8P7GzF29IBET5UVHMfhiW9PxK1znQiKIspMOpRZtDBqk/t7jxhowZLrpuGG57+Up1t/tKMFt/1rI566ehJ0alWvu1gA0dPo03mt3918BEEhfN24eFK4eF3ka8c+D9nrWGrWYVZdGVad2Crt/a1H8asLx2Zty77uRFGEJElZ7ezaeKAD1y/5MmpngzGVFvzo7N63jEtkXrfk/5NdxyiBJ32C7lQLFM/zqK+vp2SDpEzJsRM5Cp9o+y1JkuB2u+FyuaBWq2G1WlPqved5HgaDAUVFRXLBKq/Xi87OTjidTsWtMVUKJcdOPrBtyVIZvU526y82pV2r1UKv18NoNMJiscBms6G4uFje1s1sNsvTx5lUpthmc/p8JDbThXWKxZJs/GQreWe0Wq28RCEVKpUKZrNZLtDncrngcDh6vT6wPbyzPX0eUNY6+ERruFPBErdEbSo16zF29Mi8XXvUKh7jBtswaWgxhpYak07emZnDS7Hk+qkwRMwWWL6zFbf8cwN8gVBSRSdZ51O628n9J6L6fLlFh9l14VH0yLoZOp1OLgrJXDC+Uv5/hy+ETxtyt+95tivQf9bQhqv/ujYqeR83yIp/3jg949kbQ0qMqBsQnjUROcOB9E9Kve9R1tmQlCjhA5wUJiXHjl6vB8/z8Hq9Mb8fCoXgcDgQCARgMpky3vZJo9HAbDajqKgIRqMRoijC4XDIU3dJNCXHTq6xadGpJGYajUbe4iwTkQm+wWCQY99msyV9Pmz6fC4SS6DrvavRaODxeOIW5Usm+XU4HJAkKeH2b6lgMybSTYRYJ6HZbIYkSb1eH1hHQa5GyJWyHzyrvZDpjW2ynRKFdu2ZUVuK56+fCqM2nDSu2HUM3/3HeviCQlIdH2zGWKpt39PqkreDA4ALT62SC9GxmjFsxhl7f/j9fvnxZ51SAW1E4bp3vg5Xss+2bO4B/8HWo7jh+S/hiaihMKO2BM9fMwklpuxc9yJH4Tcd7ES7O73rCikcSrz2UAJfoERRRGNjY8Z7B5OTj9Jjh+M4eXuqyIumJEnwer1wOBwAuvZ2zuY6Xjb6b7VaYTKZEAqFYLfbEyYj2SAIAnw+n2Jfj0hKj51ciiwWl8p0YTblWQk3ALmYPt+dyWSSt3Lsrrf4CYVCcDqd8pKYbE6p1Wq1CAaDGSW9Wq22x/WB7XoRiU2fz9WIDesU6ssORlZ7IVvXYJVKlbA9hXrtmV5bihdumAZTRBL/6Z7j+MGbuxBMoilqtVqeRp+KN786FPX1xRHV59nMichlPTqdDoFAQP772gwazB0ZTlSXbW+BN05hwUxlqwL9a+sP4rYXNyIghP+wC0cPwN+umYxjRw9mLXbmjwov45Ak5HR2Aul7Sr320Br4ONgUOCC83pCt02HiHWcXonjHu39IsQ/57sER77hKpYIkSfIaychzYcd7O0clt6n7uVCbstsmQRDk2FFqm9jIFdtfmhWzYmuJ2Sg9gJy8ThqNBlqtFl6vV/5nMBig0+mgUqkyfp1YQuf1euXROo7jYDAYYDAYkj73fL9OrLI2+50n0/uJdSjp9Xq5AniybVKr1QgGgz0S53y3iRWtYnsj5+p1MhgMcLlccm2J7m2K9dkaDAbhcrnA8zzMZnNK77NkYo8liD6fT05c0o09tVoNi8WCYDAIr9crj2ayNcTsOhX5t8zmtZydD1tHne7rlEnseTwe8DwPjUaTlTap1Wp4vV6EQqGYbUrmc6uvrxHxYm/SEBuWXDcFN7ywHi5/V0x80diJm174Es9cNQkGrSphm9huFqyzpLc2hUIC3oiYPj+60oLRlVb5HD0eT9R2i6Ioyu83th0kx3E4f1wFlp3YQs8dEPDxzhacN74q69fyYDAoPzbd12nJ50349Xs7o37PNydU4uFLxoGH1OvnViptmlxdBKNWJY/yr9jZim+eOkiRsZdsm2IdV+r7qS/aFO9zKxdtSraTmRL4ExYvXozFixfLf8i9e/fK1UFtNhsqKyvR0tICu90u/0xZWRnKyspw+PBheS9NAKioqEBRURGampqiek0HDx4Ms9mMvXv3RgVJTU0N1Go1Ghoaos6pvr4eoVAIjY2N8jGe5zFixAh4PB60t7djz5494HkeWq0WtbW1sNvtaG4OV041mUwYMmQI2tvb0dbWJh9XYpvcbjcOHQr3GlObctOmUCgkx87w4cMV2yZJkuSbVI1Gg2PHjkEQBPmmIx+vUyAQQGtr1xo3tl5wyJAhcDgcabeJPV6v16OtrS3q4l1UVAS/34+WlpaobaKUEnuiKMLn8wHASfd+Yu1ua2tLuU1DhgyB2+3G4cOHo5KTvng/lZaWAkBOX6dQKCS/bziOg06nQ21tLRwOR9TnFmvTsWPHcPz4cfnxPp8vJ9dy1q5sXve8Xi9aW1vl97bBYIDNZoPL5cKBAwdSfp2SbdOxY8cgiqJ8o5rP9xOLJYvFgqKioqy0Sa/X49ixYzh27FjM654oinLsjBw5UpHXiESxZwXwywUVuP+jZnmLvM/2HMfVz36GBxdUYNzo+G0KhUJobm5GS0uLvIQnUZs+2tyEo3affPzMOisAoKWlBZ2dnfJnS3l5eVSb2PujqqoKRUVFqDP6oFNz8Ie6YvutjQdx3viqrF/LRVGERqOB1WpN+XVqbm7G4pVNePHrjqjfffG4Utx0qgFN+/bKHbBAdq57nChgwkAdVh/smmW0YmcLRFGCx6PM2MvW63QytynycyvXbWKf0b3hpL5eRKUwDocDNpsN7e3tsFq7LnpK7EEKhULYs2cPamtr5Z7Uk7FXjNqU3gj8vn37UFtbKxd+U2KbJEmSC1lFjm7FalMy555pm0KhkFxcT6VSRRUS661NgUAAPp9PXkPN2sN+B8M6LLxeL3i+q9Be5D7Bff06CYKAxsZG1NXVgeO4k+b9JAgCOjs7odPpYDAYUm6TJEmw2+3yVmh90SY2wm2z2WKOmmb7dWLT4VUqlVynItbnVuTjzGYzOI7L2bWcbSFZXFwsP08qbUr0OrHOLVYx3GKx5PRazirjs2378vl+8nq98Pv9KCoqytpMCQBob2+XZ1h1P/dkPreU+pnLSJKEz3cewe2vbZdH4oGuNdp/u3YKjFp1zDaJoojOzk5oNBp5dDxRm37w6ia5gB3PAat/cgYG2rpmdrHPVFaYsbdrxJ3//gr/O7Gdnk7NY/3/WwijJnppSCaxJ0kSOjs7YTKZYDAYUnqdAA6/eGcbXli9P+r57zyjDvcsqJO/TuZzK9Xr3r/WNOH+/26Xj791+yxMGGxTbOwl06ZCez/lq03BYBB79+7tkW/lqk0ulwtFRUWw2+1yHhoLJfDdsAS+tz8cIST3clnNOV2BQEDehzkyoetOkiQ5cWfTPvV6fVJFnwRBgMfjQTAYlKckK+lvcLLx+/3yHufprsvu7OyEVqtNer/2bPN4PAgEAlndp703gUAALpcLJpMp5lpp9ndlhSRzfV4sETIajQm39csEuxnL9v7v3bFintkq9JcslnDpdLqsx7LD4QDP8wn3Ri9k7P3Q6BBx3fMb4PSF62LMqC3B36+bGrfavdvtRjAY7PX96wmEMPXXH8mj/PNGluP566cBCMdMvPcj62jUaDTyVpAfbmvG9/65QX7Mo5dPwEUTB/f42XSxc+ptG9gePyeIuO/1zXjjq8NRx//feaNx05zarJ1fPIc7vZj1u4/lr+9eUI/vnzki57+X9H/J5qFUxK5ASZIEl8uV9FoJQphCih22hlZJWCEro9GIQCAAu90Or9cbNUrt8XjQ2dkJt9st35CyafO9Je8A5FHL7ttXpboNVrYVUuxkE1vGkUlSxtbB9wXWmZRqAb5MabVa6HQ6eDweuX4Cix+fzwe32w2tVpuX5B3oGvHQaDRpV6NPhkqlynnyzn4Px3F5j6nI7ceyTa1Wxy1k1x+uPX6/H2q1GpOHleHFm6bDqg8n62v2teO6JV/C7Y9d7DLZavRLt7XIyTsQXbzO7/eD5/m4n6lsdlhkMbt5I8thiTjPd74+2ntDU8B+TyrvGV9QwK0vboxK3nkO+P0l42Mm77mInUFFBowYGO5o+mQ3FbLrr5R67aEEvkCJoohDhw71mB5CSG8odjLHcV2V8m02G3Q6HbxeL+x2O5xOJ+x2u7zHr81mg8ViSTtx6r59ldPp7NN96k/G2BEEAaFQKOOOJI1GIxfiyrdQKARRFFMa4coWg8EgT/kWBAGHDh2Cx+OBx+OBTqeDyWTKe6dCKBQq+C0iOS7/+8GzjhetVpuTTgpWaDDWe6TQrz2CICAYDModH+MHF+HFm2bAZgi/J9c1tuO6JevgipHEq9VqeTlWIv/ZGF7ba9GpcdaYgQC6/n7J7D2v0+nkDj8A0KlVOPuUCvn7q3YfQ0cWt01jdW2S6dgGAJc/hOuXfCkX1wMAjYrDn6+chMumDon5M7mKnfkjw9XoNx/qxHGXP8Gjk7Nq9zE8ubwBRzpjb6NL8k+p1x5K4AkhJE08z8NoNMJms0GtVkOSJJhMJnlP+Wzd5EZuX8X2yaZ96vMjnb3fY2HTnPui84VVec7nVGuG53l52zU2euv1eqHX6/OevAOQOzFyOQqfL/neDz4YDEIUxZwtP2Dx2R+va2zHgMjryLjBNrx40/SoJP7Lpg6c/8Sn+O+mwxDE6PW1vW2F2Gz34fM94QJi546rhF6jivr9vc2ciDVL5YIJVfL/h0QJH2xrjvWjaWE1ZZLR4Q7gqufWYPW+4/Ixg0aFv107FeeOq8zaOSUrcps9SQJWZbid3GvrD+Kav6/Dn5btxqXPrI7ZkUMIQwk8IYRkiBXgYnvT5yIpYTdfNpsNRqMRwWAQdrtdro6eD6y4oNJ6onOFFRXMxtRzlUolF8TJJzaaptFo8p4sMxqNBnq9Xl5qYjAY+qwWAJtC3F8SeEnK337wPp8ParU6Zx1BrKhTX80wypVES1jGDrLhpe9OR5ExnMQ3Hffg7pc3YdHjq/DB1qNywt7bNPr/bjqMiJwfF08aJP9+tg1dMiPdOp0OoVBI/j2nDS9FiSnc8fDO10eSa3gSkkngJUnC6r3HcdlfVuPrQ+Gq31a9Gv+6aRpOH1Ge4KdzZ0p1Ccy68Hthxc70E/ith+342Vtb5a8Pd3rx1Io9GZ0f6d8ogS9QrDe2r27ISOGi2ClsbPp+UVER9Ho9PB4PvN7cT7cTRRFutxsqlQoOhwNOp1MeUe2v2NTzbK331Wg0eU9O2JTkvq4lwQoxarVaGAyGPj0XrVYrF8gsZGq1Om8JL0vocjX6DnRd29g0+ljfK9TPLTZzId515JQqG166aQbKzNHf393iwi3/2ogL/vwZVuxslTsBY3U+SZIUNX1+cLEBU4eVAEi9boFGowHP8/D7u6aEa1Q8Fo0NT6Nfve84Wh2ZdxyzzuB4CbzbH8I/1+zH2Y+twhXPrUFDq0v+XplZh1e+NxOTq0t6/T25ih2tmsfsujL561UNx6JmTSSrwx3ALf/agEAoumP8r5814sBxT8bnSTKj1GsPJfAFiud51NbWJr1uiBCGYqd/4DgORqMRBoMBXq83ap/5bBMEQd5+qKamBiaTCZIkwe12o7OzEy6XC4FAoN8l836/HyqVKmsjjn2xDp4tAeiL6fOROI6D1WpFXV1dn1972GyEQh+FZ69rPmZ1+Hw+eXp1LsVb11/In1vJXEfGVFnx0b2n47Z5w2HQRCe0Ww87cP3zX+KSp7/AxsPumNfabUcc2N0STnAvnjgIPM9FzSJKdqo6S1gif0/kNHpJAt7bknkxO7atVvfXdE+rCw++vQ0zfrsc97+1NapdQFcBuddumYnRlcntFJXL2JkXMY2+0xPEpoOdKf28IEq4+5VNONTRsxM+EBLx2/d2ZHqKJENKvfYo62xI0thWLv3thpnkHsVO/2IwGGAymeQtubL9urJ9uiVJgsVigdvthk6ng9VqlSvrC4IAl8slV95PtE4zGaIoIhgMypXK87lMIPIc2LTXbGE38PmcRt8X1efjUcq1hyUobISxkOVjHTx7L+j1+pzHEdv3vHsnl1JiJ1XsWpbM6HeRUYv7zhmFVffNx42za6BVR9+ibzzQiRv+uQnf/fdWrNvXFvW9NzZGb6d20Ynq88FgEIIgpDxzonsxu6nDSjDQGm7DO5uzk8ADJ4oXihKWbmvG1X9di4WPrMTzXzTB2W0NOM8B546rwH9uPQ01Zaakf08uY2deRCE7APhkV2tKP//YR7uxKqKCfU2ZCZOri+WvP9jWjNV7j8f6UZInSr32UAJfoERRRHNz80mzFpVkD8VO/6PT6WA2m+V9hrP1QRMMBuF0OuXRU47jomKH7VFvs9nk9f/sZ+x2OzweT8LpvawyM0vUHQ4HOjo60NnZCafTCY/Hg2AwKG9Dlk9yFeYsbpfF8zxUKlXeEni2BKCvp88zSrr2JLstl9LlYx28z+fLSiHHZMQr9qik2ElFrOJ1vSm36HD/+WOw6kfzcfWMoVDz0Z0m6w84cPlz63DdknXYcsiOoCDi7a/DCfykoUVygptu3QI2Y4B1cql4DueNC4/Cb9jfgUMdmU3vFgQBHZ4g/rKqEaf/fgVu/ucGfLanrcfjSk1a3D5/OD798Rl46qrJqLCl1hmRy9ipsOkxqsIif/3JruTXwS/b3oInPw6vczdqVfjLdybjF984BZH9ZL98d3taU/NJdij12tO3c+oIIYRkhVarhcVigcvlgtPphNlszmjKFxvR12g08j7diZIEdpNoMBgQCoUQCATg9/vh8/mgUqnkG1g2hZztCw5A3kZIpVLJ+62z9Z4AYLfb4fV6YTab4/7+bGOF37I9bS7X+5BHUsr0eSVi68cDgUBB/31YO9ge49nGpmAnWwAtU2x/+0KvTwBEF8FM529XYdPj1xeOw/dOH47HlzfgjY2HoorUfbLrGD7ZdQynDilCmyt8TWF7v7O6BeleN3U6nbz9o0qlwgUTKvH3zxvl7/9v81F8b+7wtJ5708FO/P3TPfhgWysCQuzkdOLQIlwzsxrnjquETp39bQuzZf6oAdjZ7AQAbDlsR6vThwGWxJ0MjW1u3PvKpqhjv//WeIwY2NUZcPmUIXj5y4MAgB1HHXjly4O4cvrQ7J88KViF+6lFCCEkikajgcVikfeLt1gsad04+nw+eDweaLXalLf64jgOGo0GGo1GrpYfCATg9XrlIlVsLW1kop7od7BifalsOZQJtvd7LjoM1Go1fD5fXtqipOnzSsN2dQgEAvJe9YWI4zgYDAb5/ZrtNeqpFkDLhnzvb58rvRWvS9aQEiP+eOkE3DpvOB7/qAHvbD6CyElWkeuutSoe54/v2lIt07oFWq0WHo8Hfr8fRqMRpw4pwpASAw62d63XfmfzkaQSeF9QwJ5WF3a3OLGrxYnVe49jc0Q1+Ug6NY9vTKjCNTOHYdxgW1rnnW/zRpTj6U/2yl+v2t2Gb00eHPfxnkAIt/xzQ9QSgZtm1+D88eEZDj84ayTe3XxU3kruj0t34bzxlVFbDpKTGyXwBYrjuD7ZQ5cUPoqd/k2tVsNqtcLpdMLhcMBisSSdKLI9un0+H/R6fY+tvlKNHTZ1VKvVRo22p0qn08Hn8+VtFJ5Ne81FwS72nMFgMKcJvNKmzwPKu/ZotVr4fD6EQqGcF2fLJdYR4fF45KUu2ZBOAbRsUKlUPWapKC12khEIBLJaBHN4uRlPXDERt80fjoff24YVu9t7PGbB6AEoMmrlugVGozHtv1lkMTvWyXXB+Co8dSJZ3XrYgX3HXKgt77omhwQRTcc92N3ixM5mJ3Y3O7G7xYmm4270NgN8cLEB35lRjcumDEGxKbvXrFzHzqTqYlj0ajh9Xcn2il2tcRN4SZLwk/9swa4Wp3xsWk0JfrxoVNTjyi063HlGHR56fycAoN0dwJPLG/D/zh+TkzaQ+JR67aEEvkDxPI8hQ4b09WmQAkSx0/+pVCp5Oj1L4nu7iWRV5dlNX6yiR5nETiYffmzrvHyMwifaszkb2JT2XI8wKnH6vNKuPWq1Wt6Wq5ATeHaDyZaadO94SxcrgGYyJV8wLBvYLBVRFOUZREqLnd5EJtDZNqrCiqeuOBUbGtvw3JqjWBlRBO3qGdUAsle3QKfTwe/3IxgMQqvV4oIJ4QQeAH717nbYDBrsanFhb6sLASG1dcKz60px/awazBs5ACo+NwlSrmNHo+Ixp74M721pBgB8uvsYQoIItarn7Lclnzfh7a+PyF8PtOqw+MpJ0MR47HWzhuGldQew/8RWcs9/0YQrpg/F8PL8LSUjyr32UBG7AiWKItra2hRXVIEoH8XOyYEl8SqVCk6nM2HhNFEU4XQ6EQgEYDab41Ys7svYYWtwc73nvdfrhSiKOd3vWqPRZFypvzdKnD6vxGsPG71WWoXhVLGCkmxGQTakWwAtU6yDLrIdSoydRFjxt1zNgNFqtRhTYcJzV5+K12+ZiZtPr8VTV03CrLqyrNYtYK8/a8+oCgvqBoQTyBW7juGtTUew46gjqeRdq+IxqsKCG2ZV462bTsXz103BgtEDc5a8A/mJnchq9A5fCF/F2E5uXWN71LZwGhWHp66ajHJL7CUWOrUK/++88Ih7SJTwm//RtnL5ptRrj3K65klKJElCW1sbiouLe38wIREodk4ePM/La+JdLhdMJlOPG0qWvIuiCIvFknAksi9jJx+j8IIgwOfzwWAw5HSUP7JyeC6SIyVOnweUee3RarXwer3yCGMh0+v1CAQCcLvdGU+lZwXQ8j36DsQuZKfE2Ikn0+J1yWD1QwKBAKYMK8GUYSXy97Jdt4CthWczIi4YX4VHP9qd8Gd4DhhWasKIgRaMqLBg5EALRlaYMazUBLWKl4uk5mNpRj5iZ96I8qivP9nViqkRr0mLw4fbXtyIUMRagp+fPyZqy7hYFo4egNl1ZXJ1/o93tmLl7mOY2+33kdxR6rWHEnhCCOnHOI6Tp9OzJJ7d2AmCAKezay2e1WrN6zrXdOR6LbzH4wHP8zkdfQfClcNDoVBOEnglTp9XqshEqNATeDaV3uFwyB1R6WIF0Prqb1LIhexYB1quC/+xGg6SJMmdNZIkwefzZbVuAevk8vv9MBgMuGZmNV7feFAuZldl08tJ+oiBFow8MUqv18T//YIg9Fq8tJAMsOpxSpUV2444AAArdh7Dj87uWtceCIm47cWNaHP55cdfPGmQvNwhEY7jcP/5Y7Do8VVyHYFfvbsdp909J+a0e3LyoE93Qgjp5ziOg9lshsfjgdvthiRJUKlUcLlc8ih9PraIylQuR+EDgQCCwaC8ZV4useQ6GAzmpLOAjSb3l5vjXGMJSuSa60KlVquh1+vh9XrTTuLY+u2+rM7P1sErhSRJEEUx4X/Z/4uiKG+JmUuxZo+wyvfZvK6wjhy/3w+9Xo9ikxbL752HA+1ulFv0aVVGz9eOIvk0b2S5nMBvP+pAi8OHgVY9fvveDmzY3yE/bnSlFb+5cFzS762RFRZcPaMa/1i9HwCwp9WFF9fsx3WzarLfCFIwCvuT6iTGcRxsNhvdoJGUUeycnNjoHEuAnU6nvE4+2aRFCbGTi7XwkiTB4/FAo9HkbcSRjTBme+11KBSCIAiKLMqmhPiJJTL56Q/YEhDWWZcqtgtDPreO606lUsnLTID8xY4kSQgGg/B4PHA4HOjs7ERHRwc6Ojpgt9vhcDjgcrngdrvh8/kQCATktbEqlQparRZGozEvu2VEzh5hclW3QKfTQRRFeVaEVs2jboAl7W3NWCdHPuQrduZHrIMHgJW7juHNrw7h+S+a5GM2gwZ/uXoyDNrU2v79hSOi/taPftSADncgwU+QbFHq5xaNwBconudRWVnZ16dBChDFzsnNaDSC53kIgpDyFkNKiJ1cjMKz0VeLxZKFM0yORqOB1+vN+hZmwWAwZ1vgZUoJ8RML2+orEAj0adKaLRzHwWg0wul0yqOmyWJTsLNRAC0TLAFl7/FcxQ5L2Nmaf5agsvcQ26mA47ge/1XCDX3kNHpBEBAKhXLSeaBWq6FSqeD3+zO+trBzzdd7LV/XnVOHFMFm0MDu7eoI/Oea/WhoDW8Xx3HAY98+FUNLU9+ZoNikxT0L6/GLd7YDAOzeIB79aDd++c2x2Tl5EpdSP7doBL5AiaKIo0ePKq4qIlE+ih2i1+vT2tdUKbGTzVH4fBWu644V6sr2Ol+2JZoSkovulBI/sWi1Wnn6cX+g0Wig0+ng9XqjisH1hlXk7+uODJ7nwfO8/P7IVuyw5QFshL2jowMulwt+vx88z8NoNMJms6G4uBhms1neUlOn0/VI6JVAq9XKW1/6fL6cTt1nOzZk4zUAkLfrbb6uO+oT28kxWw7b4QuGf+c9C0b0GKVPxdUzqqN2AHhx7QHsjthPnuSGUj+3KIEvUJIkwW63F/zWNyT/KHZIupQSO2wUPhAIpJScxJKvwnXdRa6DzxZBECAIgmKLsSklfmJhf7PI6ciFjq1h93g8Sf+Mz+eDRqNRxPpklUolv7/TjR2W3Ho8HtjtdnR2dsLlciEQCMRM2PV6vSLaniw2e8Tv98szSHLVuZCt9wh7TfP1d87ndSdegn7GqAG484y6jJ5bo+Jx//nhbeUEUcKv3t2uyOtpf6LUzy1K4AkhhBScbIzCs8J1qS4lyBaNRpPVdfCs+rwSp88rHc/z0Gg0/SqBZwlqMBiU9/FOJBgMQhCEvHdmxZNpJXpJkuB2u+WEXaVSwWQywWazoaioqCAT9li0Wi1CoVDO6xZEFrPLhCAI8lKE/ub0GNu7DS0x4tHLTgWfhb3u544oxxmjwp0Enza04aMdrRk/Lyk8/e/dQwghpN/LdBS+LwrXdccS7WyNwit5+nwhYIlQprM6lESr1Ubt451Irqdgp6p7IbtUSJIkJ+5ms1lO2HU6XcEn7N2x1yuXo++MTqeT19qnqz9WoGfKLTqMH2yTv9ZrePzlO5NhM2bvPfWz80ZDHdEZ8Jv/bYc/1H+uWSQ5lMAXKI7jUFZWRjdqJGUUOyRdSoudTEbhWeE6ozH1gkLZwopzZWMdvNKnzwPKi5/u2NZ7/WkUHoAc44mm0guCkLNtDdPFCtmx0eVkY4cl78FgEBaLRdHviWxQqVSwWq0wGAw5/12sBkAmo/D5rEAP5P+683/njoZew8Nm0ODxb0/E6EprVp9/eLkZ1502TP666bgHL0RUus+HHUcduP3FjTjr0ZV4ZOkudHr61zUzklI/tzhJaZP6+5jD4YDNZoPdbofVmt03HSGEkOzy+XzweDyw2WxJ3xQKggC73Q6DwZCXm95EXC4XBEGAzWbr/cEJeL1e+Hw+FBUVKe5Go5Bk6/VQGr/fD7fbDbPZHDOhdbvdCAaDitsuqbOzU96aLRmSJMHpdEIQBJjNZsXMJuhP2LXGZrMlNQ1ekiSIoih3Mvp8Puj1+j6/9uZSSBDhD4kw6XKz2ZfdG8T8P36C9hNbyVl0anz8w3kot+S2+GRTmxuPLNuNdzYfQWT2aNapce1p1bhxdi1KTP27wyzXks1DaQS+QImiiIMHDyquKiJRPoodki4lxk46o/B9VbguFo1GA0EQMv6bFsL0eSXGT3darTbjKcJKxKqox5pKL4oi/H5/XqZgp4qtg08mdkRRlJN3i8VCyXuOsMr33Zf+sH3i/X4/PB4PnE4n7HY7Ojo6YLfb4XK55GUa+ZwV0RfXHbWKz1nyDnTtJ/+Ds0bIXzv9Ifxp6a6c/b5muw8/fWMLFjyyEm9/HZ28A4DLH8LiFXsx++GP8bv3d+K4K7M6CUqi1M8t2ge+QLHiLDSBgqSKYoekS4mxk+q+8KxwndlsVkSywpKMUCiU9k0tG9lS+oiWEuOnO9YJEggE5Cnc/YXJZILdbofX64XJZJKP+/3+nBdAS5dKpZK390sUOyx5F0URFoul3712SsLqJPh8PrlmhCAIUa8Nz/Py41jdAbZkKN8K4bqTjm9PHYp/rt6Pnc1dW8m9sv4grp5RjbGDsjd7qN0dwNOf7MELq/cjEOqZwJaatDjuDk+f9wQEPLNyL174ognfmVmNm0+vRZlZedeVVCg1fmgEnhBCSEFLdhReCYXrumM3upkUsqPq89nDcRy0Wq28H3p/wqrS+/1+Od4kSYLP54NWq1VkVXC1Wi1PwY6Hkvf80+v18og7m81kNpthtVpRXFyMoqIiWCwWGI1G6PV6aDQaRcZXIVPxHB644BT5a0kC/u/NLVi+oyXjNelOXxCPfbQbp/9+BZ77tLFH8j6tpgSv3zITa/9vAR65bAJqy0xR3/cGBTy7ah9mP/wxfv3udrQ6fRmdD+mJrnKEEEIKWrKj8KxwncViyfMZJpbpdlmFMH2+kLCtskKhUL/rFNHpdPJ6eJvNJndUKGE5SSzsvRzv/SEIAlwuFyRJgtVq7bfVzZVGo9GguLi4r0/jpDdzeCnOOaUCH2xrBgBsPmTHjS+sBwDUDTBjSnUxJlcXY8qwEgwr7X27VF9QwL/W7MfiFXvQ4enZqTx2kBU/OnsUTq8PF3W7eNJgfPPUQXjn6yN44uMG7Dvmjng+EX/9rBH/XLMfV04filvmDsdAqzKvNYWGith1UyhF7CRJgt1uV1zBGaJ8FDskXUqOHXZuarUaZrO5x/eVVLiuu0AgAJfLBY7j5Kmmkf8SjVyxdsUrTqYkSo6fSJIkweFwQK/XK3JaeaYEQYDD4YBOp0MwGATP84rr1IrE3tfdi+wJggCns2v6sMVioeSdxFQo1510HTjuwcJHV8ac4h6p1KQ9kcwXY3J1CcYOskKn7nrPBAURr60/hCeWN6DZ0XO0fHi5CT84ayQWja1I+DcURAnvbj6CJz/egz2trh7f16p5XDmtK5GvsBVGIp/v+Ek2D6UEvptCSeAJIYRES1SRnhW3UuJNHCsIxdaSsn8Mm2bf/R/HcVR9PkckSerXf0/2XgGg+IJvLpcLoihG3ZNR8k5I2LLtLXh8+W5sP+KAmGRWp1XzmDDYhnGDivDxzhY0He+5zeSgIgPuXliPiycOglqV/BIIUZTw3tajeGJ5A3a3xEjkVTyumVmNH50zUu5EIF0ogU9ToSTwoiiiqakJw4YNo3VFJCUUOyRdSo+deKPwbIS7EEapGUmSeiT03avVq1QqiKIIjUYTc9aB0ig9fk4mbLs1SZIUv2Wez+eD2+1GZ2cnampq5DXvbOYAxRJJ5GS67jh9QWw62In1TR3YsL8DXx3ogDsg9P6D3ZSZtbhjfh2umD40owRbFCV8sK0ZTyxvkIvtRZpSXYynr56c8+3vMpHv+Ek2D6U18AVKkqR+WWSH5B7FDkmX0mMn1lp4JRauSwbHcVCr1T0KcsVK7AtlmrfS4+dkwnEcLBZLQbwW7H3MdpBg20BS8k6ScTJddyx6DebUl2NOfTmArv3odzY7sWF/B9bv78CGpnYcsccvKGfRq3HL3OG47rRhWdkGj+c5nDuuEuecUoGl21vwxPIGbD/qkL+/fn8Hvvnnz/DsNVOyWj0/m5QaP5TAE0II6Td0Oh18Ph+8Xi/MZrNiC9elK15iT0iqOI4riGUCkbHucrnkGSeUvBOSmFrFY+wgG8YOsuHa04YBAI50euVkfv3+Duw46oBZp8ZVM6pxy+nDYTNmfzkNz3M4Z2wFzj5lID7Y2oyfvLEFdm9Xkbwjdh++9cwX+MO3JuCCCVVZ/939Fd0BEEII6TciR+EDgQB8Ph8MBgOtkSWkQLHijpIkQaVSUfJOSAaqigz4RpEB3ziRLAcFEZoU1rdnguM4LBpXiTFVVtz0wno0nCh05wuKuPPfX2FXsxP3njkCPK/8jsW+Rmvgu2FrD9rb2+W1BxzHged5iKIYNYUi3nGe58FxXNzjkcWJ2HEAPfY5jXecrXt0uVwwGo1yLzrP8z32S0313PuyTfHOndqU3TaxKcVGo1FOagq9TcmcO7Up8zax2GFTb5XaJiBctI7jOFit1qjrZH9/nZTapnifW4Xcpv74OimxTV6vFx6PB8XFxfKIfKG3KZnj1KbM2yRJkjwjK97nVqG1KfLcC/V1cgcE3PPyJizf2Rr1e88cMxB/unQ8TNpwp3tftkkQBLjd7rQ/t1J9nVwuF4qKimgNfLIWL16MxYsXy3/IvXv3ykWBbDYbKisr0dLSArvdLv9MWVkZysrKcPjwYbjd4X0PKyoqUFRUhKamJgQCAfn44MGDYTabsXfv3qggqampgVqtRkNDQ9Q51dfXIxQKobGxUT7G8zxGjBgBj8eDI0eOyMe1Wi1qa2tht9vR3NwsHzeZTBgyZAja29vR1tYmH1dim9xuNw4dOkRtojZRmxTeJo7jcOjQIUW3qby8HK2trXC73Whtbe21Tf3xdVJamxwOR79rU398nZTcpvb29n7XJqD/vU5KbBPHcWhsbOxXbSr01+nnZwxEuTaAlzd3yt9btr0F33hiFR44YyAqLZo+b1NHR0deX6fS0lIkg0bguymUEfhQKIQ9e/agtrZW3k6IevqoTcm0SRAE7Nu3D7W1tfLWQYXepmTOndqUeZsEQUBjYyPq6urAcZzi28T2uE6nrUptUyHHXrzPrUJuU398nZTYpmQ+twqtTckcpzZl3qZkPrcKrU2R594fXqd3Nh/FT97YAl8w/JxFBg2evOJUnDa8tM/vI/bu3Zv251aqrxONwGeI7bMbKfJGMJPj8dZipnKc47iY58lxXMzHZ+vcc92mVI5TmzJrE7sQdT8e7/GMktuU7nFqU/LnyD6ACqFN8fa2Phlep3SP57pN7Gciv1/obeqPr5NS29Tb51Yhtqm349SmzM+9t8+tQmxTb8cLqU0XThyMugEWfPcf63H0RJX8Tm8Q1z2/Hj8/fwyumVmd8rnHO55um3L1udX9d7LrW29iPxshhBBCCCGEEJJjYwfZ8PYdszG5ulg+JogSHnh7G/7vzS0IhMQEP33yoQSeEEIIIYQQQkifKbfo8NJ3p+OyKYOjjv973UFc9dc1aHP5++jMlIfWwHfD1sD3tvagr0mShEAgAK1Wm/R0C0IAih2SPoodkgmKH5Iuih2SLoqdwiNJEp7/ogm//t8OCGI4TR1UZMCz10zGKVW2vJ5LPuMn2TyURuALGNtKhZBUUeyQdFHskExQ/JB0UeyQdFHsFBaO43D9rBq8cP002AzhWjaHO7245Okv8NhHu+Hyh/J2PkqMH0rgC5QoimhoaOhRYZGQ3lDskHRR7JBMUPyQdFHskHRR7BSu2fVl+O/ts1A3wCwf8wVFPPZRA07//Qr87bNG+IJCgmfInFLjhxJ4QgghhBBCCCGKMqzMhDdvOw0LRg2IOt7uDuBX727HGX/8BK9+eRAhQVkJdq5RAk8IIYQQQgghRHEseg2eu2YKfn/JeFTZ9FHfO2L34b7/bMbZj63Ce1uO4mQp7UYJPCGEEEIIIYQQReJ5DpdNHYKPfzgP958/BiUmbdT39x5z47YXN+Ibf/4cq3Yf6/eJPFWh76aQqtCLogie56mqJkkJxQ5JF8UOyQTFD0kXxQ5JF8VO/+Tyh/C3Txvx3Kf7Yha0m1FbgvvOGYVJQ4tj/HTy8h0/VIX+JBAK5a8CI+lfKHZIuih2SCYofki6KHZIuih2+h+zTo27F9Zj1X3z8d05NdCqo1PaNfvacfFTX+CmF9ZjV7Mzo9+lxPihBL5AiaKIxsZGxVVFJMpHsUPSRbFDMkHxQ9JFsUPSRbHTv5WYtPjZeWOw8kfzcMW0IVDx0aPkH+1owTmPr8L3X9mEA8c9KT+/UuOHEnhCCCGEEEIIIQWp0mbAQxePx7Lvn44LJlRFfU+SgDe/Oowz/vQJXt9wqI/OMLsogSeEEEIIIYQQUtBqy8148oqJ+N9dszF/ZHmP708dltmaeKWgBL6A8Ty9fCQ9FDskXRQ7JBMUPyRdFDskXRQ7J59TqmxYcv00vPq9mXLSfvnUIaguNaX8XEqMH6pC302hVKEnhBBCCCGEEBKfJEn4ZNcxjKmyYqBV3/sP9CGqQt/PSZIEl8vV7/c5JNlHsUPSRbFDMkHxQ9JFsUPSRbFDOI7D/FED0krelRo/lMAXKFEUcejQIcVVRSTKR7FD0kWxQzJB8UPSRbFD0kWxQzKh1PihBJ4QQgghhBBCCCkAlMATQgghhBBCCCEFgBL4AsVxHLRaLTiO6+tTIQWGYoeki2KHZILih6SLYoeki2KHZEKp8UNV6LuhKvSEEEIIIYQQQvKJqtD3c5IkobOzU3FVEYnyUeyQdFHskExQ/JB0UeyQdFHskEwoNX4ogS9QoiiiublZcVURifJR7JB0UeyQTFD8kHRR7JB0UeyQTCg1fiiBJ4QQQgghhBBCCgAl8IQQQgghhBBCSAGgBL5AcRwHk8mkuKqIRPkodki6KHZIJih+SLoodki6KHZIJpQaP1SFvhuqQk8IIYQQQgghJJ+oCn0/J4oi2traFFdUgSgfxQ5JF8UOyQTFD0kXxQ5JF8UOyYRS44cS+AIlSRLa2toUt60BUT6KHZIuih2SCYofki6KHZIuih2SCaXGDyXwhBBCCCGEEEJIAaAEnhBCCCGEEEIIKQCUwBcojuNgs9kUVxWRKB/FDkkXxQ7JBMUPSRfFDkkXxQ7JhFLjh6rQd0NV6AkhhBBCCCGE5BNVoe/nRFHE0aNHFVcVkSgfxQ5JF8UOyQTFD0kXxQ5JF8UOyYRS44cS+AIlSRLsdrviqiIS5aPYIemi2CGZoPgh6aLYIemi2CGZUGr8UAJPCCGEEEIIIYQUAHVfn4DSsB4Wh8PRx2eSmCAIcLlccDgcUKlUfX06pIBQ7JB0UeyQTFD8kHRR7JB0UeyQTOQ7flj+2duIPyXw3TidTgDAkCFD+vhMCCGEEEIIIYScTJxOJ2w2W9zvUxX6bkRRxJEjR2CxWBS3ZUAkh8OBIUOG4ODBg1Qtn6SEYoeki2KHZILih6SLYoeki2KHZCLf8SNJEpxOJ6qqqsDz8Ve60wh8NzzPY/DgwX19GkmzWq10QSJpodgh6aLYIZmg+CHpotgh6aLYIZnIZ/wkGnlnqIgdIYQQQgghhBBSACiBJ4QQQgghhBBCCgAl8AVKp9PhgQcegE6n6+tTIQWGYoeki2KHZILih6SLYoeki2KHZEKp8UNF7AghhBBCCCGEkAJAI/CEEEIIIYQQQkgBoASeEEIIIYQQQggpAJTAE0IIIYQQQgghBYASeEIIIYQQQgghpABQAl+AFi9ejGHDhkGv12P69OlYt25dX58SUaBVq1bhggsuQFVVFTiOw1tvvRX1fUmS8POf/xyVlZUwGAxYuHAhGhoa+uZkiaI89NBDmDp1KiwWCwYMGIALL7wQu3btinqMz+fD7bffjtLSUpjNZlxyySVoaWnpozMmSvH0009j/PjxsFqtsFqtmDlzJt5//335+xQ3JFm/+93vwHEc7rnnHvkYxQ+J58EHHwTHcVH/Ro0aJX+fYockcvjwYVx99dUoLS2FwWDAuHHjsH79evn7SrtnpgS+wLzyyiu499578cADD2Djxo2YMGECzj77bLS2tvb1qRGFcbvdmDBhAhYvXhzz+7///e/xxBNP4JlnnsHatWthMplw9tlnw+fz5flMidKsXLkSt99+O9asWYNly5YhGAzirLPOgtvtlh/z/e9/H++88w5ee+01rFy5EkeOHMHFF1/ch2dNlGDw4MH43e9+hw0bNmD9+vU444wz8M1vfhPbtm0DQHFDkvPll1/iL3/5C8aPHx91nOKHJHLKKafg6NGj8r/PPvtM/h7FDomno6MDs2bNgkajwfvvv4/t27fjT3/6E4qLi+XHKO6eWSIFZdq0adLtt98ufy0IglRVVSU99NBDfXhWROkASG+++ab8tSiKUkVFhfSHP/xBPtbZ2SnpdDrp3//+dx+cIVGy1tZWCYC0cuVKSZK6YkWj0Uivvfaa/JgdO3ZIAKTVq1f31WkShSouLpb++te/UtyQpDidTqm+vl5atmyZNHfuXOnuu++WJImuOySxBx54QJowYULM71HskER+/OMfS7Nnz477fSXeM9MIfAEJBALYsGEDFi5cKB/jeR4LFy7E6tWr+/DMSKFpbGxEc3NzVCzZbDZMnz6dYon0YLfbAQAlJSUAgA0bNiAYDEbFz6hRozB06FCKHyITBAEvv/wy3G43Zs6cSXFDknL77bfjvPPOi4oTgK47pHcNDQ2oqqpCbW0trrrqKhw4cAAAxQ5J7O2338aUKVNw6aWXYsCAAZg4cSKee+45+ftKvGemBL6AtLW1QRAEDBw4MOr4wIED0dzc3EdnRQoRixeKJdIbURRxzz33YNasWRg7diyArvjRarUoKiqKeizFDwGALVu2wGw2Q6fT4ZZbbsGbb76JMWPGUNyQXr388svYuHEjHnrooR7fo/ghiUyfPh3PP/88PvjgAzz99NNobGzEnDlz4HQ6KXZIQvv27cPTTz+N+vp6fPjhh7j11ltx11134YUXXgCgzHtmdZ/8VkIIIQXh9ttvx9atW6PWEhKSyMiRI7Fp0ybY7Xa8/vrruPbaa7Fy5cq+Pi2icAcPHsTdd9+NZcuWQa/X9/XpkAKzaNEi+f/Hjx+P6dOno7q6Gq+++ioMBkMfnhlROlEUMWXKFPz2t78FAEycOBFbt27FM888g2uvvbaPzy42GoEvIGVlZVCpVD2qZra0tKCioqKPzooUIhYvFEskkTvuuAPvvvsuVqxYgcGDB8vHKyoqEAgE0NnZGfV4ih8CAFqtFnV1dZg8eTIeeughTJgwAY8//jjFDUlow4YNaG1txaRJk6BWq6FWq7Fy5Uo88cQTUKvVGDhwIMUPSVpRURFGjBiBPXv20LWHJFRZWYkxY8ZEHRs9erS8BEOJ98yUwBcQrVaLyZMnY/ny5fIxURSxfPlyzJw5sw/PjBSampoaVFRURMWSw+HA2rVrKZYIJEnCHXfcgTfffBMff/wxampqor4/efJkaDSaqPjZtWsXDhw4QPFDehBFEX6/n+KGJLRgwQJs2bIFmzZtkv9NmTIFV111lfz/FD8kWS6XC3v37kVlZSVde0hCs2bN6rFV7u7du1FdXQ1AmffMNIW+wNx777249tprMWXKFEybNg2PPfYY3G43rr/++r4+NaIwLpcLe/bskb9ubGzEpk2bUFJSgqFDh+Kee+7Br3/9a9TX16Ompgb3338/qqqqcOGFF/bdSRNFuP322/HSSy/hv//9LywWi7zGy2azwWAwwGaz4cYbb8S9996LkpISWK1W3HnnnZg5cyZmzJjRx2dP+tJPf/pTLFq0CEOHDoXT6cRLL72ETz75BB9++CHFDUnIYrHIdTYYk8mE0tJS+TjFD4nnhz/8IS644AJUV1fjyJEjeOCBB6BSqXDFFVfQtYck9P3vfx+nnXYafvvb3+Kyyy7DunXr8Oyzz+LZZ58FAHAcp7x75j6pfU8y8uSTT0pDhw6VtFqtNG3aNGnNmjV9fUpEgVasWCEB6PHv2muvlSSpa1uM+++/Xxo4cKCk0+mkBQsWSLt27erbkyaKECtuAEhLliyRH+P1eqXbbrtNKi4uloxGo3TRRRdJR48e7buTJopwww03SNXV1ZJWq5XKy8ulBQsWSEuXLpW/T3FDUhG5jZwkUfyQ+C6//HKpsrJS0mq10qBBg6TLL79c2rNnj/x9ih2SyDvvvCONHTtW0ul00qhRo6Rnn3026vtKu2fmJEmS+qbrgBBCCCGEEEIIIcmiNfCEEEIIIYQQQkgBoASeEEIIIYQQQggpAJTAE0IIIYQQQgghBYASeEIIIYQQQgghpABQAk8IIYQQQgghhBQASuAJIYQQQgghhJACQAk8IYQQQgghhBBSACiBJ4QQQgghhBBCCgAl8IQQQgjpE88//zw4jsP69ev7+lQIIYSQgkAJPCGEENKPsSQ53r81a9b09SkSQgghJEnqvj4BQgghhOTeL3/5S9TU1PQ4XldX1wdnQwghhJB0UAJPCCGEnAQWLVqEKVOm9PVpEEIIISQDNIWeEEIIOck1NTWB4zj88Y9/xKOPPorq6moYDAbMnTsXW7du7fH4jz/+GHPmzIHJZEJRURG++c1vYseOHT0ed/jwYdx4442oqqqCTqdDTU0Nbr31VgQCgajH+f1+3HvvvSgvL4fJZMJFF12EY8eORT1m/fr1OPvss1FWVgaDwYCamhrccMMN2f1DEEIIIQpHI/CEEELIScBut6OtrS3qGMdxKC0tlb/+xz/+AafTidtvvx0+nw+PP/44zjjjDGzZsgUDBw4EAHz00UdYtGgRamtr8eCDD8Lr9eLJJ5/ErFmzsHHjRgwbNgwAcOTIEUybNg2dnZ24+eabMWrUKBw+fBivv/46PB4PtFqt/HvvvPNOFBcX44EHHkBTUxMee+wx3HHHHXjllVcAAK2trTjrrLNQXl6On/zkJygqKkJTUxPeeOONHP/VCCGEEGWhBJ4QQgg5CSxcuLDHMZ1OB5/PJ3+9Z88eNDQ0YNCgQQCAc845B9OnT8fDDz+MRx55BADwox/9CCUlJVi9ejVKSkoAABdeeCEmTpyIBx54AC+88AIA4Kc//Smam5uxdu3aqKn7v/zlLyFJUtR5lJaWYunSpeA4DgAgiiKeeOIJ2O122Gw2fPHFF+jo6MDSpUujnuvXv/51Nv40hBBCSMGgKfSEEELISWDx4sVYtmxZ1L/3338/6jEXXnihnLwDwLRp0zB9+nS89957AICjR49i06ZNuO666+TkHQDGjx+PM888U36cKIp46623cMEFF8Rcd88Sdebmm2+OOjZnzhwIgoD9+/cDAIqKigAA7777LoLBYAZ/BUIIIaSw0Qg8IYQQchKYNm1ar0Xs6uvrexwbMWIEXn31VQCQE+qRI0f2eNzo0aPx4Ycfwu12w+VyweFwYOzYsUmd29ChQ6O+Li4uBgB0dHQAAObOnYtLLrkEv/jFL/Doo49i3rx5uPDCC3HllVdCp9Ml9TsIIYSQ/oBG4AkhhBDSp1QqVczjbKo9x3F4/fXXsXr1atxxxx04fPgwbrjhBkyePBkulyufp0oIIYT0KUrgCSGEEAIAaGho6HFs9+7dcmG66upqAMCuXbt6PG7nzp0oKyuDyWRCeXk5rFZrzAr2mZgxYwZ+85vfYP369XjxxRexbds2vPzyy1n9HYQQQoiSUQJPCCGEEADAW2+9hcOHD8tfr1u3DmvXrsWiRYsAAJWVlTj11FPxwgsvoLOzU37c1q1bsXTpUpx77rkAAJ7nceGFF+Kdd97B+vXre/ye7kXsetPR0dHjZ0499VQAXVvQEUIIIScLWgNPCCGEnATef/997Ny5s8fx0047DTzf1Z9fV1eH2bNn49Zbb4Xf78djjz2G0tJS3HffffLj//CHP2DRokWYOXMmbrzxRnkbOZvNhgcffFB+3G9/+1ssXboUc+fOxc0334zRo0fj6NGjeO211/DZZ5/JhemS8cILL+Cpp57CRRddhOHDh8PpdOK5556D1WqVOw0IIYSQkwEl8IQQQshJ4Oc//3nM40uWLMG8efMAANdccw14nsdjjz2G1tZWTJs2DX/+859RWVkpP37hwoX44IMP8MADD+DnP/85NBoN5s6di4cffhg1NTXy4wYNGoS1a9fi/vvvx4svvgiHw4FBgwZh0aJFMBqNKZ373LlzsW7dOrz88stoaWmBzWbDtGnT8OKLL0b9TkIIIaS/46RU57ERQgghpF9pampCTU0N/vCHP+CHP/xhX58OIYQQQuKgNfCEEEIIIYQQQkgBoASeEEIIIYQQQggpAJTAE0IIIYQQQgghBYDWwBNCCCGEEEIIIQWARuAJIYQQQgghhJACQAk8IYQQQgghhBBSACiBJ4QQQgghhBBCCgAl8IQQQgghhBBCSAGgBJ4QQgghhBBCCCkAlMATQgghhBBCCCEFgBJ4QgghhBBCCCGkAFACTwghhBBCCCGEFID/Dwg9YMPiTRSUAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1200x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_trials(file_path):\n",
    "    # 1. Load the JSON data\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    results = data.get('results', [])\n",
    "    all_val_losses = []\n",
    "    best_overall_loss = float('inf')\n",
    "    best_trial_index = -1\n",
    "\n",
    "    # 2. Extract losses and identify the best trial\n",
    "    for i, entry in enumerate(results):\n",
    "        attachments = entry.get('trial', {}).get('attachments', {})\n",
    "        # The history key follows the pattern \"ATTACH::[index]::history\"\n",
    "        history_key = f\"ATTACH::{i}::history\"\n",
    "        \n",
    "        if history_key in attachments:\n",
    "            val_losses = attachments[history_key].get('val_losses', [])\n",
    "            all_val_losses.append(val_losses)\n",
    "            \n",
    "            # Check if this trial contains the best (lowest) loss seen so far\n",
    "            if val_losses:\n",
    "                min_loss = min(val_losses)\n",
    "                if min_loss < best_overall_loss:\n",
    "                    best_overall_loss = min_loss\n",
    "                    best_trial_index = i\n",
    "\n",
    "    # 3. Create the plot\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    \n",
    "    # Plot all trials in a muted light gray\n",
    "    for i, losses in enumerate(all_val_losses):\n",
    "        if i == best_trial_index:\n",
    "            continue  # Skip the best one to plot it last (on top)\n",
    "        plt.plot(losses, color='lightgray', alpha=0.4, linewidth=1)\n",
    "\n",
    "    # Highlight the best trial in a bold color\n",
    "    if best_trial_index != -1:\n",
    "        plt.plot(all_val_losses[best_trial_index], \n",
    "                 color='#1f77b4',  # Professional blue\n",
    "                 linewidth=2.5, \n",
    "                 label=f'Best Trial (Index {best_trial_index}, Loss: {best_overall_loss:.4f})')\n",
    "\n",
    "    # 4. Formatting\n",
    "    plt.title('Training Progression: Validation Loss across all Trials', fontsize=14)\n",
    "    plt.xlabel('Epochs', fontsize=12)\n",
    "    plt.ylabel('Validation Loss', fontsize=12)\n",
    "    \n",
    "    # Use log scale if there is a massive range in loss values\n",
    "    plt.yscale('log') \n",
    "    \n",
    "    plt.grid(True, which=\"both\", linestyle='--', alpha=0.5)\n",
    "    plt.legend()\n",
    "    \n",
    "    print(f\"Visualization complete. Best trial found at index {best_trial_index}.\")\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure 'test.json' is in the same directory as this script\n",
    "    visualize_trials('all_trials.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc72a561-4b54-4c8d-bc7d-73e9040c385f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Best Trial Found ---\n",
      "Index: 1\n",
      "Lowest Loss: 0.16972512834107698\n",
      "Parameters: {\n",
      "    \"batch_size\": 96,\n",
      "    \"cnn_dense\": 128,\n",
      "    \"cnn_dropout\": 0.23288671170703942,\n",
      "    \"cnn_kernel_size_1\": 3,\n",
      "    \"cnn_kernel_size_2\": 5,\n",
      "    \"cnn_kernels_1\": 64,\n",
      "    \"cnn_kernels_2\": 96,\n",
      "    \"learning_rate\": 0.0006894204402451877,\n",
      "    \"lstm_dense\": 128,\n",
      "    \"lstm_hidden_size\": 128,\n",
      "    \"lstm_layers\": 2,\n",
      "    \"optimizer\": \"rmsprop\",\n",
      "    \"dropout\": 0.23288671170703942\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def extract_best_params(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        best_overall_loss = float('inf')\n",
    "        best_params = None\n",
    "        best_trial_index = -1\n",
    "\n",
    "            \n",
    "        # Iterate through the results list\n",
    "        for i, entry in enumerate(data.get('results')):\n",
    "            params = entry.get('params')\n",
    "            attachments = entry.get('trial', {}).get('attachments', {})\n",
    "            # The history key follows the pattern \"ATTACH::[index]::history\"\n",
    "            history_key = f\"ATTACH::{i}::history\"\n",
    "            \n",
    "            if history_key in attachments:\n",
    "                val_losses = attachments[history_key].get('val_losses', [])\n",
    "                \n",
    "                # Check if this trial contains the best (lowest) loss seen so far\n",
    "                if val_losses:\n",
    "                    min_loss = min(val_losses)\n",
    "                    if min_loss < best_overall_loss:\n",
    "                        best_overall_loss = min_loss\n",
    "                        best_trial_index = i\n",
    "                        best_params = params\n",
    "        \n",
    "        if best_params:\n",
    "            print(f\"--- Best Trial Found ---\")\n",
    "            print(f\"Index: {best_trial_index}\")\n",
    "            print(f\"Lowest Loss: {best_overall_loss}\")\n",
    "            print(f\"Parameters: {json.dumps(best_params, indent=4)}\")\n",
    "\n",
    "            return best_params\n",
    "        else:\n",
    "            print(\"No valid loss data found in the JSON file.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{file_path}' was not found.\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error: Failed to decode JSON. Check the file format.\")\n",
    "\n",
    "best_params = extract_best_params('all_trials.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6535bb83-2d15-43e5-b0c9-3c628a4c9201",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Train Loss: 9.5466 Acc: 0.4097 | Val Loss: 4.5989 Acc: 0.4812\n",
      "Epoch 002 | Train Loss: 2.6209 Acc: 0.4648 | Val Loss: 1.9507 Acc: 0.5152\n",
      "Epoch 003 | Train Loss: 1.5912 Acc: 0.5010 | Val Loss: 0.9348 Acc: 0.5528\n",
      "Epoch 004 | Train Loss: 1.0375 Acc: 0.5688 | Val Loss: 0.9115 Acc: 0.5982\n",
      "Epoch 005 | Train Loss: 0.8210 Acc: 0.6212 | Val Loss: 0.6380 Acc: 0.6779\n",
      "Epoch 006 | Train Loss: 0.7240 Acc: 0.6656 | Val Loss: 0.5834 Acc: 0.7588\n",
      "Epoch 007 | Train Loss: 0.6537 Acc: 0.7011 | Val Loss: 0.5510 Acc: 0.7528\n",
      "Epoch 008 | Train Loss: 0.6154 Acc: 0.7195 | Val Loss: 0.4918 Acc: 0.7633\n",
      "Epoch 009 | Train Loss: 0.5657 Acc: 0.7398 | Val Loss: 0.4352 Acc: 0.7934\n",
      "Epoch 010 | Train Loss: 0.5396 Acc: 0.7580 | Val Loss: 0.4970 Acc: 0.7403\n",
      "Epoch 011 | Train Loss: 0.5102 Acc: 0.7765 | Val Loss: 0.4686 Acc: 0.7893\n",
      "Epoch 012 | Train Loss: 0.4890 Acc: 0.7851 | Val Loss: 0.4501 Acc: 0.7928\n",
      "Epoch 013 | Train Loss: 0.5075 Acc: 0.7824 | Val Loss: 0.4445 Acc: 0.8239\n",
      "Epoch 014 | Train Loss: 0.4700 Acc: 0.7923 | Val Loss: 0.4415 Acc: 0.8003\n",
      "Epoch 015 | Train Loss: 0.4431 Acc: 0.8092 | Val Loss: 0.4479 Acc: 0.8066\n",
      "Epoch 016 | Train Loss: 0.4371 Acc: 0.8147 | Val Loss: 0.4768 Acc: 0.8081\n",
      "Epoch 017 | Train Loss: 0.4070 Acc: 0.8242 | Val Loss: 0.6348 Acc: 0.7770\n",
      "Epoch 018 | Train Loss: 0.4096 Acc: 0.8237 | Val Loss: 0.3757 Acc: 0.8490\n",
      "Epoch 019 | Train Loss: 0.4007 Acc: 0.8236 | Val Loss: 0.3416 Acc: 0.8352\n",
      "Epoch 020 | Train Loss: 0.3768 Acc: 0.8400 | Val Loss: 0.5090 Acc: 0.8179\n",
      "Epoch 021 | Train Loss: 0.3533 Acc: 0.8527 | Val Loss: 0.3822 Acc: 0.8513\n",
      "Epoch 022 | Train Loss: 0.3434 Acc: 0.8633 | Val Loss: 0.5167 Acc: 0.8087\n",
      "Epoch 023 | Train Loss: 0.3103 Acc: 0.8763 | Val Loss: 0.3762 Acc: 0.8543\n",
      "Epoch 024 | Train Loss: 0.3105 Acc: 0.8768 | Val Loss: 0.3221 Acc: 0.8872\n",
      "Epoch 025 | Train Loss: 0.2933 Acc: 0.8846 | Val Loss: 0.3660 Acc: 0.8878\n",
      "Epoch 026 | Train Loss: 0.2801 Acc: 0.8956 | Val Loss: 0.2844 Acc: 0.9134\n",
      "Epoch 027 | Train Loss: 0.2670 Acc: 0.8994 | Val Loss: 0.2391 Acc: 0.9260\n",
      "Epoch 028 | Train Loss: 0.2382 Acc: 0.9088 | Val Loss: 0.4042 Acc: 0.8612\n",
      "Epoch 029 | Train Loss: 0.2436 Acc: 0.9151 | Val Loss: 0.5667 Acc: 0.8140\n",
      "Epoch 030 | Train Loss: 0.1996 Acc: 0.9254 | Val Loss: 0.2150 Acc: 0.9373\n",
      "Epoch 031 | Train Loss: 0.2011 Acc: 0.9260 | Val Loss: 0.2023 Acc: 0.9307\n",
      "Epoch 032 | Train Loss: 0.1826 Acc: 0.9332 | Val Loss: 0.1849 Acc: 0.9364\n",
      "Epoch 033 | Train Loss: 0.1671 Acc: 0.9420 | Val Loss: 0.1276 Acc: 0.9618\n",
      "Epoch 034 | Train Loss: 0.1521 Acc: 0.9469 | Val Loss: 0.1163 Acc: 0.9612\n",
      "Epoch 035 | Train Loss: 0.1453 Acc: 0.9497 | Val Loss: 0.1053 Acc: 0.9696\n",
      "Epoch 036 | Train Loss: 0.1205 Acc: 0.9571 | Val Loss: 0.1532 Acc: 0.9496\n",
      "Epoch 037 | Train Loss: 0.1183 Acc: 0.9602 | Val Loss: 0.1043 Acc: 0.9648\n",
      "Epoch 038 | Train Loss: 0.1245 Acc: 0.9577 | Val Loss: 0.2048 Acc: 0.9242\n",
      "Epoch 039 | Train Loss: 0.0990 Acc: 0.9683 | Val Loss: 0.1230 Acc: 0.9552\n",
      "Epoch 040 | Train Loss: 0.0934 Acc: 0.9708 | Val Loss: 0.3295 Acc: 0.9069\n",
      "Epoch 041 | Train Loss: 0.0867 Acc: 0.9723 | Val Loss: 0.4161 Acc: 0.8928\n",
      "Epoch 042 | Train Loss: 0.0969 Acc: 0.9715 | Val Loss: 0.0526 Acc: 0.9827\n",
      "Epoch 043 | Train Loss: 0.0906 Acc: 0.9698 | Val Loss: 0.1099 Acc: 0.9651\n",
      "Epoch 044 | Train Loss: 0.0689 Acc: 0.9781 | Val Loss: 0.0680 Acc: 0.9776\n",
      "Epoch 045 | Train Loss: 0.0857 Acc: 0.9718 | Val Loss: 0.0882 Acc: 0.9642\n",
      "Epoch 046 | Train Loss: 0.0790 Acc: 0.9752 | Val Loss: 0.0731 Acc: 0.9704\n",
      "Epoch 047 | Train Loss: 0.0575 Acc: 0.9815 | Val Loss: 0.0656 Acc: 0.9827\n",
      "Epoch 048 | Train Loss: 0.0762 Acc: 0.9771 | Val Loss: 0.0468 Acc: 0.9878\n",
      "Epoch 049 | Train Loss: 0.0596 Acc: 0.9820 | Val Loss: 0.0685 Acc: 0.9806\n",
      "Epoch 050 | Train Loss: 0.0600 Acc: 0.9801 | Val Loss: 0.0557 Acc: 0.9830\n",
      "Epoch 051 | Train Loss: 0.0621 Acc: 0.9801 | Val Loss: 0.0333 Acc: 0.9896\n",
      "Epoch 052 | Train Loss: 0.0604 Acc: 0.9798 | Val Loss: 0.0748 Acc: 0.9794\n",
      "Epoch 053 | Train Loss: 0.0492 Acc: 0.9845 | Val Loss: 0.0685 Acc: 0.9773\n",
      "Epoch 054 | Train Loss: 0.0621 Acc: 0.9825 | Val Loss: 0.0686 Acc: 0.9788\n",
      "Epoch 055 | Train Loss: 0.0458 Acc: 0.9865 | Val Loss: 0.0554 Acc: 0.9830\n",
      "Epoch 056 | Train Loss: 0.0679 Acc: 0.9807 | Val Loss: 0.0691 Acc: 0.9758\n",
      "Epoch 057 | Train Loss: 0.0419 Acc: 0.9870 | Val Loss: 0.0693 Acc: 0.9740\n",
      "Epoch 058 | Train Loss: 0.0430 Acc: 0.9873 | Val Loss: 0.0487 Acc: 0.9824\n",
      "Epoch 059 | Train Loss: 0.0487 Acc: 0.9845 | Val Loss: 0.0466 Acc: 0.9851\n",
      "Epoch 060 | Train Loss: 0.0405 Acc: 0.9879 | Val Loss: 0.1171 Acc: 0.9657\n",
      "\n",
      "val Accuracy: 0.9656716417910448\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97       268\n",
      "           1       0.99      0.96      0.97       682\n",
      "           2       0.98      0.94      0.96       919\n",
      "           3       0.95      0.99      0.97      1481\n",
      "\n",
      "    accuracy                           0.97      3350\n",
      "   macro avg       0.98      0.96      0.97      3350\n",
      "weighted avg       0.97      0.97      0.97      3350\n",
      "\n",
      "[[ 254    0    1   13]\n",
      " [   0  652   12   18]\n",
      " [   0    1  864   54]\n",
      " [   0    7    9 1465]]\n"
     ]
    }
   ],
   "source": [
    "# build dataloaders from the existing train_ds/test_ds in this session\n",
    "train_loader = DataLoader(train_ds, batch_size=best_params['batch_size'], shuffle=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=best_params['batch_size'], shuffle=False)\n",
    "\n",
    "# create model (note we pass dropout into lstm dropout and cnn dropout)\n",
    "model, criterion, optimizer = get_model(best_params)\n",
    "\n",
    "# Train with modest epochs; early stopping inside fit handles rest\n",
    "history = model.fit(\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    epochs=60,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    patience=10\n",
    ")\n",
    "acc, report, matrix = get_validation(model, test_loader, device)\n",
    "\n",
    "print(\"\\nval Accuracy:\", acc)\n",
    "print(report)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "16d952c1-9d35-4bc1-bf41-942e5dddab77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_accs'], label='Training Accuracy')\n",
    "plt.plot(history['val_accs'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['train_losses'], label='Training Loss')\n",
    "plt.plot(history['val_losses'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7d160b-68be-4b06-976c-97bcbd423f07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# map params to integers where needed\n",
    "params = {'batch_size': 64, 'cnn_dense': 256, 'cnn_dropout': np.float64(0.3571401842400106), 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 32, 'cnn_kernels_2': 16, 'learning_rate': np.float64(1.8587640600578385e-05), 'lstm_dense': 64, 'lstm_hidden_size': 96, 'lstm_layers': 2, 'optimizer': 'sgd'}\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "unique_subjects = list(df['Window'].unique())\n",
    "accs = []\n",
    "variances = []\n",
    "batch_size = params['batch_size']\n",
    "\n",
    "K_FOLDS = 11\n",
    "fold_size = len(unique_subjects) // K_FOLDS\n",
    "\n",
    "random.shuffle(unique_subjects)\n",
    "\n",
    "cyclic = itertools.cycle(unique_subjects)\n",
    "batched_cyclic = batched(cyclic, n=fold_size)\n",
    "folds = itertools.islice(batched_cyclic, K_FOLDS)\n",
    "    \n",
    "for i, fold in enumerate(folds):\n",
    "    print(f\"Starting fold {i + 1}/{K_FOLDS}\")\n",
    "\n",
    "    train_df = df[~df['Window'].isin(fold)]\n",
    "    test_df   = df[df['Window'].isin(fold)]\n",
    "\n",
    "    train_loader = get_dataset(train_df, batch_size=batch_size)\n",
    "    test_loader  = get_dataset(test_df, batch_size=batch_size)\n",
    "    model, criterion, optimizer = get_model(params)\n",
    "\n",
    "    # Train with modest epochs; early stopping inside fit handles rest\n",
    "    history = model.fit(\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        epochs=100,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        patience=15\n",
    "    )\n",
    "\n",
    "    acc, *_ = get_validation(model, test_loader, device)\n",
    "    accs.append(acc)\n",
    "\n",
    "    print(f\"Fold {i + 1} Accuracy:\", acc)\n",
    "\n",
    "print(f\"k-Fold CV Mean Accuracy: {np.mean(accs):.4f} ± {np.std(accs):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6a4bb1-479f-46a1-bd12-50d340d54b0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "params = readable\n",
    "\n",
    "unique_samples = list(df['Window'].unique())\n",
    "subject_count = df['ID'].nunique()\n",
    "accs = []\n",
    "variances = []\n",
    "batch_size = params['batch_size']\n",
    "\n",
    "fold_size = len(unique_samples) // subject_count\n",
    "random.shuffle(unique_samples)\n",
    "\n",
    "cyclic = itertools.cycle(unique_samples)\n",
    "batched_cyclic = batched(cyclic, n=fold_size)\n",
    "folds = itertools.islice(batched_cyclic, subject_count)\n",
    "\n",
    "for i, fold in enumerate(folds):\n",
    "    print(f\"[{get_timestamp()}] Starting fold {i + 1}/{subject_count}\")\n",
    "\n",
    "    train_df = df[~df['Window'].isin(fold)]\n",
    "    test_df   = df[df['Window'].isin(fold)]\n",
    "\n",
    "    train_loader = get_dataset(train_df, is_train=True, batch_size=batch_size)\n",
    "    test_loader  = get_dataset(test_df, is_train=False, batch_size=batch_size)\n",
    "    model, criterion, optimizer = get_model(params)\n",
    "\n",
    "    # Train with modest epochs; early stopping inside fit handles rest\n",
    "    history = model.fit(\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        epochs=60,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        patience=15\n",
    "    )\n",
    "\n",
    "    acc, *_ = get_validation(model, test_loader, device)\n",
    "    accs.append(acc)\n",
    "\n",
    "    print(f\"Fold {i + 1} Accuracy:\", acc)\n",
    "\n",
    "print(f\"LOSOCV Mean Accuracy: {np.mean(accs):.4f} ± {np.std(accs):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3934c02-1f26-4db2-8c1b-6d45ad199e8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "params = {'batch_size': 48, 'cnn_dense': 64, 'cnn_dropout': np.float64(0.5130373328759822), 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 16, 'learning_rate': np.float64(0.000462968156587811), 'lstm_dense': 256, 'lstm_hidden_size': 128, 'lstm_layers': 1, 'optimizer': 'adam'}\n",
    "\n",
    "df = pd.read_csv(\"./processed_clustered_adhdata.csv\")\n",
    "\n",
    "frequency_count = len(df['Frequency'].unique())\n",
    "window_count = len(df['Window'].unique())\n",
    "numeric_df = df.drop(['ID', 'Window'], axis=1)\n",
    "\n",
    "display(df.head())\n",
    "\n",
    "# shape: (windows, frequencies, electrodes)\n",
    "full_ndarray = numeric_df.values.reshape((window_count, frequency_count, numeric_df.shape[1]))\n",
    "\n",
    "X = full_ndarray[:, :, 2:]     # drop ID/Class columns\n",
    "y = full_ndarray[:, 0, 0]      # class label is repeated across freq rows\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "# Add channel dimension (N, 1, freq, electrodes)\n",
    "X_train = X_train[..., np.newaxis]   # (N, freq, electrodes, 1)\n",
    "X_test  = X_test[...,  np.newaxis]\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)  # (N, freq, electrodes, 1)\n",
    "train_ds = EEGDataset(X_train, y_train)\n",
    "test_ds  = EEGDataset(X_test,  y_test)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
    "\n",
    "unique_samples = list(df['Window'].unique())\n",
    "subject_count = df['ID'].nunique()\n",
    "accs = []\n",
    "variances = []\n",
    "batch_size = params['batch_size']\n",
    "\n",
    "model, criterion, optimizer = get_model(params)\n",
    "\n",
    "# Train with modest epochs; early stopping inside fit handles rest\n",
    "history = model.fit(\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    epochs=60,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    patience=15\n",
    ")\n",
    "\n",
    "acc, *_ = get_validation(model, test_loader, device)\n",
    "accs.append(acc)\n",
    "\n",
    "print(f\"Fold {i + 1} Accuracy:\", acc)\n",
    "\n",
    "print(f\"LOSOCV Mean Accuracy: {np.mean(accs):.4f} ± {np.std(accs):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c75e90b-0cac-4fd9-8f54-5d2d380581ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, report, matrix = get_validation(model, test_loader, device)\n",
    "\n",
    "print(\"\\nval Accuracy:\", acc)\n",
    "print(report)\n",
    "print(matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
