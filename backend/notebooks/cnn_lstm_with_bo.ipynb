{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0250446",
   "metadata": {},
   "source": [
    "#### **EEG ADHD Classification: CNN-LSTM with Bayesian Optimization**\n",
    "\n",
    "This notebook implements a hybrid CNN-LSTM deep learning model for ADHD classification using EEG data. The model combines convolutional neural networks for spatial feature extraction with LSTM networks for temporal pattern recognition. Hyperparameter optimization is performed using Tree-structured Parzen Estimator (TPE) algorithm via Hyperopt, with an iterative convergence-based approach to ensure robust hyperparameter selection.\n",
    "\n",
    "#### **Key Features**\n",
    "- **Dual-stream architecture**: Combines raw EEG spatial-temporal features with engineered frequency band powers\n",
    "- **Iterative Bayesian Optimization**: Runs multiple BO searches until standard deviation of results converges\n",
    "- **Reproducible experiments**: Seed management for consistent results across runs\n",
    "- **Comprehensive evaluation**: Multiple cross-validation strategies including Leave-One-Subject-Out\n",
    "- **Early stopping**: Prevents overfitting with configurable patience parameter\n",
    "\n",
    "#### **Table of Contents**\n",
    "\n",
    "1. [First Imports](#first-imports) - Essential libraries for data processing, ML, and deep learning\n",
    "2. [Read the Processed Dataset](#read-the-processed-dataset) - Load preprocessed EEG data with frequency features\n",
    "3. [Group the Data](#group-the-data) - Reshape tabular data into 3D tensors for CNN-LSTM\n",
    "4. [Dataset Loading](#dataset-loading) - Custom PyTorch Dataset with dual-stream architecture\n",
    "5. [Model Creation](#model-creation) - Hybrid CNN-LSTM model with fusion and classification heads\n",
    "6. [First Model Training](#first-model-training) - Baseline model training with default hyperparameters\n",
    "7. [Helper Functions](#helper-functions) - Utilities for model creation, evaluation, and data loading\n",
    "8. [Search Space Definition](#search-space-definition) - Hyperparameter search space for Bayesian Optimization\n",
    "9. [Search Objective Definition](#search-objective-definition) - Objective function with train/test split or k-fold CV\n",
    "10. [Hyperparameter Search](#hyperparameter-search) - TPE search with configurable iterations and seeding\n",
    "11. [Results Visualization](#results-visualization) - Iterative BO with convergence tracking and analysis\n",
    "    - Convergence-based optimization with stability criteria\n",
    "    - Summary visualization of all BO runs\n",
    "    - Best parameter extraction and final model training\n",
    "12. [Cross-Validation Experiments](#cross-validation-experiments) - Window-based k-fold and LOSOCV evaluation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9fc3d7-ba3e-48bb-88a9-8ee38f5f1ab9",
   "metadata": {},
   "source": [
    "#### **First Imports**\n",
    "\n",
    "Import essential libraries for data manipulation, machine learning, and deep learning:\n",
    "- **NumPy/Pandas**: Data processing and manipulation\n",
    "- **Scikit-learn**: Train/test splitting, metrics, and label encoding\n",
    "- **PyTorch**: Deep learning framework for building and training neural networks\n",
    "- **Torch Optimizers**: Adam, RMSprop, and SGD for model optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "181da2fc-dae6-4638-8d23-8214b7560c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam, RMSprop, SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2168c3-c347-46fa-b7d7-9f352e31fdd3",
   "metadata": {},
   "source": [
    "#### **Read the Processed Dataset**\n",
    "\n",
    "Load the preprocessed and clustered ADHD EEG dataset. This dataset contains:\n",
    "- **Frequency domain features**: Power spectral density values across different frequency bands\n",
    "- **Window segments**: Temporal windows extracted from continuous EEG recordings\n",
    "- **Electrode channels**: Data from 19 EEG electrodes plus 7 band power features\n",
    "- **Subject IDs**: For cross-validation and subject-independent testing\n",
    "- **Class labels**: ADHD diagnostic categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af714077-585f-4de6-b357-1e50dc4993ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./processed_clustered_adhdata.csv\")\n",
    "\n",
    "frequency_count = len(df['Frequency'].unique())\n",
    "window_count = len(df['Window'].unique())\n",
    "numeric_df = df.drop(['ID', 'Window'], axis=1)\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39df14e-ecc4-4a5d-80c9-66305657bc36",
   "metadata": {},
   "source": [
    "#### **Group the Data**\n",
    "\n",
    "Transform the 2D tabular data into 3D tensors suitable for CNN-LSTM processing:\n",
    "\n",
    "**Reshaping Process:**\n",
    "1. Group rows by window number to reconstruct temporal segments\n",
    "2. Create shape: `(WINDOW_COUNT, FREQUENCY_PER_WINDOW, ELECTRODES)`\n",
    "3. Separate features (X) from labels (y)\n",
    "4. Add channel dimension for CNN compatibility: `(N, freq, electrodes, 1)`\n",
    "\n",
    "**Train/Test Split:**\n",
    "- 80% training, 20% testing\n",
    "- Stratified sampling to maintain class distribution\n",
    "- Ensures balanced representation of all diagnostic categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe8775d-db75-48e4-a074-6190d80c2ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape: (windows, frequencies, electrodes)\n",
    "full_ndarray = numeric_df.values.reshape((window_count, frequency_count, numeric_df.shape[1]))\n",
    "\n",
    "X = full_ndarray[:, :, 2:]     # drop ID/Class columns\n",
    "y = full_ndarray[:, 0, 0]      # class label is repeated across freq rows\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "# Add channel dimension (N, 1, freq, electrodes)\n",
    "X_train = X_train[..., np.newaxis]   # (N, freq, electrodes, 1)\n",
    "X_test  = X_test[...,  np.newaxis]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(\"Train shape:\", X_train.shape)  # (N, freq, electrodes, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed81cb7-3951-4889-a095-e59b7e0ea1a4",
   "metadata": {},
   "source": [
    "#### **Dataset Loading**\n",
    "\n",
    "Custom PyTorch Dataset class for EEG data with dual-stream architecture:\n",
    "\n",
    "**Features:**\n",
    "- **EEG Raw Features**: Spatial-temporal patterns from 19 electrodes (1, 77, 19)\n",
    "- **Band Power Features**: Pre-computed frequency band powers (7 features)\n",
    "- **Data Augmentation Ready**: Supports future augmentation strategies\n",
    "- **Batch Processing**: Efficient DataLoader integration\n",
    "\n",
    "**Architecture Rationale:**\n",
    "The dual-stream approach allows the model to learn both:\n",
    "1. Fine-grained spatial-temporal patterns via CNN-LSTM\n",
    "2. Engineered frequency domain features via dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2595397-63c0-4fd7-9f9a-4242370f5bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "        # -> (N, 1, freq, electrodes)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]          # (1, 77, 26)\n",
    "\n",
    "        x_eeg  = x[:, :, :19]    # (1, 77, 19)\n",
    "        x_band = x[0, 0, 19:]    # (7,)\n",
    "\n",
    "        return x_eeg, x_band, self.y[idx]\n",
    "\n",
    "train_ds = EEGDataset(X_train, y_train)\n",
    "test_ds  = EEGDataset(X_test,  y_test)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1002d8ce-9fbc-49a2-bc2b-cd3f38ed0d1b",
   "metadata": {},
   "source": [
    "#### **Model Creation**\n",
    "\n",
    "Hybrid CNN-LSTM architecture with dual-stream feature fusion:\n",
    "\n",
    "**CNN Branch (Spatial Feature Extraction):**\n",
    "- Conv2D layers with configurable kernels and sizes\n",
    "- Average pooling for downsampling\n",
    "- Dropout for regularization\n",
    "- Dense layer before LSTM for dimensionality reduction\n",
    "\n",
    "**LSTM Branch (Temporal Pattern Recognition):**\n",
    "- Multi-layer LSTM for sequence modeling\n",
    "- Configurable hidden size and layer depth\n",
    "- Dropout between LSTM layers\n",
    "- Final timestep aggregation\n",
    "\n",
    "**Band Power Branch:**\n",
    "- Dense layers for frequency domain features\n",
    "- ReLU activation and dropout\n",
    "\n",
    "**Fusion & Classification:**\n",
    "- Concatenate CNN-LSTM output with band power features\n",
    "- Two-layer classification head\n",
    "- Supports multi-class ADHD categorization\n",
    "\n",
    "**Training Features:**\n",
    "- Early stopping with patience parameter\n",
    "- Best model checkpoint restoration\n",
    "- Training/validation history tracking\n",
    "- Overfitting detection capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e658ff5-0dd2-4898-9f4a-d71d0cc79bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class EEGCNNLSTM(nn.Module):\n",
    "    def __init__(self, num_band_features=7, num_classes=4,\n",
    "                 cnn_kernels_1=32,\n",
    "                 cnn_kernel_size_1=3,\n",
    "                 cnn_kernels_2=32,\n",
    "                 cnn_kernel_size_2=3,\n",
    "                 cnn_dropout=0.3,\n",
    "                 cnn_dense=16,\n",
    "                 lstm_hidden_size=32,\n",
    "                 lstm_layers=4,\n",
    "                 lstm_dense=64,\n",
    "                 dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        pad1 = cnn_kernel_size_1 // 2\n",
    "        self.conv1   = nn.Conv2d(1, int(cnn_kernels_1), kernel_size=cnn_kernel_size_1, padding=pad1)\n",
    "        self.pool1 = nn.AvgPool2d(2)\n",
    "        \n",
    "        pad2 = cnn_kernel_size_2 // 2\n",
    "        self.conv2 = nn.Conv2d(int(cnn_kernels_1), int(cnn_kernels_2), kernel_size=cnn_kernel_size_2, padding=pad2)\n",
    "        self.cnn_dropout = nn.Dropout(cnn_dropout)\n",
    "\n",
    "        # Compute flatten size dynamically\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 1, X_train.shape[1], 19)\n",
    "            out = self._forward_cnn(dummy)   # [B, C, H, W]\n",
    "            b, c, h, w = out.shape\n",
    "            self.seq_len = h                      # sequence length (rows)\n",
    "            self.cnn_feat_dim = c * w             # CNN features per timestep\n",
    "\n",
    "        # Dense layer BEFORE LSTM\n",
    "        self.cnn_dense = nn.Linear(self.cnn_feat_dim, int(cnn_dense))\n",
    "\n",
    "        # Two stacked LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=int(cnn_dense),\n",
    "            hidden_size=int(lstm_hidden_size),\n",
    "            num_layers=int(lstm_layers),\n",
    "            batch_first=True,\n",
    "            dropout=dropout if lstm_layers > 1 else 0.0\n",
    "        )\n",
    "        # self.lstm_dense = nn.Linear(int(lstm_hidden_size), int(lstm_dense))\n",
    "        self.band_fc = nn.Sequential(\n",
    "            nn.Linear(num_band_features, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        \n",
    "        # final classifier (match your original final style: dropout + linear)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(int(lstm_hidden_size) + 32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes),\n",
    "        )\n",
    "\n",
    "    def _forward_cnn(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.cnn_dropout(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x_eeg, x_band):\n",
    "        # 1️⃣ CNN feature extraction\n",
    "        x = self._forward_cnn(x_eeg)             # [B, C, H, W]\n",
    "\n",
    "        # 2️⃣ Prepare sequence for LSTM\n",
    "        x = x.permute(0, 2, 1, 3)                 # [B, H, C, W]\n",
    "        x = x.contiguous().view(x.size(0), x.size(1), -1)  # [B, H, C*W]\n",
    "\n",
    "        # 3️⃣ Dense layer for each timestep\n",
    "        x = F.relu(self.cnn_dense(x))     # [B, H, dense_size]\n",
    "\n",
    "        # 4️⃣ Two-layer LSTM\n",
    "        lstm_out, _ = self.lstm(x)                # [B, H, hidden_size]\n",
    "\n",
    "        # 5️⃣ Use last time step (or mean/attention if preferred)\n",
    "        eeg_feat = lstm_out[:, -1, :]\n",
    "        # eeg_feat = lstm_out.mean(dim=1)                    # [B, hidden_size]\n",
    "        # eeg_feat = self.lstm_dense(eeg_feat)\n",
    "\n",
    "        # --- Band features ---\n",
    "        band_feat = self.band_fc(x_band)        # [B, 32]\n",
    "\n",
    "        # --- Fusion ---\n",
    "        fused = torch.cat([eeg_feat, band_feat], dim=1)\n",
    "\n",
    "        # 6️⃣ Fully connected head\n",
    "        x = self.classifier(fused)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def fit(self, train_loader, test_loader, epochs, criterion, optimizer, device, patience=100):\n",
    "        best_val_loss = float('inf')\n",
    "        no_improve = 0\n",
    "\n",
    "        train_losses, train_accs = [], []\n",
    "        val_losses, val_accs     = [], []\n",
    "\n",
    "        best_state = None\n",
    "        for epoch in range(epochs):\n",
    "            # --- Train ---\n",
    "            self.train()\n",
    "            train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "            for xb_eeg, xb_band, yb in train_loader:\n",
    "                xb_eeg = xb_eeg.to(device)\n",
    "                xb_band = xb_band.to(device)\n",
    "                yb = yb.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                out = self(xb_eeg, xb_band)\n",
    "                loss = criterion(out, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item() * xb_eeg.size(0)\n",
    "                train_correct += (out.argmax(1) == yb).sum().item()\n",
    "                train_total += yb.size(0)\n",
    "\n",
    "            train_loss /= train_total\n",
    "            train_acc  = train_correct / train_total\n",
    "            train_losses.append(train_loss)\n",
    "            train_accs.append(train_acc)\n",
    "\n",
    "            # --- Validate ---\n",
    "            self.eval()\n",
    "            val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "            with torch.no_grad():\n",
    "                for xb_eeg, xb_band, yb in test_loader:\n",
    "                    xb_eeg = xb_eeg.to(device)\n",
    "                    xb_band = xb_band.to(device)\n",
    "                    yb = yb.to(device)\n",
    "                    out = self(xb_eeg, xb_band)\n",
    "                    loss = criterion(out, yb)\n",
    "                    val_loss += loss.item() * xb_eeg.size(0)\n",
    "                    val_correct += (out.argmax(1) == yb).sum().item()\n",
    "                    val_total += yb.size(0)\n",
    "\n",
    "            val_loss /= val_total\n",
    "            val_acc  = val_correct / val_total\n",
    "            val_losses.append(val_loss)\n",
    "            val_accs.append(val_acc)\n",
    "\n",
    "            print(f\"Epoch {epoch+1:03d} | \"\n",
    "                  f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n",
    "\n",
    "            # if val_loss - train_loss > 0.2:\n",
    "            #     print(\"Overfitting detected.\")\n",
    "            #     break\n",
    "\n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_state = self.state_dict()\n",
    "                no_improve = 0\n",
    "            else:\n",
    "                no_improve += 1\n",
    "                if no_improve >= patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break\n",
    "\n",
    "        if best_state is not None:\n",
    "            self.load_state_dict(best_state)\n",
    "        return {\n",
    "            \"train_accs\": np.array(train_accs),\n",
    "            \"train_losses\": np.array(train_losses),\n",
    "            \"val_accs\":   np.array(val_accs),\n",
    "            \"val_losses\": np.array(val_losses)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ec1eaa-dcab-40b1-a171-1cf366326818",
   "metadata": {},
   "source": [
    "#### **First Model Training**\n",
    "\n",
    "Initial baseline model training with default hyperparameters:\n",
    "\n",
    "**Purpose:**\n",
    "- Establish baseline performance metrics\n",
    "- Verify model architecture and data pipeline\n",
    "- Identify potential issues before hyperparameter optimization\n",
    "\n",
    "**Training Configuration:**\n",
    "- 60 epochs with early stopping\n",
    "- Adam optimizer with L2 regularization\n",
    "- Cross-entropy loss for multi-class classification\n",
    "- Automatic GPU detection and utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f87827b-31e7-417d-b64f-8b93427bc531",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01683741-6af7-471c-8440-c72a1b8fd652",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = EEGCNNLSTM().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "history = model.fit(train_loader, test_loader, epochs=60, criterion=criterion,\n",
    "                    optimizer=optimizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20ab9a3-cf9a-4fbf-b321-8d29d0797a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for xb_eeg, xb_band, yb in test_loader:\n",
    "        xb_eeg = xb_eeg.to(device)\n",
    "        xb_band = xb_band.to(device)\n",
    "        preds = model(xb_eeg, xb_band).argmax(1).cpu().numpy()\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(yb.numpy())\n",
    "\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(all_labels, all_preds))\n",
    "print(classification_report(all_labels, all_preds))\n",
    "print(confusion_matrix(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ba6aae-6a97-4f20-9edd-a31e8d0a355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_accs'], label='Training Accuracy')\n",
    "plt.plot(history['val_accs'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['train_losses'], label='Training Loss')\n",
    "plt.plot(history['val_losses'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159c22db-c687-481f-8529-b23ffae9a542",
   "metadata": {},
   "source": [
    "#### **Helper Functions**\n",
    "\n",
    "Utility functions for hyperparameter optimization and model evaluation:\n",
    "\n",
    "**Core Functions:**\n",
    "- `batched()`: Iterator utility for creating batches (for cross-validation folds)\n",
    "- `get_timestamp()`: Logging timestamp generation\n",
    "- `get_model()`: Model instantiation with custom hyperparameters\n",
    "- `get_validation()`: Comprehensive model evaluation with metrics\n",
    "- `get_dataset()`: Dynamic dataset creation with configurable batch sizes\n",
    "\n",
    "**Evaluation Metrics:**\n",
    "- Accuracy score\n",
    "- Classification report (precision, recall, F1-score)\n",
    "- Confusion matrix\n",
    "- Cross-validation support\n",
    "\n",
    "These functions enable efficient experimentation and reproducible results across different hyperparameter configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0e739b5-97ce-4907-8c34-e24550813939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def batched(iterable, n, *, strict=False):\n",
    "    # batched('ABCDEFG', 2) → AB CD EF G\n",
    "    if n < 1:\n",
    "        raise ValueError('n must be at least one')\n",
    "    iterator = iter(iterable)\n",
    "    while batch := tuple(itertools.islice(iterator, n)):\n",
    "        if strict and len(batch) != n:\n",
    "            raise ValueError('batched(): incomplete batch')\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49309de8-22a6-422f-a577-69ee748d5178",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def get_timestamp():\n",
    "    ts = time.time()\n",
    "    return datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "def get_model(params):\n",
    "    model = EEGCNNLSTM(\n",
    "        cnn_kernels_1=params['cnn_kernels_1'],\n",
    "        cnn_kernel_size_1=params['cnn_kernel_size_1'],\n",
    "        cnn_kernels_2=params['cnn_kernels_2'],\n",
    "        cnn_dropout=float(params['cnn_dropout']),\n",
    "        cnn_dense=params['cnn_dense'],\n",
    "        lstm_hidden_size=params['lstm_hidden_size'],\n",
    "        lstm_layers=params['lstm_layers'],\n",
    "        lstm_dense=params['lstm_dense'],\n",
    "        dropout=float(params['cnn_dropout']),  # use cnn_dropout as a simple shared dropout param\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if params['optimizer'] == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'], weight_decay=1e-4)\n",
    "    elif params['optimizer'] == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=params['learning_rate'], weight_decay=1e-4)\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=params['learning_rate'], momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "    return model, criterion, optimizer\n",
    "\n",
    "def get_validation(model, data_loader, device, matrix=True):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb_eeg, xb_band, yb in data_loader:\n",
    "            xb_eeg = xb_eeg.to(device)\n",
    "            xb_band = xb_band.to(device)\n",
    "            preds = model(xb_eeg, xb_band).argmax(1).cpu().numpy()\n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(yb.numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    report = classification_report(all_labels, all_preds)\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds) if matrix else None\n",
    "\n",
    "    return acc, report, conf_matrix\n",
    "\n",
    "def get_dataset(df, is_train=False, batch_size=36):\n",
    "    frequency_count = len(df['Frequency'].unique())\n",
    "    window_count = len(df['Window'].unique())\n",
    "    numeric_df = df.drop(['ID', 'Window'], axis=1)\n",
    "\n",
    "    # shape: (windows, freqs, features)\n",
    "    full_ndarray = numeric_df.values.reshape((window_count, frequency_count, numeric_df.shape[1]))\n",
    "\n",
    "    X = full_ndarray[:, :, 2:]     # drop ID/Class columns\n",
    "    y = full_ndarray[:, 0, 0]      # class label is repeated across freq rows\n",
    "\n",
    "    # Add channel dimension (N, 1, freq, electrodes)\n",
    "    X = X[..., np.newaxis]          # (N, freq, electrodes, 1)\n",
    "\n",
    "    print(X.shape)\n",
    "\n",
    "    return DataLoader(EEGDataset(X, y), batch_size=batch_size, shuffle=is_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfcbdbb-a909-4bd8-b736-258d0265d095",
   "metadata": {},
   "source": [
    "#### **Search Space Definition**\n",
    "\n",
    "Hyperparameter search space for Bayesian Optimization using Hyperopt:\n",
    "\n",
    "**CNN Architecture:**\n",
    "- `cnn_kernels_1/2`: Number of convolutional filters [16, 32, 48, 64, 96]\n",
    "- `cnn_kernel_size_1/2`: Kernel dimensions [3x3, 5x5]\n",
    "- `cnn_dropout`: Regularization rate [0.0 - 0.7]\n",
    "- `cnn_dense`: Dense layer size before LSTM [32, 64, 128, 256]\n",
    "\n",
    "**LSTM Architecture:**\n",
    "- `lstm_hidden_size`: Hidden state dimensions [32, 64, 96, 128]\n",
    "- `lstm_layers`: Number of stacked LSTM layers [1-6]\n",
    "- `lstm_dense`: Dense layer size after LSTM [32, 64, 128, 256]\n",
    "\n",
    "**Training Configuration:**\n",
    "- `learning_rate`: Log-uniform distribution [1e-5, 1e-2]\n",
    "- `optimizer`: Choice of Adam, RMSprop, or SGD\n",
    "- `batch_size`: Samples per batch [32, 36, 48, 64, 80, 96]\n",
    "\n",
    "**Optimization Strategy:**\n",
    "Tree-structured Parzen Estimator (TPE) algorithm balances exploration and exploitation to efficiently search the hyperparameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32e30627-801f-426c-985f-887d0b0b04c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK\n",
    "\n",
    "# -------------------------\n",
    "# Hyperopt search space\n",
    "# -------------------------\n",
    "space = {\n",
    "    'cnn_kernels_1'    : hp.choice('cnn_kernels_1', [16, 32, 48, 64]),\n",
    "    'cnn_kernel_size_1': hp.choice('cnn_kernel_size_1', [3, 5]),\n",
    "    'cnn_kernels_2'    : hp.choice('cnn_kernels_2', [16, 32, 64, 96]),\n",
    "    'cnn_kernel_size_2': hp.choice('cnn_kernel_size_2', [3, 5]),\n",
    "    'cnn_dropout'      : hp.uniform('cnn_dropout', 0.0, 0.7),\n",
    "    'cnn_dense'        : hp.choice('cnn_dense', [32, 64, 128, 256]),\n",
    "    'lstm_hidden_size' : hp.choice('lstm_hidden_size', [32, 64, 96, 128]),\n",
    "    'lstm_layers'      : hp.choice('lstm_layers', [1, 2, 3, 4, 5, 6]),\n",
    "    'lstm_dense'       : hp.choice('lstm_dense', [32, 64, 128, 256]),\n",
    "    'learning_rate'    : hp.loguniform('learning_rate', np.log(1e-5), np.log(1e-2)),\n",
    "    'optimizer'        : hp.choice('optimizer', ['adam', 'rmsprop', 'sgd']),\n",
    "    'batch_size'       : hp.choice('batch_size', [32, 36, 48, 64, 80, 96])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3db2be-a073-42a6-9466-4468ebd16381",
   "metadata": {},
   "source": [
    "#### **Search Objective Definition**\n",
    "\n",
    "Objective function for Hyperopt optimization with two strategies:\n",
    "\n",
    "### Strategy 1: K-Fold Cross-Validation (Commented)\n",
    "**Approach:**\n",
    "- Subject-stratified K-fold cross-validation\n",
    "- Cyclic fold generation for balanced distribution\n",
    "- Score calculation: `mean_loss + variance_of_tail_losses`\n",
    "- More robust but computationally expensive\n",
    "\n",
    "**Metrics:**\n",
    "- Mean validation loss across all folds\n",
    "- Variance of final validation losses (stability measure)\n",
    "- Combined score penalizes both high loss and instability\n",
    "\n",
    "### Strategy 2: Single Train/Test Split (Active)\n",
    "**Approach:**\n",
    "- Uses predefined train/test split\n",
    "- Faster iteration for initial hyperparameter search\n",
    "- Early stopping with patience=10\n",
    "\n",
    "**Return Value:**\n",
    "- Minimizes best validation loss\n",
    "- Includes training history for analysis\n",
    "- STATUS_OK for successful trials\n",
    "\n",
    "**Note:** The objective function is called by Hyperopt's `fmin()` and should return a dictionary with 'loss' and 'status' keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69653680-cc67-4a43-9456-5551beeed641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# -------------------------\n",
    "# k-Fold CV for Hyperopt\n",
    "# -------------------------\n",
    "def objective(params):\n",
    "    print(\"Trial params:\", params)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    unique_subjects = df['ID'].unique()\n",
    "    losses = []\n",
    "    variances = []\n",
    "    batch_size = params['batch_size']\n",
    "\n",
    "    K_FOLDS = 5\n",
    "    fold_size = len(unique_subjects) // K_FOLDS\n",
    "\n",
    "    cyclic = itertools.cycle(unique_subjects)\n",
    "    batched_cyclic = batched(cyclic, n=fold_size)\n",
    "    folds = itertools.islice(batched_cyclic, K_FOLDS)\n",
    "\n",
    "    for i, fold in enumerate(folds):\n",
    "        print(f\"Starting fold {i + 1}/{K_FOLDS}\")\n",
    "\n",
    "        train_df = df[~df['ID'].isin(fold)]\n",
    "        test_df  = df[df['ID'].isin(fold)]\n",
    "\n",
    "        print(train_df.shape, test_df.shape)\n",
    "\n",
    "        train_loader = get_dataset(train_df, batch_size=batch_size)\n",
    "        test_loader  = get_dataset(test_df, batch_size=batch_size)\n",
    "        model, criterion, optimizer = get_model(params)\n",
    "\n",
    "        # Train with modest epochs; early stopping inside fit handles rest\n",
    "        history = model.fit(\n",
    "            train_loader=train_loader,\n",
    "            test_loader=test_loader,\n",
    "            epochs=60,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "            patience=15\n",
    "        )\n",
    "\n",
    "        acc, *_ = get_validation(model, test_loader, device)\n",
    "        loss = history['val_losses']\n",
    "        mean_loss = np.min(loss)\n",
    "        losses.append(mean_loss)\n",
    "\n",
    "        last_5_or_less = history[\"val_losses\"]\n",
    "        last_5_or_less = last_5_or_less[-min(len(last_5_or_less), 5):]\n",
    "        variance = np.var(last_5_or_less) if len(last_5_or_less) > 1 else 1\n",
    "        variances.append(variance)\n",
    "\n",
    "        print(f\"Fold {i + 1} Accuracy:\", acc)\n",
    "\n",
    "    loss = np.mean(losses)\n",
    "    tail_variance = np.var(variances)\n",
    "    print(variances)\n",
    "    score = loss + tail_variance\n",
    "\n",
    "    print(f\"k-Fold CV Mean Loss: {loss:.4f} ± {np.std(losses):.4f}\")\n",
    "    print(f\"k-Fold CV Tail Variance: {tail_variance:.4f}\")\n",
    "\n",
    "    # Hyperopt minimizes -> return negative accuracy\n",
    "    return {'loss': score, 'status': STATUS_OK, 'attachments': {'history': history}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fdbeed8-2f69-45bb-b4ee-d5e73b4c2f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Single Objective for Hyperopt\n",
    "# -------------------------\n",
    "def objective(params):\n",
    "    print(\"Trial params:\", params)\n",
    "\n",
    "    # build dataloaders from the existing train_ds/test_ds in this session\n",
    "    train_loader = DataLoader(train_ds, batch_size=params['batch_size'], shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=params['batch_size'], shuffle=False)\n",
    "\n",
    "    # create model (note we pass dropout into lstm dropout and cnn dropout)\n",
    "    model, criterion, optimizer = get_model(params)\n",
    "\n",
    "    # Train with modest epochs; early stopping inside fit handles rest\n",
    "    history = model.fit(\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        epochs=60,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        patience=10\n",
    "    )\n",
    "\n",
    "    best_val_loss = float(np.min(history['val_losses'])) if len(history['val_losses']) > 0 else 0.0\n",
    "\n",
    "    # Hyperopt minimizes -> return negative accuracy\n",
    "    return {'loss': best_val_loss, 'status': STATUS_OK, 'attachments': {'history': history, 'best_val_loss': best_val_loss}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bbef8d-8af5-4fbc-bf88-ad2a8ae921af",
   "metadata": {},
   "source": [
    "#### **Hyperparameter Search**\n",
    "\n",
    "Execute Tree-structured Parzen Estimator (TPE) search for optimal hyperparameters:\n",
    "\n",
    "**Search Configuration:**\n",
    "- `max_evals`: Number of trials (default: 30, increase for thorough search)\n",
    "- Algorithm: TPE (adaptive Bayesian optimization)\n",
    "- Tracks all trials in Hyperopt Trials object\n",
    "\n",
    "**Process:**\n",
    "1. Initialize trials tracking object\n",
    "2. Run TPE algorithm over search space\n",
    "3. Convert raw indices to interpretable values\n",
    "4. Save best parameters to JSON\n",
    "\n",
    "**Output:**\n",
    "- **Best hyperparameters**: Both raw indices and interpreted values\n",
    "- **Search duration**: Total optimization time\n",
    "- **Parameters JSON**: Serialized configuration for model reproduction\n",
    "\n",
    "**Multiple Runs:**\n",
    "The second cell executes 10 independent search runs to:\n",
    "- Assess hyperparameter sensitivity\n",
    "- Identify robust configurations\n",
    "- Build ensemble of candidate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086894b3-97e9-4755-9b94-886320886f39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from hyperopt import Trials, fmin\n",
    "\n",
    "def hyperparameter_search(max_evals=30, seed=None):\n",
    "    trials = Trials()\n",
    "    \n",
    "    # Create seeded random state for Hyperopt\n",
    "    rstate = np.random.RandomState(seed) if seed is not None else None\n",
    "    \n",
    "    print(\"Starting TPE search...\")\n",
    "    if seed is not None:\n",
    "        print(f\"Using seed: {seed}\")\n",
    "    t0 = time.time()\n",
    "    best = fmin(\n",
    "        fn=objective,\n",
    "        space=space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=max_evals,   # increase for more thorough search\n",
    "        trials=trials,\n",
    "        rstate=rstate  # CRITICAL: seed Hyperopt's random state\n",
    "    )\n",
    "    \n",
    "    print(\"Best hyperparameters:\", best)\n",
    "    t1 = time.time()\n",
    "    duration = t1 - t0\n",
    "    print(f\"TPE search finished in {duration:.2f} seconds\")\n",
    "    print(\"Best (raw indices):\", best)\n",
    "    \n",
    "    # Convert choice indices back to values for readability:\n",
    "    def choice_value(key, val):\n",
    "        mapping = {\n",
    "            'cnn_kernels_1': [16, 32, 48, 64],\n",
    "            'cnn_kernel_size_1': [3, 5],\n",
    "            'cnn_kernels_2': [16, 32, 64, 96],\n",
    "            'cnn_kernel_size_2': [3, 5],\n",
    "            'cnn_dense': [32, 64, 128, 256],\n",
    "            'lstm_hidden_size': [32, 64, 96, 128],\n",
    "            'lstm_layers': [1, 2, 3, 4, 5, 6],\n",
    "            'lstm_dense': [32, 64, 128, 256],\n",
    "            'optimizer': ['adam', 'rmsprop', 'sgd'],\n",
    "            'batch_size': [32, 36, 48, 64, 80, 96]\n",
    "        }\n",
    "        return mapping[key][int(val)] if key in mapping else val\n",
    "    \n",
    "    readable = {k: choice_value(k, v) if k in ['cnn_kernels_1','cnn_kernel_size_1','cnn_kernels_2',\n",
    "                                               'cnn_kernel_size_2', 'cnn_dense','lstm_hidden_size',\n",
    "                                               'lstm_layers','lstm_dense','optimizer','batch_size'] else v\n",
    "                for k,v in best.items()}\n",
    "    print(\"Best (interpreted):\", readable)\n",
    "\n",
    "    params = dict(readable)\n",
    "    params['cnn_kernels_1'] = int(params['cnn_kernels_1'])\n",
    "    params['cnn_kernel_size_1'] = int(params['cnn_kernel_size_1'])\n",
    "    params['cnn_kernels_2'] = int(params['cnn_kernels_2'])\n",
    "    params['cnn_kernel_size_2'] = int(params['cnn_kernel_size_2'])\n",
    "    params['cnn_dense'] = int(params['cnn_dense'])\n",
    "    params['lstm_hidden_size'] = int(params['lstm_hidden_size'])\n",
    "    params['lstm_layers'] = int(params['lstm_layers'])\n",
    "    params['lstm_dense'] = int(params['lstm_dense'])\n",
    "    params['batch_size'] = int(params['batch_size'])\n",
    "    params['cnn_dropout'] = float(params['cnn_dropout'])\n",
    "    params['dropout'] = float(params['cnn_dropout'])\n",
    "\n",
    "    return trials, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f549aa-3b7b-4bce-bd19-460597af1f5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trials, params = hyperparameter_search()\n",
    "with open(\"best_parameters.json\", \"w+\") as f:\n",
    "    import json\n",
    "    json.dump(params, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0526fe-c2b6-44b6-801c-b85489073f88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trial_results = []\n",
    "\n",
    "for i in range(10):\n",
    "    trials, params = hyperparameter_search()\n",
    "    trial_results.append({\"trials\": trials, \"params\": params})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3309ea-f41e-4d51-a860-176d3821da54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def to_json_safe(obj):\n",
    "    \"\"\"Recursively convert objects to JSON-serializable types.\"\"\"\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    if isinstance(obj, (np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    if isinstance(obj, (np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.isoformat()\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: to_json_safe(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, (list, tuple)):\n",
    "        return [to_json_safe(v) for v in obj]\n",
    "    return obj\n",
    "\n",
    "\n",
    "def serialize_trial(trial):\n",
    "    \"\"\"Extract only JSON-safe and meaningful parts of a Hyperopt Trial.\"\"\"\n",
    "    print(trial.attachments)\n",
    "    return {\n",
    "        \"attachments\": to_json_safe(trial.attachments)\n",
    "    }\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# DROP-IN REPLACEMENT STARTS HERE\n",
    "# ===============================\n",
    "\n",
    "json_results = []\n",
    "\n",
    "for entry in trial_results:\n",
    "    json_results.append({\n",
    "        \"params\": to_json_safe(entry.get(\"params\")),\n",
    "        \"trial\": serialize_trial(entry.get(\"trials\")),\n",
    "    })\n",
    "\n",
    "with open(\"all_trials.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {\"results\": json_results},\n",
    "        f,\n",
    "        indent=4,\n",
    "        ensure_ascii=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ffed76",
   "metadata": {},
   "source": [
    "#### **Results Visualization**\n",
    "\n",
    "Comprehensive hyperparameter optimization with iterative Bayesian Optimization and convergence tracking:\n",
    "\n",
    "---\n",
    "\n",
    "### **Iterative Convergence-Based Bayesian Optimization**\n",
    "\n",
    "**Multi-Run Strategy:**\n",
    "- Starts with 3 initial BO searches with different random seeds\n",
    "- Runs 2 additional iterations and recalculates standard deviation\n",
    "- Continues until std dev stabilizes (relative change < 0.1%)\n",
    "- Tracks convergence to ensure robust hyperparameter selection\n",
    "\n",
    "**Reproducibility:**\n",
    "- Each BO run uses a unique sequential seed (base_seed + run_index)\n",
    "- Seeds control PyTorch, NumPy, Python random states, and Hyperopt's TPE sampler\n",
    "- Full seed tracking for experiment reproduction\n",
    "\n",
    "**Stability Criteria:**\n",
    "- Convergence: `|current_std - previous_std| / previous_std < 0.001`\n",
    "- Reduces sensitivity to random initialization\n",
    "- Ensures identified hyperparameters are stable across multiple searches\n",
    "\n",
    "**Safety Measures:**\n",
    "- Maximum limit of 20 total BO runs\n",
    "- Prevents infinite loops while allowing thorough exploration\n",
    "\n",
    "---\n",
    "\n",
    "### **Comprehensive Visualization and Analysis**\n",
    "\n",
    "**Performance Summary:**\n",
    "- Table of all BO runs with seeds and validation losses\n",
    "- Ranking of configurations by performance\n",
    "- Statistical analysis: mean, std dev, min/max, range\n",
    "\n",
    "**Convergence Plots:**\n",
    "- Best validation loss across all runs (bar chart with annotations)\n",
    "- Standard deviation evolution over iterations\n",
    "- Percentage change annotations between measurements\n",
    "- Color-coded convergence indicators (green when < 0.1% change)\n",
    "\n",
    "**Visual Highlights:**\n",
    "- Best performing run highlighted in green\n",
    "- Mean and minimum loss reference lines\n",
    "- Convergence threshold visualization\n",
    "\n",
    "---\n",
    "\n",
    "### **Best Parameters Extraction**\n",
    "\n",
    "**Selection Process:**\n",
    "- Systematically searches across all BO runs\n",
    "- Identifies trial with lowest validation loss\n",
    "- Extracts complete hyperparameter configuration\n",
    "\n",
    "**Output:**\n",
    "- Best run index and seed for reproducibility\n",
    "- Complete hyperparameter dictionary\n",
    "- Best validation loss achieved\n",
    "- Saves to `best_parameters.json` for deployment\n",
    "\n",
    "---\n",
    "\n",
    "### **Final Model Training with Optimal Hyperparameters**\n",
    "\n",
    "**Evaluation:**\n",
    "- Trains model with best discovered hyperparameters\n",
    "- Reports test set accuracy, classification report, confusion matrix\n",
    "- Visualizes training history (accuracy and loss curves)\n",
    "- Confirms model generalization performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faee892-23fa-430f-8bde-2e9ff855827a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def run_bo_with_seed(seed, max_evals=30):\n",
    "    \"\"\"Run Bayesian Optimization with a specific seed.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running BO with seed: {seed}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    set_seed(seed)\n",
    "    # CRITICAL: Pass seed to hyperparameter_search to seed Hyperopt\n",
    "    trials, params = hyperparameter_search(max_evals=max_evals, seed=seed)\n",
    "    \n",
    "    # Extract best validation loss from this run\n",
    "    best_val_loss = float('inf')\n",
    "    for trial in trials.trials:\n",
    "        if trial['result']['status'] == 'ok':\n",
    "            loss = trial['result']['loss']\n",
    "            if loss < best_val_loss:\n",
    "                best_val_loss = loss\n",
    "    \n",
    "    return {\n",
    "        'seed': seed,\n",
    "        'trials': trials,\n",
    "        'params': params,\n",
    "        'best_val_loss': best_val_loss\n",
    "    }\n",
    "\n",
    "# Initialize tracking variables\n",
    "all_bo_runs = []\n",
    "std_devs = []\n",
    "run_counts = []  # Track number of runs for each std_dev calculation\n",
    "base_seed = 42\n",
    "\n",
    "print(\"Starting iterative Bayesian Optimization with convergence tracking...\")\n",
    "print(f\"Convergence criterion: Standard deviation change < 0.1%\\n\")\n",
    "\n",
    "# Initial 3 runs\n",
    "print(\"Phase 1: Running initial 3 BO searches...\")\n",
    "for i in range(3):\n",
    "    seed = base_seed + i\n",
    "    result = run_bo_with_seed(seed, max_evals=30)\n",
    "    all_bo_runs.append(result)\n",
    "    \n",
    "    print(f\"\\nRun {i+1}/3 completed:\")\n",
    "    print(f\"  Seed: {seed}\")\n",
    "    print(f\"  Best Val Loss: {result['best_val_loss']:.6f}\")\n",
    "\n",
    "# Calculate std dev only after all 3 initial runs\n",
    "best_losses = [r['best_val_loss'] for r in all_bo_runs]\n",
    "current_std = np.std(best_losses)\n",
    "std_devs.append(current_std)\n",
    "run_counts.append(len(all_bo_runs))\n",
    "\n",
    "print(f\"\\nPhase 1 Summary:\")\n",
    "print(f\"  Total Runs: {len(all_bo_runs)}\")\n",
    "print(f\"  Std Dev: {current_std:.6f}\")\n",
    "\n",
    "# Iterative refinement\n",
    "iteration = 2\n",
    "converged = False\n",
    "prev_std = std_devs[-1]\n",
    "\n",
    "while not converged:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Phase {iteration}: Running 2 additional BO searches...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Run 2 more BO searches\n",
    "    for i in range(2):\n",
    "        seed = base_seed + len(all_bo_runs)\n",
    "        result = run_bo_with_seed(seed, max_evals=30)\n",
    "        all_bo_runs.append(result)\n",
    "        \n",
    "        print(f\"\\nRun {len(all_bo_runs)} completed:\")\n",
    "        print(f\"  Seed: {seed}\")\n",
    "        print(f\"  Best Val Loss: {result['best_val_loss']:.6f}\")\n",
    "    \n",
    "    # Calculate new std dev after both runs complete\n",
    "    best_losses = [r['best_val_loss'] for r in all_bo_runs]\n",
    "    current_std = np.std(best_losses)\n",
    "    std_devs.append(current_std)\n",
    "    run_counts.append(len(all_bo_runs))\n",
    "    \n",
    "    # Check convergence\n",
    "    std_change = abs(current_std - prev_std) / prev_std if prev_std > 0 else 1.0\n",
    "    \n",
    "    print(f\"\\n--- Convergence Check ---\")\n",
    "    print(f\"Total Runs:       {len(all_bo_runs)}\")\n",
    "    print(f\"Previous Std Dev: {prev_std:.6f}\")\n",
    "    print(f\"Current Std Dev:  {current_std:.6f}\")\n",
    "    print(f\"Relative Change:  {std_change*100:.4f}%\")\n",
    "    \n",
    "    if std_change < 0.001:  # 0.1% threshold\n",
    "        print(f\"\\n✓ Convergence achieved! Std dev change < 0.1%\")\n",
    "        converged = True\n",
    "    else:\n",
    "        print(f\"\\n→ Not converged yet, continuing...\")\n",
    "        prev_std = current_std\n",
    "        iteration += 1\n",
    "        \n",
    "        # Safety limit\n",
    "        if len(all_bo_runs) >= 20:\n",
    "            print(f\"\\n⚠ Reached maximum of 20 runs, stopping.\")\n",
    "            converged = True\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Optimization Complete!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total BO runs: {len(all_bo_runs)}\")\n",
    "print(f\"Final Std Dev: {std_devs[-1]:.6f}\")\n",
    "print(f\"Std Dev tracked at runs: {run_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc72a561-4b54-4c8d-bc7d-73e9040c385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Create summary table\n",
    "summary_data = []\n",
    "for i, run in enumerate(all_bo_runs):\n",
    "    summary_data.append({\n",
    "        'Run': i + 1,\n",
    "        'Seed': run['seed'],\n",
    "        'Best Val Loss': run['best_val_loss'],\n",
    "        'Rank': 0  # Will be filled after sorting\n",
    "    })\n",
    "\n",
    "# Sort by best val loss and assign ranks\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df = summary_df.sort_values('Best Val Loss')\n",
    "summary_df['Rank'] = range(1, len(summary_df) + 1)\n",
    "summary_df = summary_df.sort_values('Run')  # Resort by run order\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY OF BAYESIAN OPTIMIZATION RUNS\")\n",
    "print(\"=\"*70)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate statistics\n",
    "mean_loss = summary_df['Best Val Loss'].mean()\n",
    "std_loss = summary_df['Best Val Loss'].std()\n",
    "min_loss = summary_df['Best Val Loss'].min()\n",
    "max_loss = summary_df['Best Val Loss'].max()\n",
    "\n",
    "print(f\"\\nStatistics:\")\n",
    "print(f\"  Mean Best Loss: {mean_loss:.6f}\")\n",
    "print(f\"  Std Dev:        {std_loss:.6f}\")\n",
    "print(f\"  Min Loss:       {min_loss:.6f}\")\n",
    "print(f\"  Max Loss:       {max_loss:.6f}\")\n",
    "print(f\"  Range:          {max_loss - min_loss:.6f}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Best validation losses across runs\n",
    "ax1 = axes[0]\n",
    "runs = summary_df['Run'].values\n",
    "losses = summary_df['Best Val Loss'].values\n",
    "colors = ['#2ecc71' if loss == min_loss else '#3498db' for loss in losses]\n",
    "\n",
    "bars = ax1.bar(runs, losses, color=colors, alpha=0.7, edgecolor='black', linewidth=1.2)\n",
    "ax1.axhline(y=mean_loss, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_loss:.4f}')\n",
    "ax1.axhline(y=min_loss, color='green', linestyle='--', linewidth=2, alpha=0.5, label=f'Best: {min_loss:.4f}')\n",
    "\n",
    "ax1.set_xlabel('BO Run', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Best Validation Loss', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Best Validation Loss per BO Run', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "ax1.set_xticks(runs)\n",
    "\n",
    "# Annotate the best run\n",
    "best_run_idx = summary_df['Best Val Loss'].idxmin()\n",
    "best_run = summary_df.loc[best_run_idx, 'Run']\n",
    "best_loss = summary_df.loc[best_run_idx, 'Best Val Loss']\n",
    "ax1.annotate(f'Best\\n{best_loss:.4f}', \n",
    "             xy=(best_run, best_loss),\n",
    "             xytext=(best_run, best_loss + (max_loss - min_loss) * 0.1),\n",
    "             ha='center',\n",
    "             fontsize=9,\n",
    "             fontweight='bold',\n",
    "             color='darkgreen',\n",
    "             arrowprops=dict(arrowstyle='->', color='darkgreen', lw=1.5))\n",
    "\n",
    "# Plot 2: Standard deviation convergence\n",
    "ax2 = axes[1]\n",
    "\n",
    "# Use the tracked run_counts for accurate x-axis\n",
    "ax2.plot(run_counts, std_devs, marker='o', linewidth=2.5, markersize=8, \n",
    "         color='#e74c3c', markerfacecolor='white', markeredgewidth=2)\n",
    "ax2.set_xlabel('Number of BO Runs', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Std Dev of Best Losses', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Convergence of Standard Deviation', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add convergence threshold line\n",
    "if len(std_devs) > 1:\n",
    "    converged_std = std_devs[-1]\n",
    "    ax2.axhline(y=converged_std, color='green', linestyle='--', linewidth=2, \n",
    "                alpha=0.5, label=f'Final: {converged_std:.4f}')\n",
    "    ax2.legend(fontsize=10)\n",
    "\n",
    "# Annotate percentage changes between consecutive measurements\n",
    "for i in range(1, len(std_devs)):\n",
    "    pct_change = abs(std_devs[i] - std_devs[i-1]) / std_devs[i-1] * 100 if std_devs[i-1] > 0 else 0\n",
    "    mid_x = (run_counts[i] + run_counts[i-1]) / 2\n",
    "    mid_y = (std_devs[i] + std_devs[i-1]) / 2\n",
    "    \n",
    "    # Color code: red if change > 0.1%, green if <= 0.1%\n",
    "    box_color = 'lightgreen' if pct_change <= 0.1 else 'yellow'\n",
    "    \n",
    "    ax2.annotate(f'{pct_change:.2f}%', \n",
    "                xy=(mid_x, mid_y),\n",
    "                fontsize=8,\n",
    "                ha='center',\n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor=box_color, alpha=0.5, edgecolor='gray'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Visualization complete.\")\n",
    "print(f\"\\nConvergence History:\")\n",
    "for i, (runs, std) in enumerate(zip(run_counts, std_devs)):\n",
    "    if i == 0:\n",
    "        print(f\"  After {runs} runs: Std Dev = {std:.6f}\")\n",
    "    else:\n",
    "        prev_std = std_devs[i-1]\n",
    "        pct_change = abs(std - prev_std) / prev_std * 100 if prev_std > 0 else 0\n",
    "        status = \"✓ Converged\" if pct_change < 0.1 else \"→ Continuing\"\n",
    "        print(f\"  After {runs} runs: Std Dev = {std:.6f} (Change: {pct_change:.2f}%) {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7356bf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def extract_best_params_from_runs(all_runs):\n",
    "    \"\"\"Extract parameters from the best performing BO run.\"\"\"\n",
    "    best_overall_loss = float('inf')\n",
    "    best_params = None\n",
    "    best_run_index = -1\n",
    "    best_seed = None\n",
    "    \n",
    "    for run_idx, run_data in enumerate(all_runs):\n",
    "        if run_data['best_val_loss'] < best_overall_loss:\n",
    "            best_overall_loss = run_data['best_val_loss']\n",
    "            best_params = run_data['params']\n",
    "            best_run_index = run_idx + 1\n",
    "            best_seed = run_data['seed']\n",
    "    \n",
    "    if best_params:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"BEST PERFORMING CONFIGURATION\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Run Index:        {best_run_index}/{len(all_runs)}\")\n",
    "        print(f\"Seed:             {best_seed}\")\n",
    "        print(f\"Best Val Loss:    {best_overall_loss:.6f}\")\n",
    "        print(f\"\\nHyperparameters:\")\n",
    "        print(json.dumps(best_params, indent=4))\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Save best parameters\n",
    "        output_data = {\n",
    "            'run_index': best_run_index,\n",
    "            'seed': best_seed,\n",
    "            'best_val_loss': float(best_overall_loss),\n",
    "            'parameters': best_params\n",
    "        }\n",
    "        \n",
    "        with open(\"best_parameters.json\", \"w\") as f:\n",
    "            json.dump(output_data, f, indent=4)\n",
    "        \n",
    "        print(f\"\\n✓ Best parameters saved to 'best_parameters.json'\")\n",
    "        \n",
    "        return best_params\n",
    "    else:\n",
    "        print(\"Error: No valid parameters found.\")\n",
    "        return None\n",
    "\n",
    "# Extract best parameters from all BO runs\n",
    "best_params = extract_best_params_from_runs(all_bo_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6535bb83-2d15-43e5-b0c9-3c628a4c9201",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# build dataloaders from the existing train_ds/test_ds in this session\n",
    "train_loader = DataLoader(train_ds, batch_size=best_params['batch_size'], shuffle=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=best_params['batch_size'], shuffle=False)\n",
    "\n",
    "# create model (note we pass dropout into lstm dropout and cnn dropout)\n",
    "model, criterion, optimizer = get_model(best_params)\n",
    "\n",
    "# Train with modest epochs; early stopping inside fit handles rest\n",
    "history = model.fit(\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    epochs=60,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    patience=10\n",
    ")\n",
    "acc, report, matrix = get_validation(model, test_loader, device)\n",
    "\n",
    "print(\"\\nval Accuracy:\", acc)\n",
    "print(report)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d952c1-9d35-4bc1-bf41-942e5dddab77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_accs'], label='Training Accuracy')\n",
    "plt.plot(history['val_accs'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['train_losses'], label='Training Loss')\n",
    "plt.plot(history['val_losses'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4e5135",
   "metadata": {},
   "source": [
    "#### **Cross-Validation Experiments**\n",
    "\n",
    "Comprehensive model evaluation using multiple cross-validation strategies to assess generalization:\n",
    "\n",
    "---\n",
    "\n",
    "### **Experiment 1: K-Fold Window-Based Cross-Validation**\n",
    "\n",
    "**Configuration:**\n",
    "- K=11 folds\n",
    "- Split by temporal window segments (not subjects)\n",
    "- Random shuffle before folding\n",
    "- Uses cyclic iterator for balanced fold distribution\n",
    "\n",
    "**Purpose:**\n",
    "- Assess model generalization to unseen temporal windows\n",
    "- Tests ability to classify different time segments\n",
    "- Faster than subject-based CV\n",
    "- Subject data can appear in both train and test sets\n",
    "\n",
    "**Characteristics:**\n",
    "- May overestimate performance (data leakage from same subject)\n",
    "- Good for assessing temporal pattern learning\n",
    "- Less representative of real-world deployment\n",
    "\n",
    "---\n",
    "\n",
    "### **Experiment 2: Leave-One-Subject-Out Cross-Validation (LOSOCV)**\n",
    "\n",
    "**Configuration:**\n",
    "- Folds equal to number of unique subjects\n",
    "- Each fold: one subject for testing, all others for training\n",
    "- Windows from same subject grouped together\n",
    "- No data leakage between train/test\n",
    "\n",
    "**Purpose:**\n",
    "- **Gold standard** for subject-independent evaluation\n",
    "- Tests true generalization to new, unseen individuals\n",
    "- Most realistic clinical deployment scenario\n",
    "- Accounts for inter-subject variability\n",
    "\n",
    "**Clinical Relevance:**\n",
    "- Simulates real diagnostic workflow\n",
    "- Model must generalize to patients not in training set\n",
    "- Identifies subject-specific overfitting\n",
    "- More conservative performance estimate\n",
    "\n",
    "---\n",
    "\n",
    "### **Evaluation Metrics**\n",
    "\n",
    "Both experiments report:\n",
    "- **Mean accuracy** across all folds\n",
    "- **Standard deviation** (indicates model stability)\n",
    "- **Per-fold accuracy** for detailed analysis\n",
    "\n",
    "**Interpretation:**\n",
    "- Lower std dev = more stable, reliable model\n",
    "- Window-based: baseline generalization capability\n",
    "- Subject-based: clinically relevant performance\n",
    "\n",
    "---\n",
    "\n",
    "### **Trade-offs Summary**\n",
    "\n",
    "| Aspect | Window-Based K-Fold | Leave-One-Subject-Out |\n",
    "|--------|---------------------|------------------------|\n",
    "| **Speed** | Faster | Slower (more folds) |\n",
    "| **Realism** | Less realistic | Highly realistic |\n",
    "| **Performance** | Often higher | More conservative |\n",
    "| **Data Leakage** | Possible | None |\n",
    "| **Clinical Value** | Limited | High |\n",
    "| **Use Case** | Quick validation | Final evaluation |\n",
    "\n",
    "Both approaches validate model robustness from complementary perspectives and help identify different types of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7d160b-68be-4b06-976c-97bcbd423f07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# map params to integers where needed\n",
    "params = {'batch_size': 64, 'cnn_dense': 256, 'cnn_dropout': np.float64(0.3571401842400106), 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 3, 'cnn_kernels_1': 32, 'cnn_kernels_2': 16, 'learning_rate': np.float64(1.8587640600578385e-05), 'lstm_dense': 64, 'lstm_hidden_size': 96, 'lstm_layers': 2, 'optimizer': 'sgd'}\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "unique_subjects = list(df['Window'].unique())\n",
    "accs = []\n",
    "variances = []\n",
    "batch_size = params['batch_size']\n",
    "\n",
    "K_FOLDS = 11\n",
    "fold_size = len(unique_subjects) // K_FOLDS\n",
    "\n",
    "random.shuffle(unique_subjects)\n",
    "\n",
    "cyclic = itertools.cycle(unique_subjects)\n",
    "batched_cyclic = batched(cyclic, n=fold_size)\n",
    "folds = itertools.islice(batched_cyclic, K_FOLDS)\n",
    "    \n",
    "for i, fold in enumerate(folds):\n",
    "    print(f\"Starting fold {i + 1}/{K_FOLDS}\")\n",
    "\n",
    "    train_df = df[~df['Window'].isin(fold)]\n",
    "    test_df   = df[df['Window'].isin(fold)]\n",
    "\n",
    "    train_loader = get_dataset(train_df, batch_size=batch_size)\n",
    "    test_loader  = get_dataset(test_df, batch_size=batch_size)\n",
    "    model, criterion, optimizer = get_model(params)\n",
    "\n",
    "    # Train with modest epochs; early stopping inside fit handles rest\n",
    "    history = model.fit(\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        epochs=100,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        patience=15\n",
    "    )\n",
    "\n",
    "    acc, *_ = get_validation(model, test_loader, device)\n",
    "    accs.append(acc)\n",
    "\n",
    "    print(f\"Fold {i + 1} Accuracy:\", acc)\n",
    "\n",
    "print(f\"k-Fold CV Mean Accuracy: {np.mean(accs):.4f} ± {np.std(accs):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6a4bb1-479f-46a1-bd12-50d340d54b0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "params = readable\n",
    "\n",
    "unique_samples = list(df['Window'].unique())\n",
    "subject_count = df['ID'].nunique()\n",
    "accs = []\n",
    "variances = []\n",
    "batch_size = params['batch_size']\n",
    "\n",
    "fold_size = len(unique_samples) // subject_count\n",
    "random.shuffle(unique_samples)\n",
    "\n",
    "cyclic = itertools.cycle(unique_samples)\n",
    "batched_cyclic = batched(cyclic, n=fold_size)\n",
    "folds = itertools.islice(batched_cyclic, subject_count)\n",
    "\n",
    "for i, fold in enumerate(folds):\n",
    "    print(f\"[{get_timestamp()}] Starting fold {i + 1}/{subject_count}\")\n",
    "\n",
    "    train_df = df[~df['Window'].isin(fold)]\n",
    "    test_df   = df[df['Window'].isin(fold)]\n",
    "\n",
    "    train_loader = get_dataset(train_df, is_train=True, batch_size=batch_size)\n",
    "    test_loader  = get_dataset(test_df, is_train=False, batch_size=batch_size)\n",
    "    model, criterion, optimizer = get_model(params)\n",
    "\n",
    "    # Train with modest epochs; early stopping inside fit handles rest\n",
    "    history = model.fit(\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        epochs=60,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        patience=15\n",
    "    )\n",
    "\n",
    "    acc, *_ = get_validation(model, test_loader, device)\n",
    "    accs.append(acc)\n",
    "\n",
    "    print(f\"Fold {i + 1} Accuracy:\", acc)\n",
    "\n",
    "print(f\"LOSOCV Mean Accuracy: {np.mean(accs):.4f} ± {np.std(accs):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3934c02-1f26-4db2-8c1b-6d45ad199e8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "params = {'batch_size': 48, 'cnn_dense': 64, 'cnn_dropout': np.float64(0.5130373328759822), 'cnn_kernel_size_1': 5, 'cnn_kernel_size_2': 5, 'cnn_kernels_1': 16, 'cnn_kernels_2': 16, 'learning_rate': np.float64(0.000462968156587811), 'lstm_dense': 256, 'lstm_hidden_size': 128, 'lstm_layers': 1, 'optimizer': 'adam'}\n",
    "\n",
    "df = pd.read_csv(\"./processed_clustered_adhdata.csv\")\n",
    "\n",
    "frequency_count = len(df['Frequency'].unique())\n",
    "window_count = len(df['Window'].unique())\n",
    "numeric_df = df.drop(['ID', 'Window'], axis=1)\n",
    "\n",
    "display(df.head())\n",
    "\n",
    "# shape: (windows, frequencies, electrodes)\n",
    "full_ndarray = numeric_df.values.reshape((window_count, frequency_count, numeric_df.shape[1]))\n",
    "\n",
    "X = full_ndarray[:, :, 2:]     # drop ID/Class columns\n",
    "y = full_ndarray[:, 0, 0]      # class label is repeated across freq rows\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "# Add channel dimension (N, 1, freq, electrodes)\n",
    "X_train = X_train[..., np.newaxis]   # (N, freq, electrodes, 1)\n",
    "X_test  = X_test[...,  np.newaxis]\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)  # (N, freq, electrodes, 1)\n",
    "train_ds = EEGDataset(X_train, y_train)\n",
    "test_ds  = EEGDataset(X_test,  y_test)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
    "\n",
    "unique_samples = list(df['Window'].unique())\n",
    "subject_count = df['ID'].nunique()\n",
    "accs = []\n",
    "variances = []\n",
    "batch_size = params['batch_size']\n",
    "\n",
    "model, criterion, optimizer = get_model(params)\n",
    "\n",
    "# Train with modest epochs; early stopping inside fit handles rest\n",
    "history = model.fit(\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    epochs=60,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    patience=15\n",
    ")\n",
    "\n",
    "acc, *_ = get_validation(model, test_loader, device)\n",
    "accs.append(acc)\n",
    "\n",
    "print(f\"Fold {i + 1} Accuracy:\", acc)\n",
    "\n",
    "print(f\"LOSOCV Mean Accuracy: {np.mean(accs):.4f} ± {np.std(accs):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c75e90b-0cac-4fd9-8f54-5d2d380581ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, report, matrix = get_validation(model, test_loader, device)\n",
    "\n",
    "print(\"\\nval Accuracy:\", acc)\n",
    "print(report)\n",
    "print(matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
